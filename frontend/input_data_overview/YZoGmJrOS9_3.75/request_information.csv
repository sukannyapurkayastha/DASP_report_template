Request Information,Frequency,Comments
Explanation,1.0,"[['Reviewer B8PQ', ['Line 255 mentions that there is only a single run performed for each model-task pairs, yet many plots feature confidence intervals. Where do these confidence intervals come from?', 'Why do you pick the model subsets that you do in constructing each plot? For instance, plot 3b has a title ""Decision Tree: All Architectures"" but only plots GPT-2 and Mamba (what about Llama, and the other variations?) - Why apply Mamba only to Llama, and not to GPT-2? Since you mentioned compute was a bottleneck, why not keep either GPT-2 or Llama as a base model?', 'How is the ICL regression score meant to be interpreted? It seems to depend strongly on the task, where some tasks will feature a score uniformly less than 1 across models, and greater than 1 for others.']], ['Reviewer q9Lv', ['It is interesting to find that even a small change in architecture (e.g., adding RMS to GPT-2) will lead to noticeable differences on some tasks (e.g., sparse linear). It would be interesting to investigate the root reason behind that.', ""As stated above, what's the main contribution/innovation of the codebase? What conclusion/takeaway/intuition can we learn from the empirical results?""]], ['Reviewer xju7', ['The paper is not very well-motivated. Why are hybrid architectures (especially the two that are focused on) important to study? What intuitions or profound reasons drive the authors to make the experimental design choices that they did?', 'The results are presented in a manner where the performance metrics are reported for the 12 architectures and 5 tasks but it is not very clear what the reader or the scientific community working on ICL should take away from the results. Are there patterns regarding why certain hybrid architecture + task combinations make ICL shine compared to the baselines and why some others do not? A lot of the interpretation of such results is left to the reader to figure out. A lack of a deeper understanding and intuition about the reasons behind the results makes it hard to see solid/impactful takeaways that others could build on top of.', 'Why are the specific hybrid architectures chosen (GPT2-Llama and Llama-Mamba)? Is it only because no one has studied them before or are there other profound reasons why these could be of interest to study?', 'On line 35, what is meant by “richly benchmarking”?', 'Could you briefly describe what kind of structural changes were necessary to the codebase of Garg et al.? A general understanding will make it more clear why the codebase itself is a major contribution as listed currently in the draft.', 'Why aren’t the baselines and detailed results for Vector MQAR discussed in the paper draft? Why does Mamba struggle with MQAR but does pretty well on all other 5 tasks?', 'Why are some hybrid architectures converging to suboptimal regression schemes versus others are not? Do you have some intuition behind this finding?', 'Similarly, why do some models escape suboptimal regression schemes while others fail to converge entirely?', 'In Figure 6(b), what about GPT2-RMS might explain its inability to learn the decision tree function while all other hybrid architectures used do pretty well? Why is the RMS norm as the distinguishing factor for this hybrid architecture detrimental to the decision tree and sparse linear tasks but not others?', 'In lines 244-246, the authors mention that “certain hybrid architecture variations may place inductive biases on certain solution forms, resulting in convergence times when these solution forms greatly vary from the optimal predictor’s form”. Can the authors give some examples of such inductive biases for some architectures studied in this work?', 'Why are some values in the last column of Table 3 not filled in?']], ['Reviewer y95k', ['Do the authors have intuition or an explanation which can be explored to explain this? Arguably, this is one of the most important contributions to be made in a large empirical review like the presented paper, i.e., to make sense of the experiments to gain a greater intuition for why an LLM behaves in a certain way.']]]"
Improvement,1.0,"[['Reviewer B8PQ', ['A main objective of the paper is to compute numerics comparing models, but only one training run was executed for each experiment. If the compute budget is very limited, a more valuable approach may be to consider a smaller subset of models (e.g. keeping either GPT-2 or Llama, but not both) and simpler task parameterizations. Doing so will enable you to sweep across many more settings and increase your experiment replications, generating more convincing numerics.', 'Additional minor formatting comments: - Line 25: consider compressing citations (e.g. with sort&compress) - Table 1: third hline from the top intersects with text - Consider plotting figures with point-markers for each data point, to clarify where exactly your data points fall - For related models that vary by parameter (e.g. training iteraitons), consider using the same color but with different shadings / style - Figure 15: text is unreadable - Figure text overall is small and hard to read']], ['Reviewer q9Lv', ['The codebase for studying in-context learning ability could be useful to understand capabilities and limitations of hybrid models, which will accelerate research in this area.']], ['Reviewer xju7', ['Additionally, use cases for ICL itself are not well-motivated. Are there any practical use cases that warrant such extensive evaluation? The writing is not easy to understand for a reader not very up-to-date with the ICL literature.', 'Line 202: “Sparse Linear ~~on~~ adopts a suboptimal..” - References to result tables (Table 3 for lines 213, 230) and model descriptions (Figure 1a for lines 194, 204, 201, 16 etc.) could further enhance readability and the user’s understanding.']], ['Reviewer y95k', ['### Clarity There is significant room to improve the clarity of the presented work. Currently, several important details are missing (discussed below), key concepts are not fully explained, and the current paper could benefit from an editorial pass (it is currently difficult to read). It is also important to note that, while the authors presented the results of model + task pairs and detailed where models failed, no possible explanations and follow up ablation studies were conducted. the question of ""why"" remains for all the presented results. E.g., > Specific hybrid architectures can hesitate to learn/converge for certain function classes.', 'Wrt important missing information: - ""We replicate the function classes Linear Regression, Sparse Linear Regression, 2-Layer MLP Regression, and Decision Tree Regression from Garg et al. [6] as they present a wide range of ""difficulty"" for sequence models. In addition, to capture the existence of some ICL ability, we also regress onto the two function classes examined in Park et al. [14]: parity function with induced sparsity (Sparse Parity) and parallel associative recall (Vector MQAR)."" <- - How training instances are produced per task? How many test samples are produced per task? If this follows Garg et al., then each model is trained from scratch on 40 samples per task. Can you please clarify and state these in the main text?', '""To determine task-specific ICL ability, our sequence models regress onto the functions shown above [14]."" <- It would help to clearly state the paper trains the models ""from scratch"" to in-context learn, as in previous works.']]]"
Clarification,0.75,"[['Reviewer B8PQ', [""The results are sometimes difficult to interpret. At times, this is simply because the plots are unreadable, with text that is too small. I'm unsure how to interpret the in-context regression score. It looks like it's often <1 across models. Does this mean they all fail to outperform the baseline? Is this score comparable across different tasks?""]], ['Reviewer xju7', ['What is the main motivation behind studying hybrid architectures for ICL? For a reader not well-versed with the ICL literature, it is not clear from the paper.', 'Lines 60-61: What optimal train loss is being referred to here? There is no loss associated with ICL itself, right (as that only consists of in-context examples/demonstrations)? Does this loss refer to the ground truth functions being learned or the baselines?', 'Line 80: which models are being referred to here? There is no “real training” during ICL itself. Are the functions/regression targets being referred to here or the baselines? The terminology used in the paper should be clarified.', 'It is not clear what data the 12 hybrid architectures are trained on. Do the authors train them from scratch? If yes, what datasets are used? Else are pretrained weights used for the different blocks? Are the models finetuned? Something else?', 'What do the authors mean by “context length” in most of their figures? Does it refer to the number of ICL examples or the number of prompt tokens? Can the authors share a few examples of sample prompts that were used for ICL with varying context lengths?']], ['Reviewer y95k', ['It is not clear what the author\'s mean by ""zero estimator."" Is this the zero shot prediction? Correspondingly, it is not clear exactly what the presented ICL regression score represents.']]]"
Experiment,0.25,"[['Reviewer xju7', ['For Fig 6(a), could you try training Mamba checkpoints longer than 500k checkpoints to verify if it actually converges to the baseline (GPT-2’s performance)? Were these checkpoints trained by the authors themselves or used from a pretrained model? What data was used for training?']]]"
Typo Fix,0.25,"[['Reviewer xju7', ['Some typos: - Line 161: Mention the word “Figure” before 2a.', 'Line 164: “Figure 2b” instead of “Table 2b” - Line 185: Park et al. [14] show that ..']]]"
