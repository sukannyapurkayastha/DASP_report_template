Given that object manipulation is the specific application of interest, the comparison with DIAYN and the combined objective with DIAYN is interesting but little motivation or discussion has been provided in the paper.			MET
The real data experiments (sections 4.2 and 4.3) are not very convincing, not only because of the very small size of N, but also because there is no comparison with the other approaches.			DAT_EXP
The MTurk experiment gives a qualitative picture, but it could be improved with comparisons to pairwise distances learned through alternative means using the RGB image itself (given that images would permit such a comparison).			EXP
The message of synthetic experiments would be stronger if more of them were available and if the comparison between LOE, TSTE, and OENN was made on more of them.			EXP
I am not sure if this is the first work in this direction of proving bounds assuming an oracle or if there is some background work that the authors could provide to put this into context.			RWK
Attacking CRBMs is highly relevant and should be included as a baseline.			RWK
However, the experiments conducted generally show that SAVP offers only a trade-off between the visual quality of GANs and the coverage of VAEs, and does not show a clear advantage over current VAE models (Denton & Fergus, 2018) that with simpler architectures obtain similar results.			RWK_EXP
There should be sufficient details for a reader to implement this model, thought there are some minor details missing regarding the experimental setup, which will be addressed below.			EXP
The main weakness of the paper is that it does not situate itself within existing literature in this area.			NONE
Over the last few years, researchers in the speech community have invested significant effort in learning better speech representations, and this is not discussed.			NONE
These studies should be mentioned.			NONE
2. The human score of 91.4% is based on majority vote, which should be compared with an ensemble of deep learning prediction.			RES
To compare the author should use the average score of human.			NONE
As I have mentioned above, actually a lot of methods have been developed to address general image restoration tasks.			MET
Some works actually also incorporate generative and/or discriminative networks into MAP inference process for these tasks.			NONE
Thus I believe authors must compare their method with these state-of-the-art approaches.			MET
The model-based method achieves better validation error than the other baselines that use actual data.			MET
Also, to demonstrate the superiority of the proposed method an appropriate comparison against previous work is needed.			MET_RWK
Also, this would allow to compare the references against each other (filling in the missing number in Table 4) and this would allow an evaluation of the evaluation itself: while perfect scores are unlikely, the human references should be much better than the systems.			NONE
However, the idea is very related to Yeh et al.’s work which has already published but not mentioned at all.			RWK
Yeh, Raymond A., et al. "Image Restoration with Deep Generative Models." 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2018.			BIB
There are tons of algorithms for image inpainting, denoising, and super-resolution, but the proposed method was not compared with them.			MET
While the authors say "attributing a deep network’s prediction to its input is well-studied" they don't compare directly against these methods.			MET
* Comparison with other methods did not take into account a variety of hyperparameters.			MET
Besides, as the base classifier is different for various baselines, it is hard to compare the methods.			MET_RWK
For the bAbi task, although there is a significant improvement (43%) in the mean error rate compared to the original DNC, it's important to note that performance in this task has improved a lot since the DNC paper was release. Since this is the only non-toy task in the paper, in my opinion, the authors have to discuss current SOTA on it, and have to cite, for example the universal transformer[1], entnet[2], relational nets [3], among others architectures that shown recent advances on this benchmark.			RWK
- For semi-supervised classification, the paper did not report the best results in other baselines.			RWK
E.g., in Table 1 and 2,  the best result of VAT (Miyato et al., 2017) is VAT+Ent, 13.15 for CIFAR-10 (4000 labels) and 4.28 for SVHN (1000 labels).			RWK_RES
The paper also misses several powerful baselines of semi-supervised learning, e.g. [1,2].			RWK
The experimental results are not very convincing because many importance baselines are neglected.			EXP
Hence the theoretical sample complexities contributed are not comparable to those of MIME.			MET
With regards to the fMRI experiments, good baselines are missing: DEMINE is compared to Pearson correlation.			RWK_EXP
Additionally, I believe the experimental tasks are new, and as a result all implementations of competing techniques are by the paper authors. This makes it difficult to have confidence in the higher reported performance of the proposed techniques.			MET_EXP
1. The paper should also talk about the details of ARNet and discuss the difference, as I assume they are the most related work			RWK
4. Most importantly, for table4, authors are comparing to the ResNet18 ARNet instead of ResNet50? which is better than the proposed method.			MET_TNF
Perhaps I missed it, but I believe Dan Ciresan's paper "Multi-Column Deep Neural Networks for Image Classification" should be cited.			BIB
It would have been useful to compare the general models here with some specific math problem-focused ones as well.			MET
1) Some of the benchmark datasets for the proposed task as well as some well-known methods (see Battaglia et al’18 and references in there) are missing.			RWK
2) Applying the model of Hartford et al’18 to problems where interacting sets are identical is similar to applying convolution layer to a feature vector that is not equivariant to translation.			MET_RWK
I don't think this is really a fair comparison; I would have liked to have seen results for the unmodified reward function.			RES
However, I think since this is few-shot learning with domain adaptation, there is no domain adaptation baselines being mentioned in comparison.			RWK
The reviewer is also interested to see how the the generalization bound introduced in this paper is related to the recent theoretical works [3],[4] on MDL.			RWK
- Jointly learning an inference network (Q) has certainly been done before, and I am not sure authors provide an elaborate enough explanation of what is the difference with adversarially learned inference /  adversarial feature learning.			MET_RWK
The proposed approach should compared to other stronger methods such as graph convolution neural network/message passing neural networks/structure2vec.			MET
However, it’s noteworthy that embedding to a vector could be useful too if the embedding espace is representative of the entire history and the timing of the events.			MET
The experimental sections look rather mechanical. I would have put some results on the learned embedding. Or some demonstration of the embedded history or probability to intuitively convey the idea and how it works.			RES_EXP
Similarly, it would be good to formally connect the capacity to the rate of memorization before making a statement about them being related (as suggested in the initial review).			NONE
Furthermore, no comparisons were provided to any baselines/alternative methods.			MET_RWK
For example, in Sections 4 and 5 I was hoping to see comparisons to methods like MAML.			MET
- The paper should refer to 1) the reward shaping literature, 2) the growing line of works concerned with control variates for REINFORCE (such as VIMCO, MuProp, REBAR) and 3) the growing line of works concerned about combinatorial optimization with reinforcement learning (Neural Combinatorial Optimization with Reinforcement Learning, etc.)			RWK
- novelty: the additions proposed are small modifications to existing algorithms and there are other methods of attention on graphs which have been discussed in the paper but not directly compared to (e.g. this method builds heavily on Velickovic et al 2017)			MET_RWK
There is no direct comparison of performance.			NONE
Furthermore, the baseline models did not use other almost standard regularisation techniques (weight decay, dropout, batch-norm).			MET_RWK
1. Can you clarify why the proposed approach is better than the Meta-RevGrad baseline?			MET
The authors acknowledge this connection, but I think they should begin by introducing CE and then explain how Cakewalk generalizes it.			MET
It would probably help to position the VAE component more precisely w.r.t. one of the two baselines, by indicating the differences.			RWK
The tasks are explicitly designed to exploit these additional parameters - so framing the synthetic experiments as, "here are some simple functions for which we would need the additional parameters that we define" makes sense; but arguing that Hartford et al. "fail approximating rather simple functions" (page 7) is misleading because the functions are precisely the functions on which you would expect Hartford et al. to fail (because it's designed for a different setting).			RWK
But there are no comparisons between the proposed training method and previous related works.			MET_RWK
The comparisons are also absent in experiments.			EXP
My introduction suggestion: do not talk about Convolutional neural networks (CNNs). There is a *lot* of work on graph convolutional networks (GCNs).			NONE
In numerical experiments, there is no comparison with major competitors besides random sampling in the active learning setup.			EXP
- In active learning, the proposed method should be compared with other methods such as Bayesian DNN using dropout, etc.			MET
However, the un-standard design of the LSTM models makes it unclear whether the comparisons are solid enough.			MET
My main concerns are about the evaluation and comparison of standard neural models.			MET
I think these issues are against the goal of evaluating standard neural models on the benchmark and will raise doubts about the comparison between different models.			MET
There should be a better discussion of related work on the topic.			RWK
The paper currently only mentions the most related work for the proposed method,  using the whole section 2 to describe VCL and use section 3 to describe FSM and half of section 5 to describe SSR.			MET_RWK
While the comparison with HSIC is a helpful one, it is also required to compare with the competing method closest to yours, i.e.			MET
Thus, comparison with the prior work in Statistics and Machine Learning is relevant, since these tests have the same aims and scope as your tests.			NONE
If the authors claim that the proposed MaxConfidence attack method is more powerful than the MaxLoss based attacks, they should provide more comparisons between these methods.			MET
3) The experiments lack comparisons to several important baselines from self-supervised learning community, and methods using soft labels for training (as mentioned in 2) above).			EXP
My main concern about this paper is why this algorithm has a better performance than CW attack?			MET
I would suggest comparing with CW attack under different sets of hyper-parameters.			MET
The theoretical proof depends on the convexity assumption, I would also suggest comparing the proposed attack with CW and other benchmarks on some simple models that satisfy the assumptions.			MET_EXP
It makes a nice story that the theoretical properties justify the observations, but they may be as well completely unrelated.			NONE
One of the main area which is missing in the paper is the comparison to two other class of RL methods: count-based exploration and novelty search.			MET
While the section 4 has a discussion on related papers, there's no systematic experimental comparison across these methods.			MET_RWK
While the paper shows experimentally that they aren't as successful as the RFs or IDFs, there's no further discussion on the reasons for poor performance.			EXP
=> Baselines: The comparison provided in the paper is weak.			RWK
But there are better baselines possible.			NONE
Moreover, in spite of the authors writing that their goal is “completely different” from [Lee at al 18a, Ma et al 18a], I found the two cited papers having a similar intent and approach to the problem, but a comparison is completely missing.			RWK
Without a proper comparison (formal and experimental) with these lines of work, the paper is incomplete.			EXP
A proper baseline should have been compared.			RWK
Third, the comparison to baseline and “DeepSet” is not fair.			RWK
RNN-based approaches are with better “complexity” comparing to your sum baseline and “Deepset” approach.			MET_RWK
So, I have some doubts about the experimental results.			EXP
My concern is that is highly dependent  on the order of hyperparameter searched, which could impact the tunability significantly.			NONE
The author never explains. E.g., link to NRMSE and PFC to the Table.			TNF
It is fine, but it seems weird that the author still mentioned the setup of MNIST+SVHN in the main text.			MET
4. From Figure 6 and Figure 8-11, it looks like the bandit is more or less on par with fixed exploration policies.			TNF
4. Comparison with past works.			RWK
1) The only comparison to another non-fixed sampling baseline is Kool et al. 2019.			RWK
The visualization and a thorough comparison were missing in MNIST classification.			NONE
This baseline was also missing in image reconstruction.			RWK
2:    The authors should compare against several costs/algorithms (e.g. l_0 with OMP, l_1 with LARS, etc.), and across various N_0/sparsity penalties, and across several datasets.			MET_DAT
The choice of RBMs is not motivated well and without any comparisons to alternatives, it comes across			MET
It also adds capacity and it is not at all made clear whether the comparison is fair since no analysis on number of parameters are shown.			ANA
- Relatedly, better baselines should be used; for example, if the memory used by the generative model is merely put to storing randomly chosen instances from the tasks, how will the results compare? Clearly storing instances bypasses the forgetting problem completely (as memory size approaches the dataset size it turns into the joint problem) and it's not clear how many instances are really needed per task, especially for these simpler problems.			MET_RWK
This is true throughout; as stated before it is not clear how many parameters and how much memory these methods need, which makes it impossible to compare.			MET
In summary, the paper presents what seems like an effective strategy for continual learning, by combining some existing methods together, but does not make it precise what the contributions are and the methodology/analysis make it hard to determine if the comparisons are fair or not.			MET
There is no experiment clearly shows convincing evidence of the correctness of the proposed approach or its utility compared to existing approaches (Xie & Ermon (2019); Kool et al. (2019); PlÂšotz & Roth (2018) ).			MET_RWK
Why is the second objective log(#params) instead of just #params when the introduction mentions explicitly that tuning the scales between different objectives is not needed in LEMONADE?			MET
This seems like a very weak baseline---there are any number of other reasonable ways to encode this.			RWK
3. In the introduction, the Authors point that prior methods have trouble dealing with textureless, reflective or transparent approaches, but it's not clear form the paper where it addresses these cases, and if yes, what is the mechanism for that.			INT
In this light, experiments demonstrating comparisons between GANs and VAEs as the reference generative model for explanations would have made the paper stronger (as the proposed approach relies explicitly on how good the generative model is).			MET_EXP
Citations used for Gradcam are wrong -- Sundarajan et al., 2016 should be changed to Selvaraju et al., 2017.			BIB
- Experimental results provided in the paper are only qualitative -- as such, I do not find the comparisons (and improvements) over the existing approaches convincing enough.			RES_EXP
However, there is no comparison with ENAS and DARTS in experiments.			EXP
Thus, I suggest the authors could show the space and time comparisons with the baseline methods to show effectiveness of the proposed method.			MET_RWK
In addition, I observe that in Table 1, the proposed method does not outperform the Joint Training in SVHN with A_10.			MET_TNF
I hope the author could explain this phenomenon.			NONE
Why not compare results to WGAN-GP in this case? Since the proposal of GANs, many papers addressed the mode collapse problem.			RWK
The experiments on synthetic data could be improved: for reproducibility, many works on GANs used the same synthetic data as VEEGAN.			EXP
There should be some kind of comparison with test set results from other state-of-the-art work on these datasets.			RWK_DAT
Three of these methods, i.e. Lime, GradCam, PDA, are not designed for producing contrastive explanations, so I am not sure to what extend this comparison is appropriate.			MET
However, attack in Wasserstein distance and some other methods can also do so.			MET
It's hard for me to judge of the experimental results of section 5.3, given that there are no other			RES_EXP
For e.g. the results on the oscillating behavior of Dirac-GAN are described in related works (e.g. Mescheder et al. 2018), and in practice, WGAN with no regularization is not used (as well as GAN with momentum, as normally beta1=0 in practice).			RWK
I am also wondering if the comparison with the baselines is fair.			RWK
In other words, although in the present results, the proposed NF-SGAN/WGAN outperforms the baseline, the reported performance of the baselines is worse than in related works on CIFAR10.			RWK
Especially if the lower-level component of the hierarchy is pre-trained in a non-MARL setup, it can just be seen as part of the environment from the point of view of the MARL training, offerring limited new insight into MARL.			MET
Given that you observed that SVHN has higher likelihood on all three model types (PixelCNN, VAE, Glow), why investigate a component specific to just flow-based models (the volume term)?			MET
On a more minor note, the paper [1] seems to report 4.95 accuracy for MixMatch on CIFAR-10 with 4k labels, while in this paper it’s being reported as 6.24.			RWK
However, there are papers empirically analyzing novelty detection using generative model -- should analyze or at least cite:			RWK
Furthermore, the experimental section does not compare to other forms of hierarchical approaches for MARL, and generally only provides a single comparison to PPO & MADDPG.			MET_EXP
Besides, in the experiments, the proposed method is not compared to other transfer learning methods.			MET_EXP
Weakness: It would be good to see some comparison to the state of the art			RWK
The proposed method is not appropriately compared with the other methods in experiments.			MET_EXP
1. Even though K-matrices are aimed at structured matrices, it would be curious either to empirically compare K-matrices to linear transformations in fully-connected networks (i.e. dense matrices) or to provide some theoretical analysis.			ANA
4) In the experiments, more comparisons with methods in [1] or [2] should be conducted given they are all parallelizing the backpropagation algorithm and achieve speedup in the training.			MET_EXP
The main issue of this paper is the fair comparisons with other works.			RWK
The paper should compare the proposed work to the InfoGAN work both quantitatively and qualitatively to justify its novelty.			MET
- Baseline missing: Random actions from expert			RWK
- Baseline missing: Simple RNN policies that communicate hidden states.			RWK
Another baseline could be to simply model the imitator and probing policies as RNNs and let them communicate with each other via the hidden states.			RWK
- I’d also like to see more extensive comparisons between FICM and ICM across different datasets, for example, Super Mario Bros. and the Atari games, instead of only comparing FICM against ICM on ViZDoom.			DAT
Finally, the CNN architecture should be compared with modern SAT solvers which have been participating to SAT competitions.			MET
If the two NNs used in DNN and DNN(resized) are different, I believe it’s still possible to apply the algorithm in DNN(resized) to the NN used in DNN, and get a full trace in the figure as noise and projection changes, which would lead to more fair comparison.			MET
Though Kaleidoscope include these as special cases, it is not clear whether when given the same resources (either memory or computational cost), Kaleidoscope would outperform them.			MET
- For the squeezenet and latent permutation experiments, would be nice if there is a comparison to other parameterizations of permutation matrices, e.g. gumbel-sinkhorn.			EXP
Finally, the paper needs to compare its parameter-reduction approaches against other compression and hyperparameter optimization techniques.			MET
And while there is an argument to be made for the inherent interestingness of exploring these questions, this angle would be more compelling if multiple encoder architectures were explored and compared.			MET
- comparison should be made to the linear composition method in the Arora, Liang, Ma ICLR 2017 paper			RWK
Have the authors considered to use categorical or binary variables?			MET
Since this work takes the approach of allowing stale weight updates, the author should also compare with existing distributed training approaches that use asynchronous updates, with or without model parallelism, for example, Dean et al., 2012.			MET_RWK
However, there is no comparison against existing work.			RWK
Learning disentangled representations with deep generative models is very much an active area.			NONE
https://openreview.net/references/pdf?id=Sy2fzU9gl			BIB
https://arxiv.org/abs/1802.05822			BIB
https://arxiv.org/abs/1802.05983			BIB
https://arxiv.org/abs/1802.04942			BIB
However, memory overhead is still an issue compared to existing method.			RWK
Given that BERT is most robust against their manipulation, it would be good to see a more powerful recurrent model for comparison.			MET
Alternatively why not try using a factorization technique to reduce the rank and then see how well the method does for different ranks?			MET
1a. Comparison to other exploration methods.			MET
It is important to place the contributions in this paper in context of these other works.			RWK
A number of these references are missing and no experimental comparison to these methods has been made.			RWK
- Since the proposed method starts from Laplace transform, it would be helpful to further discuss the connection between other methods that regularises the eigenvalues of the Jacobian (such as spectral-normalisation), which work in the frequency domain from a different perspective.			MET
Spare few lines of equations to define what are DeepSets and PointNetSeg in the paper and point out the difference since these networks are used throughout the paper extensively without proper mathematical definition.			MET
But the only negative aspect is the basis competitor algorithms are very simple in nature without any form of learning and that are very old.			MET
The authors also rely mostly on the FID metric, but do not show if and how there is improvement upon visual inspection of the generated images (i.e. is resolution improved, is fraction of images that look clearly 'unnatural' reduced etc.)			RES
Otherwise, it is hard to tell how significant these correlations are. Since the end goal is to determine transferability of tasks and not the methods, it does seem like there are simpler baselines that you could compare against.			MET
- I believe one should not compare the distance shown between the left and right columns of Figure 3 as they are obtained from two different models.			TNF
My biggest concern is the lack of comparison with other representation learning methods, which is a very well studied problem.			MET
However, it looks like authors only compared with VIB which is similar to the proposed method in terms of the objective function.			MET
* In related work, no reference to previous work on "statistical" approaches to NN			RWK
- The paper mentions the MSVV algorithm twice but no reference or explanation is provided.			MET
- No comparison has been made between their approach and other previous approaches.			MET_RWK
-  The authors should compare to at least some popular previous approaches that use a feature engineering based methodology such as - IntPred			MET_RWK
While the authors are using an existing model, they shouldn't assume that the reader has read the paper describing that model.			MET
