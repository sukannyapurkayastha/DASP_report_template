How to understand this phenomenon? Does this mean that the distribution of the layer outputs will not change too much if the layer is deep enough?			MET
In particular, the node-level pretraining described in section 3.1.1. seems rather complicated to implement as a context graph needs to be computed for each node in the graph.			MET
However, since quantitative exploration of large real-world datasets may be challenging and expensive to collect, the synthetic experiments could have been more detailed.			DAT_EXP
Since quantitative real-world results are challenging to obtain, improved presentation of the qualitative results would be helpful as well.			RES
- The definition of g should depend on only \theta_k^I and \hat{\delta}_k^M, not \theta_k^k.			MET
- The equation (1) should hold for any \theta’, not \theta.			MET
- The equation (1) should contain \rho, not p.			MET
However, the theoretical justification and experimental results are not.			RES_EXP
* The use of the name "batch-norm" for the layer wise normalization is both wrong and misleading.			MET
For example, it is unclear to me why some larger models are not amenable to truncation.			MET
Even though this is mostly an empirical investigation, I think some more efforts should be put in understanding and explaining why some of those behaviors are shown, as I think it can bootstrap future work more easily.			NONE
- In Section 3.1 : “Across runs in Table 1, we observe that without Orthogonal Regularization, only 16% of models are amenable to truncation compared to 60% with Orthogonal Regularization.” For me, this is not particularly clear.			TNF
- In Appendix D, the Figures could be slightly clarified by using a colored heatmap to color the curve, with colors corresponding to the threshold values.			TNF
One minor issue is that the first sentence of the third paragraph in Section 4 is not a full sentence and therefore difficult to understand.			NONE
This paper is very well written, although very dense and not easy to follows, as many methods are referenced and assume that the reviewer is highly familiar with the related works.			RWK
This poses a challenge in evaluating this paper.			OAL
- the notation q appeared in the middle part of page 3 before the definition of q is shown in the last paragraph of p.3.			MET
1. In section 3.1, the authors selected a snippet from each section, but this was not rigorously defined.			NONE
At the end of the subsection, the authors argue that 4D convolution is just k times larger than 3D kernels, which sounds like contradicting.			MET
-- It is not clear how the  per-example scalar sigma-i is learned. (for Eq 2)			MET
But the authors do not seem to describe how they chose the hyperparameters for the baselines algorithms.			MET
Hence it is unclear how large the running time improvement is compared to a well-tuned baseline.			RWK
Since finding an adversarial with smaller perturbation is a harder problem, it is unclear how the algorithms compare for finding adversarial examples with similar distortion.			MET
However, the real-world experiments are not necessarily the easiest to read.			EXP
3. Section 4.2 need more clarity.			NONE
Authors should clarify the justification behind experimenting only on 'first 500 test images'.			EXP
In this sense, claims about the low-variance of GO-gradient wrt to other reparametrization baed estimators should be removed, as they are the same.			MET
2. Annotation is not clear in this paper.			NONE
" There are multiple new variables in this paragraph, without specifying the dimension and meaning for each attribute, it's really hard to understand.			NONE
On page 4, State update function, what is the meaning of variable "Epsilon" in the equation?			MET
From the supplementary, it seems Epsilon means the environment?			MET
Finally, the style (font) of the paper does not adhere to the ICLR style template, and must be changed.			OAL
- some parts of the paper are quite unclear			OAL
- In Equation (2), How is P_ij defined exactly, are they parameters? I am quite confused about this part			MET
The primary difficulty in reviewing this paper is the poor presentation of the paper.			OAL
There are many typos and mistakes (e.g., the last paragraph of the paper starts with a sentence that does not make any sense), missing references (e.g., there is an empty parenthesis at the end of the second paragraph on the second page) and in at least two cases, there are references to a formula that is not in the manuscript (e.g., reference to formula 15 on line 3 of page 5).			NONE
This issues makes reviewing this paper very difficult.			OAL
Fourth, please make it clear that the proposed method aims to estimate "causality-in-mean" because of the formulation in terms of regression.			MET
While I could imagine human judges preferring them as they are fluent, I think they are wrong as they express a different meaning than the SPARQL query they are supposed to express.			MET
It seems there is no conclusion to take away from the experiments in section 5 (convolutions).			EXP
There are many typos and grammar errors			OAL
The maths is not clear, in particular the gradient derivation in equation (8).			MET
But again, it's not super clear how the paper estimates this derivative.			MET
This should at the very least be explained more clearly.			NONE
Weaknesses: Paper could have been written better. I had hard time understanding it.			OAL
The notations are overall confusing and not explained well.			OAL
1. Are e_{i,t} and lambda_{i,t} vectors?			MET
Also, it took me a long time to figure out that ‘i’ is used to index each entity (it is mentioned later).			NONE
The paper is written in a way that makes following it a bit difficult, for example, the experimental setups.			EXP
Also, the writing can be improved by making the writing more concise and formal (examples of informal: "spoil the network", "model is spoiled", "problem of increased classes", "many recent researches have been conducted", "lots of things to consider for training", "supervised learning was trained" etc.).			NONE
The contributions of the method could also be underlined more clearly in the abstract and introduction.			INT
The description of consistency regularisation methods in section 2.2 is not very clear and I would like to get better understanding of temporal ensembling and SNTG methods here as they play an important role in the experiments.			MET
Then, the important hyperparameter of the method---threshold---seems to be hard to select (both in sections 4.1 and 4.2).			MET
I also had a hard time going through the paper - there aren't many details.			OAL
Section 2.1 is where the method is proposed, yet most of the descriptions there are unclear. Without these details it is impossible to judge the novelty of the "rule-based concept extractor", which is the key technical innovation.			MET
The paper lacks rigor and the writing is of low quality, both in its clarity and its grammar.			OAL
An example of lack of mathematical rigor is equation 4 in which the same variable name is used to describe the weights before and after pruning, as if it was computer code instead of an equation.			MET
Also pervasive is the use of the asterisk to denote multiplication, again as if it was code and not math.			MET
"Sparse Mixture of Sparse of Sparse Experts"			NONE
"if we only search right answer"			NONE
"it might also like appear"			NONE
"which is to design to choose the right"			NONE
sparsly			NONE
"will only consists partial"			NONE
"with γ is a lasso threshold"			NONE
"an arbitrarily distance function"			NONE
"each 10 sub classes are belonged to one"			NONE
"is also needed to tune to achieve"			NONE
However, there are some key issues with the paper that are not clear.			OAL
1) In the abstract, I find the message for motivating the masking from the sentence  "content based look-up results... which is not present in the key and need to be retrieved."  hard to understand by itself. When I first read the abstract, I couldn't understand what the authors wanted to communicate with it. Later in 3.1 it became clear.			NONE
3) First paragraph in page 5 uses definition of acronyms DNC-MS and DNC-MDS before they are defined.			NONE
4) Table 1 difference between DNC and DNC (DM) is not clear. I am assuming it's the numbers reported in the paper, vs the author's implementation?			TNF
- q_theta was introduced in Eq. (8) before it is defined in Eq. (11).			MET
In the algorithm, the authors need to define the HT function in (3) and (4).			MET
There are some typos that can be easily found, such as “of the out algorithm”.			NONE
It leaves it unclear to the reader when someone should choose to utilize a spherical method or when the proposed method would then be preferred compared to other spherical methods.			MET
Some details weren't clear to me.			NONE
- This paper is a slightly difficult read - not because of the			OAL
Each one of these topics is worthy of			NONE
The introduction contains too much related work, which should be divided in another section.			NONE
It is difficult to understand the goal of Section 4.2.			NONE
Also, it is not clear why those are called axioms since they are not use to build anything else.			MET
Also, second sentence is not a complete sentence			NONE
- The interchangeable use of the term "conductance of neuron j" for equations 2 and 3 is confusing.			MET
- As said in the my main comments, I am not convinced by the use of the term Axiom. They are not use as building blocks, and are rather used as desirable properties for which the authors prove that only "path methods" can satisfy.			MET
- Footnote 2 on page 5 it difficult to read.			NONE
- Figures 1-4 are difficult to interpreted on a printed version.			TNF
1) How to solve the constrained problem (8) is unclear.			NONE
I don't have a good sense of whether this is a reasonable theoretical model to explore and a lot of very basic questions remain unanswered for me.			MET
* I thought there was a bit over-selling in intro.			NONE
I take issue with the usage of the phrase "skill discovery".			MET
Here, there is only a single (unconditioned) policy, and the different "skills" come from modifications of the environment -- the number of skills is tied to the number of environments.			MET
This is not to say that this way of doing things is wrong, but rather that it is misleading in the context of prior work.			RWK
The language around skills and the extent of prior knowledge still downplays things a bit too much for my liking.			MET
- It is extremely hard to follow what exactly is going on; I believe a few illustrative examples would help make the paper much clearer; in fact the idea is not that abstract.			PDI
- Minor grammatical mistakes (missing "a" or "the" in front of some terms, suggest proofread.)			OAL
A reader’s most natural question is whether there is a large enough improvement to offset the extra computational cost, so the fact that wall-clock times are relegated to the appendix is a significant weakness.			RES
2) The writing is poor and hard to follow.			OAL
First, many details are missing, especially in the experiments, which makes the proposed method suspicious and non-convincing.			MET_EXP
- figures 2 & 3 should be a lot larger in order to be readable			TNF
In this paper "large compression ratios" means little compression, which I found confusing.			NONE
The paper does not follow a smooth story line, where an open research question is presented and a solution to this problem is developed in steps.			NONE
The flowchart in Fig 1 is rather a system design consisting of many components, the functionality of which is not clearly described and existence of which is not justified.			NONE
This complex flowchart does not even describe the complete task.			NONE
It is in the end plugged into a continual learning algorithm which also performs domain transformation.			MET
-	Section 5 is somewhat less clear than the previous sections.			NONE
The purpose of the public set is explained only in section 5.2.			MET
Particularly, the authors mixed their observations up with the results of published works, making it hard to identify the contributions of this paper.			RES
- the paper is poorly written and sentences are generally very hard to parse. For example, section 3.1 is opened by statements such as "(we use) a multi-task evaluator which trains on the principal and auxiliary tasks, and evaluates the performance of the auxiliary tasks on a meta set"??			OAL
The paper is easy to understand but several sections (starting from section 2) could use an english language review (e.g. "search right" -> "search for the right", "predict next word" -> "predict the next word", ...) In section 3, can you be more specific about the gains in training versus inference time?			NONE
-Something is a bit weird with the FGM results. While it is a weaker attack, a 0%/100% disparity between it and every other			MET
Also, it seems quite strange to me that making the FW overshot and then projecting back would be beneficial.			MET
- I am not sure what you mean in 5.4 “we omit all grid search/ binary search steps			MET
- Paper is often hard to follow, and contains a significant number of typos.			OAL
As is, the counter example proof itself is quite confusing: it would really help if the proof was more formal.			MET
On the downside, everything here is an extension of existing work, and the body of the paper is hard to read (though this may be inevitable, there's a lot of background to go over here).			NONE
- There are better words than "decent" to describe the performance of DARTS, as it's very similar to the results in this work!			RES
However, the weakness is that the condition is opaque and it is not entirely clear how broad of class of problems this condition would apply to.			MET
In the experiment there is no details on how you set the hyperparameters of CW and EAD.			EXP
- in the whole paper there is $y$ which is not defined.			NONE
- Writing sometimes a bit overcomplicated (e.g., Sect. 4.5 could just be a figure of a commuting diagram and two sentences...)			NONE
This is discussed on p4, but it's unclear to me how keeping $\sigma$ separate is benefitial for the analysis, and this is not picked up again explicitly again?			ANA
- page 5, paragraph 3: "we from some" -> "we start from some"			NONE
- p6par1: "much cheaper then computing" -> than			NONE
- p6par6: "on formulas that with" -> no that			NONE
- p6par7: "measure how rate" -> "measure the rate"			NONE
However, I don't understand the use of $\alpha$ here.			MET
I believe that's what "Pred (One Step)" expresses, but it would maybe be generally helpful to be more precise about the notation			MET
The paper can benefit from a proofreading.			OAL
There are a few typos throughout the paper such as:			OAL
* The text is quite hard to read.			OAL
There are many typos (see below).			OAL
* Equations (1, 2): z and \phi are not consistently boldfaced			MET
As pointed out by R2, with depth there are a lot more number of possible ways in which one could carve out decision boundaries to separate data points, thus, it is not clear that the loose linear upper bound holds Specifically, as one might expect with depth it could be possible that linear capacity increase is a lower bound (I am not suggesting that it is, but that possibility should be considered and explained in the paper).			MET_DAT
In general, I feel the paper is interesting but would benefit from a major revision which makes the message of the paper more clear, and addresses these and other issues raised in the review phase. Thus I am holding my current rating.			OAL
4. Fig. 3 (right): It is not clear			TNF
On the other hand it is clear that using the confidence of the model to predict the dataset is a useful property, but the right side of the Fig. is very confusing.			TNF
It is not clear to me what the point of Sec. 5 is, given a trained model, one wants to figure out if an image was present in the training of the model.			EXP
It is also not clear why Table. 3 does not report the Bayes baseline results.			RES_TNF
1. It is not clear that training a network to classify a set from another set is necessarily equivalent to ``memorization’’.			MET
2. Figure 3: it is confusing to call the cumulative distribution of the maximum classification score as the CDF of the model (y-axis fig. 3 left) as CDF means something else generally in such contexts, as the CDF of a predictor.			TNF
I believe that the presentation of the proposed method can be significantly improved.			MET
The method description was a bit confusing and unclear to me.			MET
Sometimes these are capitalized, but the use is inconsistent throughout the paper.			NONE
I had trouble to understand some parts of this paper, since some of the sentences do not make sense to me. For example			OAL
- the sentence under eq. (2)			MET
- the sentence "Bacause the identity of the datapoint can never be learned by ..." What is the identity of a dat point?			MET
Somehow, those connections are not clear to me.			NONE
- Table 3 is a bit confusing as-is (lower is only better when controlling on the quality of the best sample.			TNF
- It isn't clear from the tables that OCE_0.1 outperforms REINFORCE_Z (as is mentioned in the discussion).			TNF
It’s not obvious to me how useful the ME bias would be when there is a large number of classes since the probability assigned to the true novel class with a perfect ME bias would be 1 / (# unseen classes).			MET
- It is unclear how MISC-p is performed. Please elaborate on how MISC-p works for prioritization.			MET
- Also, for MISC-r experiments, the weights between the intrinsic reward bonus and the extrinsic reward are not specified in the paper.			EXP
* It is unclear to me whether the "efficient method for SN in convolutional nets" is more efficient than the power iteration algorithm employed in previous work, such as Miyato et al. 2018, which also used SN in conv nets with different strides.			MET_RWK
4. I do not understand this: "to fit well the method overfitting rate" in Section 3.3.			MET
(1) than -> that			NONE
(2) Eq. (3): is there a superscription "(j)" on z_canon in decoder?			MET
2. In the caption of figure 2, there should be a space after `":".			TNF
- Section 3.3: “Different from pruning, which the search space is usually quite limited”. “which” should be “whose”?			MET
- Section 4.4.1: “DSO-NAS can also search architecture [...]”  -> “DSO-NAS can also search for architectures [...]”			MET
However, it isn’t entirely clear if the primary contribution is showing that ‘curiosity reward’ is a potentially promising approach or if game environments aren’t particularly good testbeds for practical RL algorithms — given the lack of significant results on more realistic domains, my intuition leans toward the later (the ant robot is interesting, but one can come up with ‘simulator artifact’ based explanations).			MET
The difference with the other reference model (SVG) is less clear.			MET_RWK
While the general description of the model is clear, details are lacking.			MET
Similarly, you did not indicate what the deterministic version of your model is.			MET
The description of the generator in the appendix is difficult to follow.			NONE
Also, it is not clear from the discussion on z, whether sampling is performed once for each video of for each frame.			MET
Another somewhat jarring fact about the alt-az convolution is that it is not well defined on the south pole.			MET
The paragraph motivating the alt-az convolution on page 4 is not very clear, and some claims are questionable.			MET
I agree that local SO(2) invariance is too limiting. But it is not true that rotating filters is not effective in planar/volumetric CNNs, as shown by many recent papers on equivariant networks.			MET_RWK
I would suggest rewriting this paragraph to make it clearer and less speculative, and acknowledge that although rotating filters might increase computational complexity, it has often been shown very effective.			MET
-Some technical details  are missing.			MET
- The digression at the bottom of the first page about neural architecture search seem out of context and interrupts the flow of the introduction.			INT
- First line page 4 you mention AF, without introducing the acronym ever before.			NONE
- "to speed up and trade off between evaluating fitness and evolving new species" Unclear sentence. speed up what			NONE
- Sec 3.4 can you recap all the parameters after eq.11? going through Sec 3.2 and 2.2 to find them is quite annoying.			MET
Reading it feels like reading about lattices when the work is about general graphs, and lattices provide no intuition about the proposed solution.			MET
It is still not clear why self-modulation stabilizes the generator towards small conditioning values.			MET
- In figure 3 (c) "number |T of input" should be  "number |T| of input"			TNF
- In figure 5 (a) "cencept" should be "concept"			TNF
- In figure 8 "Each column corresponds to ..." should be "Each row corresponds to ...".			TNF
- In the last paragraph of A1 "growth of the layer width respect" should be "growth of the layer width with respect"			NONE
- In the second paragraph of A2 "hypothesize the that relation" should be "hypothesize that the relation".			NONE
- In section 4.3 last paragraph, first sentence: "with the maximunm number" should be "with the maximum number"			NONE
The clarity of the presentation (in particular the description of when the method is applicable) and the technical correctness of the paper are somewhat lacking.			MET
Because of the above many discussions about discrete vs. continuous variables are missleading.			MET
I find the background on ELBO and GANs unnecessary occluding the clarity at this point.			RWK
In particular, it is unclear what the assumption on the size of the unlabelled test set is.			DAT
I think this is a very interesting direction, but the present paper is somewhat unclear.			OAL
In particular, the example in section 3.1 says that a noisy information bottleneck is introduced, but then says that the modified and unmodified models have "training algorithms that are exactly equivalent." I think this example needs to be clarified.			EXP
Many of the parameters here are also unclear and not properly defined/introduced.			MET
What is the relationship between $\theta$ and $\tilde\theta$ exactly?			MET
Overall, I think this paper has some interesting ideas, but those need to be fleshed out and clearly explained in a future revision.			OAL
- The end of the 2nd line of lemma 1: P, G should be \mathbb{P}, \mathbb{G}			MET
- The 3rd line of lemma 1: epsilon1 -> epsilon_1			MET
- Page 14, Eq(14), \lambda should be s			MET
- Page 14, Eqs(13, 14), w(\mathbb{P}, \mathbb{G}) should appear on the right.			MET
This quantity never mentioned before.			NONE
Reading the Lemmas of Section 4 (Lemma 4.4) you can see that it is the smoothness parameter of function $H$. Thus, is not necessary to have it there (not important for the definition).			MET
The paper is relatively well-written, although the description of the neural models can be improved.			OAL
First, it is somewhat misleading about its contributions: it's not obvious from abstract/introduction that the whole model is the same as DIP except for the proposed architecture.			INT
Specifically, the first contribution listed in the introduction makes it look like this paper introduces the idea of not learning the decoder on the dataset (the one that starts with “The network is not learned and itself incorporates all assumptions on the data.”).			INT_PDI
Fig 4 is very confusing.			TNF
First, it doesn’t label the X axis.			TNF
Second, the caption mentions that early stopping is beneficial for the proposed method, but I can’t see it from the figure.			TNF
Third, I don’t get what is plotted on different subplots.			TNF
Uses unintroduced (at that point) notation and is very confusing.			NONE
For example, it is not clear the strong emphasis on the robust MDP formalization and the fact that MCTS finds a Nash equilibrium.			MET
* The issue of possible false positives due to sample dependence in fMRI data has again been ignored in the rebuttal.			DAT
Some statements don't make sense, however, eg. "HSIC-based estimators tend to have loose confidence intervals due to the need to bound generalization error of kernels f and g on unseen data points.			MET
" First, f and g are functions, not kernels.			NONE
This should be stated clearly in the main text, and not disclosed in the final sentence of the final page of the appendix.			NONE
It is not clear from the presentation that this has been confirmed.			NONE
The authors do not explicitly define continual learning, incremental learning, and catastrophic forgetting problem.			MET
The paper is not well-organized and not written in a consistent way. For the method and the experiment sections, I need to jump back and forth several times in order to understand what the authors are trying to say.			NONE
1. Typo: Third paragraph in section 1, "...which is makes use of ...".			NONE
2. It's very confusing when the authors introduce \sigma and \omega in the beginning of section 4: why would you need two networks predict the same thing?			MET
3. Mentioning "merging \sigma and \omega, is left for future work" is confusing before formally introducing \sigma and \omega.			NONE
4. Even when the authors formally introduce \sigma and \omega in 4.2, it is still not clear that why both of them are used for modelling the success probability.			MET
5. In fact, I don't know why \omega needs to output p. It's never mentioned in the experiment section.			EXP
7. Typo: Page 5 last paragraph, "... negative instances for for each ...".			NONE
9. It is not clear that baseline 1 and 2 correspond to which baselines in later experiments.			RWK_EXP
10. Reading the baselines before the experiments is very confusing.			EXP
11. Baseline 2 is actually referred to as "usage baseline" but this name is not introduced in the itemized part.			RWK
The first one is clarity.			NONE
The paper is not very self-contained, and I have to constantly go back to Lee et al. and Xu et al. in order to read through the paper.			RWK
The paper can be made more mathematically precise.			NONE
The input and output types of each block in Figure 1. should be clearly stated.			TNF
For example, in Section 3.2, it can be made clear that the domain of the quantization function is the real and the codomain is a sequence k bits.			MET
Since the paper relies so heavily on Lee et al., the authors should make an effort to summarize the approach in a mathematically precise way.			MET_RWK
The figures are almost useless, because the captions contain very little information.			TNF
For example, the authors should at least say that the "D" in Figure 2. stands for delay, and the underline in Figure 4. indicates the bits that are not pruned.			TNF
Many more can be said in all the figures.			TNF
Although the idea behind this paper is fairly simple, the paper is very difficult to understand.			PDI
Furthermore, the usage of the evaluation method unclear as well, it seems to be designed for evaluating the effectiveness of different adversarial attacks in Figure 2.			TNF
Again, this confuses me on what is the main topic of this paper.			NONE
The main criticism I have is that I found the paper harder to read.			OAL
In particular, the exact difference between the proposed method and the ES baseline is not as clear as it could be.			MET_RWK
This makes the contribution of this paper in terms of the method			MET
Hence, the claims in multiple places of this paper and the names for the two networks are misleading.			NONE
#3) There are several Equations that can be combined, such that to save enough white space in order to discuss further some actual technical details.			MET
- What is dt in Algorithm 1 description?			MET
-typo “implmented”			NONE
-What’s the 3d plot supposed to represent?			TNF
--The notation for the proposed parameters theta, theta’, phi, phi’ are not consistent with the notation in the intro section, where phi was used for the encoder and theta for the decoder.			MET
In later sections they use theta and theta’ for encoder/decoder resp.			MET
→ what do the authors mean “			NONE
This paper presents non-trivial theoretical results that are worth to be published but as I argue below its has a weak relevance to practice and the applicability of the obtained results is unclear.			RES
Adaptivity ratio is mentioned in the intro but not defined there.			NONE
Why mention it here, if it's not being defined.			MET
Also it's not clear from the details in the paper what are the architectures for the VAE and RFs (there's a reference to the code but would've been better to have sufficient details in the paper).			MET
=> The results shown in Figure-4 (Section-4.2) seems unclear to me.			TNF
In general I find Section 3 pretty difficult to follow.			NONE
What does “keeping notes” mean?			NONE
I think it is necessary to make it clearer how s_{post_read} and attn_copy are computed with the updated {h^i_t} and what u^i is expected to encode.			MET
- It is unclear why the results on WikiSQL is presented in Appendix. Combining the results on both datasets in the experiments section would be more convincing.			DAT_RES_EXP
Minor:  Since the action is denoted by "a",  it will be more clear if the authors use another symbol to denote the parameter of q(z) instead of "\alpha" at Eq 10 and 15.			MET
Minor, 1/2 is missing in the last line of Eq 19.			MET
* "data  tripets" on page 2			NONE
* Figure 5 should appear after Figure 4.			TNF
Regarding the presentation, I found odd having some experimental results (page 5) before the Section on experience even have started.			RES_EXP
The authors did not comment on the computational overhead of training their LDA.			NONE
(4) The writing quality is not satisfactory.			OAL
Here are a few examples: The ICLR citation style needs to use sometimes \citep.			BIB
The authors instead used \cite everywhere, making the paper hard to read.			NONE
The authors are suggested to use unified notations to denote vectors/matrices (e.g. all bold).			NONE
The introduction can start at a lower level (such as flat/hyperbolic neural networks).			INT
The labels of figures are hard to read.			TNF
The definition of the tuning budgets is not clear, is it the number of trials or the time/computation budgets?			NONE
The authors seems interchangeably using “runs” and “iterations”, which makes the concept more confusable.			MET
- Lack of clarity in the following passage: “In our setting, each point xi in the point cloud can be considered to correspond to single images when we train GANs over images”			MET
Nevertheless, its presentation and Figure 1 makes this elegant idea seems over-complicated.			NONE
2. [Phrasing.] There are too many unconcise or informal phrases in the paper.			NONE
For example, I don't understand what does it mean in "However, if training data is complete, ..... handle during missing data during test." Another example would be the last few paragraphs on page 4; they are very unclear.			DAT
Also, the author should avoid using the word "simply" too often (see the last few paragraphs on page 5).			NONE
3. [Presentation.] The presentation is undesirable. It may make the readers hard to follow the paper.			OAL
c. Figure 1 is over-complicated.			TNF
e. What are the two modalities in Table 2? The author should explain.			TNF
Likewise, it is not well explained what is the value of the learnt relationships, and how uncertainty and errors in the causal learning are relevant to the downstream use of the learnt model.			MET
Here the end goal is less clear; this is understandable in the sense that the work is positioned as a piece in a grand objective, but it would seem valuable to nevertheless describe some concrete example(s) to elucidate this aspect of the algorithm (use case / error effects downstream).			MET
I don’t understand the claim that “GANs prioritize matching joint distributions of pixels over per-pixel reconstruction” and its implication that VAEs do not prioritize joint distribution matching.			MET
The explanation provided by the authors is thus not sufficiently precise and I recommend the retraction of this claim.			NONE
A minor comment is that the mutual information I(., .) being a function of two variables suddenly became a function of a single variable in Eq. (1) and in the text which precedes it.			MET
Indeed, without referencing the original Pointer Network and (and especially the) Transformer papers, it would not be possible to understand this paper at all.			RWK
1. While I see the value in designing an automatic exploration mechanism, the complexity of the underlying approach makes the contribution of the bandit-based algorithm difficult to discern from the large number of other bells and whistles in the experiments.			MET_EXP
2. I don’t understand Figure 4.			TNF
3. The key contribution of the paper that the authors could highlight better is that they do not add new hyper-parameters.			MET
I am confused by Figure 4, and in general with the relative rank metrics.			TNF
3. This apart, I think that the experiment section is pretty hard to read, given all the metrics and methodology is in the Appendix.			EXP
this analysis is not currently included in the main body of the manuscript, but rather in the appendix, which I find rather annoying.			ANA
Also, please place the related work earlier on in the paper.			RWK
Otherwise, it is impossible for a reader to correctly and objectively relate your proposed approach to previous literature.			MET_RWK
The details of the approach is not entirely clear and no theoritcal results are provided to support the approach.			MET_RES
It is not clear how smaller RBMs (and their associated parameters) are combined to obtain the larger RBM model.			MET
The term "Combinatorial optimization", which is used in the title and throughout the body of paper, sounds a bit confusing to the reviwer.			NONE
Weakness: the presentation of the tables/bar charts in the experiment is a bit unclear.			TNF
More explanations are needed.			NONE
Moreover if x must be equal to 0 Ax \leq 0 and at that point Ax = 0, then that means there exists no x for which Ax < 0, so why not just say this outright?			NONE
Perhaps a cleaner definition would just be “A is full rank and there does not exist any X such that Ax < 0?			MET
Also perhaps better to use the curly sign for vector inequality.			MET
Overall the paper, while interesting is unacceptably messy.			OAL
The first two pages have no paragraph breaks!			NONE
!! This means either that the author are separating paragraphs with \\ \noindent or that they have modified the style file to remove paragraph breaks to save space.			NONE
Either choice is unreadable and unacceptable.			NONE
The paper is also littered with typos and vague statements (many enumerated below under *small issues*).			OAL
In this case, they add up to make a big issue.			NONE
The notation at the top of page 4 — see (1) and (2) — comes out of nowhere and requires explanation.			NONE
|_{y>0} x + b |_{y>0}  <— what is the purpose of the subscripts here? Why is this notation never introduced?			MET
The paper offers some analysis, suggesting when each of the conditions occurs, upper bounds the smallest singular value of D A (where the example dependent diagonal matrix D incorporates the ReLU activation (shouldn’t this be more clearly introduced and notated?).			ANA
“From a high-level perspective both of these approaches” --> missing comma after “perspective”			MET
"as well as the gradient correspond to the highest			NONE
"analyzing the lowest possible response" what does "response' mean here?			NONE
"We provide upper bounds on the smallest singular value" -- the singular value of what? This hasn't been stated yet.			NONE
"reverse view on adversarial examples" --- what this means isn't clear from the preceding text.			MET
Notation section -- need a sentence here at the beginning, can't just have a section heading followed by bullets.			NONE
"realated"			NONE
- The term "combinatorial optimization" is used in a confusing way -- addition would not usually be called a combinatorial optimization problem.			NONE
- It would be better to provide the details of the procedure of the proposed method (e.g., Algorithm 1 and each processing of Algorithm 1) in the paper, not in the Appendix.			MET
- The method is very confusingly presented and requires both knowledge of HAT as well as more than one reading to understand.			MET
The fact that HAT-like masks are used for a generative replay approach is clear, but the actual mechanism of "growing capacity" is not made clear at all especially in the beginning of the paper.			MET
2) Using the word "task" in describing "joint training" of the generative, discriminative, and classification networks is very confusing (since "task" is used for the continual learning description too,			MET
The method is difficult to understand, missing many details and essential explanation, and generally does not support a significant contribution.			MET
3. The paper is not nicely written or rather easy to follow.			OAL
- Some of the figures your report are compelling but it is a bit unclear to the reader if the results are general (e.g., the examples could have been hand-picked). Are there any quantitative measures you could provide (in addition to Tables 1 and 2 which don't measure the quality of the approach)?			TNF
In equations 1 and 2, should a, b be written in capital? Since they represent random variables.			MET
But the way it is presented and explained lacks of clarity: for instance in Section 2, some notations are not well defined (e.g what is f?) .			NONE
Moreover, the way classical models are casted under the InfoNCE principle is badly written: it assumes that readers have a very good knowledge of the models, and the paper does not show well the mapping between the loss function of each model and the InfoNCE criterion.			MET
It gives technical details that could (in my opinion) get ignored, and I would clearly prefer to catch the main differences between the different models that being flooded by technical details.			MET
Here again, the article moves from technical details (e.g "hidden state of the first token (assumed to be a special start of sentence symbol ") without providing formal definitions.			MET
J_MLM is also not clear since x_i is never defined (I assume it is x_{i:i}).			MET
but the way the paper is written makes it very difficult to follow, and the description of the proposed model is unclear (making the experiments difficult to reproduce) and lacks of a better discussion about the interest of mixing multiple loss.			MET
The explanation in Fig 2 on why this is the case seem to me not so clear. Are you trying to show that the Wave-U-Net does not work since there is no 1/f^2 law for clean audio signals?			TNF
The explanation of this section is slightly unclear.			NONE
There is a little typo in Formula 1 for the STFT spectrogram, I would use the modulus |.| rather than || . ||.			MET
However, the setting is less clear to me for both the unsupervised speech/music restoration and supervised source separation problems.			NONE
I am a bit unclear about how comparisons are made to other methods that do not optimize for small numbers of parameters? Do you compare against the lowest error network found by LEMONADE? The closest match in # of parameters?			MET
I identify this ambiguity between BERTScore versions as an important weakness of the paper.			MET
- It's unclear throughout whether words or wordpieces are the main token being considered.			NONE
I recommend adjust the language to be more consistent throughout.			OAL
Notably, I'm actually unsure whether you compute IDF over words or word pieces, and how this is applied.			MET
When it comes to clarity and organization I find the paper a bit "messy" in that it is a collection of quite a few findings on the very specific topic of binary classification with quite strong assumptions.			NONE
Especially given the very specific nature of the topic I miss a strong and clear path through the paper.			OAL
Tiny detail: The axes of several of the plots given in the paper mis the lables which makes it hard to read. Straightforward to fix, but worth mentioning nevertheless.			TNF
I found the method section a bit difficult to read though, and even after several readings I cannot get my head around it. Specifically, here are some issues that I hope the Authors could clarify.			MET
- There is a typo in equation 6			MET
In terms of writing, the paper is a bit confusing in terms of motivations and notations.			OAL
In addition, Procedure 1 is not referenced in the text which makes is hard to understand the utility of the same.			MET
In Section 4.1			NONE
- I am confused what is the fixed reference in Figure 6. It is not explained in the main paper.			TNF
The paper is well-written but the structure is a bit disconnected; most notably, I didn't see clearly how Section 2 and 3 fit together.			OAL
- I understand the upper bounds on the singular values, but I am not sure how they relate to inverse stability.			MET
The paper seems to lack clarity on certain design/ architecture/ model decisions.			MET
For example, the authors did not justify why VGMM model was chosen and how does it compare to other density estimators.			MET
Also, I had to go through a large chunk of the paper before coming across the exact setup.			MET
I think the paper could benefit from having this in the earlier sections.			OAL
3.			NONE
First, this paper is not easy to follow.			OAL
For example, in Section 4.1, I am not sure the equation (3), (4), (5), (6) are the contributions of this paper or not since a large number of citations appear.			NONE
Furthermore, I do not see legend in Figure 3 and thus I cannot figure out what the curves represent.			TNF
Fourth, there are some grammar mistakes and typos.			OAL
For example, there are two "the" in the end of the third paragraph in Related Work.			RWK
In the last paragraph in Related Work, "provide" should be "provides".			RWK
In page 8, the double quotation marks of "short-term" are not correct.			NONE
Third, the writing in the paper has some significant lapses in clarity.			OAL
I was a substantial way through the paper before understanding exactly what the set-up was;  in particular, exactly what "state" meant was not clear.			NONE
- This sentence  was particularly troublesome:  "Each  state s_t also includes the state of the achieved goal, meaning the goal state is a subset of the normal state.			NONE
- Also, it's important to say what the goal actually is, since it doesn't make sense for it to be a point in a continuous space.			NONE
I found the paper confusing at times.			OAL
It is well-written from a syntactical and grammatical point of view, but some key concepts are stated without being explained, which gives the impression that the authors have a clear understanding of the material presented in the paper but communicate only part of the full picture to the reader.			OAL
I have two issues with the chosen example: 1) the connection with combinatorial optimization is not clear to me, and 2) it’s not very well explained.			NONE
The connection to combinatorial optimization, however, is much less clear to me.			NONE
The related work section looks incomplete with some missing related references as mentioned above, and copy of a segment that appears in the introduction.			RWK
The imagenet experiment lacks details.			EXP
- Page 8: "differntiable methods for NAS." differentiable is misspelled.			MET
I did not completely follow the arguments towards directed graph deconvolution operators.			MET
There is lack of clarity and the explanation seems lacking in parts in this particular section; especially since this is the key contribution of this work			NONE
Typo:. The “Inf” in Tabel 1			TNF
A clearer explanation of the theory here would help, as I think Fourier's theorem nicely supports your claims.			MET
If this is what these graphs show, consider using a different visualization to make it clearer that you're improving the final performance, not just the training process.			TNF
Clarity: The clarity is below average.			OAL
However, the motivation and benefits of introducing a parent and child variational approximation are not discussed adequately.			MET
I also struggled a little to understand what is the difference between forward interpolate and filtering.			MET
The clarity of this paper needs to be strengthened.			OAL
While the generation of this type of explanations is somewhat novel, from the text it seems that the proposed method may not be able to indicate what part of the image content drove the model to predict class A. Is this indeed the case?			MET
1) There is no clear rationale on why we need a new model based on Transformers for this task.			MET
Readability suggestion: the paper starts with a very nice motivating example, but when the setup is provided, i.e., that (x,c) pairs are the input to the learner, the intended content of c is not immediately clear- control variates could assume anything from general context information to privileged information.			NONE
A similarly informative example would be great!			NONE
Clarification regarding lemma 1: it seems that if the true posterior cannot be expressed by q, a gap will necessarily remain, even in the “limit” of perfect learning. Is this correct?			MET
Some portion? It is not clear from reading. This would be a serious impediment to reproducibility.			NONE
Also, Table 1 is really confused. I would not understand the meaning if I am not familiar with the experiment settings.			TNF
- Lambda sim and lambda s are used interchangeably. Please make it consistent.			MET
- The caption of Table 1 is a little vague. Please clearly state the meaning of the numbers in the table.			TNF
2. In Figure 3, the baseline got different perplexity between 3(a) and 3(b).			TNF
- "... is that instead of trying to probability ..." => "... tying ..."			NONE
- "... All models sare trained ..." => "... are			NONE
- "... Tho get the feature ..." => ?			NONE
2) The main contributions of this paper are not quite clear to me.			OAL
I am not convinced that the measure theoretic perspective is always			MET
4.4, law of total variation -- define			MET
The authors do not focus (in the main paper) on GAN variants used currently, and it is not clear if the proposed approach provides improvement relative to the current state of the art implementations (see next paragraph)			MET
- Abstract: ‘converge better’ it is not clear to me in what sense (faster/better final performances)			NONE
I believe in the second paragraph of 4.1 the authors provide some insight into this matter, however, I have to admit I do not understand this paragraph:			MET
In summary, I think this is interesting work, but a clearer explanation of the relationship between HRL and MARL, as well as a clearer main argument, supported by experimental evidence, would greatly improve this paper.			EXP
Unfortunately, I don't feel like it sufficiently addresses my questions and concerns.			NONE
However, what was not clear to me is how this reduces the non-stationarity of MARL.			MET
I feel like in the paragraph in questions, a lot of causes and effects are mixed up and more careful descriptions of the benefits of the algorithm would help.			MET
Apart from global vs. local, can the authors provide more examples of what sort of information this approach can disentangle? (Even for global vs. local, is there a transformation that can remove local information as opposed to global information?)			MET
- What is the choice of beta in the beta-VAE training objective?			MET
- Why were 30 discrete categories used in the clustering experiment? Is this still comparable to the approaches that use 10, which would correspond to the number of classes?			MET
1. What is M in Algorithm 1 ?			MET
2. First paragraph in related work is very unrelated to the current subject, please remove.			RWK
Additionally, some readers may find this paper a little difficult to read due to (1) lack of clarity in the writing, e.g., the first three paragraphs in Section 3; (2) omitted details, e.g., how much overlap exists between kernels (Figs. 1, 2, and 4 suggests there is no overlap - this should be made clear); and (3) poor grammar and nonstandard terminology, e.g., the authors' use of the word "energy" and the phrase "degradation problem".			NONE
All of these issues should be addressed in a future version of the paper.			NONE
Not sure why Eqns. 2 and 9 need any parentheses			MET
.  They should be removed.			NONE
1)	on page 4, Section 3, the first paragraph, shouldn’t “C_p^{val} of 55” be “C_p^{test} of 55”?			NONE
- The parameter count of a 2-layer network with $h$ hidden units and input dimension $d$ is $O(dh)$. So perhaps it makes sense to study an asymptotic regime where $dh/n$ approaches $\gamma$, instead of both d and h growing linearly in n. While this issue is hinted at in the discussion section, I don't understand the statement "the mechanism that provably gives rise to double descent from previous works Hastie et al. (2019); Belkin et al. (2019) might not translate to optimizing two-layer neural networks."			MET_RWK
- Lack of a strong explanation for the results or a solution to the problem			RES
However, the explanation of the strategy wasn’t very clear for me, and the authors didn’t frame it as a major contribution.			MET
For the learnable data augmentation it would be great if the authors could provide more insight into the method, how it works, and why is it better than the alternatives.			MET
In terms of presentation, the notation and statement of theorems are precise, however, the presentation is rather dry, and I think the paper can be significantly more accessible.			OAL
This second part is indeed what is used in the proof of the universality of the permutation invariant version of DeepSets, making the connection more visible.			MET
Generalizing this to the multi-channel input as the next step could make the proof more accessible.			MET
1) I do not really understand the emphasis on optimisation while all the proofs are related to the convergence to the stationary distributions.			MET
- There is no need for such repetitive citing (esp paragraph 2 on page 2).			BIB
Sometimes the same paper is cited 4 times within a few lines.			BIB
While it’s great that so much prior work was acknowledged, mentioning a paper once per paragraph is (usually) sufficient and increases readability.			RWK
- Some parts of the paper feel long-winded and aimless.			OAL
Section 2 background takes too much space.			NONE
- On page 4 footnote 2, as far as I know the paper did not define BPD.			NONE
- There are two lines of text between Fig. 4 and Fig. 5, which is confusing.			TNF
There are probably many other ways to trade BLEU score for efficiency, and without showing those other methods (and the point drops they have), it's not clear that K-matrices are a good way to speed up decoding a bit.			MET
The experimental section includes various mistakes (see under minor) and misses to describe figures, leading to the assumption that additional time is required for a more detailed evaluation of the algorithm (including more domains and in particular baselines).			EXP
- Direct duplication of text between parts of section 5.3 and 8.3 leading to the duplication of the error of describing the value function learning rate as 0.000.			MET
- Self-referential sentences in the supplementary materials (i.e. referral to itself)			NONE
- Missing references on page 3			BIB
- The egocentric velocity field is not described (section 5)			MET
- The wording new paradigm in MARL might be unsuited given existing work on complex domains.			MET
‘Our proposed approach represents the first physics-based simulation of its kind that supports MARL.’ This sentence remains unclear as the authors do not propose a simulation engine.			MET
- Text on experiment figures is much too small.			TNF
However, the  transfer learning model is unclear.			MET
Cons:  unclear transfer learning model, insufficient experiments.			EXP
Detail: section 4 describes the transfer learning model used in the work, but the description is unclear.			MET
It is unknown the used model is a new model or existing model.			MET
Minor nitpick: There seems to be some space crunching going on via Latex margin and spacing hacks that the authors should ideally avoid :)			NONE
I have doubts on applying the proposed method to higher dimensional inputs.			MET
In			NONE
section 6.3, the authors show an experiments in this case, but only on a dense			EXP
ReLU network with 2 hidden layers, and it is unknown if it works in general.			MET
Additionally, in section 6.4, the results in Figure 2 also does not look very			RES_TNF
positive - it unlikely to be true that an undefended network is predominantly			MET
robust to perturbation of size epsilon = 0.1. Without any adversarial training,			NONE
adversarial examples (or counter-examples for property verification) with L_inf			MET
This change is noted, but it would be useful to make a least a brief (i.e. one sentence) comment on the motivation for this change.			MET
I like the drawings, however, the font on the drawings is too small - making it hard to read.			NONE
1. the difficult to train the network			EXP
2. table 2: Dynamic -> Adaptive?			TNF
9.2.	In formula (1.5) “<=>” seems to be used at different levels (?) it would be good to use brackets to make clear which level “<=>” refers to.			MET
10.	The title suggests that the paper studies multiple VQA models but only a single model is studied.			INT
- The problem setting description is neither formal nor intuitive which made it very hard for me to understand exactly the problem you are trying to solve.			PDI
- The formalization in Paragraph 3 of the Intro is not very formal. I guess S and i are both simply varying-length sequences in U.			NONE
-In describing Eqn 3 there are some weird remarks, e.g. "N is the sum of all frequencies". Do you mean that N is the total number of available frequencies? i.e.			MET
- Your F and \tilde{f} are introduced as infinite series.			MET
- In general, you have to introduce the notation much more carefully.			NONE
'C[1,...,B]' is informal abusive notation.			NONE
You should clearly state using both mathematical notation AND using sentences what each symbol means.			NONE
- Still it is unclear where 'fj' comes from. You need to state in words eg "C[b] contains the accumulation of all fj's such that h(j)=b; i.e. for each sequence j \in U, if the hash function h maps the sequence to bin b (ie $h(j)=b$), then we include the *corresponding frequency* in the sum."			MET
- What I don't understand is how fj is dependent on h. When you say "at the end of the stream", you mean that given S, we are analyzing the frequency of a series of sequences {i_1,...,i_N}?			NONE
- The term "sketch" is used in Algorithm1, like 10, before 'sketch' is defined!!			MET
-I'm not going to trudge through the proofs, because I don't think this is self-contained (and I'm clearly not an expert in the area).			MET
Honestly, this paper is very difficult to follow.			OAL
For example, the related works paper would read better if it wasn't one long block (i.e. break it into several paragraphs)			RWK
While the idea of the submission might be useful in some applications, the work is rather vaguely written, it is in draft phase:			NONE
1. Abbreviations, notations are not defined: GAN, WGAN-GP, DNN, FID (the complete name only shows up in Section 4), softplus, sigmoid, D_{\theta_{old}}, ...			NONE
2. While the primary motivation of the work is claimed to be 'mode collapse', it does not turn out from the submission what mode collapse is.			MET
While the idea of the work might be useful in practice, the current submission requires significant revision and work before publication.			NONE
Since the results are far from state of the art, a clean and neat presentation of the theoretical advantages and contributions is crucial.			RES
2) The presentation is not professional, hard to follow and the submission overall looks very rushed:			OAL
- In equations, please use \inf, \sup, and \text{...} for text such as distance, data, ...			NONE
- The set F in Definition 3.5 looks odd, as it appears to be recursive and might not be unique.			MET
- The writing looks very rushed, and should be improved.			OAL
For example, I have trouble understanding the sentence "So the existed algorithms should be heuristic or it can get a bad result even we train the neural networks with lots of datasets." in the introduction.			DAT
- The aspect ratio in Fig. 5 should be fixed.			TNF
For example, the setting of “No strong aug.” and “No weak aug.” are not clear.			NONE
-  Figure 2 are hard to read for different M's. It would be better if the authors can show the exact accuracy numbers rather than the overlapped lines			TNF
- Again, Figure 3, it is hard to see the benefits for increasing M from the visualizations for different clusterings.			TNF
This paper has problems with clarity/polish and experimental design that are sufficiently severe			EXP
I am generally not super picky about these things, but there does have to be some standard.			NONE
This paper looks very hastily put together, especially pages 7 and 8.			OAL
There are many typos and unclear statements.			OAL
What does this mean? People have mostly settled on using FID for this.			NONE
Isn't this just restating the point made in the first sentence?			MET
But this paper doesn't propose a way to solve that problem, so it's strange to mention this here in this way.			NONE
3, The notations in Eq. (1) and (2) are messy.			MET
Although I can figure their meaning from the context, one may clarify certain notations if they appear at the first time.			NONE
However, the method, particularly on the weight sharing, lacked a bit of important background on adaptive softmax.			MET
It is unclear how well the proposed method works in general.			MET
The plot (Figure2 (d)) is very blurry and people cannot really tell local structure from it.			TNF
It is hard to understand the results without discussing it.			RES
1. The usage of footnote 2 is incorrect.			NONE
Furthermore, I had some trouble understanding how a SAT instance is solved using algorithm 1.			MET
Specifically the text in Section 3.3 that explains Algorithm 1 is a bit confusing.			MET
Some figures, like Figure 3 and 4, are hard to read.			TNF
In the start of Section 3, it is not clear why having the projection be sparse is desired.			MET
To change to a firm accept, I think the paper needs some changes in written style mainly, to make it friendlier to newcomers to the area, which can easily be implemented in the camera ready stage of paper preparation.			OAL
For instance, the omission of a results discussion section or a conclusion are clearly not reader friendly.			RES
Overall, the paper is a little confusing.			OAL
eqn (8): use something else to denote the function 'U'.			MET
You used 'U' before to denote the set.			NONE
Unfortunately, it is riddled with grammatical errors and should be proof-read carefully. A lot of singular/plurals are off, and some formulations are odd or downright unclear.			OAL
- Grammatical errors and odd formulations all over the place			OAL
• Not all of the arrows in Figure 1 are pointing to the right lines.			TNF
• In the abstract, it might be good to clarify that the exponentiation is elementwise.			NONE
5. Clarity: The first half of this paper was easy to follow and clear. The experimental section had a couple of areas that left me confused. In particular:			EXP
6. Validation data / test sets: Throughout this work, it is unclear what / how validation is performed.			DAT
You have 3 datasets worth of data for the regression task (it is still unclear, however, what is being used for evaluation), but it doesn't look like this is addressed in the larger scale experiments at all.			DAT
It is unclear to me how this will avoid these.			NONE
Additionally, it is unclear how you will have vanishing rewards given the structure of the learned controller.			MET
This clipping will also introduce bias, this is not discussed, and will probably lower variance.			MET
8. "Beyond fixed schedules, automatically adjusting the training of G and D remains untacked" -- this is not 100% true.			EXP
This detail is not discussed in this paper, and some details -- such as the learning rate for the meta-optimizer I was unable to find.			MET
Current representation is difficult to read / parse.			OAL
It would be better if the authors were a little more careful in their use of terminology here.			MET
*The strategy proposed to introduce weak-supervision is too ad-hoc.			MET
If this is the case, this should be properly discussed in the paper.			NONE
*It is not clear why the latent variables modelling the generative factors are defined using a Gaussian prior.			MET
How the case where two images have a very similar latent factor is avoided while generating pairs of images for the Siamese network?			MET
-The strategy proposed to provide weak-supervision to the model is too ad-hoc and it is not clear how to apply it in general applications			MET
-The experimental section do not clarify the benefits of the proposed approach.			MET_EXP
Without the comparison it’s not clear how much improvement this approach provides compared to existing work that perform stale updates.			MET_RWK
Although a detailed discussion is provided related to the memory consumption between the proposed method and PipeDream, no detailed discussion is provided with respect to GPipe.			MET
My main hesitation comes from a lack of clarity about the main lesson we have learned.			NONE
* page 2, "We show that..." I'd break this into two sentences to make it easier to parse.			NONE
With the current table, one has to compare cells in the top half of the table to those in the bottom half of the table, which is quite difficult to do.			TNF
Q2: Can the authors structure the experimental results with different sections? Currently it is just a single section which is difficult to read.			RES_EXP
There were some experimental details that were poorly explained but in general the paper was readable.			EXP
- (W4) Metrics for ranking of transfer don't make sense (and some are missing) : I also don't understand how "precision" and NDCG are used as metrics.			MET
In most cases I would care about how well my model would perform on transfer not just which tasks I should transfer from. I would have wanted to understand something like the correlation of these produced scores with the actual ground truth performance numbers.			MET
If I were to go through the computation of then why not just train a smaller version of that representation technique instead and **directly** see how well it can encode data in k dimensions via that technique / for that task?			MET_DAT_EXP
For example say we had two task with exact opposite labels.			NONE
They would have a very low weight difference score though they are ideal representations for each other.			RES
- My main concern about the analysis is that it shows why several methods (e.g., momentum, multiple update steps) are *not* helpful for stabilising GANs, but does not tell why training with these methods, as well as others such as gradient penalty, *do converge* in practice with properly chosen hyper-parameters?			MET_ANA
I am quite confused with the name PointNetST.			NONE
The gain of using a sufficiently more complicated approach to assess the overall distance between the test and training dataset is not clear, comparing it to the very insightful histograms.			DAT
- (top of p. 2) What exactly is the difference between "implicit feature combinations" and "explicit (?), expressive feature combinations"			MET
- (top of p. 2) "encourage parameter sharing" - between what and what? at which level? [reading on, I realized this applies to groups of features; it should maybe be made clear earlier]			MET
The plots in figure 4 are too small.			TNF
It would be useful to have a table, like the one on the last page, which clearly shows the baseline vs. the random-projection model, with some description of the results in the main body of the text.			RES_TNF
The overall presentation could be better, and I would encourage the authors to tidy the paper up in any subsequent submission.			OAL
For example, there are lots of typos such as "instead of trying to probability of a target word".			NONE
* Some turns of phrase like "recently gained a flourishing interest", "there is still a wide gap in quality of results", "which implies a variety of underlying factors", ... are vague / do not make much sense and should probably be reformulated to enhance readability.			RES
* Introduction, top of page 2: should read "does not learn" instead of "do not learns".			NONE
* Section 3.1, "amounts to optimizing" instead of "amounts to optimize"			NONE
* "circle-consistency" should read "cycle-consistency" everywhere.			NONE
* The model name "FILM-poi" is only used in the "implementation details" section, it doesn't seem to be referred to anywhere else. Is this a typo?			NONE
Also, the sentence, "We demonstrate the implementation and performance of our pipelined backpropagation in PyTorch on 2 GPUs using ResNet, achieving speedups of up to 1.8X over a 1-GPU baseline, with a small drop in inference accuracy.", is confusing. If I use data parallelization, the gain should be also around 2.			EXP
The exposition is not particularly clear in several places:			NONE
- U^m in Eq 1 is undefined and un-discussed.			MET
What probability term does it correspond to? It is supposed to make probabilities of different cardinalities comparable, but the exact mechanism is unclear.			MET
- The term p(w) disappears on the left hand side of Eq 2.			MET
- Notation in Sec. 3.2 is very cumbersome, making it hard to follow.			MET
Furthermore, I found the description ambiguous, preventing me from understanding how exactly the permutation head output is used in Eq 5.			MET
Specifically, there is some confusion about estimation of w~, which seems based on frequency estimation from past SGD iterations (Eq 3).			MET
- The network architecture is never described, especially the transition from Conv to Dense and the layer sizes, making the work hard to reproduce.			MET
I do not understand the "deep integration of MARL and HRL" that is claimed in the Introduction.			INT
I also do not agree with another claim that "We consider the simulation and training environment to be another novel contribution... few simulator support more than one agent, at most 2".			NONE
Second, the writing can be greatly improved.			OAL
Almost half of the technical details are buried in "8. Supplementary material".			NONE
Since it is not fair to use "Supplementary material" as a way to extend the page limit, I will make my judgement of the paper solely based on the contents up to Section 7.			NONE
In the main text (up to Section 7), there is no mentioning of how the low-level controllers are learned, and how to combine PPO in a MARL partial parameter sharing setting.			MET
Most of these should be moved to the main text.			NONE
1) Certain paragraphs in the main text can be significantly shortened, such as the reward shaping in Section 5.2.			NONE
2) It would be great if the paper can clearly define the experiments: "waypoint", "oncoming", "mall", and "bottleneck".			EXP
3) The paper needs a thorough proof-reading. There are many grammar mistakes, typos, missing citations. For example,			OAL
Is there a constraint on the CFS and classifiers that ensure the difference between the weights really captures what is suggested?			MET
- The marketing phrase "the blind-spot attach" falls short in delivering what one may expect from the paper after reading it.			NONE
The paper would read much better if the authors better describe the phenomena based on the gap between the two distribution than using bling-spot.			MET
Though the paper is not suggesting that, it would help to clarify it in the paper.			NONE
Is it because the ML solution would be faster to compute with big instances? Is it because with the proposed approach one can curate sophisticated heuristic solutions when provable optimality is out of reach?			MET
This needs more elaboration. Is this way of training results expected? What is the lesson learned?			EXP
- the exposition could be improved, in particular the description of the plots is not very clear, I'm still not sure exactly what they show			TNF
- not clear what the target audience of the first part (section 2) is, it is too technical for a survey intended for outsiders, and discusses subtle points that are not easy to understand without more knowledge, but at the same time seems unlikely to give additional insight to an insider			NONE
- I believe that in particular section 2 goes into too many mathematical details and subtleties that do not really add a lot.			MET
I think that either the reader already understand those concepts well (which I admit, I don't really, I'm merely curious about GANs and have been following the action from a distance, hence my low confidence rating to this review), or if they does not, it will be very hard to get much out of it.			MET
- "the variance of models obtained by Guassian Process regression is handled implicitely so we tran each model once"? I do not understand what this means, and I work with hyper-parameter tuning using gaussian processes daily.			MET
Although the method the authors suggest is a plausible way to explicitly model the relationship between syntactic pairs and to create a combined embedding for them, their presentation does not make this obvious and it takes effort to reach the conclusion above.			MET
If this is the case, authors should state it more clearly in the paper that a large proportion of the gap in performance between MixMatch and ReMixMatch is the introduction of stronger transformations (Autoaugment style).			MET
- The discussion/explanation of the differing performance of tying or not tying each part of the embedding weights for the different datasets is confusing; I think it could benefit from tightening up the wording but mostly I just had to read it a couple times. Perhaps all that's complicated is the distinction between embedding and projection weights; it would definitely be helpful to be as explicit about that as possible upfront.			MET
p2-3, Section 3.1 - I found the equations impossible to read. What			MET
- The importance mixing does not seem to provide a better performance and could therefore be shortened in the paper			MET
The graphs were difficult to parse.			TNF
I was able to make them out, but perhaps separating the top row (FID and diversity graphs) into separate figures, separate lines, or something would have reduced some confusion.			TNF
In addition, different charts presenting only one loss function, with their spectral normalization and gradient penalty variants, would have made the effects of the normalization more obvious on the FID distribution graphs.			NONE
If this can be changed before publication, I would strongly suggest it.			NONE
It is unclear how the model actually operates and uses attention during execution.			MET
These			NONE
* In the introduction, "the classical approach" is mentioned but to be the latter is			INT
* From page 3 onwards: I was truly confused by the use of [x] throughought the text			NONE
* In Section 4, it took me some time to understand that the considered metrics do not			NONE
3) I would suggest a different name other than Neural-LP-N, as it is somewhat underselling this work. Also it makes Table 2 not that easy to read.			TNF
- Section 1.1 presents results with too many details without introducing the problem.			RES
It is hard to follow the flow of ideas present in the paper when similar things are not together.			NONE
I would suggest restructuring the paper into a more classical structure such as: <intro without detailed results - previous work & problematic - approach taken with more details for reproducibility - description of the two tasks - description of experiments with more details for reproducibility - results - conclusion>.			NONE
It is very hard to understand sentences referring to this.			NONE
- typo at the beginning of section 3.1: missing 'be' in  "This can either *be* by an ..."			NONE
- typo at the beginning os section 4:  missing 'be' in "... the algorithm must irrevocably *be* allocated to ..."			MET
Therefore, I recommend to accept the paper but after revision because the presentation and explanation of the ideas contain multiple typos and lacking some details (see bellow).			OAL
-	The spacing between sections is not consistent.			NONE
-	Figures 1 is way too abstract given the complicated set-up of the proposed architecture.			TNF
-			NONE
Featuremetric BA / Feature-metric BA / Featuremetric BA / ‘Feature-metric BA’			NONE
-	Talking about depth parametrization use ‘basis’ or ‘bases’ not both and clearly defined the meaning of this important notion.			MET
-	Attention should be given to the notation in formulas (3) and (4).			MET
The projection function there is no longer accepts a 3D point parametrized by 3 variables.			MET
Instead only depth is provided.			MET
In addition, the subindex ‘1’ of the point ‘q’ is not explained.			MET
-	Attention should be given to the grammar, formatting in particular the bibliography.			NONE
I don't understand what is meant by cognitively plausible but not human-like; perhaps an example of a cognitively implausible mechanism would help clarify this issue.			MET
In general, I don't understand why we would want a visual question answering system that returns approximate answers --			NONE
* Another vague concept that is used without clarification: it is argued that if the network implements something like the Approximate Number System, that shows that it can "learn and utilize higher-level concepts than mere pattern matching".			MET
It is a pity to see that the authors provide acronyms without explicitly explaining them such as DDPG and TD3, and this right from the abstract.			NONE
The parer is  in general interesting, however the clarity of the paper is hindered			NONE
by the existence of several typos, and the writing in certain passages can be improved. Example of typos include  “an surrogate gradient”, “"an hybrid algorithm”,  “most fit individuals are used ” and so on…			NONE
