- While the authors mention multiple times that #rhs/#lhs = 1 and 2 are more challenging than #rhs/#lhs=18, they do not sufficiently explain why this is the case anywhere in the paper.
However, to state the result of Theorem 4.3, k should be bigger than M c_\eta from the dentition of \tilde{\rho}_k^M, as shown under the equation (4).
The approach is not linked to so called Tandem approach that was/is popular in speech recognition where a generative model (GMM) is trained on top of features extracted by a neural network model.
- The proposed approach is a fairly specific form of self-modulation.
Can you clarify how you view the relationship between the approaches mentioned above?
- important features for the new task should be in similar locations as important features of the old task (for example, one would expect that the proposed approach would negatively affect learning the new task if the important features of the old task were all located in the bottom of the image, while all important features for the new task are in the top)
For example, by increasing the amount or size of image regions to be considered, the classifier may accidentally become more robust on an old task.
- The randomized weight is not very practical. Though it may be the standard approach of mean field,
Why should we use embedding to compare the similarity?
This means that it is not apriori clear if using this solver instead of standard SGD, ADAM is any good.
I find these assumptions too strong for the task of learning disentangled representation.
The proposed method is very simple and frames the problem basically as a supervised learning problem.
They can indeed be subsumed by generalization bounds based on VC theory.
The bound obtained in Theorem 2 comes rather easily from the bounded assumption on the non-linearity and is this not very interesting.
- The search space of the proposed method, such as the number of operations in the convolution block, is limited.
One question which is not addressed is the reason for only one RBM layer.
Furthermore, If I deploy the learning algorithm on a physical robot, will the temporally consistent exploration cause less wear and tear to the actuators when the robot explores.
My assumption is the visual feature already contains the label information for image captioning.
Is the number right?
It adds extra complexity, requires you to do function composition which may be less expressive and takes more coomputation, etc. And to what end?
The function composition doesn't capture that.
2. Besides the computational expensive-ness of the three-step approach (vector quantization, BERT, acoustic model training), the combined model complexity is large because these steps do not share neural network architecture.
This claim is unfortunately unfounded for a very important reason: the LSTM performance is not at all close to that which can be achieved by LSTMs in general.
Though this may be true, these models still undermine the claim that “neuromodulated plastic LSTMs...outperform standard LSTMs on a benchmark language modeling task”.
Concerns: I find the claim on deep networks kind of irresponsible.
Second, the conclusion of Theorem 2 seems to be flawed.
This illustrates that the conclusion of Theorem 2 may be wrong.
The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.
Don't the updates in Eq 1 and 2 take care of this? The ablation does not test this, right?
Another modeling choice that is not clear is regarding how the model processes the text -- reading prefixes of the paragraph, rather than one sentence at a time. What happens if the model is changed to be read one sentence at a time?
Is \eta_{\mathcal{H}} is a better estimation that previous adaptability term in multi-source DA? What is the role of \eta_{\mathcal{H}} in the algorithm design? How to control it in empirical algorithm?
The paper does not provide very significant evidence that this method is useful.
-1 The mathematics in section 3.1 is unclear and potentially flawed (more below).
-4 Talking about "agents" and "Multi-Agent" is a somewhat confusing given the slightly different use of the same term in the reinforcement literature. Why not just "mapping" or "network"?
At the very least we need another "partial" sign in front of the "\delta" function in the numerator.
So is the REINFORCE estimator used or something? Not that the importance sampling matter is orthogonal.
As with ensembles, clearly it only helps to have multiple agents (N>2) if the additional agents are distinct from f_1 (again without loss of generality this applies to g as well).
However, the proposed upper bound in the paper involves other parameters, such as the model complexity and the number of training samples.
1). Why minimizing the upper bound in this scenario would help to minimize the loss on the target domain, when the upper bound can be substantially huge? How about just evaluate \alpha by the closeness of the source domain with the target domain?
Intuitively, these criteria are well motivated, but unfortunately, the combination of all the intuitions (including "selection network" with threshold) is not very principled.
To me this paper is just not good enough - the method essentially i) use "a professor and two teaching assistants" to build a "rule-based concept extractor" for problems, then ii) map problems into this "concept space" and simply treat them as words. There are several problems with this approach.
Moreover, the main difference between math problems and other problems is that there are math expressions; I do not think that using words/concept labels only is enough without touching on the math expressions.
Second, the proposed method does not sound scalable - the use of a professor and two teaching assistants to construct the concept extractor, and the use of an expert TA to select a small set of informative words.
Algorithm 1 does not include mitosis, which may have an effect on the resulting approximation.
This claim is bigger than just CNNs and needs to be studied in a theoretical framework not an empirical one.
Also, one simple way to stop these adversarial cases would be to explore using Sigmoid as opposed to softmax.
This restriction exists in Dreamer, and the method cannot be applied to discrete control tasks unless approximation techniques such as Gumbel-softmax are used.
Dreamer does not use any variance reduction technique, so the gradient estimates could have very large variance.
The definition of “recovering true factor exactly” need to be given.
The descent lemma used by the author is not valid for the stochastic result.
Objectively saying that the author's method is better than CycleGAN is difficult. How does their ensemble method compare to just their single-agent dual method? Is there a noticeable difference there?
I feel the approach to implicitly assume that the classifiers to be compared are already "reasonably accurate"; since if not, both classifiers might be easily falsified by certain trivial examples, making the "disagreed examples" not as meaningful.
However, since the authors adopted an affinity-aware distance, two incorrect predictions can still be compared based on their semantic tree distances to the true class.
The advantage of INN, however, is not crystal clear to me versus other generative methods such as GAN and VAE.
-	In section 4, it is unclear why only the maximal activation of the softmax layer is used to characterize a sample? Why not considering the full distribution that should contain richer information? Why just focusing on the output layer and why not using the info available at intermediate layers?
This might be because the Bayes and MAT attacks are too simplistic.
Again, why not using the distribution of the outputs of all layers? Why focusing only on the output of the last layer?
Therefore, it is an overclaim that the KL-divergence bound (2) provides an immediate justification for AGZ’s core learning algorithm.
Furthermore, the robust MDP view of the AGZ in sequential decision-making problems is not so impressive either.
It isn’t clear why the particular proposals are necessary or to which of the proposed extensions the inflated OOD uncertainties can be attributed:
2. The  objective? The proposed objective,  Eq 5, trades off stochastically approximating the (conditional) marginal likelihood against not deviating too much from p(\theta) =  MN(0, I, I) in the KL sense.
-  Instead of relying on the KL regularizer, a standard approach to learning the model in Eq 3 would be to use well understood MCMC or variational methods that explicitly retain uncertainty in \theta and induce predictive uncertainties.  Were they explored and found to be not effective?
b) Down weighting the KL term by lambda for the VI techniques unfairly biases the comparison.
However, this can be tricky in practice since it requires having an estimate for Q_r(s,a) as a function of the state (in order to ensure that the state-dependent lower bound can indeed be satisfied).
Thus, the trained model will also only be able to handle one test domain, not much different than regular meta-learning models.
In short, the meta-learning part stays in the regular few-shot learning module (which is implemented as a prototypical network), and has nothing related to domain adaptation.
Specifically, what if the few-shot learning component is removed, and the network is trained with standard domain adaptation.
It is not clear how MULANN can work in this situation and how its performance vary with the noisy signals conveyed in those false pseudolabeled samples.
- can you motivate why you are not using perplexity in section 3.2?
-The average distortion metric (that’s unfavourable to your method anyway) doesn’t really mean anything as the constraint optimization has no incentive to find a value smaller than the constraint.
- I am not sure what the authors mean by “the Frank-Wolfe gap is affine invariant”. If we scale the input space by a, the gap should be scaled by a^2 - how/why is it invariant?
- In remark 4.8 in the end option I and II are inverted by mistake
- In the proof you wrongfully use the term telescope sum twice, there is nothing telescopic about the sum it is just bound by the max value times the length.
- Authors claim to identify a fundamental problem with the existing generative models for point clouds, yet Section 2 tries to show that a _specific version_ that uses DeepSet does not satisfy theoretical guarantees. What if we use e.g. a recurrent network instead?
- I don't understand why the authors say that their space "interpolates smoothly" just because the limit in the curvature is the same from the left and right side.
- One thing that I didn't see discussed by the authors is that there are subtle difference between hyperbolic and spherical spaces.
For example, the weighted midpoint of Def. 3.2 doesn't immediately extend to spherical space (or at least won't be unique).
- For the synthetic tree, why is the number of edges 2(|V|-1) rather than |V|-1?
However, the implementation details are hazy, and some design choices (which operations, hyperparameters etc.) aren't well justified.
- What are the motivations to use Frank-Wolfe ? Usually this algorithm is used when the constraints are to complicated to have a tractable projection (which is not the case for the L_2 and L_\infty balls) or when one wants to have sparse iterates which do not seem to be the case here.
- In Theorem 4.7 an expectation on g(x_a) is missing
- Sec 3.3 the argmin is a set, then it is LMO $\in$ argmin.
- One drawback is that this method requires the mask during training. How can it be adapted for scenarios where the mask is not present? In other words, we only see multiple modalities as input, but we are not sure which are noisy and which are not?
I am still a bit confused about the difference between "zero-confidence attacks" and those that don't fall into that category such as PGD.
Since all of these are approximations, and we cannot know how far we are from the true margin, I don't see why these categories help.
My main concern is that sampling the target values for the unobserved modalities from the prior would almost necessarily lead to blurry synthetic “ground truth” for these modalities, which in turn means that the model would produce underconfident predictions for them.
Furthermore, it is not obvious to me why these prior samples would be sensible at all, given that all modalities have independent latents by construction.
Without any comparisons it’s hard to tell how difficult the tasks under consideration are and what would amount to good performance on the held-out tasks.
3. What are the limitations of the F-pooling? It is good to me that the authors discuss one limitation on the imaginary part of output and I would like to hear more on other potential limitations for this method.
Besides some issues in the technical details, the major problem of this paper is that it uses the data processing inequality (DPI) in a **wrong** way.
However, based on the definition of \theta and \tilde{\theta} given in the first sentence of section 2.3, the relation between \theta, \tilde{\theta} and D
Either case, I don't think we can have the inequality in eq. (5).
The usefulness of the approach is also lessened by the greater importance of the choice of the optimizer.
- In order to pre-train the discriminator network, additional (s,a,s') experiences are required, thus it seems difficult to say that it is better for exploration than VIME.
1. I had hard time to understand latent canonicalization.
Do you mean that each latent variable is fixed to a value, such that this factor of variation is disabled? Are the canonicalizers pre-specified using meta-labels? Are they updated/learned during model training?
More explanation of canonicalization is needed.
2. The learning of the proposed model relies on meta-data description such that the learning is supervised. Can the method be applicable to situations where no meta-data and no class labels are available?
3. How can the proposed method be generalized to non-image data?
However, it seems a little unexplainable to apply a technique developed from classification to analyze RNNs, since the main task of RNNs never should be classification.
2. The distribution proposed in section 3.2 assumes independence between the elements $x_j$. This seems problematic for some relatively simple problems.
Is there any explanation for this?
It seems like a large assumption that the same learning rate would work for different methods, especially when some of them are normalizing the objective function.
The comparison of different observation representations doesn’t include any analytical component, the empirical component is primarily inconclusive, and the position statements are fairly non-controversial (and not really conclusively supported).
- doesn’t answer the one question regarding observation representation that it set out to evaluate
- presumably, the sample complexity is ridiculous
Dealing with highly stochastic environments seems a potential fatal flaw of the assumptions of this method.
To start with, the set of rotations R(phi, nu, 0) called the alt-az group in this paper is not a group in the mathematical sense.
This easy to see, because a composition of rotations of the form Rz(phi) Ry(nu) is not generally of that form.
This would mean that the layer is actually SO(3)-equivariant, but it has been proven [1], that any rotation equivariant layer between scalar spherical feature maps can be expressed as an azimuthally isotropic convolution.
Since the alt-az convolution is not isotropic and maps between scalars on S2, it cannot be equivariant.
But it is easy to see that eq. 10 will give different results for each of these coordinates, because they correspond to different rotations of the filter about the Z-axis.
In particular, I believe that any algorithms you compare against, you should optimize both G and theta, since optimizing purely the hardware is unfair.
This statement is ambiguous and potentially unsupported by evidence. how do you define complex? that can or that did discover?
#5 is weak, and tell us more about the limitations of random search and naive ES than necessarily a merit of your approach.
- Sec 2.2: "(GNNs) are very effective" effective at what? what is the metric that you consider?
- Sec 3 "(PS), where weights are reused" can you already go into more details or refer to later sections?
- Sec 3.1: the statements about MB and MF algorithms are inaccurate.
- Sec 4.1:
- A.4 makes it sound like eps_t needs to be assumed to be bounded, when all that is required is the bound on eps_0.
However, when there is no defense deployed in the training process, it is not intuitive to see why the proposed attack is more effective and persistent than the centralized attack, given that a smaller trigger usually results in a worse attack performance.
1. The classification of base class into super classes seems questionable to me.
Hence, to characterize different tasks, as far as I am concerned, the classification should take both the graph G and the label Y into consideration, instead of solely the graph.
- I don't see a discussion about the downsides of the method (for example, the large number of triplet comparison examples needed for training; and possible methods to overcome this problem).
The reparameterization trick does not apply to all continuous random variables, only to such that the reparameterization satisfies certain smoothness conditions.
Discrete variables are supported by the method only in the case that the distribution factors over all discrete variables conditionally on any additional “continuous variables” (to which the reparameterization trick is applicable).
A Bernoulli(p) random variable is discrete, yet it is reparametrizable as [Z>p] with Z following standard logistic distribution, whose density and cdf is smooth.
The notation of the true distribution as “q” the model as p and the approximate posterior of the model as “q” again is inconsistent.
Equations (5) and (6) require a theorem of differentiating under integral (expectation), such as Leibnitz rule, which in case of (6) requires q_gamma(y)f(y) to be continuous in y and q_gamma(y) continuously differentiable in gamma.
- Some assumptions are not explicitly stated.
- The overall method seems to be not very principled, and requires a lot of "tweaks and tunes", with additional losses and regularizers, to work.
Also, the definition of mutual information used in this paper uses the inferred distribution q (e.g., in eq. 2), which is somewhat unusual.
Authors should analyze the stability of their method in details.
1. The observation that gradients explode in spite of BN is quite counter-intuitive. Can you give an intuitive explanation of why this occurs?
I think having this in mind, showing the convergence of Theorems 3.2-3.4 is somehow trivial.
5) In Theorem 5.2, the term 1/sqrt(2) is missing from the final bound.
The use of “blank inputs (referred to as “thinking steps”)” in “Simple LSTM” and “Attentional LSTM" doesn’t seem to be a standard approach.
In the attentional LSTM, the use of “parse LSTM” is also not a standard approach in seq2seq models and doesn’t seem to work well in the experiment (similar result to “Simple LSTM").
Roughly, the theorem states that if for example we fix any matrix B of size e.g. 256 x k and matrix U of size 512 x 256 and then compute U relu(B C) where C is the vector of parameters of size k x 1, AND if k < 2.5 (i.e. if we use at most 2 parameters), then it would be very hard to fit 512 iid gaussian values (i.e. min_C ||U relu(B C) - eta|| where eta ~ N(0, 1)).
From my experience, aligning embedding spaces is something that usually does not work very well, especially in high dimension.
The role of \sigma seems very redundant given \omega.
How is this a reasonable assumption?
Considering the optimization problem involved in the learning process, it is hard to judge whether the effect of such a procedure from the optimization perspective.
C. Your method involves a hyperparameter to be tuned which affects the shape of the schedule.
This hyperparameter itself benefits from (requires?) some scheduling.
The more interesting question of how to generalize to unseen items (how would that be possible given that items have no representation at all) is not discussed at all and seems not to be realizable, which makes the starting point of such methods (items have no representation) questionable.
-	The paper has a specific form of formulation for abductive reasoning, where there are exactly two observations and one proceeds the other; the explanation happens in between. I can see this helps collect and annotate data, but also limit the form of abductive reasoning and how models should be developed.
If the compression rate is optimized on the test set, then the compressed model is nothing but a model overfit to the test set.
I am a making a recommendation for reject for this paper with the main reason being that I believe the primary derivations for their method appear flawed.
--In the main section describing the approach (Section 4), the authors start with a claim that Equation 1 and 2 are equal; I don’t believe 1 and 2 are equal.
--In Section 4.1, it appears that they are instead making a claim about Equation 2 being a bound for equation 1; but even this derivation appears to have a problem.
If one were to interpret the second one as the unnormalized distribution on z defined via the likelihood for c given z; even this has an issue because then the expression for KL where we plug the unnormalized density in place of the normalized need not be positive which is something they need to derive their bound.
--Another issue is that the regularization lambda should apply to both the terms in the bound but in Equation (7) only appears selectively for one of the two terms.
--“Redundant weights” seems like not a very strong constraint especially for a small cardinality label space (like 10, in the case of this paper).
Otherwise, some poorly performed models could lead to near-random or adversarial inter-model discrepancies, failing the proposed approach.
Another potential issue is that the proposed approach cannot handle training set bias. If all models are biased in similar ways (e.g., toward a particular class or domain), they will not reveal informative discrepancies for the images over which they all make similar mistakes.
Hence, while I agree with the authors that existing approaches to comparing deep neural network classifiers could be improved, I think the proposed solution is not a good alternative yet.
Comparison to "SGD BN removed" is not fair because the initialization is different (application of BN re-initializes weight scales and biases).
- It seems that the model is not very scalable; while the authors do provide a way of reducing the necessary parameters that the hypernetwork has to predict, minibatching can still be an issue as it is implied that you draw a separate random weight matrix for each datapoint due to the input specific distribution (as shown at Algorithm 1).
Is this how you implemented minibatching in practice? How easily is this applied to convolutional architectures?
When you say that full-matrix computation "requires taking the inverse square root", I assume you know that is not really correct?
As a matter of good implementation, one never takes the inverse of anything.
There are several issues convolved here: one is ``full-matrix,'' another is that this is really a low-rank approximation to a matrix and so not full matrix, another is that this may or may not be implementable on GPUs.
There is a great deal of discussion about full-matrix preconditioning, but there is no full matrix here.
It is difficult to know what the theory says about the empirical results, given the tweaks discussed in Sec 2.2, and so it is difficult to know what is the benefit of the method versus the tweaks.
You say that you "informally state the main theorem."  The level of formality/informality makes it hard to know what is really being said.  You should remove it if it is not worth stating precisely, or state it precisely.  (It's fair to modularize the proof, but as it is it's hard to know what it's saying, except that your method comes with some guarantee that isn't stated.)
How would the given graph network compare to this?
Does the controller also start from scratch? If so, why? Also, it is not clear what is the meaning of generations if the graph is fixed, can't it be learned altogether at once?
Could it be that what we are seeing is the attack being denoised?
I am puzzled and looking forward to answers to the above questions. I don't yet understand what is the thing that makes this approach appear to work, or why you were able to drop the Bayes inference inversion altogether as done by Schott.
Does such approximation guarantee the policy improvement?
(6) If the authors jointly and simultaneously optimize \theta and \phi, why a regularization term about q_{\phi}(z)  is missing in Eq 12 while a regularization term about \pi_{\theta|z} does appear in Eq 12?
There might be other constructions that are more efficient and less restrictive.
I don’t follow this argument: this is just part of the classifier. White box attacks are by definition performed with the knowledge of the model, what
I appreciate the idea of testing full white-box adversarial attacks here. But I don’t understand how it is possible that DDGC is more robust, with higher adversarial test accuracy, than in Table 3.
This aspect of the task seems somewhat contrived, and it makes me wonder whether the striking failure of the non-modulated RNNs depends on this detail.
The major concern I have is that the ensemble of MC-Dropout models is not an approximation of the posterior anymore.
Each MC-Dropout model is an approximation of the posterior, but the ensemble of them may not.
Therefore, it is a little misleading to still call it Bayesian active learning.
Also, the ensemble of MC-Dropout models does not have the theoretic support from the Bayesian perspective.
The motivation for the proposed method is to solve the mode collapse problem of MC-Dropout, but using ensemble loses the Bayesian support benefit of MC-Dropout.
Since this paper select the top-k images in D, if k is large the annotating for S will be very tedious, however if k is relatively small the method seems very sensitive to selected examples, which will make the comparison not totally convincing.
My major concern is that the authors assume the hyperparameters to be independent (section 3.2), which is not necessarily true.
a. In Eq. (3), it surprises me to see the symbol \epsilon without any explanation.
b. In Eq. (6), it also surprises me to see no description of \phi and \psi.
- why do you need a conditional GAN discriminator, if you already model similarity by L1?
Instead here you seem to suggest to use L1 and GAN to do basically the same thing, or with significant overlap anyways.
Please explain the logic for this architectural choice.
However, when the weights are set to zero the weight matrix became sparser but still requires the whole weight matrix to be used by the computing resources, as removing some of the weights based on the sorting will not remove a node, only removes some of the connection with that node.
Therefore, it is not clear how the proposed framework is helping the model compression techniques.
Thus, based on this approach, the reconstruction image may not be optimal with respect to the resolution which might be critical for cryo-EM reconstruction.
Even without the "train to convergence" question above, I don't think the authors have demonstrated that their claims on the properties of their algorithms/formulations are generally true.
However, it does exclude some popular activation families, such as the polynomial activation, which were proven effective in multiple areas.
- It is not clear whether pushing the catastrophic forgetting problem into the generator is the best approach.
As such, I find it surprising that simply storing instances would do as poorly as stated in this paper which says cannot provide enough diversity.
It also seems strange to say that storing instances "violates the strictly incremental setup" while generative models do not.
1. The approach is not well justified either by theory or practice.
Additionally, the Wave-U-Net appears to reach the same accuracy as the Harmonic Convolution with many fewer iterations (while also continuing to get much higher PSNRs).
A significant concern/confusion for me is that this doesn't seem to be a mixed membership model, and so I don't know how meaningful it is to generate a level distribution from a Dirichlet and then draw from that mixture one time.
Moreover, the equation J_DIM seems to be wrong since it contains g_\omega twice while I think (but maybe I am wrong) that it has also to be defined by g_\psi.
One other limitation seems to be that Theorem 1 requires using a step size which seems to be much smaller than what one may hope to use in practice. Can you comment on this?
It is a minor weak point that the algorithm can work only when the abstract state is obtained by the RAM state.
In some RL tasks, it is not allowed to access the RAM state.
The algorithm assumptions are strong.
The authors state that proposed framework can be added to any baseline model, but miss to clearly mention the limitations.
The sentence reads "... and helps develop intuitions about behaviors observed in more general settings." Given the restrictive nature of your set-up I find it very hard to believe that this extends to more general settings.
PS: After discussion, I think the motivation of the method is not clear to understand why the proposed method works.
- While the authors suggest that a latent model over the input distribution needs to be trained only once and is applicable off-the-shelf for any further contrastive explanations regarding any network operating on the same dataset -- learning such a model of the input space is an overhead in itself.
- Typographical Errors: Section 3.1 repeats the use of D for a discriminator as well as the input distribution.
Procedure 1 and Procedure 2 share the same titles -- which is slightly misleading.
Since, there is no clear metric to evaluate contrastive explanations -- human studies to judge the class-discriminativeness (or trust) of the proposed approach would have made the paper stronger.
It is not well explained in the paper how this proxy correlates with the Learning progress criteria.
- The way the authors adapt the modulation z (or at least its description in the paper) seems not technically sounded for me.
It is not clear how those observations can affect practical algorithms and this is something I hope the author can address.
However, in this way, when more and more tasks come, the generator will become larger and larger.
The proposed method is also heuristic and lacks promising guarantee.
First, the mechanism provided has no mathematical justification--it seems fairly arbitrary.
I have a substantial concern that this method might end up assigning a high likelihood of resampling trajectories where something unusual happened, not because of the agent's actions, but because of the world having made a very unusual stochastic transition.
However, the assumption is too strong.
third paragraph: Your claim of invariance to time rescaling is technically correct, but I am not convinced that a model can learn the correct omega values for an arbitrary rescaling (e.g. if the period is smaller than the time unit).
(5) The authors override the CW distance: first in Theorem 3.1 they define it as a distance between two finite point clouds, and later in Theorem 3.2 they redefine it as a distance between a point cloud and the Gaussian distribution.
The loss term is motivated by the idea that we want the output distribution to retain some information about the context, but why should that be the case?
However, the proposed technique does not seem to be handling the problem foundationally well.
-The proposed technique seems to include very heavy feature engineering and several ad-hoc practical steps--that is far from the motivation of using NN in tabular data.
- Sim(delta) should be Dissim(delta) which measures the dissimilarity between channels.
The paper positions itself generally as dealing with arbitrary transformations T, but really is
It is not clear to me if NF would improve stability/performances in general games.
- Page 3, Eq. 5: as D and G are functions of time here, eq 5 should maybe be written in more detail to include t
However, as far as I can tell, there is no difference between applying such a hierarchy to the MARL case and to the single agent problem.
I also understand that HRL can reduce the number of parameters, but I don't see how structured exploration reduces the number of parameters.
And lastly, I also can see how parameter sharing can simplify the learning, but I still don't see how it would allow agents to estimate the behaviour change of other agents easier.
My concern here is that beta might be affecting the result more than the proposed training algorithm.
How do performance and model size trade off?
How were the number of layers and kernels chosen?
Was the 5x10x20x10 topology used for MNIST the only topology tried?
It seems reasonable to suspect that the phenomenon may be due to a common cause in all three model types.
Negative aspects: One major concern I have with the paper is the notion of privacy considered.
The notion of privacy considered in the paper makes two assumptions which I am not comfortable with: i) The protection that the notion assures is against reconstruction attacks.
I do not see the GAN style approach taken by the paper, ensures this.
The second issue I would like to raise is related to discussions around the non-universality of the vanilla PointNet model.
- I think the comparison between prior lifetimes and humans mastering a language doesn’t hold up and is distracting
The bijection needs to map training images to a high-density region of the Gaussian, and that aspect would make the model think twice before making the volume term too large.
Section 5 is based on a 2nd order expansion on the $log p(x)$ given by a deep network -- I shouldn't be the judge of this, but from a realistic perspective this does not mean much.
This seems to call the validity of the anomalous into question. Curious what the authors have to say about this.
Minimization of Eq. 3 and Eq. 4 contradict each other and the objective function does not converge obviously.
Without such a guarantee, the proposed method is not very useful because we
Algorithm 1, step 20, the covariance matrix is updated after the RL step so regardless of how the RL policies are generated, the search distribution on the next distribution includes them.
In both this work, and Khadka & Tumer, the RL updates lead to policies that differ from the search distribution (indeed that is the point), and there is no guarantee in this work that the TD3 updates result in policies close to the starting point.
1. Why there is still a need to combine adaptive convolutions with regular convolutions? What would the model performance be for a model with only adaptive kernels?
1) The paper claims to want to predict unordered sets, yet the model is clearly indicating a dependence in the order of the outputs and the input p_m(\pi | x_i, w) (1); this feels like a very odd choice to me.
2) The paper still makes very odd choices even if one ignores the above and wants to model some orderings as more likely than others.
3) Supposing even still that one does want a mixture model with as many components as permutations, there are still some issues.
It is very unclear how the dependence on \pi drops out when getting a MAP estimate of outputs in section 3.3.
Also, the paper claims that it will use a super script m to denote a known cardinality, yet omits \mathcal{Y}_i^{m_i} in the training set of the first sentence in 3.1.
It is not clear why the authors neglect these classical concepts, and are talking about 'random functional perturbations', ... It is also unclear where the optimized transformation (T) lives; the authors are trying to differentiate over some function space which is undefined.
(3) I am slightly concerned about the sample complexity of keeping track of the probability of worker i finishing goal g within t steps with a bonus b. This scales linearly in parameters which usually would be large (such as the number of time-steps).
However, although the proposed method achieves good performance over various (smaller) benchmarks, the method seems ad-hoc and complicated.
1. The objective of the update equation of CTAugment’s learned weights seems contradicted with the purpose of how data augmentation is used in the consistency-based SSL method.
It is not clear how well these methods scale, and for example using k=8 adds computation which may hinder training scalability.
The major issue I have with this paper is Theorem 3.1 on the ergodic convergence rate of the proposed PowerSGD.
At the first glance, it is $O(\frac{1}{T})$ which is faster than the conventional SGD convergence rate $O(\frac{1}{\sqrt{T}})$.  But after a closer look, this rate is obtained by a very strong assumption on the batch size $B_{t}=T$.
I consider this assumption unrealistic.
In this case, it is basically a GD, not SGD any more.
It's unclear how this would scale to more complex tasks with higher-dimensional state spaces such as Atari, Starcraft II or if this would work with tasks with continuous state and action spaces such as mujoco.
4. The biggest flaw that I see in this method is the practicality of it's use.
If you are able to obtain or code your own agent, then you've already solved the task and there is no need to do any sort of imitation learning in the first place.
Thus, the question is 1) how well will this method work with a human acting as the demonstrator? and 2) how can this method work if you are not able to have access to a demonstrator long periods of time (or even at all)?
To me these two reasoning statements are not particularly convincing. One could also say:
While I agree with this to some extent, I also think this argument may not be fully right. When the probing agent is testing the expert, it is essentially showing the imitator many different configurations of the environment.
It may not be that it changes in the first time step (for obvious reasons), but it is essentially showing it many configurations of the expert.
The Dual FW algorithm does not need to be used along with the hinge loss (SVM loss).
This should be explained in detail since Line 15 is the main output of the algorithm, which is fed (Line 16) to an existing solver for completing the assignment.
- The metrics are tightly coupled to the encoder used, and no exploration of encoder architectures is performed.
Page 4: Other Connections with Lower bounds: The first line " "we may also consider ... ". This line is vague. How will you ensure the amount of deformation is such that the set \bar{U} is contained in U ?
It needs to be ascertained when equality holds in the bounds you propose, so that we know how tight they are. What if the bounds are too loose to be practical?
This implies that the amount of work (in FLOPs) done by the algorithm (at least the version being analyzed) is quadratic in T, which makes the convergence rates a bit misleading.
But a very similar assumption is hidden in the bounded-gradient-variance assumption Assumption 3.2; for example, Assumption 3.2 is clearly not satisfied by the least-squares regression problem
The empirical comparisons to UNIT are reasonably thorough, though I would have preferred more in-depth evaluation of the MoVE model as well.
However, the more pressing issue is that evaluation is done either sample-wise within-domain (reconstruction), or distribution-wise across domains (transfer).
Do people leave the negations in when modifying such examples for entailment or neutrality, thus breaking the simple correspondence?
It would be easy to use ELMo here, if the main question is about Transformers vs recurrent models.
I believe the proposed techniques have some flaws which hurt the eventual method.
My concern is that the flaws in the method do not make it conducive to use as is.
- (W1) Adversely affected by rotations: One of my big concerns with the work is the way the CFS is computed.
This again greatly concerned me as I am not certain how stable these metrics are.
Based on my understanding the authors rank (which itself is questionable) the different tasks in order of potential for transfer and then call this the "gold" set. How is precision and NDCG calculated from this?
The proposed  CLF weight difference method has some concerning aspects as well.
Likewise looking at a difference of weight vectors seems arbitrary in other ways as well.
Thus the metric is not a proper metric.
As for GAN, due to the inexact update, it is not really solving the min-max problem.
This discrepancy exists for GANs, but it is a bigger issue for the duality gap interpretation.
The paper tries to control for it, however that description is rather short and vague (not clear how the proposed reward is computed without there being a grid, or how is the grid useful without the intrinsic reward).
Mapping techniques from "non-directional" problems (where the action space is not a direction) and then projeting on the sphere is sub-optimal (the variance is too big).
The question is, why one would exlude the mixture-of-softmax approach here?
It remains somewhat unclear, why this bigram-centered regularization would strongly contribute for prediction in general.
It remains totally unclear, how the presented approaches would perform on more realistically sized tasks and within actual applications.
A more general function is
Then, can the function family the authors used in the paper approximate this function?
- what is the benefit brought by the 'Structural Knowledge' transfer? is this quantified anywhere? based on the description, I don't understand if this is an add-on to TabNN or whether it is incorporated in TabNN.
I don't really understand how learning lambda would _guarantee_ that the optimization will converge to a better solution.
-In k-means clustering (E-step and M-step), is it correct to multiply \tilde x to (c-v)?
I think that the error arising from quantizing v into c is only affected by a subset of rows of \tilde x. For example, if v is the first subvector of w_j, then I think that only 1-st, m+1-th, 2m+1-th, … rows of \tilde x affect to the error.
Given its limitation to small and low-dimensional environments, it cannot be said how well the approach will scale with respect to these factors and the resulting, more complex agent behaviours.
Overall, while the agent behaviour modelling focused on a type of inner state (based on past trajectories) provides benefits in the evaluated examples, it is unsure how well the approach scales to more complex domains based on strong similarity and simplicity of the tested toy scenarios (evaluation on sorting problems is an interesting step towards to address this shortcoming).
* the ablation study designed to compare with a Gauss-Newton-like approach does not seem correct.
- for GN optimization, lambda should be set to 0 - not a constant value.
- the image features should be re-trained for the GN framework:  Since the features are learned for the LM iteration, they are adapted to the use of the predicted lambda, but they are not necessarily suitable to GN optimization.
Thus, the advantage of using a LM optimization scheme is not very convincing.
- the state vector Chi is not defined for the proposed method, only for the standard bundle adjustment approach. If I understand correctly is made of the camera poses.
- the name 'Bundle Adjustment' is actually not adapted to the proposed method.
Eq (2) cannot have Delta Chi on the two sides.
before Eq (3): the 'photometric ..' -> a 'photometric ..'
But there are some minus ones in the random projection?
However in the approach proposed here, the negative examples are missing.
So there is no guarantee this algorithm will minimise the overall regret.
- It does not seem necessary to predict cumulative mixture policies (ASN network).
Since you assume the number of information nodes is large, you cannot minimize the l2 loss over all states. Do you assume you generate states by following some policy? Which policy?
* The fact that the model makes use of pitch class and octave labels also raises questions about applicability -- if I understood correctly, transfer can only be done when this information is present.
I think the main point of transfer over a regular generative model that goes from labels to audio is precisely that it can be done without label information.
* The use of fully connected layers also implies that it requires fixed length input, so windowing and stitching are necessary for it to be applied to recordings of arbitrary length. Why not train a convolutional model instead?
* I think the choice of a 3-dimensional latent space is poorly justified. Why not use more dimensions and project them down to 3 for visualisation and interpetation purposes with e.g. PCA or t-SNE?
* I found the description in Section 3.1 a bit confusing as it initially seems that the approach requires paired data (e.g. "matching samples").
The only problem I see is that phrase similarity part is not convincing.
It is not clear that the notion of similarity through classifier weights makes sense, but see below for clarification questions.
Although they show that the Ski Rental problem can also be learned to solve though it is trivial and does not even use the framework the authors propose in its full extent, ie. problem instances are not generated by use of a machine learning model, which is one of main claims the authors are making.
The authors claim that the Arora's RAND-WALK model does not capture any syntactic information.
That is, the current design hasn't well considered the local and global stability relation, but just simply sum them up.
I think it is misleading to call it a convolution, as (a) it is not a convolution mathematically, and (b) fast convolution techniques (Fourier, Winograd) cannot be applied, so claims to greater efficiency may be misleading.
In (2) is (N+1)x(N+1) the kernel size (sums are over 0,1,...,N?)??
Is the output of the first convolution a single HxW feature planes, or a HxWx(N+1)x(N+1) tensor?
x is already present within the indicator, no need to add yet
However, hyperparameter searching is not extending any ML technique, it is just an approach to find a good training configuration and show robustness in different hyperparameters settings.
It is thus unclear if the approach is robust against different hyperparameter settings.
- I think the authors want to make an explicit connection to counterfactuals as understood in the causality community. Then they shy away from it saying they are inspired by it. May be a formal exposition in the supplement about counterfactuals and generating mechanisms could help readers from other communities (NLP) even it means repeating standard/synthetic examples. Its good to say what exactly in a counterfactual generation process, the "people" in amazon turk were substituting.
1. I found the argument about FID in section 2.1 unconvincing. Are there proofs or citations for the claim that real images don't follow multivariate gaussian distribution after applying FID? Copying is indeed an issue that FID cannot detect, but it may be tangential to model collapse for real world concerns like privacy.
2. The statement "IS, FID and MODE score takes both visual fidelity and diversity into account." under "Evaluation of Mode Collapse" is contradictory to the description in sec 2.1 that IS in fact does not measure diversity.
