However, it seems the only Figure showing D’s loss when unconstrained is Figure 26, in which it is hard to notice any significant jump in the loss.
- In Appendix F, Figure 20 d), the title seems wrong. It seems to report sigma^2 values, but the title says “losses”.
A quick glance at Table 1 suggests that the bounds obtained through Theorem 3 are exponential in t and are mostly vacuous.
Furthermore, Figure 6 and Figure 7 in general show SAVP performing worse than SVG (Denton & Fergus 2018), a VAE model with a significantly less complex generator, including for the metric (VGG cosine similarity) that the authors introduce arguing that PSNR and SSIM do not necessarily indicate prediction quality.
- In the outputs shown in Table 3, the questions generated by the scratchpad encoder often seem to be too general compared to the gold standard, or incorrect.
However, the issue still exists in Dreamer, since there seems to be an upper limit of effective horizon length (perhaps around 40, according to Figure 4).
a) The uncertainties produced by CDN in Figure 2 seems strange.
- p8par1: "approximate embedding $\alpha(e(\gamma'(...)))$ - $e$ is undefined and should probably be $e'$ (this is also the case in the caption of Fig. 5), and $c'$ should probably be included as well.
The samples from MNIST in Figure 3 are indeed very blurry, supporting this.
It did also not match any numbers in Tab. 4 of the appendix.
For Figure 2, Did you try applying canonicalizations in different orders? Do they give the same results?
Table 8 rises some concerns.
Also, since the synthetic image pairs are not multimodal in nature, it is unclear to me for what the messages are conveyed in Figure 3 and 4.
(b) a significant clarification of Figure 4.
When I look at Figure 4abcd, it appears that the Convolution and Dilated Convolutions fit a clean signal faster (it is just not as clean.
Perhaps I am misreading this plot, but it is not obvious to me that this plot supports the claims the authors are making.
Also, Figure 6 is referenced in the text in the context of binary multiplication ("[...] is able to outperform a multiplier created just by training, as can be seen in Figure 6"), but presents results for addition and factorization only.
- In Table 1, for ImageNet, Shadow Attach does not always generate adversarial examples that have on average larger certified radii than the natural parallel, at least for sigma=0.5 and 1.0. Could the authors explain the reason?
- In Table 2, it is not clear to me what is the point for comparing errors of the natural images (which measures the misclassification rate of a natural image) and that of the adversarial images (which measures successful attacks rate), and why this comparison helps to support the claim that the attack results in a stronger certificates.
- Figure 6(a) clearly suggests that the data mean for SVHN and CIFAR are very different, instead of similar.
The transfer metrics (MMD and kNN) are opaque to the reader: for instance, in table 1, is a knn score of 43173 qualitatively different than 43180?
- Why are there missing BLEU scores and the number of parameters in Table 1?
Table 1 tries to provide some comparisons on Atari, however number of samples is different for different methods making the comparisons invalid.
caption of Fig 1: extractS
typo in absolute in caption of Fig 4
* The differences between UNIT (GAN; C-po) and UNIT (MMD; C-po) in Table 1 seem very small and I'm not convinced that they are significant. Why does the MMD version constitute an improvement? Or is it simply more stable to train?
* The descriptor distributions in Figure 3 don't look like an "almost exact match" to me (as claimed in the text).
In Table 2 I saw some optimizers end up with much lower test accuracy.
-	In Figure 2.b I’m surprised by the difference obtained in the feature maps for images which seems very similar (only the lighting seems to be different). Is it three consecutive frames?
-  Is the romantic/ horror flips and their absence the only spurious thing in Figure 4 ?
