- The hyperparameter selection regime (and the experiments used to find them) is not described
2. during sampling, either training or testing, how do authors handle temporal overlap or make it overlap?
3) The paper only conducts comparison experiments with fixed-alpha baselines.
Without an empirical example in a realistic setting, it is hard to judge whether the benefits of introducing an ME bias for better classification of new inputs belonging to new classes can outweigh the negatives via a potential increase in misclassification of those belonging to old classes.
4. Can the authors show concrete examples on how the attacks are generated?
- for the experiment with Imagenet images, it is not very clear how many pictures are used. Is this number 2500?
However it's not clear how is this range used in practice ? Do you sample uniformly $\alpha$ in this range to train the linear interpolation ?
3. Shouldn't random indexing produce non-uniform numbers of non-zero entries depending on alpha? Why did you have an exact number of non-zero entries, s, in the experiments?
It would be nice if the authors pointed to a git repository with their code an experiments.
2)	It is not clear what the “replicates” refer to in the experiments.
Given this, I wonder why all experiments are conducted on sets of two to five images, even for Kitti where standard evaluation protocols would demand predicting entire sequences.
I think all claims about running time should be corroborated by controlled experiments.
