Therefore, it may be a good idea for the authors to analyze the correlation between FSM changes and accuracy changes.
- no qualitative analysis on how modulation is actually use by the systems.
-2 Diversity of additional "agents" not analyzed (more below).
- Missed opportunity of better analysis of which theorem/rewrite rule properties are more likely to fail
Presumably, whether the benefit scales well will depend on the loss function - in any case, this warrants some analysis/discussion especially given that there are no experiments to test it.
- One thing I am confused about is the residual model, which seems quite important for the pipeline but I cannot find details describing it and much analysis on this component.
4. Mean field analysis, although it lends an insight into the statistics of the activations, needs to connected with empirical observations.
For the modeling contribution, although it shows some improvements on the benchmarks and some nice analysis, the paper really doesn’t explain well the intuition of this “write” operation/Scratchpad (also the improvement of Scratchpad vs coverage is relatively limited). Is this something tailored to question generation? Why does it expect to improve on the question generation or it can improve any tasks which build on top of seq2seq+att framework (e.g., machine translation, summarization -- if some results can be shown on the most competitive benchmarks, that would be much more convincing)?
\alpha^i_t and u^i are also pretty complex and it would be good to conduct some ablation analysis.
I would have liked to see a bit more analysis as to why some pre-training strategies work over others.
- It would be nice if more network architectures were analysed (such as VGG and DenseNets).
- It would be nice if different stopping criteria were analysed.
Moreover, said the analysis is in my opinion not sufficiently rigorous, with hand-wavy arguments, no formal proof and unclear terms (e.g. how do you define near-optimal?)
In that regard, I would like the authors to comment on the worst-case computational complexity of the numerical analysis for determining the volume of a preimage through multiple layers.
While this assumption seems plausible,  no analysis has been done to verify it in a systematic way.
This may also help to understand some of the limitations of this analysis.
And the analysis of the "dynamic range" of the algorithim is missing.
- The premises of the analyses are not very convincing, limiting the significance of the paper.
2. The authors should provide ablation study and analysis of their CTAugment.
4. The authors hypothesize that “stronger augmentation can result in disparate predictions, so their average may not be a meaningful target.” However, they do not show any analysis to support this hypothesis.
-- The analysis is relatively straightforward and boils down to bucketing the error and integration over the buckets.
-- The machine learned advice is assumed to be flawless at identifying the Heavy Hitters, authors might want to consider incorporating errors in the analysis.
2. It is hard to understand what the model has learned compared to hand-crafted schedule. Are there any analysis other than the results alone?
If that's the goal, however, a more detailed error analysis would need to be included.
(This seems like something that could easily be done in the empirical analysis as well and would provide richer empirical signals as well)
Recommendations for the authors: Would it be possible to provide an analysis of the cases when TabNN is expected to outperform GBDT by a sizable margin? Or, if not, are there other reasons why using a neural network would make more sense than just simply running GBDT?
- The analysis has focused on a very simple case of having a linear discriminator which for example in WGAN, forces the first moments to match. How does the analysis extend to more realistic cases?
I feel that the idea deserves a broader analysis beyond just a single choice of disentanglement.
- Lack of theoretical analysis. It could have been nice if the authors could show the observed phenomenon analytically on some simple distribution.
