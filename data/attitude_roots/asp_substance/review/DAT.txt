which is much smaller than the number of time series usually involved say in gene regulatory network data
2.	Data size is too small, and the baselines
Did the authors also examine the relationship between mutual information and generalization error for CIFAR data sets? Does it not make sense to examine this for all (most of) the setups considered in Table 4, 8, and 9.
Due to the different objects used in the different datasets, some of the experiments have a smaller set of words.
It will be also interesting to see how does the proposed method perform on large-scale datasets such as DomainNet and Office-Home dataset:
Weaknesses: The dataset created here is entirely synthetic, and the paper only includes one single small real-world case; it seems like it would be easy to generate a larger and more varied real world dataset as well (possibly from the large literature of extant solved problems in workbooks).
The descriptions of the datasets used are not clear, e.g., the number of classes for each data.
First, the labeled data portion is fixed and is relatively high
For example, the number of labeled examples in Table 1 is fairly large and inconsistent (4K, 1K, 10K for the 3 organic datasets).
BSD 500 only contains 500 images, and it would be good if more diverse set of images are considered.
- Some of the datasets the authors currently test on are quite toy, especially for the image-based MNIST and SVHN datasets.
Is it better to decay learning rates for toy data sets?
It is difficult to judge the performance of the proposed model based on so small data set.
The first point would be: what's the meaning of synthetically generating training curves other than proving that transformer achieves good performance in modeling discrete distribution? Most practical problems would not have the same distribution as the previously gathered public dataset, thus the data is not representative, and synthetic training curves just does not make sence.
* There is still no comparison with competing nonparametric tests on the fMRI data.
- How much does the image matter for the single-image data set?
The author can perhaps consider the datasets used by Tsai et al. There are seven datasets, and they can all be modified to the setting of partially-observable multimodal data.
Apart from the assumption H1 of linear separability of the data (which I don't mind), the results require very strong assumptions, in particular hypothesis H2 stating "at the beginning of training data points from different classes do not activate the same neurons".
It would have been very interesting to study the quality of interpolations on more models and datasets, and compare their generalization capabilities as well as the bias present in the different datasets.
Third, the datasets used in this paper are rather limited.
What is the minimum/maximum size of the data set? Do we really need a large data set or just a subset that covers the data distribution?
- Fitting 6 parameters to 42-49 data points raises concerns about overfitting.
Consider doing cross validation over those 42-49 data points, and report the mean of deviations computed on the test folds.
My only issue here is that very little information was given about the size of the training sets. Did they use all the samples?
However, in the proposed domain of tabular data, often data sets are significantly more high dimensional in reality and include at least one set of sparse large dimensional features
However, these were done using 1 dataset and also a simple feed-forward network (rather than LSTM).
Thus, it might be helpful to test the result on another dataset (e.g. WikiText).
Comparing to this approach, I wonder if it would be better to provide samples that exhibit a particular factor, or samples that conceal the factor?
However image captioning datasets are not mentioned.
It would make sense to use image captioning data to create the image lookup.
Given the title of the paper, it would have been nice if this paper explored more than just MNIST vs NotMNIST and SVHN vs CIFAR10, so that the readers can gain a better feel for when generative models will be able to detect outliers.
The second order analysis is good but it seems to come down to just a measure of the empirical variances of the datasets.
- Lack of an extensive exploration of datasets
- In particular, Section 4 is a series of empirical analyses, based on one dataset pair.
According to the conclusions of the paper, such dataset pairs should be easy to find -- just find a dataset that "lies within" another. Did you try e.g. CIFAR-100 train and CIFAR-10 test?
7. The evaluation on two datasets seems to be rather limited, additional comparisons should be included.
First, larger data sets such as Wikitext-2 and Wikitext-103, and/or the billion-word benchmark, are needed to understand how well the approach works in practical LM settings.
* There was some analysis of the augmented IMDB dataset, but none of the SNLI dataset.
That's a very simple kind of question; more generally, I'd like to see more analysis of the new dataset.
Q1: Has the authors tried more complicated datasets such as CIFAR-10 to evaluate the pGAN method? It would make the paper more convincing to add results on more complex datasets.
o use of the Penn Treebank dataset only;
- it is not really clear to be what data the graphs show: the boxplots show 5% of what data? does it also include the models obtained by gaussian process regression? and what about the line plots, is it the best model so far as you train more and more models? if so, how are those models chosen and ordered? are they the results of single models or average of multiple ones?
However, my concern is that the paper focuses only on a very specific application domain,  and an improvement over the niche dataset with much more supervision (from the extension) is not surprising at all. In the mean time, the notion of "Neural Stethoscopes" could be much more  generally applied. Without applications in other domains, it is not immediately clear what the paper's implication is.
1. If we do not have access to the model parameter, the training data, or the artifacts during training like the discriminator, what are some of the real world situations that fit this description? In those cases, is it too much to assume that we can control the random seed input to G?
2. Is it reasonable to assume some constraints on how much data we can get from the blackbox generator?
