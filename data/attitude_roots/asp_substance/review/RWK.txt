2. Why were more baselines from the related work not included? I understand the experiments are a proof of concept, but it would be nice to get a feeling for what some of the other methods do.
- For the automatic evaluation measures there should be multiple references per SPARQL query since this is how BLEU et al are supposed to be used.
The selected baselines are not sufficient.
The improvement from the baselines is also limited.
In addition, it could be worthwhile to compare and benchmark on existing evaluations: https://arxiv.org/pdf/1802.06806.pdf
Better literature review to reflect the relevant previous video action recognitions, especially those on video compositional models.
little improvements over the baselines or even significantly worse. More importantly,
However, as presented these ideas are poorly justified and careful comparisons against sensible baselines are missing.
Moreover, comparison with some of the DA baselines (ADDA[1], DSN[2]) is missing.
- Pioneering work is not necessarily equivalent to "using all the GPUs"
I feel the baseline in domain adaptation area is a bit limited.
The connection between mutual information and generalization has been studied in several contexts [see, e.g., the references in this paper and https://arxiv.org/abs/1511.05219 https://arxiv.org/abs/1705.07809 https://arxiv.org/abs/1712.07196 https://arxiv.org/pdf/1605.02277.pdf https://arxiv.org/abs/1710.05233 https://arxiv.org/pdf/1706.00820.pdf ] and further exploration is desirable.
A naive approach to estimate coefficient with single-source domain discrepancy measures such as [1]Mansour (2009), [2,3] Ben-David(2007, 2010), [4] Kuroki et al (2019), and W1-distance is not considered.
Questions: I would like to see more discussion about difference between this work and [Z Hu, arXiv:1905.13728].
In the absence of citation or comparison with any of the prior work on multivariate statistical dependence testing, the current submission is not suitable for publication.
For example, [1] could be used to reduce the variance of gradient w.r.t. \phi.
A missing empirical analysis is on class-conditional noise (see for example Patrini et al. 17 for a definition).
I highly encourage the authors to finetune the ImageNet pre-trained BagNet on PASCAL VOC and compare to the previous patch-based deep networks.
Since positional encoding with Fourier transforms is well known, this seems like the relevant benchmark but it receives only a brief treatment.
I would also expect more ablation studies about how to pick p_{\had d}, which seems to be the key of this approach, in MNIST and CIFAR10.
The submission could maybe improved by segmenting the work into intro / related / background (with clear equations presenting the existing GP) / analysis / approach / experiments
I do apologize if my original comment wasn't clear regarding the contribution part of the paper. What I was trying to say is not that I didn't see the individual contributions of the paper, but instead that the paper does multiple things simultaneously, without comparing against the relevant baselines for any of the individual contributions.
I believe it will not be great, but I think for completeness, you should add such a baseline.
6. On CIFAR10 the results seem to be worse that other methods.
10. Another related paper seems to be Spatial Transformer Networks (Jaderberg et al.).
I'd encourage the authors to extend the intro and position the ideas w.r.t. existing works and extend the evaluation.
For instance, I would like to see the comparison between DeepCS and SmartHash by Tang et al 2017.
- The WGAN-GP baseline is very weak, i.e. does not show any reasonable generated images (Fig. 9).
4. The scores achieved by the baseline are very far from state of the art, making the comparison mostly useless,
There is also a matter of ease of training compared to existing approximations or relaxations, e.g. Gumbel-Sinkhorn.
If one reframes the convergence rate in terms of FLOPs U=T^2 instead of iterations, then the convergence rate drops from 1/T to 1/sqrt(U), which undermines the claim in remark 3.4 that this analysis is superior to that of Yan et al. (2018).
The biased introduced via this truncation has been studied in great depth in [3] and shown to be harmful.
Minor comment: An interesting line of work is that of [3] which could be included in the discussion.
* first paragraph page 2: some references to causality literature and definition of spuriousness as common cause
This would be an effective baseline to compare. (Correct me if I am wrong here.)
- (W3) Baselines for transfer learning: I felt this was another notable oversight.
More details should be provided, and when comparisons to past works or inter-life curiosity are made, this should be controlled for.
- About setup: the paper reports using ResNets for natural images as in Mescheder et al. (2018).
- The baseline scores of Reg-SGAN and Reg-WGAN seem to be worse than those reported in Mescheder et al. (2018), which have inception scores above 6 according to Figure 6 of their paper.
* CodeSLAM (best paper at CVPR'18) is referenced but there is no comparison with it, while a comparison on the EuRoC dataset should be possible.
- Authors do not visualize the attention (as is common in previous work involving attention in e.g., NLP).
