The results and discussions in the main part of the paper are too light in my opinion; the average model accuracy across modules is not an interesting metric at all, although it does show that the Transformer performs better than recurrent networks.
Further, it seems that the results in Table 4 might be a bit obscured by the size of the downstream task dataset.
However, given that the achieved performance gains over the state-of-the-art are fairly small, it would be good to assess if the obtained improvements are statistically significant.
[-] The results indicate that SAVP offers a trade-off between the properties of GANs and VAEs, but does not go beyond its individual parts.
Most importantly, I would like to see a measure of variance/uncertainty like confidence intervals included in the results; otherwise it's impossible to assess whether the results are likely to be significant or not. Other questions:
In addition, the results seem very weak.
Perhaps the results would be a little more convincing if additional common word embeddings were also tested.
However, as the authors acknowledge the overall simplicity of the tasks being evaluated with mostly marginal improvements makes the overall evaluation fall short.
But the paper only provides empirical results on sentimental analysis and digit recognition.
* The biggest problem for me was the unconvincing results.
The empirical evaluation is limited in considering only one task (clique finding), and the results seem to be quite sensitive to the chosen optimizer.
- The conclusions focus on the importance of section 3 and
If this solution can be proven to improve a valuable metric (e.g. accuracy, interpretability, theoretical understanding, or computational efficiency) of a setup, it is then worthwhile being published.
Results are much lower in less favorable cases, sometimes close to random (see last line of Table 3).
I believe the results all relate to inference but it would be good to get an overview of the impact of training time as well.
However, I feel like the main result is a bit too predictable, and for acceptance I'd like to see a much more detailed exploration of the questions around systematic generalization.
Results on more scenes will make the performance more convincing.
- it's better to show time v.s. testing accuracy as well.
However, the results are unconvincing: only the results for epsilon = 0.1 are shown, and even so the advantage is marginal.
The authors discuss their results on k-medoids in the appendices, but it seems like these results aren't quite complete yet.
From the perspective of a purely technical contribution, there are fewer exciting results.
Moreover, the performance reported in this paper seems to be much inferior to the state-of-the-art results reported in the literature.
A. You demonstrate the results on CIFAR-10 for 10% error rates which corresponds to networks which are far from what is currently used in deep learning.
Thus, it is hard to say whether the results are applicable in practice.
However, this does not take into account the time already spent on pre-training. Perhaps the authors can include some results as to the total time taken as well as amortized total time over a number of different downstream tasks.
I think this work would have much greater impact if the authors can show that the power-law form holds for a larger variety of architectures and optimizers thus allowing researchers to more confidently incorporate the results of this work into the design and training deep neural networks.
The current tasks and problem sizes are not very convincing, and the accuracy results are not very compelling.
The unsupervised results are more interesting but not very much explored (a single set of sampled faces).
It seems that the sampled reconstruction results (Fig. 8) are not as good as VAE on CIFAR10.
I would have expected to see some analysis and results on the translation quality over systematic noise applied to the input graph.
iv) Finally, the reported results are mostly qualitative.
In this regard, I encourage the authors to update the supplementary material in order to show extended qualitative results of the explanations produced by their method.
Overall: this paper makes a convincing case that it can be used to generate higher quality images, but not that this improves the quality of the disentangled representations.
- No large corpus results.
It also shows itself in the results that final algorithm is almost indistinguishable from GBDT regarding results.
However such problems are entirely missing in the results section.
Second, I notice that compared to the result in Table 1, PGD attack can yield better results [1].
The advantage of this random encoding was to reduce the number of parameters for an embedding layer, but the results showed we gained much PPL from a 25% reduction in embedding size (Table 1).
1) Provide stronger empirical results (these are not too convincing).
More importantly, the results presented are quite meager.
If this is a method for image recognition, it would be better to present results for a more substantial image recognition problem than MNIST and CIFAR-10.
something that is either deterministic, or a probabilistic result with a small
4. How sensitive are the results to the number of adaptive kernels in the layers.
However, the results are not enough to be accepted to ICLR having a very high standard.
- The shown inception scores are far from state-of-the-art.
Unfortunately this paper offers only weak results.
First of all, only a small number of qualitative results are reported and, therefore, it is very difficult to assess the proposed method and draw any conclusion.
- The authors report only negative results for the fully-unsupervised version of UD-GAN The paper lacks and in-depth discussion about why this negative result is interesting.
In particular, the qualitative results are too limited and no quantitative evaluations is provided.
While the motivation is nice, I find the results (especially in the unguided setup) underwhelming.
My major concern is whether the results are significant enough to deserve acceptance.
If these networks can truly solve these problems, authors should report the success rate while varying the threshold, not individual accuracy of the items which can be arbitrarily high by violating constraints.
Say for block size d=4, an index is required for each block, and the resulting compression ratio is at most 4 (correct me if I understand it wrong).
Weaknesses: The main weakness of the paper is that the performance gains are extremely low compared to the next contender; perhaps they are statistically significant (this cannot be determined), but it's unclear why we wouldn't use GBDT.
Unfortunately, the results are quite disappointing in terms of sound quality, and feature many artifacts.
This seems like an unnecessary bottleneck in the model, and could partly explain the relatively poor quality of the results.
I cannot understand from that part whether T(v_a, v_b,.) addition to v_a+v_b gives any improvement or not.
- As shown in the ablation study, the main contribution on the obtained results seems to be the use of stronger transformations than in MixMatch.
- While there seems to be a consistent improvement over TD3, this improvement is in some cases small (e,g. ants).
I would suggest the authors to either introduce the two problems earlier or to simply say that near-optimal results are achieved, without giving detailed results, because it is very hard to understand them without any introduction of the task being achieved.
However, the results are a bit misleading in their reporting of the std error.
