Also, to demonstrate the superiority of the proposed method an appropriate comparison against previous work is needed.
Besides, as the base classifier is different for various baselines, it is hard to compare the methods.
2) Applying the model of Hartford et al’18 to problems where interacting sets are identical is similar to applying convolution layer to a feature vector that is not equivariant to translation.
- Jointly learning an inference network (Q) has certainly been done before, and I am not sure authors provide an elaborate enough explanation of what is the difference with adversarially learned inference /  adversarial feature learning.
Furthermore, no comparisons were provided to any baselines/alternative methods.
- novelty: the additions proposed are small modifications to existing algorithms and there are other methods of attention on graphs which have been discussed in the paper but not directly compared to (e.g. this method builds heavily on Velickovic et al 2017)
Furthermore, the baseline models did not use other almost standard regularisation techniques (weight decay, dropout, batch-norm).
But there are no comparisons between the proposed training method and previous related works.
The paper currently only mentions the most related work for the proposed method,  using the whole section 2 to describe VCL and use section 3 to describe FSM and half of section 5 to describe SSR.
While the section 4 has a discussion on related papers, there's no systematic experimental comparison across these methods.
RNN-based approaches are with better “complexity” comparing to your sum baseline and “Deepset” approach.
- Relatedly, better baselines should be used; for example, if the memory used by the generative model is merely put to storing randomly chosen instances from the tasks, how will the results compare? Clearly storing instances bypasses the forgetting problem completely (as memory size approaches the dataset size it turns into the joint problem) and it's not clear how many instances are really needed per task, especially for these simpler problems.
There is no experiment clearly shows convincing evidence of the correctness of the proposed approach or its utility compared to existing approaches (Xie & Ermon (2019); Kool et al. (2019); PlÂšotz & Roth (2018) ).
Thus, I suggest the authors could show the space and time comparisons with the baseline methods to show effectiveness of the proposed method.
Since this work takes the approach of allowing stale weight updates, the author should also compare with existing distributed training approaches that use asynchronous updates, with or without model parallelism, for example, Dean et al., 2012.
- No comparison has been made between their approach and other previous approaches.
-  The authors should compare to at least some popular previous approaches that use a feature engineering based methodology such as - IntPred
