However, the real-world experiments are not necessarily the easiest to read.
Authors should clarify the justification behind experimenting only on 'first 500 test images'.
It seems there is no conclusion to take away from the experiments in section 5 (convolutions).
The paper is written in a way that makes following it a bit difficult, for example, the experimental setups.
In the experiment there is no details on how you set the hyperparameters of CW and EAD.
It is not clear to me what the point of Sec. 5 is, given a trained model, one wants to figure out if an image was present in the training of the model.
- Also, for MISC-r experiments, the weights between the intrinsic reward bonus and the extrinsic reward are not specified in the paper.
In particular, the example in section 3.1 says that a noisy information bottleneck is introduced, but then says that the modified and unmodified models have "training algorithms that are exactly equivalent." I think this example needs to be clarified.
5. In fact, I don't know why \omega needs to output p. It's never mentioned in the experiment section.
10. Reading the baselines before the experiments is very confusing.
3. This apart, I think that the experiment section is pretty hard to read, given all the metrics and methodology is in the Appendix.
The imagenet experiment lacks details.
In summary, I think this is interesting work, but a clearer explanation of the relationship between HRL and MARL, as well as a clearer main argument, supported by experimental evidence, would greatly improve this paper.
The experimental section includes various mistakes (see under minor) and misses to describe figures, leading to the assumption that additional time is required for a more detailed evaluation of the algorithm (including more domains and in particular baselines).
Cons:  unclear transfer learning model, insufficient experiments.
section 6.3, the authors show an experiments in this case, but only on a dense
1. the difficult to train the network
This paper has problems with clarity/polish and experimental design that are sufficiently severe
5. Clarity: The first half of this paper was easy to follow and clear. The experimental section had a couple of areas that left me confused. In particular:
8. "Beyond fixed schedules, automatically adjusting the training of G and D remains untacked" -- this is not 100% true.
There were some experimental details that were poorly explained but in general the paper was readable.
Also, the sentence, "We demonstrate the implementation and performance of our pipelined backpropagation in PyTorch on 2 GPUs using ResNet, achieving speedups of up to 1.8X over a 1-GPU baseline, with a small drop in inference accuracy.", is confusing. If I use data parallelization, the gain should be also around 2.
2) It would be great if the paper can clearly define the experiments: "waypoint", "oncoming", "mall", and "bottleneck".
This needs more elaboration. Is this way of training results expected? What is the lesson learned?
