A stronger contribution is 1., which however is somewhat incremental compared to similar comparisons made in the past.
3. Technical contribution: While the authors propose the first bounds for LSTMs and MGUs, most of the analysis seems to be a marginal contribution over the work of Bartlett et al. [3]
However, this task is an instance of natural language generation: given a meaning representation (quite often a database record), generate the natural language text correspoding to it. And previous work on this topic has proposed very similar ideas to the scratchpad proposed here in order to keep track of what the neural decoder has already generated, here are two of them:
3) The novelty of this paper is incremental as the theoretical results are extended from Cortes et al (2019) and Zhao et al (2018).
- The paper does not have a significant novel contribution, but rather extends GANs (improved-GAN mostly) with a manifold regularization, which has been explored in many other works Kumar et al. (2017) and Qi et al. (2018).
Compressability is evaluated, but that was already present in the previous work.
For example, the two regimes mentioned in the paper has been identified by a few other works and the contribution of this paper is just to verify them again.
In terms of learning rate scaling, this paper gets similar conclusions as Shallue et al. (2018).
In terms of the difference between vanilla SGD and SGD with momentum, Zhang et al. (2019) already argued that the difference depends on specific batch sizes and SGD with momentum only outperforms SGD in the curvature dominated regime.
However, the novelty is rather limited as similar ideas have been undertaken (e.g., Mescheder et al 2018), but in different contexts.
The testbeds all existed previously and this is mostly the effort of pulling then together.
Beyond the core findings, the other settings are less convincingly supported by seem more like work in progress and this paper is really just a scaling-up of [Pathak, et al., ICML17] without generating any strong results regarding questions around representation, what to do about stochasticity (although the discussion regarding something like ‘curiosity honeypots’ is interesting).
All of the testbeds have been used previously.
Spectrum pooling has been used in the community of computer vision and machine learning.
Taking a random example (there are others by simple searching), in the ECCV paper "DFT-based Transformation Invariant Pooling Layer for Visual Classification, Ryu et al., 2018" The DFT magnitude pooling is almost the same as the authors' propositions, where the "Fourier coefficients are cropped by cutting off high-frequency components".
The work is rather incremental from current state-of-the-art methods.
For example, Narayanaswamy et al. [1] also propose to utilize labels to VAE.
However, there are a few, though not many, works in the literature trying to combine Choquet integral with deep learning.
This paper fails to account for a vast amount of literature on modeling natural images that predates the post-AlexNet deep-learning era.
These regimes are fairly well covered by previous works (e.g. Belkin et al as well as others).
1) The introduction makes it seem the generative replay is new, without citing approaches such as DGR (which are cited in the related work).
I must probably be missing something, and I encourage the authors to clarify what the main novelties are when compared to the several papers by Dupuis & al.
But this is too similar to the cited findings on page 6 (models assign high likelihood to constant images).
-- Since CounMin is closely related to Bloom filters the idea of using machine learning to speed it up appears to be noticeably less novel given that for Bloom filters this has already been done by Mitzenmacher’18.
- The experiments seem very similar to Wu et al. 2018, which is considered to be prior work under the ICLR guidelines.
While this paper is acknowledged in the related work, it would be helpful to expand further on the relationship between these works, so the originality and contribution of this paper can be better evaluated.
It's hard to ascertain what exactly the contributions are, and how they might not be a straightforward consequence of prior work (for example, combining results from Bietti and Mairal; and generalization bounds for linear models).
For instance, using codes and codebooks to compress the weights has already been used in [1,2].
In addition, minimizing reconstruction error has already been used in low-rank approximation[3] and network pruning[4].
It is thus very hard to know if this new approach brings any improvement to previous work.
