sentences
"This work proposes LSTNet, a self-supervised method to establish reliable 3D dense correspondences irrespective of the input point clouds’ rotational orientation."
"Specifically, LSTNet learns to formulate SO(3)-invariant local shape transform for each point in a dynamic, input-dependent manner."
"Each point-wise local shape transform can map the SO(3)-equivariant global shape descriptor of the input shape to a local shape descriptor, which is passed to the decoder to reconstruct the shape and pose of the input point cloud."
"The proposed self-supervised training pipeline encourages semantically corresponding points from different shape instances to be mapped to similar local shape descriptors, enabling LSTNet to establish dense point-wise correspondences via nearest point pairs between cross-reconstructed point clouds."
The self- and cross-reconstruction training strategy is simple yet effective.
LSTNet demonstrates state-of-the-art performance on 3D semantic matching when evaluated on the KeypointNet dataset and part segmentation label transfer when evaluated on the ShapeNet dataset.
"The performance of aligned shape pairs under the setting of I/I shows that other methods, such as CPAE, are much better than LSTNet."
The reason why other methods are much better than LSTNet under the setting of I/I should be clarified.
Lack of limitations.
"6: marginally above the acceptance threshold 2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work."
Math/other details were not carefully checked.1)
This paper proposes a self-supervised method to find semantically corresponding points for a point cloud pair;）The main idea is to decouple a point cloud feature learning process into a SO(3)-equivariant global shape descriptor and dynamic SO(3)-invariant point-wise local shape transforms;Experiments on the KeypointNet dataset show the effectiveness of the proposed method.
"This paper is generally well-written;The idea of factorizing point cloud descriptors into SO(3)-equivariant global shape descriptor and dynamic SO(3)-invariant
point-wise local shape transforms seems to be novel;Experimental results are good."
"The main weakness of this paper could be all experiments are performed on synthetic datasets, with simple point cloud."
It's good for authors' to show some examples/experiments on real-world datasets.
"For example, the 3Dmatch dataset."
"Since the proposed method can estimate dense correspondences, I wonder whether the proposed method can be used to estimate the relative rotation/translation for a point cloud pair."
"For example, the estimated dense correspondences can be fed to an ICP method to estimate the relative rotation/translation."
"The running time and GPU memory cost is blurry for me;Please compare the proposed method with more recent papers, e.g., [SC3K: Self-supervised and Coherent 3D Keypoints Estimation
from Rotated, Noisy, and Decimated Point Cloud Data]."
Please refer to the weaknesses.
"5: marginally below the acceptance threshold 4: You are confident in your assessment, but not absolutely certain."
"It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
"This paper introduces LSTNet, which leverages an SO(3)-equivariant encoder-decoder architecture(Vector Neuron Networks, VNNs) and proposes a novel function called local shape transform to further transform the learned features."
The proposed method is validated on both the 3D keypoint transfer and part segmentation label transformer tasks.
The idea of cross-reconstruction for generating inter-object correspondences in a self-supervised way is interesting;The overall writing is good and the methodology part is well-organized and easy to follow.
The novelty of this work seems insufficient for ICLR.
The whole pipeline heavily relies on VNNs and the main contribution I personally consider is the local shape transform and the self-supervised mechanism for correspondences.
"Regarding the local shape transform:From 3.1.1, the SO(3)-invariant output is $\mathbf{V}\mathbf{U}^T \in \mathbb{R}^{C \times C}$, while in 3.1.2, the obtained SO(3)-invariant features $\mathbf{V} \in \mathbb{R}^{C^\prime \times 3 \times N}$ have a different shape;The authors claimed that the local shape transform transforms the global features to local ones."
"Regarding this, I have two questions."
"First, why are the features obtained by the Encoder global?"
"They are generated by a DGCNN-based VNN, but DGCNN is not guaranteed to capture the global context, as it is graph-based and really depends on the number of layers together with the number of rings of each layer."
"Second, the so-called local shape transform is predicted by a multi-layer perception from some SO(3)-invariant features that obtained from the input."
"Why after transforming the ""global"" features by such a mechanism, the features turn to ""local""?"
I cannot see any specific design that enables it.
It should be further explained.
"(I personally do not think so)Regarding the experiments:The experiments are only conducted on synthetic data, which cannot support the proposed method can work for real applications."
"I think it would be better to have additional real-data experiments;As this paper also targets on correspondence estimation, whose typical downstream task is pose estimation."
"Therefore, I consider it worthwhile to also conduct experiments on tasks of 6D pose estimation or point cloud registration (there you always use real data), to further validate the estimated correspondences."
"In Tab.1, only CPAE proposed in 2021 is used as the baseline."
"Some recent methods, e.g., [1], should also be included."
Otherwise the results are not convincing at all (only compared to a single baseline which was proposed years ago).
And it seems CPAE is the only baseline method for all the experiments.
More baselines are required on both tasks.
The method is claimed to generate SO(3)-invariant correspondences.
"However, in Tab. 1, even on the synthetic data, the I/SO(3) and SO(3)/SO(3) experiments perform unsimilarly (I would expect to have similar results per category, as it is on synthetic and clean data)."
"Could this be explained?For the SO(3)-equivariant and -invariant methods, some works for point cloud registration"
"[2, 3, 4, 5] should also be discussed.--------------------------------------------"
[1].
Zohaib et al.
"SC3K: Self-supervised and Coherent 3D Keypoints Estimation from Rotated, Noisy, and Decimated Point Cloud Data, ICCV 2023;"
[2].
Dent et al.
PPF-FoldNet:
"Unsupervised Learning of Rotation Invariant 3D Local Descriptors, ECCV 2018

[3]."
Ao et al.
SpinNet:
"Learning a General Surface Descriptor for 3D Point Cloud Registration, CVPR 2021

[4]."
Wang et al.
"You Only Hypothesize Once: Point Cloud Registration with Rotation-equivariant Descriptors, ACM MM 2022"
[5].
Yu et al.
"Rotation-Invariant Transformer for Point Cloud Matching, CVPR 2023"
See weaknesses.
"3: reject, not good enough 4: You are confident in your assessment, but not absolutely certain."
"It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
This paper attempts to register point cloud properties to their templates without precise correspondences and exact shape matching.
"To achieve this, the authors trained a local shape transform (LST) network that produces SO(3) invariant correspondences."
The training is self-supervised.
The experimental results on ShapeNet look nice.
- Valid motivation.
"Unlike the abused topic, vanilla point cloud registration, the motivation stands and could potentially benefit practical usages."
The SO(3)-invariant network design intrinsically ensures robustness against rotations.
The joint usage of a global descriptor and a local descriptor makes sense and may help with classification and recognition directly.
The self-supervision scheme looks plausible by self and cross-reconstruction.
My major concern is with the experimental setup.
"While the experiments on ShapeNet is common in the community and shows good result, I am in general doubtful whether such an approach could be really applied to the real world."
"In motivation, the authors talk about usage in vision, graphics, and robotics."
"In vision and robotics, we are interested in fitting real-world scans to templates (e.g. [Scan2CAD, CVPR 2019]), where in most cases, only noisy, partial, and sparse point clouds are provided."
The authors do not have experiments or discussions in such cases.
The authors also take groundtruth keypoints and semantic segmentations from datasets for the experiments.
"In the real-world, however, obtaining such accurate high-level semantic information already requires a deep understanding of the point cloud, and its segmentation backbone may already be SO(3) invariant."
This impairs the strength that the authors proposed.
"Following my points in the ""weaknesses"" section, I am curious about several relevant problems in the practical setup (i.e., scan to model)."
Would SO(3) invariance be sufficient?
"Do we need SE(3) or even Sim(3) invariance, if we cannot easily normalize the input due to the noise and sparsity?"
Will the network still be functional if the density distributions are different across input and output?
Will it work out of the 16-category domain?
"Do we need more training data, or would it work out-of-box?Would non-gt and/or biased key points and semantic parts be transferred properly?"
"It would be nice if the authors could conduct a minimal set of experiments in the real-world setup (e.g., extract a reconstruction from a ScanNet model and attempt to apply keypoint/semantic part transfer)."
"Otherwise, it would be good to see a justification that this paper itself is an inevitable intermediate step toward real-world usage, and what can be done to further extend it."
"5: marginally below the acceptance threshold 4: You are confident in your assessment, but not absolutely certain."
"It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
This paper presents a method of learning dense 3D correspondence between shapes in a self-supervised manner.
"Specifically, it is built on an existing SO(3)-equivariant representation."
The input point clouds are independently encoded to SO(3)-equivariant global shape descriptor Z and dynamic SO(3)-invariant point-wise local shape transforms.
Then the network is trained via penalizing errors in self- and cross- reconstructions via the decoder.
The experiment validates the effectiveness of the proposed method.
The paper is in general well organized and easy to follow.
The proposed method is straightforward and shown to be effective on the test data.
The main issue of the proposed method lies in the experimental evaluation.
Only one learned-based method is adopted for comparison in the main paper on a rather simple dataset.
More methods including some traditional methods should be also evaluated for better comparison.
The experiment on the real dataset should be also provided to show the robustness of the proposed method.
"From Fig. 6 in the supplementary, we can see that the performance of the proposed method on the I/I scenario is much worse than the SOTA method."
More analysis of the drop of performance should be given.
"Moreover, the performance of different methods with different rotation angles should be provided for better comparison."
How about the performance of other methods with a rough alignment of the initial shape?
"If a rough alignment is enough for the existing methods, why should we learn SO(3)-invariant correspondence in an end-to-end manner?The whole method is mainly built upon the existing SO(3)-equivariant representation."
The main contribution lies in introducing this representation to the specific task.
I didn't get too much novel insight in terms of network design.
Please refer to the Weaknees part.
"5: marginally below the acceptance threshold 4: You are confident in your assessment, but not absolutely certain."
"It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
This paper targets the problem of detecting test set contamination of black-box language models.
"The proposed method is based on two hypotheses: (1) the exchangeability of many datasets (distribution won't be affected after shuffling); and (2) if a language model is contaminated, it is more likely to find certain orderings of data samples than other orderings."
Then a statistical test is proposed to compare the log probability of the dataset under the original ordering to the log probability under random permutations on sharded datasets.
"Experiments are conducted with one 1.4B-gpt2 model trained from scratch on 10 test sets, and the results prove the effectiveness of the proposed framework. -"
"This paper targets an interesting and exciting problem in the community, test set contamination."
"Based on the hypothesis, this paper proposed a contamination detection method, which is intuitive and easy to deploy in other settings."
"The method is verified with a 1.4B language model trained from scratch, and the existing Llama2 model, both showing promising results even when the test set only appears a few times in the pre-training corpus."
- I'm most concerned about the definition of contamination used in this paper.
"Currently, the most popular definition of contamination follows the n-gram analysis."
"In real-world scenarios when training large language models, it's hardly seen to directly feed original data samples in their original ordering as shown in Figure 1."
The application of this work could be greatly limited.
"From Figure 3, it seems that the parameters for shards and permutations are sensitive and have to be carefully selected when being applied to other test sets."
The paper only targets direct sentence appearance in the pre-training stage.
What about instruction-tuning data in the SFT stage?
"- Could you further explain ""high false positives"" in existing n-gram-based analyses?How did you deal with the labels for the data samples in test sets?"
6: marginally above the acceptance threshold 3: You are fairly confident in your assessment.
It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.
Math/other details were not carefully checked.
"This paper examines the issue of test set contamination in large language models (LLMs), referring to the phenomenon where LLMs memorize public benchmarks during their pretraining phase."
"Since the pretraining datasets are rarely available, this paper proposes a statistical test to identify the presence of a benchmark in the pre-training dataset of a language model without accessing the model’s training data or weights."
The intuition is the exchangeability of datasets— the order of examples in the dataset can be shuffled without affecting its joint distribution.
"If a language model shows a preference for any ordering of the dataset, it might have seen the data during pretraining."
"The test on the LLaMA-2 model identifies potential contamination in the MMLU benchmark, which is consistent with the results in the original LLaMA-2 report."
-	The idea of utilizing dataset exchangeability to identify test set contamination is novel and interesting.
"The proposed sharded likelihood comparison test addresses the tradeoff between statistical power and computational requirements of the permutation test, which is promising."
The sharded rank comparison test also provides (asymptotic) guarantees on false positive rates.
Experimental results are promising.
A GPT-2 model is trained from scratch on standard pretraining data and known test sets to verify the efficiency of the proposed method in identifying test set contamination.
"The method is also tested with an existing model, LLaMA2, on the MMLU dataset, showing general agreement with the contamination study results."
"-	Although a more efficient sharded rank comparison test is proposed, the computational complexity is still considerable."
"For example, testing 49 files using 1000 permutations per shard can take 12 hours for LLaMA2.There is no comparison with other baseline methods."
"The method relies on a strong assumption of data exchangeability, which may not hold in real-world datasets."
"If a dataset is not exchangeable, how effective is the method?"
"8: accept, good paper 3: You are fairly confident in your assessment."
It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.
Math/other details were not carefully checked.
"This paper studies the problem of identifying test set contamination in large language models, i.e., detecting that a test set is present in the pretraining data of a language model."
"The main idea behind the approach is that for test sets that have some canonical order of individual instances (e.g.: the order in which the dataset creators release the dataset), the likelihood of the test set in that order would be significantly higher than any random permutation of the dataset."
"Based on this idea, the paper proposes two versions of the test, one of which shards the test set and aggregates statistics over the shards to make the estimate more robust to potential biases in the model."
The tests are evaluated first by measuring their sensitivity when pretraining datasets are intentionally contaminated.
It is shown that they are highly sensitive when the tests sets are large or have been duplicated enough in the pretraining data.
The test is then used to measure contamination of the pretraining data used to train the Llama models and it is shown that the findings agree with prior reports.
This is clearly written paper and makes a strong contribution.
"The tests do not require access to model weights or pretraining data, making them practically useful."
The experiments do not compare the performance of the proposed tests to prior work.
"I understand that this work differs from say, the work from Carlini et al. in that this work focuses on set-level contamination, but how does aggregating instance-level statistics over a set compare?"
- How does the performance of this method compare to that of prior work (see Weakness)How sensitive is the proposed test to the model size?
"8: accept, good paper 3: You are fairly confident in your assessment."
It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.
Math/other details were not carefully checked.
The paper proposes a statistical test that given certain assumptions can indicate whether a black-box language model has been trained on certain datasets.
This is a topic of increasing interest and importance given the prevalence of pretrained models that are trained on very large amounts of data.
The authors first propose a simple permutation test and identify some weaknesses with it.
They then propose a more sophisticated sharded test.
"The authors show 2 kinds of experiments:

(1) They test on a dataset where they have injected a small amount of certain test sets to see if their approach can detect them."
(2) They apply their test to existing models such as Lllama-2 showing their approach can scale.
-Topic of large importance in the community given the direction of the field.
Novel approach with thorough empirical results.
I have some questions about the definition of test set contamination below.
Well written and interesting.
I have some questions about the definition of test set contamination below.
In Figure 1 the authors show test set contamination for BoolQ.
But the examples there are unlabeled.
Are the authors targeting unlabeled test set contamination i.e. the input is present in the pretraining data but not the label?
Would be great to have some justification and explanation of this setting.
"8: accept, good paper 3: You are fairly confident in your assessment."
It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.
Math/other details were not carefully checked.
