{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv('sentences_author.csv')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "source": [
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "import torch\n",
    "from torch.nn import Sigmoid\n",
    "\n",
    "# Load model and tokenizer (ensure they are already initialized)\n",
    "# checkpoint path: /home/nana/DASP_report_template/model_training/nlp/review_to_theme/results/checkpoint-1810\n",
    "# bert-base-uncased\n",
    "model = BertForSequenceClassification.from_pretrained(\"/home/nana/DASP_report_template/model_training/nlp/review_to_theme/results/final_model\", num_labels=11, problem_type=\"multi_label_classification\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"/home/nana/DASP_report_template/model_training/nlp/review_to_theme/results/final_model\")\n",
    "\n",
    "# Make sure the model is in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "def predict(input_text, threshold=0.5):\n",
    "    # Tokenize the input text using tokenizer (handles padding, truncation, etc.)\n",
    "    inputs = tokenizer(input_text, truncation=True, padding=\"max_length\", max_length=128, return_tensors=\"pt\")\n",
    "\n",
    "    # Run the model for prediction\n",
    "    with torch.no_grad():  # Disable gradient calculation during inference\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    # Apply sigmoid to logits (since it's multi-label classification)\n",
    "    sigmoid = Sigmoid()\n",
    "    probabilities = sigmoid(logits)\n",
    "\n",
    "    # Apply threshold to get binary predictions (0 or 1)\n",
    "    predictions = (probabilities > threshold).int()\n",
    "\n",
    "    # Return the binary predictions (as a list)\n",
    "    return predictions.squeeze().tolist()\n",
    "\n",
    "# Example usage\n",
    "input_text = \"Lack of limitations.\"\n",
    "predicted_labels = predict(input_text)\n",
    "\n",
    "print(\"Predicted Labels:\", predicted_labels)\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "source": [
    "labels = ['ANA', 'BIB', 'DAT', 'EXP', 'INT', 'MET', 'OAL', 'PDI', 'RES', 'RWK', 'TNF']\n",
    "# Find all indices of 1\n",
    "indices = [i for i, value in enumerate(predicted_labels) if value == 1]\n",
    "true_labels = [labels[i] for i in indices]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "source": [
    "true_labels"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "source": [
    "def predict_theme_category(text):\n",
    "    # Load the pretrained model and tokenizer\n",
    "    model = BertForSequenceClassification.from_pretrained(\"/home/nana/DASP_report_template/model_training/nlp/review_to_theme/results/final_model\", num_labels=11, problem_type=\"multi_label_classification\")\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"/home/nana/DASP_report_template/model_training/nlp/review_to_theme/results/final_model\")\n",
    "    model.eval()\n",
    "    threshold = 0.5\n",
    "    # Tokenize the input text using tokenizer (handles padding, truncation, etc.)\n",
    "    inputs = tokenizer(text, truncation=True, padding=\"max_length\", max_length=128, return_tensors=\"pt\")\n",
    "\n",
    "    # Run the model for prediction\n",
    "    with torch.no_grad():  # Disable gradient calculation during inference\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    # Apply sigmoid to logits (since it's multi-label classification)\n",
    "    sigmoid = Sigmoid()\n",
    "    probabilities = sigmoid(logits)\n",
    "\n",
    "    # Apply threshold to get binary predictions (0 or 1)\n",
    "    predictions = (probabilities > threshold).int()\n",
    "    predictions = predictions.squeeze().tolist()\n",
    "\n",
    "    labels = ['ANA', 'BIB', 'DAT', 'EXP', 'INT', 'MET', 'OAL', 'PDI', 'RES', 'RWK', 'TNF']\n",
    "    # Find all indices of 1\n",
    "    indices = [i for i, value in enumerate(predictions) if value == 1]\n",
    "    true_labels = [labels[i] for i in indices]\n",
    "\n",
    "    # Return the binary predictions (as a list)\n",
    "    return true_labels"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "source": [
    "df['attitude_themes'] = df['sentence'].apply(predict_theme_category)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "source": [
    "df"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "source": [
    "def create_clusters(row):\n",
    "    # Extract root and themes\n",
    "    root = row[\"attitude_root\"]\n",
    "    themes = row[\"attitude_theme\"]\n",
    "    \n",
    "    # Combine root with each theme\n",
    "    clusters = [f\"{root}({theme})\" for theme in themes]\n",
    "    return clusters"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "source": [
    "# Apply the function to create clusters\n",
    "df_cluster = df\n",
    "df_cluster[\"clusters\"] = df_cluster.apply(create_clusters, axis=1)\n",
    "# Explode clusters to create one row per cluster\n",
    "df_cluster = df_cluster.explode(\"clusters\", ignore_index=True)\n",
    "print(df_cluster)"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jitsupeer_310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
