How do we take a limit of M -> ∞ ? Does k also go ∞?
Cons: While doing this leads to better convergence, each update is still very expensive compared to standard SGD, and for instance on vision tasks the algorithm needs to run for almost double the time to get similar accuracies as an SGD, adam solver.
Since q_rel require one hot vector as input, how to sample the q_rel given the importance score and how backprob the gradient in this case?
- In Equation (6), the posterior distribution should be P(X|G) since X is the latent variable to be inferred, right
How are the lambda and threshold parameters tuned? The authors mention a validation set, are they just exhaustively explored on a 3D grid on the validation set?
How does the proposed method perform in more complicated tasks such as
Dual-1 and Dual-5 are introduced without explanation.
However, their algorithm--while much less computationally expensive than true full-matrix adaptive preconditioning---is still far more expensive than the usual diagonal version.
It is unclear whether the data augmentation techniques is applied only at training time or also at test time.
Since each domain may have different number of classes, it is not clear how the number of classes (L) is set in the classification module (maximum number of classes in all domain?).
- the problem assumptions are too simplistic and unrealistic (feature distributions of target and auxiliary data are identical), so it is questionable if the proposed algorithm has practical importance
- it wasn't clear how the sparsity percentage on page 3 was defined?
- Architecture choice unclear: Why are $\sigma$ and $\omega$ separate networks.
- page 4, Sect. 4.4: Architecture of $\alpha$ would be nice (more than a linear layer?)
The paper used very restricted Gaussian distributions for the formulation.
The problem of image classification is considered only, while authors claimed the method can be easily applied to other problems as well.
Then, there will be another question: how the two networks are trained? Are they trained separately or jointly?
First of all, the setup for the AE and VAE is not specified.
3. Scenario discussed in Sec. 4 seems somewhat impractical.
In addition, the paper would also need to show that such a model does not generalize to a validation set of images.
- in section 2.2, please explain more how gradients w.r.t hyper-parameters are computed.
- how to tune lambda? it is an important hyper-parameter, but it is set without a good principle, e.g., "For SGD-APO, we used lambda = 0.001, while for SGDm-APO, we used lambda = 0.01", "while for RMSprop-APO, the best lambda was 0.0001
In Section 3.2.2, the authors only explain how they learn example-based \sigma, but details on how to make graph construction end-to-end trainable are missing.
The proofs are quite dense and I was unable to verify them carefully.
1. For the evaluation of DBA, I assume that there are 4 adversarial parties, controlling each of the 4 local triggers. When using centralized attacks, are there still 4 adversarial parties, although they share the same global trigger, or if there is only 1 adversarial party?
Specifically, which features are perturbed, what are the values assigned as the trigger, and what is the corresponding target label?
- My main concern is reproducibility: the authors employ a number of large architectures, complex loss functions, and regularizers / "additional improvements".
In this simple model, can we not calculate the mutual information directly (i.e., without the bottleneck)?
Specifically, how can mutual information in this context be formally linked to generalization/overfitting?
First of all, I don’t understand how the main equation of the compound density network in Equation (3) is different from the general case of a Bayesian neural network? Can the authors please comment on that?
I also find weird the way that the authors arrive to their final objective in Equation (5).
Then they continue to Equation (5) which they present as the combination of the true likelihood with a KL regularisation term.
However, what the authors implicitly did was to perform variational inference for maximising their likelihood by introducing a variational distribution q(\theta) = p(\theta | g(x_n; \psi).
Is there a reason why the authors do not introduce their objective by following the variational framework?
My biggest concern in the methodology, however, has to do with the selection of the matrix variate normal prior for the weights and the imposition of diagonal covariances (diag(a) and diag(b)).
What is the purpose then for introducing the matrix variate Gaussian?
Finally, it is unclear how the authors have picked the best \lambda parameter for their approach? On page 5 they state that they “pick the value that results in a good trade-off between high uncertainty estimates and high prediction accuracy.” Does this mean that you get to observe the performance in the test in order to select the appropriate value for \lambda? If this is the case this is completely undesirable and is considered a bad practice.
You mention that negative mining should improve over this strategy. What does negative mining correspond to in this context? Are there bad rewrites better than others?
I do not understand why making use of labels is important for solving the catastrophic forgetting problem and how the labels are useful in the generative replay process.
It is also not clear to me how domain translation is relevant to continual learning.
I do not understand how the model is trained to solve multiple tasks.
Do the same model is trained for multiple tasks? Is each of the tasks trained sequentially or simultaneously?
6. The rationale of the two tower design (why not combine two) is not clearly explained.
Doesn't the classification loss have a dependency on the input condition?
--What does a "heavy classifier" imply concretely?
In contrast, the studied learning rates are asymptotic and there is a big discrepancy.
2- Why is a two-stage pre-training (Figure 2) process needed? Why not just a single stage?
However, it is not obvious that how to move from line 3 to line 4 at Eq 15.
(4) Are \theta and \phi jointly and simultaneously optimized at Eq 12?
(5) Due to the mean policy approximation, does the mean policy depend on \phi?
The authors should clearly explain how to update \phi when optimizing Eq 12.
However, the derivations about \phi are missing.
For example, how to compute the gradient w.r.t. \phi? Since the mean policy is used, it is not apparent that how to compute the gradient w.r.t. \phi.
Second, the authors claim they are using/motivated by Choquet integral, but do not have any (appendix) sections to explain how this mathematical tool is really integrated into their models.
How do you guarantee that the representation learned by the neural network still obeys the property of Choquet integral? What is your loss or your algorithm?
It is limited, however, in the sense that it (only) measures auto-encoding capabilities: to what extent can the shape be reproduced given a sample point cloud from the given shape.
As previously mentioned in public comments on this forum, some points in the paper are not very clear; specifically regarding the loss function, the definition of "edge-to-edge" convolutions and generally the architectural choice related to the conditional GAN discriminator.
- how exactly do you do a L1 loss on graphs? I'd have to assume the topology of the graph is unchanged between Gy and T(Gx) ~ and then maybe take L1 of weight matrix? But then is this general enough ~ given your stated goal of modeling different topologies? Either ways, more explanation / and perhaps equations to clarify this loss would be very helpful.
There is not sufficient detail to reproduce the models based on the paper alone.
Moreover, it seems strange that significant space was used to give equations describing simple embedding lookups (i.e., matrix multiplications with one-hot vectors), but the basic technical foundations of Transformers were not adequately explained.
For example, one question is how often a single partial tree has multiple possible completions in the data.
Why? If we are learning the distribution, would not it make sense to sample all architectures only after training the supernet at our best?
Do they here refer to the gradients with respect to the weights ONLY?
Could we say that the advantage of the Gumbel Softmax technique is two-fold? i) make the loss differentiable with respect to the arch parameters; ii) reduce the variance of the estimate of the loss gradients with respect to the network weights.
2)	Can the author discuss why the soft sampling procedure in [1] is not enough? I have an intuitive understanding of this, but I think this should be clearly discussed in the manuscript as this is a central aspect of the paper.
3)	The authors use a certain number of warmup steps to train the network weights without updating the architecture parameters to ensure that “the weights are sufficiently trained”. Can the authors discuss the choice on the number of warmup epochs?
- It would greatly benefit the reader if eq. 5 were expanded.
- The approach also seems to add a lot of complexity and heuristics/hyper-parameters.
(c) The authors should present what they mean by a dilated convolution using the notation of the paper.
- the method is not applicable to episodes of different length
At last, I am pretty sure to not be able to reproduce the model described in the paper (adding a section on that in the supplementary material would help), and many concrete aspects are described too fast (like the way to sample negative pairs).
Is Harmonic Convolution applicable to complex STFT coefficients as well?
If so it would be better to define the operator in a more general notation.
In Section 4.3 and 4.4, is the x_0 (defined in Section 2.1) complex-valued STFT coefficients or something else?
What is the L1 loss defined in Section 4.4? To obtain the final separated audio waveform, an inverse STFT is applied on what?
The main problem I see with these approaches is that they rely on sufficiently large batch sizes which could be (currently) problematic for many real-world applications.
Lastly, if the authors are not planning to release the code, the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.
Also how many steps are required to learn the linear interpolation ? How much the does it influence the quality of the interpolation ?
It is also not clear what are the assumptions made on the connectivity of the input graph and the target graph.
Do we know how does the connectedness of the  input graph affect the translation quality in the case of strongly connected directed graphs? Or what happens if the target graph has a strong connectivity?
Towards this, how does the computational complexity scale wrt to the connectedness?
A lot of clarity is required on the choice of evaluation metric; for example choice of distance measure ?
What is the L1 norm applied on?
- Trick is specific to LM.
It seems heavily dependent on GBDT.
1. I do not get the point of bringing up NCE. Did you actually use NCE loss? Did you only refer to NCE as a weight tying which can be used in a standard XENT loss [3]? The first paragraph of 3.3 did not help clarify this point either.
2) Beware of overstating: the argument that the framework is broadly applicable is not that useful, given that it's a lot of work to derive closed-form marginalized estimators.
The method presented is interesting; but it is not clear that it is present with enough detail for it's results to be replicated.
Was crossvalidation used to select the topology?
If so, what was the methodology.
Moreover, these bounds are just in the case where the teacher model is linear and while it is claimed that this could be relaxed to a more general class of functions, the specific bounds might change drastically.
So any insights on the nature of these bounds will be valuable, especially with some comments on how these bounds change if the teacher model is itself realized as a 2-layer neural network.
1 The implementation steps of the proposed method (MoVE) are not clear.
In 3/4 of the pairs the author tried, this phenomenon is not there. Whether the findings generalize to other situations where the phenomenon appears is uncertain.
2. I might have missed it, but I couldn't find any motivation on why tanh is used as nonlinearity. Would the method work with relu?
To be honest, the theoretical contribution of the paper is limited.
More discussions on these questions can be very helpful to further understand the proposed method.
2. As the problem is formulated as an RL problem, which is well-known for its difficulty in training, did we encounter similar issues? More details in the implementation can be very helpful for reproducibility.
It should be better motivated why one should use the duality gap as an upper bound for the "F-distance".
Minimizing the F-distance as is usually done seems like the more direct and simple approach.
- I have trouble understanding the overall idea behind Algorithm 1 and Eq. (22).
What is the definition of f^* and g^* in Eq. (22)? Some explanatory text would be valuable.
1. The authors should provide more details on how the hand-crafted demonstrator agents were made.
I assume something similar to an a* algorithm was probably used for the passing task, but what about the maze navigation task?
How do “we choose a specific number of assignments based on prediction probabilities”?
- The CFS metric depends on a hyperparameter (the "retention ratio"), which here is arbitrarily set to 80% without any justification.
13. Reproducibility seems like it would be hard. There are a few parameters (meta-learning rates, meta-optimizers) that I could not find for example and there is a lot of complexity.
Negative points: (1) The authors should provide more justification on equation-3.
Why do the authors directly average different loss for the discriminator and the classifer?
(2) The function of the discriminator is not very clear, especially for the classification error test.
Why can pGAN lead to the inclination? Is it possible for pGAN to control the specific error tendency?
However, it is unclear how a similar strategy could be applied if we are interested in learning variables with higher-level semantics such as the expression of a face or its pose.
3. how are predicted labels embedded? Do you learn a vector of each tag of BIOES and then take a weighted sum of these vectors with predicted probabilities as weights?
The main drawback would be the rather incrementality of that paper (basically sample before projecting is a bit better than projecting after sampling) and that this directional setting is quite limited...
2. The learning procedure is confusing.
It is highly recommended to provide the pseudocode of the proposed method.
The reasons for the use of the energy-based formulation are not clear to me.
Is the energy-based model particularly well-suited to the random-projection setup, or are there other reasons for using it, independent of the use of random projections?
The index over which the sum happens is n, but n is fixed? So this looks like a sum with just one component in it, namely the first n-gram.
- It is not clear how the initialisation (10) is implemented.
Particularly the "fusion" module remains extremely unclear.
It is not clear to me that the classifier difference metric is well-defined.
Equation (4). What is d_{k,l}? A pixel-wise target label? Where does it come from?
2) How are the rules from in Eq (2)? i.e., how is \beta_i selected for each i?
- Very little details (apart from the optimization algorithm) are given regarding the architecture used (types of input, output, neural units, activation functions, number of hidden layers, loss function, etc...), which makes it very hard to reproduce this approach.
It would be interesting to know the evaluation times for the BA-net and more importantly to have some implementation details to ensure reproducibility.
Overall it would be helpful for reproducibility if authors can visualize all the layers of all the different parts of the network as it is commonly done in the DL papers.
-	How the first camera pose is initialized?
This is all further confused by the semantic topics used for clustering the images which ignores stop words and therefore spatial relations or any grammatical nuances.
