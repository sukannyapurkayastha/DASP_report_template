I am not sure if this is the first work in this direction of proving bounds assuming an oracle or if there is some background work that the authors could provide to put this into context.
Attacking CRBMs is highly relevant and should be included as a baseline.
However, the idea is very related to Yeh et al.’s work which has already published but not mentioned at all.
For the bAbi task, although there is a significant improvement (43%) in the mean error rate compared to the original DNC, it's important to note that performance in this task has improved a lot since the DNC paper was release. Since this is the only non-toy task in the paper, in my opinion, the authors have to discuss current SOTA on it, and have to cite, for example the universal transformer[1], entnet[2], relational nets [3], among others architectures that shown recent advances on this benchmark.
- For semi-supervised classification, the paper did not report the best results in other baselines.
The paper also misses several powerful baselines of semi-supervised learning, e.g. [1,2].
1. The paper should also talk about the details of ARNet and discuss the difference, as I assume they are the most related work
1) Some of the benchmark datasets for the proposed task as well as some well-known methods (see Battaglia et al’18 and references in there) are missing.
However, I think since this is few-shot learning with domain adaptation, there is no domain adaptation baselines being mentioned in comparison.
The reviewer is also interested to see how the the generalization bound introduced in this paper is related to the recent theoretical works [3],[4] on MDL.
- The paper should refer to 1) the reward shaping literature, 2) the growing line of works concerned with control variates for REINFORCE (such as VIMCO, MuProp, REBAR) and 3) the growing line of works concerned about combinatorial optimization with reinforcement learning (Neural Combinatorial Optimization with Reinforcement Learning, etc.)
It would probably help to position the VAE component more precisely w.r.t. one of the two baselines, by indicating the differences.
The tasks are explicitly designed to exploit these additional parameters - so framing the synthetic experiments as, "here are some simple functions for which we would need the additional parameters that we define" makes sense; but arguing that Hartford et al. "fail approximating rather simple functions" (page 7) is misleading because the functions are precisely the functions on which you would expect Hartford et al. to fail (because it's designed for a different setting).
There should be a better discussion of related work on the topic.
=> Baselines: The comparison provided in the paper is weak.
Moreover, in spite of the authors writing that their goal is “completely different” from [Lee at al 18a, Ma et al 18a], I found the two cited papers having a similar intent and approach to the problem, but a comparison is completely missing.
A proper baseline should have been compared.
Third, the comparison to baseline and “DeepSet” is not fair.
4. Comparison with past works.
1) The only comparison to another non-fixed sampling baseline is Kool et al. 2019.
This baseline was also missing in image reconstruction.
This seems like a very weak baseline---there are any number of other reasonable ways to encode this.
Why not compare results to WGAN-GP in this case? Since the proposal of GANs, many papers addressed the mode collapse problem.
For e.g. the results on the oscillating behavior of Dirac-GAN are described in related works (e.g. Mescheder et al. 2018), and in practice, WGAN with no regularization is not used (as well as GAN with momentum, as normally beta1=0 in practice).
I am also wondering if the comparison with the baselines is fair.
In other words, although in the present results, the proposed NF-SGAN/WGAN outperforms the baseline, the reported performance of the baselines is worse than in related works on CIFAR10.
On a more minor note, the paper [1] seems to report 4.95 accuracy for MixMatch on CIFAR-10 with 4k labels, while in this paper it’s being reported as 6.24.
However, there are papers empirically analyzing novelty detection using generative model -- should analyze or at least cite:
Weakness: It would be good to see some comparison to the state of the art
The main issue of this paper is the fair comparisons with other works.
- Baseline missing: Random actions from expert
- Baseline missing: Simple RNN policies that communicate hidden states.
Another baseline could be to simply model the imitator and probing policies as RNNs and let them communicate with each other via the hidden states.
- comparison should be made to the linear composition method in the Arora, Liang, Ma ICLR 2017 paper
However, there is no comparison against existing work.
However, memory overhead is still an issue compared to existing method.
It is important to place the contributions in this paper in context of these other works.
A number of these references are missing and no experimental comparison to these methods has been made.
* In related work, no reference to previous work on "statistical" approaches to NN
