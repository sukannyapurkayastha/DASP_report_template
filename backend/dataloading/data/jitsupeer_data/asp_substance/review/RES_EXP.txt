3) Furthermore, for the experiments only one neural network architecture was considered and it remains an open question, how the presented results translate to other architectures.
2) The experimental results provided in this paper are weak.
-	The experimental results of section 5.2 are somewhat disappointing.
Finally, the experimental results do not show any significant advantage over PGD, either in running time (they are slower) or norm perturbation.
The experimental results are actually less impressive than what are claimed in contribution and conclusion.
- impact: the results achieved in the experiments are very small improvements compared to the baseline of RGCN (~ +0.01 in two experiments and ~ -0.04 in another) and often these small variations in results can be compensated with better baselines training (e.g. better hyper-params, ...)
- Although disc can be easily estimated in the regression task (differently from d_A distance which is a special case of disc), there are no experimental results of the regression task even in the synthetic data.
So this work has to be supported with more detailed experimental results to express the potential of this approach fully.
Both the results on the development set and on the test set should be reported for the validity of the experiments.
There is in fact no experimental evidence that the practical advantages of BN are relevant to the results proven.
- The experiments were entirely focused on uncertainty quality but we are always interested in both performance on the task at hand as as well as good uncertainty estimates. What was the performance based on e.g. classification accuracy on each of these tasks compared to the baselines? I believe that including these results will strengthen the paper and provide a more complete picture.
(1) The experimental results cannot show the usefulness of the proposed GCN.
3) In its current form, the experimental results are extremely cherry-picked, with a very small number of tasks evaluated, and for each task a single selected baseline used.
- The use of RAM is a fairly serious limitation of your experimental setting in my view. You should include results also for the pixel space, even if negative.
- Experimental results are provided only on MNIST and Fashion-MNIST.
2) The experiment results include standard errors for different replicates where such replicates correspond to different training random seeds (or different samples from the enriched set?), however, it doesnâ€™t include any significance of the sampling.
- The experiments show good results compared to existing algorithms, but not impressively so.
