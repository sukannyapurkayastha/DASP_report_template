I do not see much insight into the problem.
The main problem with this paper is that it is difficult to identify its main and novel contributions.
Summarizing, the paper addresses a relevant problem but they do not state which their main contributions are, and reintroduce some ideas previously published in the literature.
At the conceptual level, the idea of jointly modeling local video events is not novel, and can date back to at least ten years ago in the paper “Learning realistic human actions from movies”, where the temporal pyramid matching was combined with the bag-of-visual-words framework to capture long-term temporal structure.
The idea of having a separate class for out-distribution is a very interesting idea but unfortunately previously explored.
1) Although the ensemble idea is new, the idea of selective self-training is not novel in self-training or co-training of SSL as in the following survey.
The idea that introduces labels in VAE is not novel.
At a high level, the idea of imposing a mixture of gaussian structure in the feature space of a deep neural network classifier is not new.
The idea to extend the use of VAE-GANs to the video prediction setting is a pretty natural one and not especially novel.
- The idea is a simple extension of existing work.
Although the idea seems to be interesting, the paper seems to be a bit incremental and is a simple application of existing GAN techniques.
The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper.
ii) Although the idea of generating contrastive explanations is quite interesting, it is not that novel.
Last comment is that, although the concept of `deficiency` in a bottleneck setting is novel, the similar idea for tighter bound of log likelihood has already been pursed in the following paper:
* the idea of smoothing gradients is not new
- the idea is trivial and simple. I don't think there's significant novelty here: all the components are existing and combining them seems very trivial to me.
