sentences
"This work proposes LSTNet, a self-supervised method to establish reliable 3D dense correspondences irrespective of the input point clouds’ rotational orientation."
"Specifically, LSTNet learns to formulate SO(3)-invariant local shape transform for each point in a dynamic, input-dependent manner. Each point-wise local shape transform can map the SO(3)-equivariant global shape descriptor of the input shape to a local shape descriptor, which is passed to the decoder to reconstruct the shape and pose of the input point cloud."
"The proposed self-supervised training pipeline encourages semantically corresponding points from different shape instances to be mapped to similar local shape descriptors, enabling LSTNet to establish dense point-wise correspondences via nearest point pairs between cross-reconstructed point clouds."
The self- and cross-reconstruction training strategy is simple yet effective.
LSTNet demonstrates state-of-the-art performance on 3D semantic matching when evaluated on the KeypointNet dataset and part segmentation label transfer when evaluated on the ShapeNet dataset.
"The performance of aligned shape pairs under the setting of I/I shows that other methods, such as CPAE, are much better than LSTNet."
The reason why other methods are much better than LSTNet under the setting of I/I should be clarified.
Lack of limitations.
1) This paper proposes a self-supervised method to find semantically corresponding points for a point cloud pair.
2）The main idea is to decouple a point cloud feature learning process into a SO(3)-equivariant global shape descriptor and dynamic SO(3)-invariant point-wise local shape transforms.
3) Experiments on the KeypointNet dataset show the effectiveness of the proposed method.
1) This paper is generally well-written.
2) The idea of factorizing point cloud descriptors into SO(3)-equivariant global shape descriptor and dynamic SO(3)-invariant point-wise local shape transforms seems to be novel.
3) Experimental results are good.
"1) The main weakness of this paper could be all experiments are performed on synthetic datasets, with simple point cloud. It's good for authors' to show some examples/experiments on real-world datasets. For example, the 3Dmatch dataset."
"2) Since the proposed method can estimate dense correspondences, I wonder whether the proposed method can be used to estimate the relative rotation/translation for a point cloud pair. For example, the estimated dense correspondences can be fed to an ICP method to estimate the relative rotation/translation."
3) The running time and GPU memory cost is blurry for me.
"4) Please compare the proposed method with more recent papers, e.g., [SC3K: Self-supervised and Coherent 3D Keypoints Estimation from Rotated, Noisy, and Decimated Point Cloud Data]."
Please refer to the weaknesses.
"This paper introduces LSTNet, which leverages an SO(3)-equivariant encoder-decoder architecture(Vector Neuron Networks, VNNs) and proposes a novel function called local shape transform to further transform the learned features. The proposed method is validated on both the 3D keypoint transfer and part segmentation label transformer tasks."
1. The idea of cross-reconstruction for generating inter-object correspondences in a self-supervised way is interesting.
2. The overall writing is good and the methodology part is well-organized and easy to follow.
1. The novelty of this work seems insufficient for ICLR. The whole pipeline heavily relies on VNNs and the main contribution I personally consider is the local shape transform and the self-supervised mechanism for correspondences.
"2. Regarding the local shape transform: 2.1. From 3.1.1, the SO(3)-invariant output is $\mathbf{V}\mathbf{U}^T \in \mathbb{R}^{C \times C}$, while in 3.1.2, the obtained SO(3)-invariant features $\mathbf{V} \in \mathbb{R}^{C^\prime \times 3 \times N}$ have a different shape."
"2.2 The authors claimed that the local shape transform transforms the global features to local ones. Regarding this, I have two questions."
"2.2.1 First, why are the features obtained by the Encoder global? They are generated by a DGCNN-based VNN, but DGCNN is not guaranteed to capture the global context, as it is graph-based and really depends on the number of layers together with the number of rings of each layer."
"2.2.2 Second, the so-called local shape transform is predicted by a multi-layer perception from some SO(3)-invariant features that obtained from the input. Why after transforming the ""global"" features by such a mechanism, the features turn to ""local""? I cannot see any specific design that enables it. It should be further explained. (I personally do not think so) 3. Regarding the experiments: 3.1 The experiments are only conducted on synthetic data, which cannot support the proposed method can work for real applications. I think it would be better to have additional real-data experiments."
"3.2 As this paper also targets on correspondence estimation, whose typical downstream task is pose estimation. Therefore, I consider it worthwhile to also conduct experiments on tasks of 6D pose estimation or point cloud registration (there you always use real data), to further validate the estimated correspondences."
"3.3 In Tab.1, only CPAE proposed in 2021 is used as the baseline. Some recent methods, e.g., [1], should also be included. Otherwise the results are not convincing at all (only compared to a single baseline which was proposed years ago). And it seems CPAE is the only baseline method for all the experiments. More baselines are required on both tasks."
"3.4 The method is claimed to generate SO(3)-invariant correspondences. However, in Tab. 1, even on the synthetic data, the I/SO(3) and SO(3)/SO(3) experiments perform unsimilarly (I would expect to have similar results per category, as it is on synthetic and clean data). Could this be explained?"
"4. For the SO(3)-equivariant and -invariant methods, some works for point cloud registration [2, 3, 4, 5] should also be discussed."
"[1]. Zohaib et al. SC3K: Self-supervised and Coherent 3D Keypoints Estimation from Rotated, Noisy, and Decimated Point Cloud Data, ICCV 2023."
"[2]. Dent et al. PPF-FoldNet: Unsupervised Learning of Rotation Invariant 3D Local Descriptors, ECCV 2018 [3]. Ao et al. SpinNet: Learning a General Surface Descriptor for 3D Point Cloud Registration, CVPR 2021 [4]. Wang et al. You Only Hypothesize Once: Point Cloud Registration with Rotation-equivariant Descriptors, ACM MM 2022 [5]. Yu et al. Rotation-Invariant Transformer for Point Cloud Matching, CVPR 2023"
See weaknesses.
"This paper attempts to register point cloud properties to their templates without precise correspondences and exact shape matching. To achieve this, the authors trained a local shape transform (LST) network that produces SO(3) invariant correspondences. The training is self-supervised. The experimental results on ShapeNet look nice."
"Valid motivation. Unlike the abused topic, vanilla point cloud registration, the motivation stands and could potentially benefit practical usages."
The SO(3)-invariant network design intrinsically ensures robustness against rotations.
The joint usage of a global descriptor and a local descriptor makes sense and may help with classification and recognition directly.
The self-supervision scheme looks plausible by self and cross-reconstruction.
"My major concern is with the experimental setup. While the experiments on ShapeNet is common in the community and shows good result, I am in general doubtful whether such an approach could be really applied to the real world."
"In motivation, the authors talk about usage in vision, graphics, and robotics. In vision and robotics, we are interested in fitting real-world scans to templates (e.g. [Scan2CAD, CVPR 2019]), where in most cases, only noisy, partial, and sparse point clouds are provided. The authors do not have experiments or discussions in such cases."
"The authors also take groundtruth keypoints and semantic segmentations from datasets for the experiments. In the real-world, however, obtaining such accurate high-level semantic information already requires a deep understanding of the point cloud, and its segmentation backbone may already be SO(3) invariant. This impairs the strength that the authors proposed."
"Following my points in the ""weaknesses"" section, I am curious about several relevant problems in the practical setup (i.e., scan to model)."
"1. Would SO(3) invariance be sufficient? Do we need SE(3) or even Sim(3) invariance, if we cannot easily normalize the input due to the noise and sparsity?"
2. Will the network still be functional if the density distributions are different across input and output?
"3. Will it work out of the 16-category domain? Do we need more training data, or would it work out-of-box?"
4. Would non-gt and/or biased key points and semantic parts be transferred properly?
"It would be nice if the authors could conduct a minimal set of experiments in the real-world setup (e.g., extract a reconstruction from a ScanNet model and attempt to apply keypoint/semantic part transfer). Otherwise, it would be good to see a justification that this paper itself is an inevitable intermediate step toward real-world usage, and what can be done to further extend it."
"This paper presents a method of learning dense 3D correspondence between shapes in a self-supervised manner. Specifically, it is built on an existing SO(3)-equivariant representation. The input point clouds are independently encoded to SO(3)-equivariant global shape descriptor Z and dynamic SO(3)-invariant point-wise local shape transforms. Then the network is trained via penalizing errors in self- and cross- reconstructions via the decoder. The experiment validates the effectiveness of the proposed method."
1. The paper is in general well organized and easy to follow.
2. The proposed method is straightforward and shown to be effective on the test data.
1. The main issue of the proposed method lies in the experimental evaluation. Only one learned-based method is adopted for comparison in the main paper on a rather simple dataset. More methods including some traditional methods should be also evaluated for better comparison. The experiment on the real dataset should be also provided to show the robustness of the proposed method.
"2. From Fig. 6 in the supplementary, we can see that the performance of the proposed method on the I/I scenario is much worse than the SOTA method. More analysis of the drop of performance should be given. Moreover, the performance of different methods with different rotation angles should be provided for better comparison."
"3. How about the performance of other methods with a rough alignment of the initial shape? If a rough alignment is enough for the existing methods, why should we learn SO(3)-invariant correspondence in an end-to-end manner?"
4. The whole method is mainly built upon the existing SO(3)-equivariant representation. The main contribution lies in introducing this representation to the specific task. I didn't get too much novel insight in terms of network design.
Please refer to the Weaknees part.
"This paper targets the problem of detecting test set contamination of black-box language models. The proposed method is based on two hypotheses: (1) the exchangeability of many datasets (distribution won't be affected after shuffling). and (2) if a language model is contaminated, it is more likely to find certain orderings of data samples than other orderings. Then a statistical test is proposed to compare the log probability of the dataset under the original ordering to the log probability under random permutations on sharded datasets. Experiments are conducted with one 1.4B-gpt2 model trained from scratch on 10 test sets, and the results prove the effectiveness of the proposed framework."
"This paper targets an interesting and exciting problem in the community, test set contamination."
"Based on the hypothesis, this paper proposed a contamination detection method, which is intuitive and easy to deploy in other settings."
"The method is verified with a 1.4B language model trained from scratch, and the existing Llama2 model, both showing promising results even when the test set only appears a few times in the pre-training corpus."
"I'm most concerned about the definition of contamination used in this paper. Currently, the most popular definition of contamination follows the n-gram analysis. In real-world scenarios when training large language models, it's hardly seen to directly feed original data samples in their original ordering as shown in Figure 1. The application of this work could be greatly limited."
"From Figure 3, it seems that the parameters for shards and permutations are sensitive and have to be carefully selected when being applied to other test sets."
The paper only targets direct sentence appearance in the pre-training stage. What about instruction-tuning data in the SFT stage?
"Could you further explain ""high false positives"" in existing n-gram-based analyses?"
How did you deal with the labels for the data samples in test sets?
"This paper examines the issue of test set contamination in large language models (LLMs), referring to the phenomenon where LLMs memorize public benchmarks during their pretraining phase. Since the pretraining datasets are rarely available, this paper proposes a statistical test to identify the presence of a benchmark in the pre-training dataset of a language model without accessing the model’s training data or weights. The intuition is the exchangeability of datasets— the order of examples in the dataset can be shuffled without affecting its joint distribution. If a language model shows a preference for any ordering of the dataset, it might have seen the data during pretraining. The test on the LLaMA-2 model identifies potential contamination in the MMLU benchmark, which is consistent with the results in the original LLaMA-2 report."
The idea of utilizing dataset exchangeability to identify test set contamination is novel and interesting.
"The proposed sharded likelihood comparison test addresses the tradeoff between statistical power and computational requirements of the permutation test, which is promising. The sharded rank comparison test also provides (asymptotic) guarantees on false positive rates."
"Experimental results are promising. A GPT-2 model is trained from scratch on standard pretraining data and known test sets to verify the efficiency of the proposed method in identifying test set contamination. The method is also tested with an existing model, LLaMA2, on the MMLU dataset, showing general agreement with the contamination study results."
"Although a more efficient sharded rank comparison test is proposed, the computational complexity is still considerable. For example, testing 49 files using 1000 permutations per shard can take 12 hours for LLaMA2."
There is no comparison with other baseline methods.
"The method relies on a strong assumption of data exchangeability, which may not hold in real-world datasets."
"If a dataset is not exchangeable, how effective is the method?"
"This paper studies the problem of identifying test set contamination in large language models, i.e., detecting that a test set is present in the pretraining data of a language model. The main idea behind the approach is that for test sets that have some canonical order of individual instances (e.g.: the order in which the dataset creators release the dataset), the likelihood of the test set in that order would be significantly higher than any random permutation of the dataset. Based on this idea, the paper proposes two versions of the test, one of which shards the test set and aggregates statistics over the shards to make the estimate more robust to potential biases in the model."
The tests are evaluated first by measuring their sensitivity when pretraining datasets are intentionally contaminated. It is shown that they are highly sensitive when the tests sets are large or have been duplicated enough in the pretraining data. The test is then used to measure contamination of the pretraining data used to train the Llama models and it is shown that the findings agree with prior reports.
"This is clearly written paper and makes a strong contribution. The tests do not require access to model weights or pretraining data, making them practically useful."
"The experiments do not compare the performance of the proposed tests to prior work. I understand that this work differs from say, the work from Carlini et al. in that this work focuses on set-level contamination, but how does aggregating instance-level statistics over a set compare?"
How does the performance of this method compare to that of prior work (see Weakness) - How sensitive is the proposed test to the model size?
The paper proposes a statistical test that given certain assumptions can indicate whether a black-box language model has been trained on certain datasets. This is a topic of increasing interest and importance given the prevalence of pretrained models that are trained on very large amounts of data. The authors first propose a simple permutation test and identify some weaknesses with it. They then propose a more sophisticated sharded test. The authors show 2 kinds of experiments: (1) They test on a dataset where they have injected a small amount of certain test sets to see if their approach can detect them.
(2) They apply their test to existing models such as Lllama-2 showing their approach can scale.
Topic of large importance in the community given the direction of the field.
Novel approach with thorough empirical results. I have some questions about the definition of test set contamination below.
Well written and interesting.
I have some questions about the definition of test set contamination below.
In Figure 1 the authors show test set contamination for BoolQ. But the examples there are unlabeled. Are the authors targeting unlabeled test set contamination i.e. the input is present in the pretraining data but not the label?
Would be great to have some justification and explanation of this setting.
"The paper studies the emergence of the in-context ability of the GPT-style transformer model trained using autoregressive loss and arithmetic modular datasets. It analyzes the influence of the number of tasks, number of in-context examples, model capacity, etc., on the ICL capability of an appropriately trained model (i.e., using early stopping). It also provides a persuasive “task decomposition hypothesis”, which is well supported by the ablation study and various experiments. The white-box analysis on the attention heads provides convincing evidence of the proposed explanation. Although there is a gap between the grokking settings (i.e., small model and toy dataset) and practical systems, the paper does a good job of explaining many important trends and concepts related to the emergence of compositional in-context ability. I enjoy reading this paper and suggest an acceptance."
The paper is easy to follow. Good presentation!
"The experiments are well-designed, providing compelling support for the claims."
The results in Figure 5 are cool.
"The skill decomposition discussed in section 5 is great. The clear pattern in attention heads verifies it very well. (The hypotheses could be further verified if the author can link the values of $c_1, c_2$ to some weights in the network, see the question part.)"
"The emergent ability (or grokking) usually refers to a phenomenon in the model “got stuck” in a non-generalization region and suddenly gained the generalization ability. Hence some discussion about the learning dynamics, i.e., how the accuracy, loss, representation, ability, attention pattern, etc., gradually evolve during training would make the paper stronger."
"The task and batch sample selection in this paper have many constraints (e.g., the rectangular rule, the balanced number of samples in each batch, etc.). However, the practical systems usually cannot strictly satisfy all these assumptions. Hence a more detailed analysis of how these assumptions influence the generalization ability would provide more insights to practical systems."
"The paper claims in line 147 that “As the o.o.d. performance increases, the pre-training performance simultaneously degrades “. However, it is hard to read this information from Figure 3-a panel 1. Maybe a different color mapping or adding numbers on these patches would be helpful."
"Equation 2 is a bit hard to understand. How does it correlate to $z = ax+by$ ? (Although, from the latter explanations, I know the model relies on $c_1z_1^t + c_2z_2^t$ to get $z$, but it might be helpful to claim how it is derived.) - Better to define $GF(p)$, i.e., the Galois field, before using it."
"Are the results in Figure 6 coming from $d=2$ or $d=4$? I can find the figure for all 8 attention heads for $d=2$ in the appendix, what about the $d=4$ case? It might be helpful to see if the pattern in later layers (i.e., attention focusing on different $z_i$) exists in shallow layers, and vice versa."
"In line 264, the paper claims that the pattern depends on $(a,b)$, but it is hard to read that from Figure 6b."
"As also mentioned in the strength part, is it possible to find some specific value in the weight space (e.g., attention weights, readout layers, etc.) that is highly correlated to $c_1, c_2$? If so, the hypothesis that the model first learns skill 2 (scale each example) and then skill 3 (weighted combine different examples) would be further verified."
"The OOD settings studied in grokking or emergent ability setting are quite related to the compositional generalization and systematic generalization. It would be helpful to discuss them in the related works, here are some of them: [1] Schott, Lukas, et al. ""Visual representation learning does not generalize strongly within the same domain."" ICLR 2022 [2] Xu, Zhenlin, Marc Niethammer, and Colin A. Raffel. ""Compositional generalization in unsupervised compositional representation learning: A study on disentanglement and emergent language."" NeurIPS 2022 [3] Ren, Yi, et al. ""Improving compositional generalization using iterated learning and simplicial embeddings."" NeurIPS 2023"
"The authors propose a synthetic sequence learning problem that I would call 'in-context modular regression', an elegant generalisation of prior work studying modular addition and in-context linear regression."
Using carefully constructed batches the authors are able to train transformer models to perform regression for a subset of tasks (weights) and a subset of inputs.
"The authors show that under some conditions on the data distribution and model architecture, the transformers not only achieve good performance on tasks and inputs included during training, but they also generalise to new tasks and/or new inputs. The authors document the conditions governing these generalisation capabilities in detail including showing phase plots and observing that in larger models, the generalisation properties are transient (they appear and then disappear across the training process)."
The authors postulate a breakdown of skills required to correctly perform the task. They effectively isolate and examine the abilities of their models to perform each component task. They also inspect the activations of each head and identify patterns suggestive of partial mechanisms underlying the generalising behaviour of the models.
I thank the authors for submitting their excellent work which stands to have a substantial impact in the science of deep learning. The work makes a meaningful contribution to an exceptionally important and interesting topic of the emergence of capabilities and internal mechanisms in deep learning.
"The setting and experiments neatly isolate and clearly demonstrate several interesting phenomena of emergence of capabilities and shifting in the solutions found by deep networks throughout training, contributing to the field's developing catalogue of examples of these phenomena."
"Moreover, the proposed synthetic problem is both rich and elegant. I expect this framework will become a fruitful test-best for follow-up work studying emergence phenomena, helping the field to improve our empirical and theoretical understanding of these phenomena."
The authors also offer a partial behavioural and mechanistic analysis which is a solid starting point for a more detailed understanding of the learned structures that emerge in this setting.
"While some elements of the analysis are complex, the authors have done an exceptional job of clearly presenting their findings. I feel careful study of each section and figure in the main text was rewarded since there was no question that occurred to me that was not addressed in the authors' clear descriptions or figures."
The authors have acknowledged all of the related work that I am aware of.
"I have not noticed any weaknesses in the paper that would temper my overall recommendation to accept. However, I note the following weaknesses, some of which the authors have already acknowledged, and others which they may like to take into consideration if they are interested to improve the paper further."
1. Delicate training set-up. The authors explain that training transformers on multiple modular addition tasks crucially relies on following a delicately balanced batch construction methodology.
"I am left wondering if this batch construction methodology, as a further departure from the standard language modelling setting, has any other implications for the learning process that may affect the generality of the results."
Note: This weakness is not decisive because the authors clearly document their training methodology and it's not that artificial anyway.
2. The mechanistic analysis is only partial. The authors admit that they have not been able to identify an end-to-end mechanistic model of how the trained transformers perform the task. This leaves their posited skill decomposition and partial mechanistic analysis open to the possibility that they are incomplete.
"Note: I think the contribution the authors have given in terms of the setting, the generalisation phenomena, and the partial skill decomposition and mechanistic analysis are already significant."
3. Relationship to prior work. The related work section does a good job of summarising the contributions of prior work in in-context linear regression and modular arithmetic in the context of transformer models.
"However, I feel that this section could be improved if the authors attempted to offer greater insight into the relationship between these prior works and the present work. For example, the authors have an opportunity here to informally describe the in-context linear regression and the modular addition problem settings that the newly proposed setting generalises."
"4. I noticed some minor text errors as follows, which I expect the authors can easily correct. Line 94: The notation $[1, p^2]$ to me suggests a closed continuous interval, whereas you appear to mean $\lbrace1, \ldots, p^2\rbrace$, also in some cases denoted $[p^2]$."
"It seems that equation 2 should read $\ldots = (z_1^t, z_2^2) \mod p$ and the equation on line 203 should read $c_1x + c_2y \mod p$. That is, $x$ and $y$ should swap places with $z_1^t$ and $z_2^t$. Is this indeed a mistake, or am I missing something?"
"In figure 6 (top row) there is a typo: ""Qeury"" on the vertical axis."
In line 445 there is a broken link.
I have not studied all appendices in detail.
"1. Why is the title 'learning to grok'? Is this meant in the sense that the grokking of a modular addition task is occurring in-context? If so, this seems a little inaccurate, since the phenomenon analogous to 'grokking' seems to still be occurring during pre-training."
"To be honest this part of the title has puzzled me since I first looked at the paper. Even if my understanding above is wrong and the title has an accurate interpretation, that I have failed to notice it might be one data point suggesting that if you are going for a title that is both short and informative, this might not be the right choice."
"2. In the figure 1 caption, is it possible to offer a clearer summary of the difference between in-distribution generalisation and out-of-distribution memorisation? On my first read through, treating the figure and caption as an overview of the work's main results, I had trouble distinguishing these two concepts."
This paper studies the emergence of in context learning and skill composition in autoregressive models. They create an algorithmic dataset to probe how autoregressive models use tasks learned during training to solve new tasks. They find that more training tasks lead to a generalizing / algorithmic approach instead of memorization.
"This work introduces a new algorithmic dataset (with modular arithmetic tasks) that force models to learn a variety of tasks. The work finds that when the number of tasks goes from small to large, the model transitions from memorization to generalization."
This work has many interesting experiments. I found Section 5.2 (Attention Heads Implement Essential Skills) pretty interesting.
The definition of task diversity is not well defined. Is the number of pretraining tasks truly indicative of task diversity? I think the paper could benefit from some justification of this assumption.
"The paper claims that for larger models, early stopping is necessary (line 52). While I appreciate that the authors used GPT-like architectures to reflect realistic settings, the architectures in the experiments are not that large. Even amongst popular open source models, the smallest are usually around 7B parameters."
Many works in the continual learning and meta learning literature suggest that training on multiple tasks at once leads to better generalization. Perhaps it is worth including brief discussion on the connections between this point and the model’s ability to generalize ood which is predominantly determined by the number of pre-training tasks.
"Since multiplication can be viewed as repeated addition, isn’t skill 2 an extension of skill 3 (or can even be viewed as skill 3 composed with itself multiple times)? Is hierarchy of skills important here?"
"This paper develops novel insights into in-context learning and how it works in Transformers. To this end, the authors propose a generalization of the modular arithmetic task explored in several prior works on grokking. Unlike those works, the structure of the defined task is more rich, enabling an analysis of both in-distribution generalization (standard test evaluation) and out-of-distribution generalization (which is itself broken down into two variants)."
The paper is fairly well written and clear. Going beyond the standard linear regression task to study ICL was great to see as well.
"The main selling point for me are the empirics though---I really like the results! The visualization of how the model represents concepts relevant to this paper's setup is quite beautiful: the circle of circles was fascinating to look at and, arguably, not something I expected. In retrospect, I can rationalize this as making sense---we get circular embeddings in grokking, so circle of circles is the logical geometrical extension here. Results on scaling are interesting in their own right as well."
"I do not have any major apprehensions, except for the related work, which I think is relatively sparse."
"Related Work. At this point, the topic this paper is focused on has a rather rich literature and I think a more detailed related work is warranted (perhaps in the appendix if space is an issue). For example, the results by Kirsch et al. (which is cited) are very similar to what authors show, especially results on scaling effects. The main different is width scaling in that paper and no geometric analysis, but nonetheless the relationship warranted more emphasis and discussion. Similarly, several recent works have explored OOD generalization of toy ICL tasks defined in prior works (e.g., see Ahuja and Lopez-Paz [1] for work on linear regression tasks and Ramesh et al. [2] for group arithmetic tasks). Regarding grokking, there are several works exploring the phase transition-y nature of this task. For example, see Kumar et al. [3]. The transient nature of ICL also has negative results (see Reddy [4]), which are worth discussion since they are the primary conclusion in depth scaling as I see it."
[1] [2] [3] [4]
A few questions below that I would like to see answered.
"PCA variance. Given this is a rather rich geometry in 2-D, I'm slightly surprised to see PCA captured it. Did you have to do some preprocessing? How much variance is explained by the two projected components? If there are other components that are not shown but have a large variance, what do those components encode---can you try 3D plots?"
"What does the MLP do? Given the mechinterp focused on attention solely, it is unclear what role MLPs played. Two experiments to try here are: (i) train attention only models to see if MLPs are even necessary, and (ii) perform the PCA analysis to uncover representations' geometry at the level of attentions and MLPs at each block in the model. Experiment (i) may require retraining models, so I understand if the authors are unable to conduct it, but my expectation will be that you will see that model ""internalizes"" task vectors and records them in MLPs. Attention only models can solve the task, but I expect the representations' geometry will be quite different. For experiment (ii) however, I expect that's easy to run and is merely repeating the plotting script on intermediate representations as a forward pass occurs through the model. If the geometry is primarily formed at attention layers, we'll see that in this experiment. vice versa, if it forms via MLPs, we'll see it explicitly."
"The paper introduces VAR, a novel autoregressive generative model for images that treats each scale in a multi-resolution feature pyramid as a token. Unlike traditional models that predict the next token from a rasterized grid, VAR predicts the next scale in a multi-resolution grid. This approach demonstrates greater scalability than next-token prediction and extends the well-known scaling laws from language modeling to image generation. Extensive experiments show that VAR outperforms both diffusion and AR baselines while offering improved efficiency in both training and inference."
The paper addresses the significant question of bridging the performance gap between autoregressive language models and autoregressive image generation. This makes the topic highly relevant for the community and potentially impactful.
"The method is well-motivated and utilizes well-known building blocks from LLMs. Hence, VAR is an important step toward showing that with a proper scheme, widely used LLM architectures can perform competitively in the Image domain."
"The experimental section is comprehensive, demonstrating VAR's performance and efficiency in image generation on ImageNet 256 and 512. Additionally, the authors provide in-depth discussion of scaling laws for VAR."
Ablation studies in Appendix D clearly illustrate the contribution of different aspects of VAR to the final model.
"While the writing of the paper is clear for the most part, the method section could benefit from better presentation. The authors provide some details on VAR tokenization and training, but I found section 3, especially section 3.2, slightly confusing. I suggest that the authors add more details and clarification on VAR's training process, the residual tokenization, and the workings of the transformer part. Currently, these details are somewhat obscured in Algorithm 1 and 2, and Figure 4."
"Some claims in the paper are slightly exaggerated. For example, in the abstract, the authors mention that VAR brings the FID from 18.65 to 1.73. While this is true, the FID of 18.65 belongs to a relatively weak baseline for AR models. It would be better to rewrite such claims in relation to more realistic baselines, such as RQ-VAE models, which are closer to VAR's methodology - The baselines used for the diffusion part are also relatively weak. For instance, MDTv2 [1] is a transformer-based diffusion model that achieves an FID of 1.58 on ImageNet 256. Therefore, it would be more appropriate to state that VAR performs ""competitively"" with diffusion models rather than significantly outperforms them."
"[1] Gao S, Zhou P, Cheng MM, Yan S. MDTv2: Masked Diffusion Transformer is a Strong Image Synthesizer. arXiv preprint arXiv:2303.14389. 2023 Mar 25."
"1. Does VAR perform next-scale prediction completely in the latent space? For example, if the image resolution is 256 and VQ-VAE has a latent size of 32, does VAR operate on resolutions up to 32, or does it extend up to the image resolution?"
"2. Is there a loss term missing in equation (5)? VQ-GAN models typically include a commitment loss and a codebook loss, but it appears VAR's quantizer only uses one of these losses. Could you provide more details on this part?"
"3. If I understand correctly, when predicting the probabilities for each scale with a transformer, the transformer now needs to estimate a much higher-dimensional distribution (a distribution over the entire grid instead of just one point in the grid). Can you provide some intuition on why the transformer part can handle this prediction task effectively? Is it due to the strong conditioning from the lower scales?"
"4. Are the features computed by the VAR quantizer also useful for image recognition tasks, or do they perform best in image generation?"
"The paper introduces Visual AutoRegressive modeling (VAR), which uses a coarse-to-fine approach for image generation. VAR drastically improves performance, reducing FID from 18.65 to 1.73 and increasing IS from 80.4 to 350.2, with 20x faster inference. It outperforms diffusion transformers in quality, speed, efficiency, and scalability. VAR models show scaling laws like large language models and demonstrate zero-shot generalization in image editing tasks."
Solid Motivation: The scale in vision signals is a natural choice for autoregressive generation. The exploration of autoregression in visual generation is indeed a worthy topic.
Novel Method: This work is the first to explore a visual generative framework using a multi-scale autoregressive paradigm with next-scale prediction.
"Strong Performance: The paper demonstrates significant advancements in visual autoregressive model performance, with GPT-style autoregressive methods surpassing strong diffusion models in image synthesis for the first time."
Promising Scaling Law: The paper presents a promising scaling law for the proposed visual autoregressive modeling paradigm.
"Lack of Ablation Study on VQVAE: There is no ablation study on the newly proposed VQ-VAE model. In Table 3, the performance differences between the first two rows cannot be solely attributed to the model change from AR to VAR, as the VQ-VAE model has also been modified."
Resolution Flexibility: The resolutions for VAR generation appear to be pre-defined and bound to the VQ-VAE model during its pre-training. Adjusting the number of resolutions or maximum resolution without re-training the VQ-VAE model seems non-trivial.
"About VQVAE - What is the rFID of your VQVAE? Can you provide a table comparing your VQVAE and VQVAEs of other works (e.g., VQGAN, MaskGIT) - What is the pre-training cost of your VQVAE?"
"If residual quantization is not used, and instead a multi-scale token map is constructed directly by some ways like: 1) Independently downsample the VAE encoder features to multiple scales, then quantize each scale directly, or 2) Construct multiple resolution ImageNet datasets (e.g., ImageNet 16x16, 32x32…, 256x256) and independently apply vector quantization to each dataset, thereby obtaining low-resolution to high-resolution token maps.Then, can the proposed visual autoregressive modeling still be performed? In other words, is residual quantization an indispensable part of VAR modeling? (Ignoring the efficiency or complexity of these alternative tokenization methods) - What is the impact on performance if residual quantization is not used?"
"Question on Figure 7 The apples-to-apples qualitative comparison, like in Figure 7, is common in diffusion-based models because the initial noise is of the same resolution as the final output, allowing significant control over the final image when coupled with deterministic sampling. However, in VAR, the counterpart to the ""initial noise"" is only the ""teacher-forced initial tokens,"" which, if I understand correctly, are of 1x1 size. This suggests only a very loose control over the final image. Given this, why doesn't this result in a situation similar to the ""butterfly effect,"" where identical initial states and random seeds, due to different model configurations, lead to significantly different final outputs after multiple generation iterations?"
"Question on zero-shot generation - Is only the last resolution of the token map masked and then teacher-forced to generate the masked regions, or is each token map masked?"
"The model hasn't explicitly learned inter-token dependence at the spatial level during training. Could you provide an explanation of why the model can perform zero-shot editing/inpainting, which requires the model to condition on tokens at some spatial positions to generate tokens at other positions?"
"This work proposes a novel approach to image generation using an autoregressive decoder-only transformer model. Rather than decoding in a raster-scan their approach (VAR) decodes scales/resolutions conditioned on previously generated scales, reminiscent of traditional scale pyramids in computer vision. VAR demonstrates competitive performance on ImageNet in terms of generation quality, diversity and inference speed. It also demonstrates scaling laws up to 2.0B parameters."
"I am very impressed by the proposed method. It is simple, intuitive and novel. It is not difficult to see why it works well."
"The empirical results are strong, and since the approach is based on a decoder-only transformer (a tried and tested architecture for autoregressive generation) I would expect it to scale to foundation-model T2I systems easily."
"The paper is well written and easy to read, with the method/motivation/story communicated clearly to the reader."
"As the reviewing burden has been heavy for this conference (6 papers) please understand that I can only dedicate so much time to this paper. Thus, I may have made mistakes in my understanding of the paper, and I welcome the authors to correct me if this is the case."
"1. The reported inference efficiency is a bit disingenuous. The comparison with DiT uses 250 steps, which is much more than what SotA samplers require. Moreover, diffusion models can be distilled into 1-4 step models that are even faster. Compared to raster scan autoregressive models, the improvement in latency seems to be due to better use of parallel resources. However, for larger batches/measuring throughput this advantage may fade."
"2. There are some missing details and insight, especially with regards to the multi-scale VQVAE. There are also details that are present in the code that really should be in the paper, such as the number of tokens per image/scale. What is its reconstruction performance of the VQVAE (compared to e.g. the SDXL VAE)? How does one choose the number of scales? How many codes end up being used over the different scales and do different scales capture similar (spatial) information in the latent space vs the image space? It would also be great to see an ablation like Table 3 for the VQVAE. Also, the code provided doesn't give details to reproduce the VQVAE, only the transformer."
"3. The use of ""zero-shot"" to refer to the model's editing ability is different in nature to zero-shot generalisation in LLMs, so I find the link made in the paper to be a little disingenuous. Moreover, the editing performance doesn't seem to be very strong, with inpainting generation spilling outside of the box. The paper also doesn't give details on how the editing is performed."
1. I'd like to see a throughput comparison in img/s as the batch size is increased. I'd also like to see some DiT results where a more advanced sampler such as DPM-solver or SA-solver is used.
2. See above.
"3. Concretely, how is the editing performed? Are certain tokens teacher-forced? If so starting from which scale?"
"This paper introduces next-scale prediction autoregressive models that satisfy mathematical premises (unidirectional dependency of autoregressive model) and preserve the 2D spatial locality. The core method is to develop multiscale VQ-VAE. The proposed method is more efficient than the traditional autoregressive model, requiring only $O(n^4)$ compared to $O(n^6)$ of the raster autoregressive model. Furthermore, this method is proven to follow scaling laws of LLM which guarantee better performance when scaling up the training process."
1. This idea is natural and novel. It successfully solves mathematical premises violation of previous rastering scan autoregressive model.
2. The power-law scaling law is interesting and encourages follow-up work to scale up models for better performance.
3. The method’s performance is competitive to the diffusion model and other generative models.
4. Most of the paper claims seem valid to me.
1. The second stage of training VAR transformers is too short and is hard for me to fully understand how it works. I wonder about the details of how to generate $h_w \times w_h$ tokens in $r_k$ parallel using k-th position embedding map. Is the embedding 1D or 2D embedding ?. How to make sure all tokens in $r_k$ are correlated to each other ?
"2. The highest resolution of the scale $r_K$ is $16 \times 16$ and there are 10 scales (1,2,3,4,5,6,8,10,13,16). I wonder if there is any motivation to choose these scales."
"3. I think the paper should include the sampling algorithm of autoregressive model with hyper-parameter details such as temperature, top-k, top-p and CFG sampling."
4. The zero-shot generalisation algorithm should be included in the paper for clarity.
My main concerns are in the method section. I hope the author could provide more method details. See the weakness above.
