text,index,review_action,target
The paper explores and experiments on extrapolating attributes of images produced by GANs by manipulating their representations in latent space.,,arg_structuring,0
Attribute manipulation is done by predicting latent space walks (linear or non-linear) and is learned in a self-supervised way by using augmented outputs of a pretrained GAN as target images.,,arg_structuring,0
Authors experimentally show dependence of range of possible attribute manipulations on the diversity of the dataset in terms of that attribute as well as propose techniques to improve it.,,arg_structuring,0
Suggested concepts are explained in a clear way with extensive experiments confirming the findings.,,arg_structuring,0
"Techniques proposed for improving ""steerability"" of GANs are backed up by both qualitative and quantitative analysis, although missing experiments on more sophisticated datasets than MNIST.",,arg_structuring,0
"Overall,  I recommend to accept this paper.",,arg_social,0
Several questions I would like the authors to address to make some details more clear and the paper more complete:,,arg_structuring,0
"1. Why the color distribution of generated images is evaluated on a sampled subset of pixels, not full images?",,arg_request,1
"(""Quantifying steerability"" section.)",,none,0
"2. On Figure 6, which classes are outlying on transformation limitation / data variability plots (bottom-right corner) and how it may be explained?",,arg_request,1
"3. While StyleGAN can not preserve geometry of objects for shift in location-based attributes, when walks are learned in the W space, have you experimented on manipulating those attributes with z space? What are the results?",,arg_request,1
Other minor flaws include,,arg_structuring,0
1. Pictures in Fig. 2 are mixed up between G(z) and G(z + \alpha w),,arg_request,1
"2. In Fig. 2 edit(G(z, \alpha)) -> edit(G(z), \alpha))",,arg_request,1
3. In eq. (2) f^n(z) -> G(f^n(z)),,arg_request,1
4. In eq. (6) +\alpha^* -> -\alpha^*,,arg_request,1
This paper proposes a WAE variant based on a new statistical distance between the encoded data distribution and the latent prior distribution that can be computed in closed form without drawing samples from the prior (but only when it is Gaussian).,,arg_structuring,0
"The primary contribution is the new CW statistical distance, which is the l2 distance between projected distributions, integrated over all possible projections (although not calculated as so in practice).",,arg_structuring,0
"Plugging this distance into the WAE produces similar performance to existing WAE variants, but does not really advance the existing achievable performance.",,arg_evaluative,0
"Overall, I quite liked the paper and think it is well-written, but I believe the authors need to highlight at least one practical advance introduced by the CW distance (in which case I will raise my score).",,arg_request,1
Some potential options include:,,arg_structuring,0
1) Faster training times.,,arg_structuring,0
It seems to me one potential advantage of the closed-form distance would be that the stochastic WAE-optimization can converge faster (due to lower-variance gradients).,,arg_fact,0
"However, the authors only presented per-batch processing times as opposed to overall training time for these models.",,arg_evaluative,0
2) Stabler training.,,arg_structuring,0
Perhaps sampling from the prior (as needed to compute statistical distances in the other WAE variants) introduces undesirable extra variance in the training procedure.,,arg_fact,0
The authors could run each WAE training process K times (with random initialization) to see if the closed-form distance enables more stable results.,,arg_request,1
3) Usefulness of the CW distance outside of the autoencoder context.,,arg_structuring,0
"Since the novelty of this work lies in the introduction of the CW distance, I would like to see an independent evaluation of this distance as a  general statistical distance measure (independently of its use in CWAE).",,arg_request,1
Can you use this distance as a multivariate-Gaussian goodness of fit measure for high-dimensional data drawn from both Gaussian and non-Gaussian distributions and show that it actually outperforms other standard statistical distances (e.g. in two-sample testing power)?,,arg_request,1
"Without demonstrating any practical advance, this work becomes simply another one of the multitude of V/W-AE-variants that already exist.",,arg_evaluative,0
Other Comments:,,arg_structuring,0
"- While I agree that standard WAE-MMD and SWAE require some form of sampling to compute their respective statistical distance, a variant of WAE-MMD could be converted to a closed form statistical distance in the case of a Gaussian prior, by way of Stein's method or other existing goodness-of-fit measures designed specifically for Gaussians.",,arg_fact,0
See for example:,,arg_structuring,0
Chwialkowski et al: https://arxiv.org/pdf/1602.02964.pdf,,arg_other,0
which like CW-distance is also a quadratic-time closed-form distance between samples and a target density.,,arg_fact,0
"Besides having closed form in the case of a Gaussian prior (which other statistical distances could potentially also achieve), it would be nice to see some discussion of why the authors believe their CW-distance is conceptually superior to such alternatives.",,arg_request,1
- Silverman's rule of thumb is only asymptotically optimal when the underlying data-generating distribution itself is Gaussian. Perhaps you can argue here that due to CLT: the projected data (for high-dimensional latent spaces) should look approximately Gaussian?,,arg_request,1
After reading the revision: I have raised my score by 1 point and recommend acceptance.,,arg_social,0
This in an interesting paper as it tries to alleviate the burden of hyper-parameters tuning for exploration strategies Deep Reinforcement learning.,,arg_structuring,0
The paper proposes an adaptive behaviour in order to shape the data generation process for effective learning.,,arg_structuring,0
"The paper considers a behaviour policy that is parametrized by a set of variables z called modulations: for example the Boltzmann softmax temperature, the probability epsilon for epsilon-greedy, per-action biases, ..",,arg_structuring,0
The author frame the modulations search into a non-stationary multi-armed bandit problem and proposes to adapt the modulations according to a proxy to the learning progress.,,arg_structuring,0
The author provides thorough experimental results.,,arg_evaluative,0
Comments:,,arg_structuring,0
- All the variations considered for the behaviour policy performs only myopic exploration and thus provably inefficient in RL.,,arg_evaluative,0
- The proposed proxy is simply the empirical episodic return.,,arg_fact,0
It is not well explained in the paper how this proxy correlates with the Learning progress criteria.,,arg_evaluative,0
- The proxy seems to encourage selecting modulations that lead to generate most rewarding trajectories.,,arg_fact,0
How this proxy incentives the agent to explore poorly-understood regions?,,arg_request,1
"In other terms, how this proxy help to tradeoff between exploration and exploitation ?",,none,0
-  The modulation adaptation problem is framed into non-stationary multi-armed bandit problem but the authors present a heuristic to solve it instead of using provably efficient bandit algorithm such as exponential weight methods (Besbes et al 2014) or Thompson sampling (Raj & Kalyani 2017) cited in the paper.,,arg_fact,0
- The way the authors adapt the modulation z (or at least its description in the paper) seems not technically sounded for me.,,arg_evaluative,0
They estimate a certain probability at time step t by empirical frequency based on data from previous time steps.,,none,0
"But as the parameters change during the learning, the f_t’(z) at time t’ < t is not distributed as f_t(z).",,none,0
This introduces a biases in the estimate.,,none,0
- I appreciate the thorough empirical results and ablation studies in the main paper and the appendix. They are really interesting.,,arg_evaluative,0
- I am confused what is the fixed reference in Figure 6. It is not explained in the main paper.,,arg_evaluative,0
Is it a baseline with the best hyperprameters in hindsight?,,arg_request,1
"-  From the plots of learning curves in appendix, the proposed methods doesn’t seem to show a huge boost of performance comparing to the uniform bandit. Could you show aggregated comparison between the proposed method and uniform bandit similarly to what is done in Figure 4 ?",,arg_evaluative,0
This paper studies the problem of how to design generative networks for auditory signals in order to capture natural signal priors.,,arg_structuring,0
"Compared to state-of-art methods in images [Lempitsky et al., 2018], this problem is not so easy on audio signals.",,arg_structuring,0
Existing work [Michelashvili &Wolf] trains generative networks to model signal-to-noise ratio rather than the signal itself.,,arg_structuring,0
This paper proposes a new convolutional operator called Harmonic Convolution to improve these generative networks to model both signals or signal-to-noise ratio.,,arg_structuring,0
Applications on audio restoration and source separation are given.,,arg_structuring,0
The paper starts to show that an existing generative network Wave-U-Net does not capture audio signal priors.,,arg_structuring,0
The explanation in Fig 2 on why this is the case seem to me not so clear. Are you trying to show that the Wave-U-Net does not work since there is no 1/f^2 law for clean audio signals?,,arg_request,1
"The Harmonic Convolution is similar to deformable convolutions, but specifically designed to capture audio harmonics.",,arg_structuring,0
It is further combined with the idea of anchors and mixing to capture fractional frequencies.,,arg_structuring,0
The explanation of this section is slightly unclear.,,arg_evaluative,0
"There is a little typo in Formula 1 for the STFT spectrogram, I would use the modulus |.| rather than || . ||.",,arg_request,1
Is Harmonic Convolution applicable to complex STFT coefficients as well?,,arg_request,1
It seems to be yes based on Section 4.2.,,arg_fact,0
If so it would be better to define the operator in a more general notation.,,arg_request,1
Numerical experiments show that the Harmonic Convolution improves over existing regular and dilated convolutions in various settings.,,arg_fact,0
Section 4.2 aims to fit the complex STFT coefficients of corrupted signals.,,arg_fact,0
"However, the setting is less clear to me for both the unsupervised speech/music restoration and supervised source separation problems.",,arg_evaluative,0
"In Section 4.3 and 4.4, is the x_0 (defined in Section 2.1) complex-valued STFT coefficients or something else?",,arg_request,1
It seems to me x_0 = ratio mask in Section 4.4.,,arg_fact,0
"What is the L1 loss defined in Section 4.4? To obtain the final separated audio waveform, an inverse STFT is applied on what?",,arg_request,1
These details can be written in supplementary material if more space is needed.,,arg_request,1
"After all, the numerical results seem to me encouraging.",,arg_evaluative,0
Summary:,,arg_structuring,0
This paper proposes a functional form to model the dependence of generalization error on a held-out test set on model and dataset size.,,arg_structuring,0
"The functional form is derived based on empirical observations of the generalizing error for various model and dataset sizes (sections O1, O2, and O3) and on certain necessary criteria (C1, C4 and C5).",,arg_structuring,0
The parameters of the function are then fit using linear regression on observed data.,,arg_structuring,0
"The authors show that the regressed function \(\epsilon(m,n)\) is able to predict the generalization error for various \(m\) and \(n\) reasonably accurately.",,arg_structuring,0
Major Points:,,arg_structuring,0
"- While the current experiments are a good start, I do not think they are extensive enough to count as strong evidence for the  power-law form of \(\epsilon(m,n)\).",,arg_evaluative,0
"I would ideally like to see results on more optimizers, at the very least for Adam, even if for fixed hyper-parameters.",,arg_request,1
"As far as I understand, this involves only minor changes in the code since reasonable hyperparameters required for the convergence of Adam have been extensively studied.",,arg_request,1
If the form still holds true then the results from this work can be more reliably used for small-scale network development and in making trade-off choices (as discussed in section 8).,,arg_evaluative,0
"- Given the current form of the paper, the abstract and introduction should be modified to reflect the fact that only limited architectures and optimizers were experimented with, and the claims of the paper are not experimentally validated in general.",,arg_request,1
Minor Points:,,arg_structuring,0
- It would be nice if more network architectures were analysed (such as VGG and DenseNets).,,arg_request,1
- It would be nice if different stopping criteria were analysed.,,arg_request,1
- It would greatly benefit the reader if eq. 5 were expanded.,,arg_request,1
"Overall, I think this is a well written paper and provides good insight into the behaviour of the error landscape as a function of model and dataset size.",,arg_evaluative,0
The paper’s primary drawback is the restrictive setting under which the experiments are performed.,,arg_evaluative,0
"Therefore, I am not convinced that the power-law form of the generalization error would hold when the experimental settings are marginally different (like when using the Adam optimizer or a VGG-like architecture).",,arg_evaluative,0
I think this work would have much greater impact if the authors can show that the power-law form holds for a larger variety of architectures and optimizers thus allowing researchers to more confidently incorporate the results of this work into the design and training deep neural networks.,,arg_request,1
Rebuttal Response,,arg_structuring,0
I would like to thank the authors for their response.,,arg_social,0
The results of additional experiments as described in Section 6.2 and in Figure 5 do indeed provide stronger evidence of the power-law form of the error function.,,arg_evaluative,0
"In light of this, I have changed my original rating.",,arg_social,0
The idea of image classification based on patch-level deep feature in the BoF model has been studied extensively.,,arg_structuring,0
Just list few of them:,,arg_structuring,0
"Wei et al. HCP: A Flexible CNN Framework for Multi-label Image Classification, IEEE TPAMI 2016",,arg_other,0
"Tang et al. Deep Patch Learning for Weakly Supervised Object Classification and Discovery, Pattern Recognition 2017",,arg_other,0
"Tang et al. Deep FisherNet for Object Classification, IEEE TNNLS",,arg_other,0
"Arandjelović et al. NetVLAD: CNN Architecture for Weakly Supervised Place Recognition, CVPR 2016",,arg_other,0
The above papers are not cited in this paper.,,arg_request,1
There are some unique points.,,arg_evaluative,0
This work does not use RoIPooling layer and has results on ImageNet.,,arg_fact,0
"But, the previous works use RoIPooling layer to save computations and works on scene understanding images, such as PASCAL.",,arg_fact,0
"Besides, the paper uses the smallest patch among all the patch-based deep networks.",,arg_fact,0
It is interesting.,,arg_fact,0
I highly encourage the authors to finetune the ImageNet pre-trained BagNet on PASCAL VOC and compare to the previous patch-based deep networks.,,arg_request,1
The paper presents a novel hierarchical clustering method over an embedding space.,,arg_structuring,0
"In the presented approach, both the embedding space and the hierarchical clustering are simultaneously learnt.",,arg_structuring,0
The hierarchical clustering algorithm aims to recover complex clustering hierarchies which cannot be captured by previously proposed methods.,,arg_structuring,0
"The paper address a relevant problem, which is of great interest for extracting knowledge from data.",,arg_evaluative,0
"In general, the quality of the paper is high.",,arg_evaluative,0
The presented approach is based on a sound formalization of hierarchical clustering and deep generative models.,,arg_fact,0
The paper is easy to follow in spite of the technical difficulty.,,arg_evaluative,0
The experimental evaluation is really extensive.,,arg_evaluative,0
It compares against many state-of-the-art methods. And the results are promising from both a quantitative and qualitative point view.,,arg_evaluative,0
"The only issue with this paper is its degree of novelty, which is narrow.",,arg_evaluative,0
"The proposed method adapt a previously presented hierarchical clustering method in the ""standard space"" (Griffiths et al., 2004) to an embedding space defined by a variational autoencoder model.",,arg_evaluative,0
"The inference algorithm builds on standard techniques of deep generative models and, also, on previously proposed methods (Wand and Blei, 2003) for dealing with the complex hierarchical priors involved in this kind of models.",,arg_evaluative,0
Summary:,,arg_structuring,0
The paper proposes to add to the original GAN (2014) loss a zero-centered gradient penalty as the one defined in the WGAN-GP paper.,,arg_structuring,0
It also provides an analysis on the mode collapse and lack of stability of classical GANs.,,arg_structuring,0
The authors compare results using their penalty on a few synthetic examples and on image net dogs generations to results using the classical GAN loss with or without gradient penalties.,,arg_structuring,0
Positive points:,,arg_structuring,0
The paper is interesting to read and well illustrated.,,arg_evaluative,0
An experiment on imagenet illustrates the progress that can be achieved by the proposed penalty.,,arg_evaluative,0
Points to improve:,,arg_structuring,0
"If I understood correctly, the main contribution resides in the application of the GP proposed by WGAN-GP to the original setting.",,arg_fact,0
"Why not compare results to WGAN-GP in this case? Since the proposal of GANs, many papers addressed the mode collapse problem.",,arg_evaluative,0
"WGAN-GP, VEEGAN, or Lucas et al arXiv:1806.07185, ICML 2018",,arg_other,0
to name only a few.,,none,0
"The related work section looks incomplete with some missing related references as mentioned above, and copy of a segment that appears in the introduction.",,arg_evaluative,0
The submission could maybe improved by segmenting the work into intro / related / background (with clear equations presenting the existing GP) / analysis / approach / experiments,,arg_evaluative,0
"The experiments on synthetic data could be improved: for reproducibility, many works on GANs used the same synthetic data as VEEGAN.",,arg_evaluative,0
The imagenet experiment lacks details.,,arg_evaluative,0
Summary,,arg_structuring,0
This paper provides an interesting application of GAN which can generate the outlier distribution of training data which forces generator to learn the distribution of the low probability density area of given data.,,arg_structuring,0
"To show the effectiveness of the method, the author intuitively shows how it works on 2-D points data as well as the reconstructed Mnist dataset.",,arg_structuring,0
"Additionally, this approach reaches a comparable performance on semi-supervised learning and novelty detection task.",,arg_structuring,0
Paper Strengths,,arg_structuring,0
"1. The idea of this paper is novel, and the implementation of this method is easily interacted with any GAN model.",,arg_evaluative,0
"Also, due to its concise structure compared to the existing method, it saves more computational memory and is time efficiency.",,arg_evaluative,0
Paper Weaknesses,,arg_structuring,0
"1. Experimental settings are clear, however, what makes me confused is that the construction for p_{\bar{d}} is straightforward for simple distribution like 2D points dataset, however, it might be intractable for complex high dimensional data such as images.",,arg_evaluative,0
"2. The model seems to be sensitive to the hyper-parameter \alpha, is this parameter always fixed at 0.5 or needed to fine-tune for different datasets?",,arg_request,1
- Summary,,arg_structuring,0
This paper proposes a multi-objective evolutionary algorithm for the neural architecture search.,,arg_structuring,0
"Specifically, this paper employs a Lamarckian inheritance mechanism based on network morphism operations for speeding up the architecture search.",,arg_structuring,0
The proposed method is evaluated on CIFAR-10 and ImageNet (64*64) datasets and compared with recent neural architecture search methods.,,arg_structuring,0
"In this paper, the proposed method aims at solving the multi-objective problem: validation error rate as a first objective and the number of parameters in a network as a second objective.",,arg_structuring,0
- Pros,,arg_structuring,0
- The proposed method does not require to be initialized with well-performing architectures.,,arg_evaluative,0
"- This paper proposes the approximate network morphisms to reduce the capacity of a network (e.g., removing a layer), which is reasonable property to control the size of a network for multi-objective problems.",,arg_evaluative,0
- Cons,,arg_structuring,0
"- Judging from Table 1, the proposed method does not seem to provide a large contribution.",,arg_evaluative,0
"For example, while the proposed method introduced the regularization about the number of parameters to the optimization, NASNet V2 and ENAS outperform the proposed method in terms of the accuracy and the number of parameters.",,none,0
"- It would be better to provide the details of the procedure of the proposed method (e.g., Algorithm 1 and each processing of Algorithm 1) in the paper, not in the Appendix.",,arg_evaluative,0
"- In the case of the search space II, how many GPU days does the proposed method require?",,arg_request,1
"- About line 10 in Algorithm 1, how does the proposed method update the population P? Please elaborate on this procedure.",,arg_request,1
The paper introduces a new approach to combine small RBMs that are pretrained in order to obtain a large RBM with good performance.,,arg_structuring,0
This will bypass the need of training large RBMs and suggests to break them into smaller ones.,,arg_structuring,0
"The paper then provides experimental evidence by applying the method on ""invertible boolean logic"".",,arg_structuring,0
MCMC is used to find the the solution to large RBM and compare it against the combined solutions of smaller RBMs.,,arg_structuring,0
"The paper motivates the problem well, however, it is not well-written and at times it is hard to follow.",,arg_structuring,0
The details of the approach is not entirely clear and no theoritcal results are provided to support the approach.,,arg_evaluative,0
"For instance, in the introduced approach, only an example of combination is provided in Figure 1.",,none,0
It is not clear how smaller RBMs (and their associated parameters) are combined to obtain the larger RBM model.,,arg_evaluative,0
"From the experimental perspective, the experimental evidence on ""invertible boolean logic"" does not seem to be very convincing for validating the approach.",,arg_evaluative,0
"Additionally, the details of the settings of the experiments are not fully discussed.",,arg_evaluative,0
"For example, what are the atomic/smaller problems and associated RBMs? what is the larger problem and how is the corresponding RBM obtained?",,none,0
"Overall, the paper seems to be a report consisting of a few interesting observations rather than introducing a solid and novel contribution with theoretical guarantees.",,arg_evaluative,0
Remark:,,arg_structuring,0
"The term ""Combinatorial optimization"", which is used in the title and throughout the body of paper, sounds a bit confusing to the reviwer.",,arg_evaluative,0
This term is typically used in other contexts.,,none,0
Typos:,,arg_structuring,0
*,,none,0
"* Page 2 -- Paragraph 2: ""Therefore, methods than can exploit...""",,arg_request,1
*,,none,0
* Page 3 -- 2nd line of math: Super-scripts are missing for some entries of the matrices W^A and W^{A+B},,arg_request,1
*,,none,0
"* Page 5 -- Last paragraph: ""...merged logical units is more likly to get get stuck in a ...""",,arg_request,1
*,,none,0
"* Page 5 -- Last paragraph: ""...and combining their distributions using the mulistart heuristic...""",,arg_request,1
This paper presents a new approach in network quantization.,,arg_structuring,0
"The key insights of this paper is quantizing different layers with different bit-widths, instead of using fixed 32-bit width for all layer weights and activation in previous works.",,arg_structuring,0
"At the same time, this paper adopted the idea form both DARTS and ENAS with parameter sharing, and introduces a new differentiable neural architecture search framework.",,arg_structuring,0
"As the authors proposed, this DNAS framework is able to search efficiently and effective through a large search space.",,arg_evaluative,0
"As demonstrated in the Experiment section of the paper, it achieves better validation accuracy than ResNet with much smaller model size and lower computational cost.",,arg_evaluative,0
1. An improved gradient method in updating the network architecture and parameters,,arg_evaluative,0
compared to DARTS and ENAS.,,none,0
It applies the Gumbel softmax to refine the sub-graph structure without training the entire super-net through the whole process.,,arg_evaluative,0
The work is able to obtain the same level of validation accuracy on Cifar-10 as ResNet while reduce the model parameters by a large margin.,,arg_evaluative,0
2. The work is in the middle ground of two previous works: ENAS by Pham et al. (2018) and DARTS by Liu et al. (2018).,,arg_evaluative,0
"However, there is no comparison with ENAS and DARTS in experiments.",,arg_evaluative,0
ENAS samples child networks from the super net to be trained independently while DARTS trains the entire super net together without decoupling child networks from the super net.,,arg_evaluative,0
"By using Gumbel Softmax with an annealing temperature, The proposed DNAS pipeline behaves more like DARTS at the beginning of the search and behaves more like ENAS at the end.",,arg_evaluative,0
This work considers a version of importance sampling of states from the replay buffer.,,arg_structuring,0
"Each trajectory is assigned a rank, inversely proportional to its probability according to a GMM.",,arg_structuring,0
The trajectories with lower rank are preferred at sampling.,,arg_structuring,0
Main issues:,,arg_structuring,0
1. Estimating rank from a density estimator,,arg_structuring,0
- the reasoning behind picking VGMM as the density estimator is not fully convincing and (dis)advantages of other candidate density estimators are almost not highlighted.,,arg_evaluative,0
- it is unclear and possibly could be better explained why one needs to concatenate the goals (what would change if we would not concatenate but estimate state densities rather than trajectories?),,arg_request,1
2. Generalization issues,,arg_structuring,0
- the method is not applicable to episodes of different length,,arg_evaluative,0
- the approach assumes existence of a state to goal function f(s)->g,,arg_structuring,0
- although the paper does not expose this point (it is discussed the HER paper),,arg_evaluative,0
3. Scaling issues,,arg_structuring,0
- length of the vector grows linearly with the episode length,,arg_structuring,0
- length of the vector grows linearly with the size of the goal vector,,arg_structuring,0
For long episodes or episodes with large goal vectors it is quite possible that there will not be enough data to fit the GMM model or one would need to collect many samples prior.,,arg_evaluative,0
4. Minor issues,,arg_structuring,0
"- 3.3 ""It is known that PER can become very expensive in computational time"" - please supply a reference",,arg_request,1
"- 3.3 ""After each update of the model, the agent needs to update the priorities of the transitions in the replay buffer with the new TD-errors"" - However the method only renews priorities of randomly selected transitions (why would there be a large overhead?).",,arg_request,1
"Here is from the PER paper ""Our final implementation for rank-based prioritization produced an additional 2%-4% increase in running time and negligible additional memory usage""",,arg_fact,0
"The proposed method tackles class-incremental continual learning, where new categories are incrementally exposed to the network but a classifier across all categories must be learned.",,arg_fact,0
"The proposed method seems to be essentially a combination of generative replay (e.g. Deep Generative Replay) with AC-GAN as the model and attention (HAT), along with a growing mechanism to support saturating capacity.",,arg_fact,0
Quantitative results are shown on MNIST and SVHN while some analysis is provided on CIFAR.,,arg_fact,0
Pros,,arg_structuring,0
"+ The method combines the existing works in a way that makes sense, specifically AC-GAN to support a single generator network with attention-based methods to prevent forgetting in the generator.",,arg_evaluative,0
"+ The method results in good performance, although see caveats below.",,arg_evaluative,0
+ Analysis of the evolution of mask values over time is interesting.,,arg_evaluative,0
Cons,,arg_structuring,0
- The method is very confusingly presented and requires both knowledge of HAT as well as more than one reading to understand.,,arg_evaluative,0
"The fact that HAT-like masks are used for a generative replay approach is clear, but the actual mechanism of ""growing capacity"" is not made clear at all especially in the beginning of the paper.",,arg_evaluative,0
"Further the contributions are not clear at all, since a large part of the detailed approach/equations relate to the masking which was taken from previous work.",,arg_evaluative,0
The authors should on the claimed contributions.,,arg_request,1
Is it a combination of DGR and HAT with some capacity expansion?,,arg_request,1
- It is not clear whether pushing the catastrophic forgetting problem into the generator is the best approach.,,arg_evaluative,0
"Clearly, replaying data accurately from all tasks will work well, but why is it harder to guard against the generative forgetting problem than the discriminative one?",,arg_request,1
- The approach also seems to add a lot of complexity and heuristics/hyper-parameters.,,arg_evaluative,0
It also adds capacity and it is not at all made clear whether the comparison is fair since no analysis on number of parameters are shown.,,arg_evaluative,0
"- Relatedly, better baselines should be used; for example, if the memory used by the generative model is merely put to storing randomly chosen instances from the tasks, how will the results compare? Clearly storing instances bypasses the forgetting problem completely (as memory size approaches the dataset size it turns into the joint problem) and it's not clear how many instances are really needed per task, especially for these simpler problems.",,arg_request,1
"As such, I find it surprising that simply storing instances would do as poorly as stated in this paper which says cannot provide enough diversity.",,arg_evaluative,0
"It also seems strange to say that storing instances ""violates the strictly incremental setup"" while generative models do not.",,arg_evaluative,0
"Obviously there is a tradeoff in terms of memory usage, privacy, performance, etc. but since none of these methods currently achieve the best across all of these there is no reason to rule out any of the methods.",,arg_evaluative,0
Otherwise you are just defining the problem in a way that excludes other simple approaches which work.,,arg_evaluative,0
"- There are several methodological issues: Why are CIFAR results not shown in a table as is done for the other dataset? How many times were the experiments run and what were the variances? How many parameters are used (since capacity can increase?) It is for example not clear that the comparison to joint training is fair, when stating: ""Interestingly, DGM outperforms joint training on the MNIST dataset using the same architecture.",,arg_request,1
This suggests that the strictly incremental training methodology indeed forced the network to learn better generalizations,,arg_request,1
"compared to what it would learn given all the data."" Doesn't DGM grow the capacity, and therefore this isn't that surprising?",,none,0
"This is true throughout; as stated before it is not clear how many parameters and how much memory these methods need, which makes it impossible to compare.",,arg_evaluative,0
Some other minor issues in the writing includes:,,arg_structuring,0
"1) The introduction makes it seem the generative replay is new, without citing approaches such as DGR (which are cited in the related work).",,arg_request,1
"The initial narrative mixes prior works' contributions and this paper's contributions; the contributions of the paper itself should be made clear,",,arg_request,1
"2) Using the word ""task"" in describing ""joint training"" of the generative, discriminative, and classification networks is very confusing (since ""task"" is used for the continual learning description too,",,arg_request,1
3) There is no legend for CIFAR; what do the colors represent?,,arg_request,1
"4) There are several typos/grammar issues e.g. ""believed to occurs"", ""important parameters sections"", ""capacity that if efficiently allocated"", etc.).",,arg_request,1
"In summary, the paper presents what seems like an effective strategy for continual learning, by combining some existing methods together, but does not make it precise what the contributions are and the methodology/analysis make it hard to determine if the comparisons are fair or not.",,arg_evaluative,0
More rigorous experiments and analysis is needed to make this a good ICLR paper.,,arg_evaluative,0
Please consider this rubric when writing your review:,,arg_other,0
1. Briefly establish your personal expertise in the field of the paper.,,arg_other,0
2. Concisely summarize the contributions of the paper.,,arg_other,0
3. Evaluate the quality and composition of the work.,,arg_other,0
"4. Place the work in context of prior work, and evaluate this work's novelty.",,arg_other,0
5. Provide critique of each theorem or experiment that is relevant to your judgment of the paper's novelty and quality.,,arg_other,0
6. Provide a summary judgment if the work is significant and of interest to the community.,,arg_other,0
"1. I am a researcher working at the intersection of machine learning,",,arg_social,0
computational neuroscience and biological vision.,,none,0
I have experience,,none,0
with neural network models and visual neurophysiology.,,none,0
2. This paper develops and tests an adaptive homeostatic algorithm for,,arg_structuring,0
unsupervised visual feature learning (for example for learning models,,none,0
of early visual processing/V1).,,none,0
3.The work spends a lot of pages describing the general problem of,,arg_structuring,0
unsupervised feature learning and the history of the base algorithms.,,none,0
The literature review is quite extensive.,,arg_structuring,0
The new content appears to,,arg_structuring,0
"be in section 2.2 (Histogram Equalization Homeostasis - HEH), where a",,none,0
simple idea to keep all units with balanced activity over the set of,,none,0
natural images.,,none,0
The authors also develop a computationally cheaper,,arg_structuring,0
version they call HAP (Homeostasis on Activation Probability) The,,none,0
authors show that their F function is optimized quicker with the HEH,,none,0
and HAP algorithms.,,none,0
I would like to see how these curves vary with,,arg_request,1
the number of neurons (e.g. can you add X% more neurons and get,,none,0
similar convergence speed -- and if so which is more computationally,,none,0
costly)?,,none,0
4. Many groups have developed various homeostatic algorithms for,,arg_evaluative,0
"unsupervised learning, though I have not seen this exact one before.",,none,0
5.  The experiments reveal the resulting receptive fields and show the,,arg_structuring,0
decrease in the F function (error function).,,none,0
The resulting receptive fields,,arg_evaluative,0
do not seem that different to me between the different methods.,,none,0
I am also not,,arg_evaluative,0
that convinced that the faster convergence as a function of learning step is that important,,none,0
especially as the learning steps may be more computationally expensive for this method.,,none,0
"6. I am not sure how interesting this work will be for the ICLR audience,",,arg_evaluative,0
as it is not clear how important the faster convergence and more even,,none,0
utilization of neurons is (and how it would compare computationally,,none,0
with just having more neurons).,,none,0
The paper proposes to make a clear connection between the InfoNCE learning objective (which is a lower bound of the mutual information) and multiple language models like BERT and XLN.,,arg_structuring,0
"Then based on the observation that classical LM can be seen as instances of InfoNCE, they propose a new (InfoWord) model relying on the same principles, but taking inspiration from other models also based on InfoNCE.",,arg_structuring,0
"Mainly, the proposed model  differs both in the nature of the a and b variables used in InfoNCE, and also on the fact that it uses negative sampling instead of softmax.",,arg_structuring,0
"Experiments are made on two tasks and compared to a classical BERT model, and on the BERT-NCE model that is a BERT variant proposed by the authors which is somehow in-between BERT and InfoWord.",,arg_structuring,0
They show that their approach works quite well.,,arg_evaluative,0
I have a very mitigated opinion on the paper.,,arg_social,0
"I) First, I really like the idea of trying to unify different models under the same learning principles, and then show that these models can be seen as specific instances of generic principles.",,arg_evaluative,0
"But the way it is presented and explained lacks of clarity: for instance in Section 2, some notations are not well defined (e.g what is f?) .",,arg_request,1
"Moreover, the way classical models are casted under the InfoNCE principle is badly written: it assumes that readers have a very good knowledge of the models, and the paper does not show well the mapping between the loss function of each model and the InfoNCE criterion.",,arg_evaluative,0
"It gives technical details that could (in my opinion) get ignored, and I would clearly prefer to catch the main differences between the different models that being flooded by technical details.",,arg_evaluative,0
"So, my suggestion would be to improve the writing of this section to make the message stronger and relevant for a larger audience.",,arg_request,1
"II) The Infoword model can be seen as a simple instance of word masking based models, and as an extension of deep infomax for sequences (it would be certainly nice to describe a little bit what Deep InfoMax is to facilitate the reading).",,arg_request,1
"Here again, the article moves from technical details (e.g ""hidden state of the first token (assumed to be a special start of sentence symbol "") without providing formal definitions.",,arg_evaluative,0
Having a first loss function after paragraph 4 could help to understand the principle of this model (before restricting the model to n-grams),,arg_request,1
.,,none,0
"Moreover, the equation J_DIM seems to be wrong since it contains g_\omega twice while I think (but maybe I am wrong) that it has also to be defined by g_\psi.",,arg_evaluative,0
J_MLM is also not clear since x_i is never defined (I assume it is x_{i:i}).,,arg_evaluative,0
At last,,arg_request,1
",  after unifying multiple models under one common learning objective, the authors propose to mix two different losses which is strange (the effect of the second term is slightly studied in the experimental section) without allowing us to understand why it is important to have this second loss function and why the first one is not sufficient enough.",,none,0
"At last, I am pretty sure to not be able to reproduce the model described in the paper (adding a section on that in the supplementary material would help), and many concrete aspects are described too fast (like the way to sample negative pairs).",,arg_evaluative,0
"Concerning the experimental section, experiments are convincing and show that the model is able to achieve a performance which is close to classical models.",,arg_evaluative,0
"In my opinion, tis section has to be interpreted as  a proof that the proposed unified vision is a good way to easily define new and efficient models.",,arg_evaluative,0
"To summarize, the unification under the InfoNCE principle is interesting,",,arg_evaluative,0
"but the way the paper is written makes it very difficult to follow, and the description of the proposed model is unclear (making the experiments difficult to reproduce) and lacks of a better discussion about the interest of mixing multiple loss.",,arg_evaluative,0
This paper proposes an autocompletion model for UI layout based on adaptations of Transformers for tree structures and evaluates the models based on a few metrics on a public UI dataset.,,arg_structuring,0
"I like the area of research the authors are looking into and I think it's an important application. However, the paper doesn't answer key questions about both the application and the models:",,arg_evaluative,0
1) There is no clear rationale on why we need a new model based on Transformers for this task.,,arg_evaluative,0
What was wrong with LSTMs/GRUs as they've been used extensively for recursive problems including operations on trees?,,arg_request,1
"Similarly, I'd have expected baselines that included those models in the evaluation section showing the differences in performance between the newly proposed Transformer model for trees and previously used methods.",,arg_evaluative,0
2) The evaluation metrics used while borrowed from the language or IR fields doesn't seem to translate to UI design.,,arg_evaluative,0
"UI layout is about visual and functional representation of an application so if one is seeking to evaluate different models, they need to relate to those.",,none,0
"This paper addresses a problem that arises in ""universal"" value-function approximation (that is, reinforcement-learning when a current goal is included as part of the input);  when doing experience replay, the experience buffer might have much more representation of some goals than others, and it's important to keep the training appropriately balanced over goals.",,arg_structuring,0
"So, the idea is to a kind of importance weighting of the trajectory memory, by doing a density estimation on the goal distribution represented in the memory and then sample them for training in a way that is inversely related to their densities",,arg_structuring,0
.,,none,0
"This method results in a moderate improvement in the effectiveness of DDPG, compared to the previous method for hindsight experience replay.",,arg_structuring,0
"The idea is intuitively sensible, but I believe this paper falls short of being ready for publication for three major reasons.",,arg_evaluative,0
"First, the mechanism provided has no mathematical justification--it seems fairly arbitrary.",,arg_evaluative,0
"Even if it's not possible to prove something about this strategy, it would be useful to just state a desirable property that the sampling mechanism should have and then argue informally that this mechanism has that property.",,arg_request,1
"As it is, it's just one point in a large space of possible mechanisms.",,arg_fact,0
"I have a substantial concern that this method might end up assigning a high likelihood of resampling trajectories where something unusual happened, not because of the agent's actions, but because of the world having made a very unusual stochastic transition.",,arg_evaluative,0
"If that's true, then this distribution would be very bad for training a value function, which is supposed to involve an expectation over ""nature""'s choices in the MDP.",,arg_evaluative,0
"Second, the experiments are (as I understand it, but I may be wrong) in deterministic domains, which definitely does not constitute a general test of a proposed RL  method.",,arg_evaluative,0
- I'm not sure we can conclude much from the results on fetchSlide (and it would make sense not to use the last set of parameters but the best one encountered during training),,arg_evaluative,0
- What implementation of the other algorithms did you use?,,arg_request,1
"Third, the writing in the paper has some significant lapses in clarity.",,arg_evaluative,0
"I was a substantial way through the paper before understanding exactly what the set-up was;  in particular, exactly what ""state"" meant was not clear.",,arg_evaluative,0
"I would suggest saying something like s = ((x^g, x^c), g) where s is a state from the perspective of value iteration, (x^g, x^c) is a state of the system, which is a vector of values divided into two sub-vectors, x^g is the part of the system state that involves the state variables that are specified in the goal, x^c (for 'context')",,arg_request,1
"is the rest of the system state, and g is the goal.",,none,0
The dimensions of x^g and g should line up.,,none,0
"- This sentence  was particularly troublesome:  ""Each  state s_t also includes the state of the achieved goal, meaning the goal state is a subset of the normal state.",,arg_evaluative,0
"Here, we overwrite the notation s_t  as the achieved goal state, i.e., the state of the object.""",,none,0
"- Also, it's important to say what the goal actually is, since it doesn't make sense for it to be a point in a continuous space.",,arg_evaluative,0
"(You do say this later, but it would be helpful to the reader to say it earlier.)",,none,0
In this work the authors introduce a new method for neural architecture search (NAS) and use it in the context of network compression.,,arg_structuring,0
"Specifically, the NAS method is used to select the precision quantization of the weights at each layer of the neural network.",,arg_structuring,0
"Briefly, this is done by first defining a super network, which is a DAG where for each pair of nodes, the output node is the linear combination of the outputs of all possible operations (i.e., layers with different precision quantizations).",,arg_structuring,0
"Following [1], the weights of the linear combination are regarded as the probabilities of having certain operations (i.e., precision quantization), which allows for learning a probability distribution over the considered operations.",,arg_structuring,0
"Differently from [1], however, the authors bridge the soft sampling in [1] (where all operations are considered together but weighted accordingly to the corresponding probabilities) to a hard sampling (where a single operation is considered with the corresponding probability) through an annealing procedure based on the Gumbel Softmax technique.",,arg_structuring,0
"Through the proposed NAS algorithm, one can learn a probability distribution on the operations by minimizing a loss that accounts for both accuracy and model size.",,arg_structuring,0
"The final output of this search phase is a set of sampled architectures (containing a single operation at each connection between nodes), which are then retrained from scratch.",,arg_structuring,0
"In applications to CIFAR-10 and ImageNet, the authors achieve (and sometime surpass) state-of-the-art performance in model compression.",,arg_structuring,0
The two contributions of this work are,,arg_structuring,0
1)	A new approach to weight quantization using principles of NAS that is novel and promising;,,arg_evaluative,0
2)	New insights/technical improvements in the broader field of NAS.,,arg_evaluative,0
"While the utility of the method in the more general context of NAS has not been shown, this work will likely be of interest to the NAS community.",,arg_evaluative,0
I only have one major concern.,,arg_structuring,0
The architectures are sampled from the learnt probability distribution every certain number of epochs while training the supernet.,,arg_fact,0
"Why? If we are learning the distribution, would not it make sense to sample all architectures only after training the supernet at our best?",,arg_request,1
This reasoning leads me to a second question.,,arg_structuring,0
"In the CIFAR-10 experiments, the authors sample 5 architecture every 10 epochs, which means 45 architectures (90 epochs were considered).",,arg_fact,0
"This is a lot of architectures, which makes me wonder: how would a “cost-aware” random sampling perform with the same number of sampled architectures?",,arg_request,1
"Also, I have some more questions/minor concerns:",,arg_structuring,0
1)	The authors say that the expectation of the loss function is not directly differentiable with respect to the architecture parameters because of the discrete random variable.,,arg_structuring,0
"For this reason, they introduce a Gumbel Softmax technique, which makes the mask soft, and thus the loss becomes differentiable with respect to the architecture parameters.",,arg_fact,0
"However, subsequently in the manuscript, they write that Eq 6 provides an unbiased estimate for the gradients.",,arg_fact,0
Do they here refer to the gradients with respect to the weights ONLY?,,arg_request,1
Could we say that the advantage of the Gumbel Softmax technique is two-fold? i) make the loss differentiable with respect to the arch parameters; ii) reduce the variance of the estimate of the loss gradients with respect to the network weights.,,arg_request,1
"2)	Can the author discuss why the soft sampling procedure in [1] is not enough? I have an intuitive understanding of this, but I think this should be clearly discussed in the manuscript as this is a central aspect of the paper.",,arg_request,1
3)	The authors use a certain number of warmup steps to train the network weights without updating the architecture parameters to ensure that “the weights are sufficiently trained”. Can the authors discuss the choice on the number of warmup epochs?,,arg_request,1
"I gave this paper a 5, but I am overall supportive. Happy to change my score if the authors can address my major concern.",,arg_social,0
"[1] Liu H, Simonyan K, Yang Y. Darts: Differentiable architecture search. arXiv preprint arXiv:1806.09055. 2018 Jun 24.",,arg_other,0
-----------------------------------------------------------,,arg_structuring,0
Post-Rebuttal,,arg_structuring,0
---------------------------------------------------------,,arg_structuring,0
The authors have fully addressed my concerns. I changed the rating to a 7.,,arg_social,0
"The paper does not really propose a new way of compressing the model weights, but rather a way of applying existing weight compression techniques.",,arg_structuring,0
"Specifically, the proposed solution is to repeatedly apply weight compression and fine-tuning over the entire training process.",,arg_structuring,0
"Unlike the existing work, weight compression is applied as a form of weight distortion, i.e. the model has the full degree of freedom during fine-tuning (to recover potential compression errors).",,arg_structuring,0
Pros:,,arg_structuring,0
"- The proposed method is shown to work with existing methods like weight pruning, low-rank compression and quantization.",,arg_evaluative,0
Cons:,,arg_structuring,0
- The idea is a simple extension of existing work.,,arg_evaluative,0
"- In Table 4, it is hard to compare DeepTwist with the other methods because activation quantization is not used.",,arg_evaluative,0
This paper proposed the DSGAN model to generate unseen data.,,arg_structuring,0
The intuition based on standard GAN is straightforward and makes sense.,,arg_structuring,0
"The paper is well written, especially the case studies illustrate the idea clearly.",,arg_structuring,0
The designing of p_{\bar{d}} also presented the limitation of this method.,,arg_structuring,0
"Two main discussed applications, semi-supervised learning and novelty detection are important in machine learning.",,arg_evaluative,0
"In general, this is an interesting paper.",,arg_evaluative,0
"However, my concern is about the experiments.",,arg_evaluative,0
"As a generative model for unseen data, I would like to see the generated results, which is more convincing.",,arg_request,1
Only the 1/7 examples of MNIST dataset are provided in case studies.,,arg_fact,0
"I am wondering for more complicated images, how is the performance?",,arg_request,1
"In this paper, the authors introduce a new convolution-like operation, called a Harmonic Convolution, which operates on the STFT of an audio signal.",,arg_structuring,0
This Harmonic convolution are like a weighted combination of dilated convolutions with different dilation factors/anchors,,arg_structuring,0
.,,none,0
"The authors show that for noisy audio signals, randomly initialized/untrained U-Nets with harmonic convolutions can yield cleaner recovered audio signals than U-Nets with plain convolutions or dilated convolutions.",,arg_structuring,0
The authors beat a variety of audio denoising tasks on a variety of metrics for speech and music signals.,,arg_structuring,0
The authors also show that harmonic convolutions in U-Nets are better than plain and dilated convolutions in U-Nets for a particular sound separation task.,,arg_structuring,0
"I recommend a weak accept for this paper because a new architecture for audio priors was presented, with reasonable empirical data supporting that this architectural choice an improvement over other more immediate alternatives.",,arg_evaluative,0
"It is important to extend the work on deep nets for imaging to other domains, such as audio.",,arg_other,0
My recommendation is not stronger because of the following concerns.,,arg_structuring,0
I think the paper could be strengthened by,,arg_structuring,0
(a) a comparison to other methods (outside the current framework) for sound separation,,arg_request,1
(b) a significant clarification of Figure 4.,,arg_request,1
"The authors claim that this data shows that Harmonic Convolutions produce a ""cleaner signal faster"" than other methods.",,arg_fact,0
"When I look at Figure 4abcd, it appears that the Convolution and Dilated Convolutions fit a clean signal faster (it is just not as clean.",,arg_evaluative,0
"Additionally, the Wave-U-Net appears to reach the same accuracy as the Harmonic Convolution with many fewer iterations (while also continuing to get much higher PSNRs).",,arg_evaluative,0
"Perhaps I am misreading this plot, but it is not obvious to me that this plot supports the claims the authors are making.",,arg_evaluative,0
(c) The authors should present what they mean by a dilated convolution using the notation of the paper.,,arg_request,1
"(d) In Figure 2, it is unclear to me how the 1/f^2 law is observed in (a) but not in (c) or (e).",,arg_request,1
The paper proposes a new approach for quantizing neural networks.,,arg_structuring,0
"It looks at binary codes for quantization instead of the usual lookup tables because quantization approaches that rely on lookup tables have the drawback that during inference, the network has to fetch the weights from the lookup table and work with the full precision values, which means that the computation cost is remains the same as the non-quantized network.",,arg_structuring,0
"The paper presents FleXOR gates, a fast and efficient logic gate for mapping bit sequences to binary weights.",,arg_structuring,0
The benefit of the gate is that the bit sequence can be shorter than the binary weights which means that the code length can be less than 1 bit per weight.,,arg_fact,0
The paper also proposes an algorithm for end-to-end training of the quantized neural networks.,,arg_structuring,0
It proposes the use of tanh to approximate the non-differentiable Heaviside step function in the backward pass.,,arg_structuring,0
Novelty,,arg_structuring,0
The idea of using logic gates for dequantization is interesting and (as far as I know) novel.,,arg_evaluative,0
"One can imagine, that specialized hardware build on this idea could very efficient for inference (in terms of energy cost).",,arg_evaluative,0
Writing,,arg_structuring,0
The paper is very well written and completed with great visualizations and pseudocode.,,arg_evaluative,0
"Kudos to the authors, I really enjoyed reading it.",,arg_social,0
"However, I do not think it is justified to go over the 8 page soft limit.",,arg_evaluative,0
I would recommend that the authors perhaps shorten section 3 or remove figure 9 to fit it into 8 pages.,,arg_request,1
Significance/Impact,,arg_structuring,0
The paper is motivated by the high computation cost of working with full precision values.,,arg_evaluative,0
"But this paper also works with full precision weights, since it has a full precision scaling factor (alpha) and, as far as I understood, works with full precision values during forward propagation.",,arg_evaluative,0
This means that there likely are no computational savings when compared to lookup tables.,,none,0
The evaluation section lacks experiments that evaluate the computational savings.,,arg_evaluative,0
"The baselines should include quantization methods based on lookup tables, and there should be a comparison of computational costs.",,none,0
"The baselines that are presented (BWN etc.) offer a tradeoff between accuracy and computational costs, yet they are only compared in accuracy.",,none,0
I would strongly recommend including the computational cost of each method in the evaluation section.,,arg_request,1
Overall assessment:,,arg_structuring,0
"While I enjoyed reading this paper, I am leaning towards rejection due to the shortcomings of the evaluation section.",,arg_social,0
Paper Contributions,,arg_structuring,0
"This paper introduces a new text generation scoring approach using BERT, called BERTScore.",,arg_structuring,0
"Using BERT embeddings and optionally idf scores, a greedy matching is performed between all reference and candidate words, with cosine similarity between vector representations as the scoring.",,arg_structuring,0
"From this, a precision, recall and F1 score can be derived.",,arg_fact,0
"This notably outperforms BLEU, as well as other metrics, most but not all of the time.",,arg_evaluative,0
The paper offers a broad range of comparisons and analysis.,,arg_evaluative,0
Decision,,arg_structuring,0
I'm leaning towards accepting the paper on the basis of the following.,,arg_social,0
Strong points taken in consideration:,,arg_structuring,0
"- Simple, well-motivated metric that uses powerful BERT-style models, without being slow to compute either.",,arg_evaluative,0
- Good performance empirically on WMT. I'm less convinced on COCO since using the image is fair game there.,,arg_evaluative,0
"- Code is provided, and it is simple and adaptable for future work.",,arg_evaluative,0
- Experimentation is detailed and reproducible.,,arg_evaluative,0
Weaker points taken in consideration:,,arg_structuring,0
- Work conducted in parallel matches or exceeds the performance of BERTScore.,,arg_fact,0
"This shouldn't necessarily be a reason to choose not to publish this work in my opinion, but it should be taken into consideration.",,none,0
I like that the authors were open and clear regarding this in their discussion.,,arg_evaluative,0
- The authors haven't come up with a recommendation for a single configuration of their approach.,,arg_evaluative,0
"In one place they recommend F-BERT without idf, in another they argue for picking and choosing based on context, with little help about how to choose.",,none,0
"I think practitioners are only going to be willing to switch away from BLEU, for example, if a single one-size-fits-all metric is proposed instead.",,arg_fact,0
I identify this ambiguity between BERTScore versions as an important weakness of the paper.,,arg_evaluative,0
- It's unclear throughout whether words or wordpieces are the main token being considered.,,arg_evaluative,0
"Most discussion and definitions use ""words"", but in section 3, subsection Token Representation, it appears to be clearly stated that BERTScore uses a BERT model based on word pieces.",,none,0
I recommend adjust the language to be more consistent throughout.,,arg_evaluative,0
"Also, scoring examples with word pieces would be more consistent with this as well, imo.",,none,0
"Notably, I'm actually unsure whether you compute IDF over words or word pieces, and how this is applied.",,arg_evaluative,0
"- Finally, I found some weaknesses in the Importance Weighting section (though this isn't too important since IDF isn't part of the recommended BERTScore I believe).",,arg_structuring,0
The IDF scores would be stronger if they were computed on a bigger in-domain corpus than the gold test set.,,arg_evaluative,0
"This would add extra steps to using BERTScore though and make things more complicated in practice, but this should nevertheless probably be tried, or at least discussed in the paper.",,none,0
"Also, the plus-one smoothing handles unknown words (or word piece?) and I'm not sure why. If we're using the test set to compute IDF, and the sentences we're looking *are* in the test set, then there shouldn't be unknown words and no smoothing is required.",,arg_evaluative,0
"So overall, I still think this deserves publication because it's valuable information for researchers, and the metric itself could be immediately useful to some as well.",,arg_social,0
"However, the weaknesses mentioned make me hesitate to fully endorse the work.",,arg_social,0
The underlying motivation for the paper is really interesting and cuts straight to the heart of Deep Learning and strives to unravel the key understanding that we are still to a large extent missing.,,arg_evaluative,0
"When it comes to clarity and organization I find the paper a bit ""messy"" in that it is a collection of quite a few findings on the very specific topic of binary classification with quite strong assumptions.",,arg_evaluative,0
Especially given the very specific nature of the topic I miss a strong and clear path through the paper.,,arg_evaluative,0
Unfortunately the paper leaves me with the distinct feeling that there are still a lot of work needed to be able to tell the story about the problem under study.,,arg_evaluative,0
Having said that the paper does contain several individual findings.,,arg_evaluative,0
"Having said that I find the ideas leading up to what the authors refers to as ""gradient starvation"" to be really interesting and that would be a great clear idea to focus on.",,arg_evaluative,0
A few concrete questions/comments:,,arg_structuring,0
"Can you explain somewhere exactly what you mean when you say ""learning dynamics of deep learning""? Given the specific nature of the results presented in the paper it would be nice to be precise also when it comes to the overall topic under study.",,arg_request,1
Given the very specific nature of the topic treated in the paper I find the title of the paper largely misleading.,,arg_evaluative,0
"The title claims way more than what is actually delivered in the paper, despite the fact that the authors have put in an ""On"" in the beginning of the title.",,arg_evaluative,0
"In Corollary 3.3. you characterize the convergence speed in a nice way, but I am missing the link to the behaviors observed empirically in e.g. Fig. 2. What am I missing?",,arg_request,1
The final sentence in Section 2 is highly speculative and I find this hard to believe without solid backing.,,arg_evaluative,0
"The sentence reads ""... and helps develop intuitions about behaviors observed in more general settings."" Given the restrictive nature of your set-up I find it very hard to believe that this extends to more general settings.",,arg_evaluative,0
"Tiny detail: The axes of several of the plots given in the paper mis the lables which makes it hard to read. Straightforward to fix, but worth mentioning nevertheless.",,arg_request,1
"A model compression framework, DeepTwist, was proposed which makes the weights zero if they are small in magnitude.",,arg_structuring,0
They used different model compression techniques in this framework to show the effectiveness of the proposed method.,,arg_structuring,0
This paper proposes a framework intending to use fewer hardware resources without compromising the model accuracy.,,arg_structuring,0
"However, when the weights are set to zero the weight matrix became sparser but still requires the whole weight matrix to be used by the computing resources, as removing some of the weights based on the sorting will not remove a node, only removes some of the connection with that node.",,arg_evaluative,0
"Therefore, it is not clear how the proposed framework is helping the model compression techniques.",,arg_evaluative,0
The authors study properties of the learning behavior of non-linear (ReLu) neural networks.,,arg_structuring,0
"In particular, their main focus is on binary classification for the linear-separable case, when optimization is done using gradient descent minimizing either binary entropy or hinge loss.",,arg_structuring,0
There are 3 main results in the paper:,,arg_structuring,0
"1) During learning, each neuron only activates on data points of one class: hence (due to ReLu), each neuron only updates its weights when seeing data points from that class.",,arg_structuring,0
"The authors refer to this property as ""Independent modes of learning"", suggesting that the learning of parameters of the network is decoupled between the two classes.",,none,0
"2) The classification error, with respect to the number of iterations of gradient descent, exhibits a sigmoidal shape: slow improvement at the beginning, followed by a period of fast improvement, followed by another plateau.",,none,0
"3) Most frequent features, if discriminative, can prevent learning of other, less frequent, features.",,arg_structuring,0
"Apart from the assumption H1 of linear separability of the data (which I don't mind), the results require very strong assumptions, in particular hypothesis H2 stating ""at the beginning of training data points from different classes do not activate the same neurons"".",,arg_evaluative,0
"Even for a shallow net, the authors are essentially assuming that the first layer of weights W is such that each row w is already a hyperplane separating the two classes after initialization (wx > 0 for all x belonging to one class and wx' < 0 for x' in the other class).",,none,0
"In other words, at initialization, the first layer is already correctly classifying all data points.",,arg_fact,0
"This is of course an extremely stringent assumption that doesn't hold in practice (eg, the probability of such an initialization shrinks to zero exponentially in the number of dimensions and in the number of neurons).",,arg_evaluative,0
"Because of this concern, I believe the results in the paper can only really characterize the learning close to convergence, since the network is already able to provide correct classification.",,arg_evaluative,0
Pros:,,arg_structuring,0
"- Authors consider a non-linear (ReLu) neural network, as opposed to the analysis of Save et al which only considers linear nets.",,arg_evaluative,0
"- The fundamentally different behavior between Hinge and binary entropy loss is interesting, and worth analyzing further.",,arg_evaluative,0
- Sigmoidal shape of classification error as a function of number of iterations is inline with what is seen in practice.,,arg_evaluative,0
"However, I believe the assumptions needed to show this point force the analysis to only characterize learning close to convergence.",,arg_evaluative,0
Minor Cons (apart from major concern above):,,arg_structuring,0
"- Theorem 3.2: ""[...] converges at a speed proportional to [...]"". Isn't \bar{u}_t logarithmic (non-linear) in t?",,arg_request,1
"- Theorem 3.2: Even if strong, I don't mind the assumption on a dataset merely consisting of two (weighted) data points. I would suggest to simulate this case without putting any condition on the initialization of the weights (ie, without assumptions H1-H2), and compare the empirical shape of the classification error with the one you obtain analytically in Figure 2 Right.",,arg_request,1
"- Theorem 3.2 Interpretation: unfinished sentence ""We can characterize the convergence speeds more quantitatively with the""",,arg_request,1
"- Theorem 4.1: Can you give an intuition or lower/upper bounds for u(t) for the Hinge case, to make evident its difference from the binary entropy case (where u(t) ~ log(t))",,arg_request,1
"- Gradient starvation, Kaggle experiment: I'm not too convinced about the novelty/usefulness of this result. In the end, even a decision tree stump would stop growing after learning the dark/light feature as a discriminator.",,arg_evaluative,0
"What I'm trying to say is that ""gradient starvation"" is a more general problem that really doesn't have to do with gradient descent.",,arg_fact,0
"Also, the fact that the accuracy on the Kaggle non-doctored test set is low is simply because the test set is not coming from the same distribution of the training set.",,arg_evaluative,0
This paper explores the relation among the generalization error of neural networks and the model and data scales empirically.,,arg_structuring,0
"The topic is interesting, while I was expecting to learn more from the paper, instead of some well-known conclusions.",,arg_evaluative,0
"If the paper could provide some guidance for model and data selection, that would be an interesting paper for the ICLR audience.",,arg_request,1
"For instance, how deep should a model be for a classification or regression task?",,arg_request,1
What is the minimum/maximum layers of a deep model? How much data is sufficient for a model to learn?,,arg_request,1
What is the minimum/maximum size of the data set? Do we really need a large data set or just a subset that covers the data distribution?,,arg_request,1
"What's the relation between the size of a model and that of a data set? By increasing the depth/width of a neural network, how much new data should be collected for achieving a reasonable performance?",,arg_request,1
How about the gain of the task performance?,,arg_request,1
The paper introduces a novel regularized auto-encoder architecture called the Cramer-Wold AutoEncoders (CWAE).,,arg_structuring,0
"It's objective (Eq. 7) consists of two terms: (i) a standard reconstruction term making sure the the encoder-decoder pair aligns nicely to accurately reconstruct all the training images and (ii) the regularizer, which roughly speaking requires the encoded training distribution to look similar to the standard normal (which is a prior used in the generative model being trained).",,arg_structuring,0
The main novelty of the paper is in the form of this regularizer.,,arg_structuring,0
"The authors introduce what they call ""the Cramer-Wold distance"" (for definitions see Theorems 3.1 and 3.2) which is defined between two finite sets of D-dimensional points.",,arg_structuring,0
"The authors provide empirical studies showing that the proposed CWAE method achieves the same quality of samples (measured with FID scores) as the WAE-MMD model [1] previously reported in the literature, while running faster (by up to factor of 2 reduction in the training time, as the authors report).",,arg_structuring,0
"While on the AE model / architecture side I feel the contribution is very marginal, I still think that the improvement in the training speed is something useful. Otherwise it is a nicely written and polished piece of work.",,arg_evaluative,0
Detailed comments:,,arg_structuring,0
"(1) My main problem with this paper is that the novel objective proposed by the authors in Eq. 7 is equivalent to the objective of WAEs appearing in Eq. 4 of [1] (up to a heuristic of applying logarithm to the divergence measure, which is not justified but meant to ""improve the balance between two terms"", see footnote 2), where the authors use the newly introduced Cramer-Wold divergence as a choice of the penalty term in Eq. 4 of [1].",,arg_evaluative,0
"(2) When viewed in this way, CW-distance introduced in Eq (2) closely resembles the unbiased U-statistic estimate of the MMD used in WAE-MMD [1, Algorithm 2].",,arg_evaluative,0
In other words: it may be the case that there is a choice of a reproducing kernel k such that Eq. 2 of this paper is an estimate of MMD_k between two distributions based on the i.i.d. samples X and Y.,,arg_fact,0
"Note that if it is indeed the case, this corresponds to the V-statistic and thus biased: in U-statistic the diagonal terms (that is i = i' and j = j' in forst two terms of eq 2) would be omitted.",,arg_fact,0
"If this all is indeed the case, it is not surprising that the numbers the authors get in the experiments are so similar to WAE-MMD, because CWAE would be exactly WAE-MMD with a specific choice of the kernel.",,arg_evaluative,0
(3) The authors make a big deal out of their proposed divergence measure not requiring samples from the prior as opposed to WAE-MMD.,,arg_structuring,0
"However, WAE-MMD does not necessarily need to sample from the prior when used with Gaussian prior and Gaussian RBF kernel, because in this case the prior-related parts of the MMD can be computed analytically.",,arg_fact,0
"In other words, if the computational advantage of CWAE compared to WAE-MMD comes from CWAE not sampling Pz, the computational overhead of WAE-MMD can be eliminated at least in the above-mentioned setting.",,arg_fact,0
"(4) based on the name ""CW distance"" I would expect the authors to actually prove that it is indeed a distance (i.e. all the main axioms).",,arg_request,1
"(5) The authors override the CW distance: first in Theorem 3.1 they define it as a distance between two finite point clouds, and later in Theorem 3.2 they redefine it as a distance between a point cloud and the Gaussian distribution.",,arg_evaluative,0
(6) What is image(X) in Remark 4.1?,,arg_request,1
"[1] Tolstikhin et al., Wasserstein Auto-Encoders, 2017.",,arg_other,0
The paper presents a an interesting novel approach to train neural networks with so called peer regularization which aims to provide robustness to adversarial attacks.,,arg_structuring,0
The idea is to add a graph neural network to a spatial CNN.,,arg_structuring,0
A graph is defined over similar training samples which are found using a Monte Carlo approximation.,,arg_structuring,0
The regularization using graphs reminds me of recent work at ICML on semi-supervised learning (Kamnitsas et al. (2018) Semi-supervised learning via compact latent space clustering) which is using a graph to approximate cluster density which acts as a regularizer for training on labelled data.,,arg_other,0
The main problem I see with these approaches is that they rely on sufficiently large batch sizes which could be (currently) problematic for many real-world applications.,,arg_evaluative,0
"Memory and computation limitations are mentioned, but not sufficently discussed.",,arg_evaluative,0
It would be good to add further details on practical limitations.,,arg_request,1
"Experiments are limited to benchmark data using MNIST, CIFAR-10, CIFAR-100.",,arg_structuring,0
Comprehensive evaluation has been carried out with insightful experiments and good comparison to state-of-the-art.,,arg_evaluative,0
Both white- and black-box adversarial attacks are explored with promising results for the proposed approach.,,arg_evaluative,0
"However, it is difficult to draw conclusions for real-world problems of larger scale.",,arg_evaluative,0
"The authors state that proposed framework can be added to any baseline model, but miss to clearly mention the limitations.",,arg_evaluative,0
"It is stated that future work will aim at scaling PeerNets to benchmarks like ImageNet, but it is unclear how this could be done.",,arg_evaluative,0
Is there any hope this could be applied to problems like 3D imaging data or videos?,,arg_request,1
Summary:,,arg_structuring,0
The paper proposes the use of a hierarchical model for a generative modeling task.,,arg_structuring,0
They propose a framework of introducing an intermediate latent variable to enforce the independence of the control and noise variable.,,arg_structuring,0
The paper report extensive experimental results to validate the proposed hierarchical model.,,arg_structuring,0
The authors also provide the anonymized code to observe the exact implementation in TensorFlow to visualize the latent variable traversals.,,arg_structuring,0
Comments:,,arg_structuring,0
The paper proposes the use of a hierarchical model for a generative modeling task by introducing an intermediate latent variable to enforce the independence of the control and noise variable.,,arg_structuring,0
The paper report extensive experimental results to validate the proposed hierarchical model.,,arg_structuring,0
This type of framework of crude to fine hierarchical generative model has already been successfully introduced by StackGAN and it's recent variants.,,arg_fact,0
"On the unsupervised disentangled feature learning, the framework provides incremental advancement by using beta-VAE in conjunction with GAN to use the best of both the worlds.",,arg_fact,0
"Even though the proposed approach is similar to StackGAN, the experiments and the results mentioned in the paper are noteworthy.",,arg_evaluative,0
Questions to Authors:,,arg_structuring,0
There are 2 main claims of novelty made in the paper.,,arg_fact,0
1. Architectural Biases:,,arg_structuring,0
How is the approach different in comparison to the StackGAN and it's variable which also use multiple levels of crude to fine image generation?,,arg_request,1
2. Unsupervised control variable discovery:,,arg_structuring,0
This part is just the use of existing disentanglement VAEs to extract the control variables.,,arg_fact,0
So how does the paper try to make contributions to improve the disentangled features with the proposed method?,,arg_request,1
"Apart from combining these to existing ideas, what can be considered as an added novelty to improve the quality of the disentangled features?",,arg_request,1
"In summary, I find there is no novelty involved apart from combining the already existing SOTA model in disentangled feature learning (beta-VAE) and image generation (StackGAN).",,arg_evaluative,0
Review,,arg_structuring,0
This paper discusses invariances in ReLU networks.,,arg_structuring,0
"The discussion is anchored around the observation that while the spectral norm of neural networks layers (their product bounds the Lipschitz constant) has been investigated as a measure of robustness of nets, the smallest singular values are also of interest as these indicate directions of invariance.",,arg_structuring,0
"The paper consists mostly of a theoretical analysis with little empirical support, focusing on a property of matrices called omnidirectIonality.",,arg_structuring,0
The definition given seems weird — an A \in R^{m \times n} is omnidirectional if there exists a unique x \in R^n such that Ax \leq 0.,,arg_structuring,0
If there is a *unique* x then that x must be 0.,,arg_structuring,0
"Else if there were a nonzero x for which Ax \leq 0, then A(cx) also \leq 0 for any positive scalar 0 and thus x is not unique",,arg_structuring,0
.,,none,0
"Moreover if x must be equal to 0 Ax \leq 0 and at that point Ax = 0, then that means there exists no x for which Ax < 0, so why not just say this outright?",,arg_request,1
Perhaps a cleaner definition would just be “A is full rank and there does not exist any X such that Ax < 0?,,arg_request,1
Also perhaps better to use the curly sign for vector inequality.,,arg_request,1
"Overall the paper, while interesting is unacceptably messy.",,arg_evaluative,0
The first two pages have no paragraph breaks!,,arg_evaluative,0
!! This means either that the author are separating paragraphs with \\ \noindent or that they have modified the style file to remove paragraph breaks to save space.,,arg_evaluative,0
Either choice is unreadable and unacceptable.,,arg_evaluative,0
The paper is also littered with typos and vague statements (many enumerated below under *small issues*).,,arg_evaluative,0
"In this case, they add up to make a big issue.",,arg_evaluative,0
The notation at the top of page 4 — see (1) and (2) — comes out of nowhere and requires explanation.,,arg_evaluative,0
|_{y>0} x + b |_{y>0}  <— what is the purpose of the subscripts here? Why is this notation never introduced?,,arg_request,1
"Ultimately this paper focuses on the question on whether the pre-image of a ReLU layer can be concluded (based on the post-image) to be a singleton,  a compact polytope, or if it has infinite volume.",,arg_fact,0
"The paper offers some analysis, suggesting when each of the conditions occurs, upper bounds the smallest singular value of D A (where the example dependent diagonal matrix D incorporates the ReLU activation (shouldn’t this be more clearly introduced and notated?).",,arg_request,1
"Ultimately this paper is interesting but falls well below the standards of exposition that I expect from a theory paper and doesn’t go very far at connecting the analysis back to the claimed motivation of investigating practical invariances. If the authors significantly improve the quality of the draft, I’ll be happy to revisit it and re-evaluate my score.",,arg_evaluative,0
Small issues,,arg_structuring,0
The following is a *very* incomplete list of small bugs found in the paper:,,arg_structuring,0
“From a high-level perspective both of these approaches” --> missing comma after “perspective”,,arg_request,1
"""as well as the gradient correspond to the highest",,arg_request,1
"possible responds for a given perturbation"" --> incomprehensible ""corresponding?"" ""possible responds?"" do you mean ""response"", and if so what is the precise technical meaning here?",,none,0
"""analyzing the lowest possible response"" what does ""response' mean here?",,arg_request,1
"""We provide upper bounds on the smallest singular value"" -- the singular value of what? This hasn't been stated yet.",,arg_request,1
"""reverse view on adversarial examples"" --- what this means isn't clear from the preceding text.",,arg_request,1
"""we aim to theoretically derive means to uncover mechanisms of rectifier networks without assumptions on the weights"" -- what does ""mechanisms"" mean here?",,arg_request,1
"Notation section -- need a sentence here at the beginning, can't just have a section heading followed by bullets.",,arg_request,1
"""realated""",,arg_request,1
This paper presents an adaptive exploration scheme that can reduce the complexity of per-task tuning.,,arg_structuring,0
"This goal is achieve by formulating the adapting scheme as a multi-arm bandit problem with the actual ""learning progress"" as a feedback signal.",,arg_structuring,0
The paper is well written and easy to be understood.,,arg_evaluative,0
The strength of this paper is that 1) the proposed method is new in the sense that it invents an automatic way for exploration.,,arg_evaluative,0
2) The algorithm is simple yet effective by the experiment results the authors provide.,,arg_evaluative,0
Weakness: the presentation of the tables/bar charts in the experiment is a bit unclear.,,arg_evaluative,0
More explanations are needed.,,arg_request,1
"This paper makes an interesting theoretical contribution; namely, that SGD with momentum (and with a slight modification to the step-size rule) is guaranteed to quickly converge to a second-order stationary point, implying it quickly escapes saddle points.",,arg_structuring,0
"SGD with momentum is widely used in the practice of deep learning, but a theoretical analysis has remained largely elusive.",,arg_structuring,0
This paper sheds light theoretical properties justifying its use for deep learning.,,arg_structuring,0
"Although the paper makes assumptions (e.g., twice differentiable, with smooth Hessian) that are not valid for the most widely-used deep learning models, the theoretical contributions of this paper should nonetheless be of interest to researchers in optimization for machine learning.",,arg_structuring,0
I recommend it be accepted.,,arg_evaluative,0
"The experiments reported in the paper, including those used to validate the required properties, are for small toy problems.",,arg_evaluative,0
This is reasonable given that the main contribution of the paper is theoretical.,,arg_evaluative,0
"However, I would have given a higher rating if some further exploration of the validity of these properties was carried out for problems closer to those of interest to the broader ICLR community.",,arg_evaluative,0
"Even if the assumptions aren't satisfied everywhere for typical deep networks, they may be satisfied at most points encountered during training, which would make the contribution even more compelling.",,arg_evaluative,0
This may also help to understand some of the limitations of this analysis.,,arg_evaluative,0
One other limitation seems to be that Theorem 1 requires using a step size which seems to be much smaller than what one may hope to use in practice. Can you comment on this?,,arg_request,1
The paper proposes using the nested CRP as a clustering model rather than a topic model.,,arg_structuring,0
The clustering is on the latent vector input into a neural network for generating the observation.,,arg_structuring,0
A variational approach is derived.,,arg_structuring,0
The proposed model seems like a straightforward extension of the nCRP with a deep model hanging off the end of it.,,arg_evaluative,0
"A significant concern/confusion for me is that this doesn't seem to be a mixed membership model, and so I don't know how meaningful it is to generate a level distribution from a Dirichlet and then draw from that mixture one time.",,arg_evaluative,0
From the generative model it seems every data point has its own Dirichlet vector on levels.,,arg_fact,0
"For topic models this makes sense since that vector is then drawn from multiple times (once per word) from a Discrete, so there's a distribution to actually learn.",,arg_fact,0
My understanding is that this isn't being done here.,,arg_evaluative,0
The paper gives a big picture view on training objectives used to obtain static and contextualized word embeddings.,,arg_structuring,0
"This is very handy since classical static word embeddings, such as SGNS and GloVe, have been studied theoretically in a number of works (e.g., Levy and Goldberg, 2014; Arora et al., 2016; Hashimoto et al., 2016; Gittens et al., 2017; Allen and Hospedales, 2019; Assylbekov and Takhanov, 2019), but not much has been done for the modern contextualized embedding models such ELMo and BERT - I personally know only the work of Wang and Cho (2019), and please correct me if I am wrong.",,arg_structuring,0
"""There is nothing as practical as a good theory"", and the authors confirm this statement: their theory suggests them to modify the training objective of the masked language modeling in a certain way and this modification proves to benefit the embeddings in general when evaluated on standard tasks.",,arg_structuring,0
I don't have any major issues to raise.,,arg_evaluative,0
"A minor comment is that the mutual information I(., .) being a function of two variables suddenly became a function of a single variable in Eq. (1) and in the text which precedes it.",,arg_request,1
~The authors build a new method to recapitulate the 3D structure of a biomolecule from cryo-EM images that allows for flexibility in the reconstructed volume.~,,arg_structuring,0
I thought this paper is very well written and tackles a difficult project.,,arg_evaluative,0
There is a previous work that these authors should cite:,,arg_request,1
"Ullrich, K., Berg, R.V.D., Brubaker, M., Fleet, D. and Welling, M., 2019. Differentiable probabilistic models of scientific imaging with the Fourier slice theorem. arXiv preprint arXiv:1906.07582.",,arg_request,1
How does your method compare to this paper?,,arg_request,1
"In Ullrich et al., they report “Time until convergence, MSE [10^-3/voxel], and Resolution [Angstrom]).",,arg_fact,0
"I think these statistics would be useful to report in your work, as they are more familiar with folks in the cryoEM field.",,arg_request,1
"In Equation 3, how does one calculate Z, the normalization constant?",,arg_request,1
"For the decoder, how large of the 3D space are you generating? What are the units? Are you using voxels to represent atomic density? What is the voxel size? Is it the same as on Page 11?",,arg_request,1
I think more description of the neural network architecture would be useful (more than what is reported on page 12).,,arg_request,1
Summary: This paper introduces the task of using deep learning for auto-completion in UI design.,,arg_structuring,0
"The basic idea is that given a partially completed tree (representing the design state of the UI), the goal is to predict or ""autocomplete"" the final tree.",,arg_structuring,0
"The authors propose a transformer-based solution to the task, considering three variants: a vanilla approach where the tree is flattened to a sequence, a pointer-network style approach, and a recursive transformer.",,arg_structuring,0
Preliminary experiments indicate that the recursive model performs best and that the task is reasonable difficulty.,,arg_structuring,0
"Assessment: Overall, this is a borderline paper, as the task is interesting and novel, but the presentation is lacking in technical detail and there is a lack of novelty on the modeling side.",,arg_evaluative,0
"In particular, the authors spend a bulk of the paper describing the three different baselines they implement.",,arg_fact,0
"However, despite the fact that most of the paper is dedicated to the explanation of these baselines.",,arg_fact,0
There is not sufficient detail to reproduce the models based on the paper alone.,,arg_evaluative,0
"Indeed, without referencing the original Pointer Network and (and especially the) Transformer papers, it would not be possible to understand this paper at all.",,arg_evaluative,0
Further technical background and detail would drastically improve the paper.,,arg_request,1
"Moreover, it seems strange that significant space was used to give equations describing simple embedding lookups (i.e., matrix multiplications with one-hot vectors), but the basic technical foundations of Transformers were not adequately explained.",,arg_evaluative,0
"In addition, only the transformer baselines were considered, and it would seem natural to consider LSTM-based baselines, or some other related techniques.",,arg_request,1
"In general, the space that was used to explain the Transformer baselines---which are essentially straightforward ways to adapt transformers to this task---could have been used to give more detail on the dataset.",,arg_evaluative,0
"For example, one question is how often a single partial tree has multiple possible completions in the data.",,arg_request,1
"A major issue---mainly due to the lack of technical details and the lack of promise to provide code/data (unless I missed this)---is that the paper does not appear to be reproducible. Given the intent to have this be a new benchmark, ensuring reproducibility seems critical.",,arg_evaluative,0
Reasons to accept:,,arg_structuring,0
- Interesting new application of GNNs,,arg_evaluative,0
Reasons to reject:,,arg_structuring,0
- Incremental modeling contribution,,arg_evaluative,0
- Lack of sufficient technical detail on models and dataset,,arg_evaluative,0
- Does not appear to be reproducible,,arg_evaluative,0
This paper develops a multi-arm bandit-based algorithm to dynamically adapt the exploration policy for reinforcement learning.,,arg_structuring,0
"The arms of the bandit are parameters of the policy such as exploration noise, per-action biases etc.",,arg_structuring,0
A proxy fitness metric is defined that measures the return of the trajectories upon perturbations of the policy z; the bandit then samples perturbations z that are better than the average fitness of the past few perturbations.,,arg_structuring,0
I think this paper is just below the acceptance threshold.,,arg_social,0
My reservations and comments are as follows.,,arg_structuring,0
"1. While I see the value in designing an automatic exploration mechanism, the complexity of the underlying approach makes the contribution of the bandit-based algorithm difficult to discern from the large number of other bells and whistles in the experiments.",,arg_evaluative,0
"For instance, the authors use Rainbow as  the base algorithm upon which they add on the exploration.",,none,0
"Rainbow itself is an extremely complicated algorithm, how can one be certain that the improvements in performance are caused by the improved exploration and not a combination of the bandit’s actions with the specifics of Rainbow?",,none,0
2. I don’t understand Figure 4.,,arg_evaluative,0
The score defined in Appendix is the average over games for which seed performs better.,,none,0
Why is the random seed being used to compare the performance of different arms? Do you instead mean that s and s’ are two values of the arm in Figure 4?,,arg_request,1
"If not, how should one interpret Figure 4, no fixed arm is always good because the performance varies across the seeds.",,none,0
The curated bandit does not seem to be doing any better than a fixed arm.,,none,0
I have a few more questions that I would like the authors to address in their rebuttal or the paper.,,arg_structuring,0
1. The proxy f(z) does not bear any resemblance to LP(z).,,arg_evaluative,0
Why discuss the LP(z) then.,,none,0
"The way f(z) is defined, it is just the value function averaged over perturbations  of the policy.",,none,0
"If one were to consider z as an additional action space that is available to the agent during exploration, f(z) is the value function itself.",,none,0
The exploration policy is chosen not to maximize the E_z [f(z)] directly but to maximize the lower bound in Markov’s inequality (P(f(z) >= t) <= E_z [f(z)]/t) in Section 4.,,none,0
2. Can you elaborate more on the metric for measuring the learning progress LP? Why does the myopic metric make sense in spite of the there being plateaus in the training curves?,,arg_request,1
3. The key contribution of the paper that the authors could highlight better is that they do not add new hyper-parameters.,,arg_evaluative,0
"In this aspect, the auto-tuner for exploration is a plug-and-play procedure in other RL algorithms.",,none,0
"4. From Figure 6 and Figure 8-11, it looks like the bandit is more or less on par with fixed exploration policies.",,arg_evaluative,0
What is the benefit of the added complexity?,,none,0
"The paper proposes a learning-based adaptive compressed sensing framework in which both the sampling and the task functions (e.g., classification) are learned jointly end-to-end.",,arg_structuring,0
The main contribution includes using the Gumbel-softmax trick to relax categorical distributions and use back-propagation to estimate the gradient jointly with the tas neural network.,,arg_structuring,0
"The proposed solution has the flexibility of able to be used in several different tasks, such as inverse problems ( super-resolution or image completion) or classification tasks.",,arg_structuring,0
The paper is very well written.,,arg_evaluative,0
The paper locates itself well in current baselines and explains Experiments mostly well.,,arg_evaluative,0
"However, there are significant limitations in demonstrating the effectiveness/impact of the proposed technique:",,arg_evaluative,0
1) The only comparison to another non-fixed sampling baseline is Kool et al. 2019.,,arg_evaluative,0
The visualization and a thorough comparison were missing in MNIST classification.,,arg_evaluative,0
This baseline was also missing in image reconstruction.,,arg_evaluative,0
2) Compressive Sensing incorporates vast literature of algorithms focusing on different aspects of improvements; algorithms focused on classification and inverse problems.,,arg_evaluative,0
"Even if done disjointly, how does the proposed joint learning is compared to those algorithms in these domains?",,arg_request,1
"3) Top row of Figure 3 nicely explains how the learned sampling paradigm performs compared to other mechanisms (such as uniform, random, low-pass). But there is no comparision against other non-fixed techniques.",,arg_request,1
The paper presents a novel scheme of distributing PPO reinforcement learning algorithm for hundreds of GPUs.,,arg_structuring,0
Proposed technique was validated for pointgoal visual navigation task on recently introduced Habitat challenge and sim.,,arg_structuring,0
"Besides the technical contribution, paper shows that when have enough computational power of billions simulation runs, it is possible to learn nearly perfect visual navigation (given RGBD + GPS inputs) via reinforcement learning.",,arg_evaluative,0
"Authors also study the task itself and show that it is yet not possible to achieve a good results without dense (each step) GPS signal, while the ""Blind"" agent, which has only GPS+compass error achieves quite high results given the billion-scale training time.",,arg_evaluative,0
This suggests that PointGoal navigation with dense GPS signal is might be a poor choice to benchmark RL algorithms and we should proceed to harder tasks.,,arg_evaluative,0
Overall I like the paper a lot and think that it should be accepted.,,arg_evaluative,0
***,,arg_other,0
I haven`t changed my mind after the rebuttal: the paper is good and should be accepted.,,arg_evaluative,0
The paper presents a variational inference approach for locally linear dynamical models.,,arg_structuring,0
"In particular,  the latent dynamics are drawn from a Gaussian approximation of the parent variational distribution,",,arg_structuring,0
"enabled by Laplace approximations with fixed point updates, while the parameters are optimized the resulting stochastic ELBO.",,none,0
"Experiments demonstrate the ability of the proposed approach to learning nonlinear dynamics, explaining data variability, forecasting and inferring latent dimensions.",,arg_structuring,0
Quality: The experiments appear to be well designed and support the main claims of the paper.,,arg_evaluative,0
Clarity: The clarity is below average.,,arg_evaluative,0
In Section 2 the main method is introduced.,,arg_structuring,0
"However, the motivation and benefits of introducing a parent and child variational approximation are not discussed adequately.",,arg_evaluative,0
"It would be helpful to move some of the stuff in the appendix to the main text, and present in a neat way.",,arg_request,1
I also struggled a little to understand what is the difference between forward interpolate and filtering.,,arg_request,1
"Originality: Given the existing body of literature, I found the technical novelty of this paper rather weak. However, it seems the experiments are thoroughly conducted.",,arg_evaluative,0
"In the tasks considered, the proposed method demonstrates convincing advantages over its competitors.",,arg_evaluative,0
Significance: The method shall be applicable to a wide variety of sequential data with nonlinear dynamics.,,arg_evaluative,0
"Overall, this appears to be a board-line paper with weak novelty.",,arg_evaluative,0
"On the positive side, the experimental validation seems well done.",,arg_evaluative,0
The clarity of this paper needs to be strengthened.,,arg_request,1
Minor comments:,,arg_structuring,0
"- abstract: uncover nonlinear observation? -> maybe change ""observation"" to ""latent dynamics""?",,arg_request,1
- The authors proposed a novel method for cryo-EM reconstruction that extends naturally to modeling continuous generative factors of structural heterogeneity.,,arg_structuring,0
"To address intrinsic protein structural heterogeneity, they explicitly model the imaging operation to disentangle the orientation of the molecule by formulating decoder as a function of Cartesian coordinates.",,arg_structuring,0
- The problem and the approach are well motivated.,,arg_evaluative,0
- This reviewer has the following comments:,,arg_structuring,0
1) VAE is known to generate blurred images.,,arg_fact,0
"Thus, based on this approach, the reconstruction image may not be optimal with respect to the resolution which might be critical for cryo-EM reconstruction.",,arg_evaluative,0
What's your opinion?,,arg_request,1
"2) What's the relationship between reconstructed performance, heterogeneity of the sample and dimensions of latent space?",,arg_request,1
"3) It would be interesting to show any relationship, reconstruction error with respect to the number of discrete multiclass.",,arg_request,1
4) How is the proposed method generalizable?,,arg_request,1
This paper proposes a method for learning disentangled representations.,,arg_structuring,0
The approach is used on both supervised (where the factors to be disentangled are known) and unsupervised settings.,,arg_structuring,0
The authors demonstrate the efficacy of their approach in both settings on several datasets with both quantitative and qualitative results.,,arg_structuring,0
This task is an important one.,,arg_evaluative,0
"However, I found that the contribution of this paper is fairly small.",,arg_evaluative,0
The proposed approach seems reasonable but it is mostly a work of engineering and provides little insights into the problem nor the proposed model.,,arg_evaluative,0
The setup where labeled data (c) also seems a bit unnatural (this also seems to be confirmed by the fact that the authors had to build datasets for the problem).,,arg_evaluative,0
Perhaps the authors could give examples of situations where this would naturally arise.,,arg_request,1
"In practice, it seems difficult to obtain these data for all required variables to be disentangled.",,arg_fact,0
The unsupervised results are more interesting but not very much explored (a single set of sampled faces).,,arg_evaluative,0
I was also curious as to why the learned Y's are blurry.,,arg_request,1
"This sort of two-stage generation is also potentially interesting, I was wondering if the authors had ideas to generalize this idea.",,arg_request,1
I also was not convinced by the experiments which are mostly qualitative. I did not find that this set of experiments provide enough support to the proposed method.,,arg_evaluative,0
Detailed comments:,,arg_structuring,0
- It is a bit unclear to me how the authors propose to obtain independent posteriors over z and c. Is it purely empirical or is there a formal reason that guarantees it?,,arg_request,1
"- Some of the figures your report are compelling but it is a bit unclear to the reader if the results are general (e.g., the examples could have been hand-picked). Are there any quantitative measures you could provide (in addition to Tables 1 and 2 which don't measure the quality of the approach)?",,arg_evaluative,0
"- Comparing to CGAN seems reasonable but given the task at hand, it seems like other methods could have been tried (although I do realize that no one may have done this before for deep generative models).",,arg_fact,0
Other comments:,,arg_structuring,0
"- In Figure 3, it would be good to label the upper trapezoid.",,arg_request,1
- Some paragraphs are very long and the manuscript may benefit from segmenting them into multiple paragraphs.,,arg_request,1
"The primary innovation of this paper seems focused towards increasing the generalization of GANs, while also maintaining convergence and preventing mode collapse.",,arg_structuring,0
"The authors first discuss common pitfalls concerning the generalization capability of discriminators, providing analytical underpinnings for their later experimental results.",,arg_structuring,0
"Specifically, they address the problem of gradient explosion in discriminators.",,arg_structuring,0
The authors then suggest that a zero-centered gradient penalty (0-GP) can be helpful in addressing this issue.,,arg_structuring,0
"0-GPs are regularly used in GANs, but the authors point out that the purpose is usually to  provide convergence, not to increase generalizability.",,arg_structuring,0
"Non-zero centered penalties can give a convergence guarantee but, the authors, assert, can allow overfitting.",,arg_structuring,0
A 0-GP can give the same guarantees but without allowing overfitting to occur.,,arg_fact,0
"The authors then verify these assertions through experimentation on synthetic data, as well as MNIST and ImageNet.",,arg_structuring,0
My only issue here is that very little information was given about the size of the training sets. Did they use all the samples?,,arg_request,1
Some portion? It is not clear from reading. This would be a serious impediment to reproducibility.,,arg_evaluative,0
"All in all, however, the authors provide a convincing  combination of analysis and experimentation.",,arg_evaluative,0
I believe this paper should be accepted into ICLR.,,arg_fact,0
"Note: there is an error on page 9, in Figure 3.",,arg_request,1
The paragraph explanation should list that the authors' 0-GP is figure 3(e).,,none,0
They list (d) twice.,,none,0
This paper attempts to mitigate catastrophic problem in continual learning.,,arg_structuring,0
"Different from the previous works where episodic memory is used, this work adopts the generative replay strategy and improve the work in (Serra et al., 2018) by extending the output neurons of generative network when facing the significant domain shift between tasks.",,arg_evaluative,0
Here are my detailed comments:,,arg_structuring,0
"Catastrophic problem is the most severe problem in continual learning since when learning more and more new tasks, the classifier will forget what they learned before, which will be no longer an effective continual learning model.",,arg_fact,0
"Considering that episodic memory will cost too much space, this work adopts the generative replay strategy where old representative data are generated by a generative model.",,arg_evaluative,0
"Thus, at every time step, the model will receive data from every task so that its performance on old tasks will retain.",,arg_fact,0
"However, if the differences between tasks are significant, the generator cannot reserve vacant neurons for new tasks or in other words, the generator will forget the old information from old tasks when overwritten by information from new tasks.",,arg_fact,0
"Therefore, this work tries to tackle this problem by extending the output neurons of the generator to keep vacant neurons to retain receive new information.",,arg_structuring,0
"As far as I am concerned, this is the main contribution of this work.",,arg_structuring,0
"Nevertheless, I think there are some deficiencies in this work.",,arg_structuring,0
"First, this paper is not easy to follow.",,arg_evaluative,0
"The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper.",,arg_evaluative,0
"For example, in Section 4.1, I am not sure the equation (3), (4), (5), (6) are the contributions of this paper or not since a large number of citations appear.",,arg_evaluative,0
"Second, the authors mention that to avoid storing previous data, they adopt generative replay and continuously enlarge the generator to tackle the significant domain shift between tasks.",,arg_structuring,0
"However, in this way, when more and more tasks come, the generator will become larger and larger.",,arg_evaluative,0
The storing problem still exists.,,arg_evaluative,0
Generative replay also brings the time complexity problem since it is time consuming to generate previous data.,,arg_evaluative,0
"Thus, I suggest the authors could show the space and time comparisons with the baseline methods to show effectiveness of the proposed method.",,arg_request,1
"Third, the datasets used in this paper are rather limited.",,arg_evaluative,0
Three datasets cannot make the experiments convincing.,,arg_evaluative,0
"In addition, I observe that in Table 1, the proposed method does not outperform the Joint Training in SVHN with A_10.",,arg_request,1
I hope the author could explain this phenomenon.,,arg_request,1
"Furthermore, I do not see legend in Figure 3 and thus I cannot figure out what the curves represent.",,arg_request,1
"Fourth, there are some grammar mistakes and typos.",,arg_request,1
"For example, there are two ""the"" in the end of the third paragraph in Related Work.",,arg_request,1
"In the last paragraph in Related Work, ""provide"" should be ""provides"".",,arg_request,1
"In page 8, the double quotation marks of ""short-term"" are not correct.",,arg_request,1
"Finally yet importantly, though a large number of works have been proposed to try to solve this problem especially the catastrophic forgetting, most of these works are heuristic and lack mathematical proof, and thus have no guarantee on new tasks or scenarios.",,arg_evaluative,0
The proposed method is also heuristic and lacks promising guarantee.,,arg_evaluative,0
The paper proposes a novel method for sampling examples for experience replay.,,arg_structuring,0
It addresses the problem of having inbalanced data (in the experience buffer during training).,,arg_structuring,0
The authors trained a density model and replay the trajectories that has a low density under the model.,,arg_structuring,0
Novelty:,,arg_other,0
"The approach is related to prioritized experience replay, PER is computational expensive because of the TD error update, in comparison, CDR only updates trajectory density once per trajectory.",,arg_structuring,0
Clarity:,,arg_other,0
The paper seems to lack clarity on certain design/ architecture/ model decisions.,,arg_evaluative,0
"For example, the authors did not justify why VGMM model was chosen and how does it compare to other density estimators.",,arg_evaluative,0
"Also, I had to go through a large chunk of the paper before coming across the exact setup.",,arg_evaluative,0
I think the paper could benefit from having this in the earlier sections.,,arg_evaluative,0
Other comments about the paper:,,arg_other,0
-  I do like the idea of the paper.,,arg_evaluative,0
It also seems that curiosity in this context seems to be very related to surprise? There are neuroscience evidence indicating that humans turns to remember (putting more weights) on events that are more surprising.,,arg_request,1
"- The entire trajectory needs to be stored, so the memory wold grow with episode length.",,arg_structuring,0
I could see this being an issue when episode length is too long.,,arg_evaluative,0
"The paper tackles Structure from Motion, one of the canonical problems in computer vision, and proposes an approach that brings together geometry and physics on one hand and deep networks on the other hand.",,arg_structuring,0
Camera unprojection and warping (of depth maps and features) are used to build a cost volume onto hypothetical planes perpendicular to the camera axis.,,arg_structuring,0
"Similarly, various camera poses are sampled around an initial guess.",,arg_structuring,0
A deep network regresses form the cost volume to a camera pose and a depth map.,,arg_structuring,0
"The method can be applied iteratively, using the outputs of the current stage as the initial guess of the next one.",,arg_structuring,0
"Training is supervised, and the the results are evaluated on multiple datasets.",,arg_structuring,0
"I am inclined to recommend accepting the paper for publication, because it addresses a canonical problem, outperforms the state of the art on multiple datasets and brings together geometry / physics and deep learning, which is IMO very a promising and underexplored direction.",,arg_evaluative,0
"I found the method section a bit difficult to read though, and even after several readings I cannot get my head around it. Specifically, here are some issues that I hope the Authors could clarify.",,arg_evaluative,0
"1. In Sec. 3 the Authors write ""We then sample the solution space for depth and pose respectively around their initialization"".",,arg_fact,0
"However in Sec 3.2 they write ""we uniformly sample a set of L virtual planes {dl} Ll=1 in the inverse-depth space"".",,arg_fact,0
"In what way are the planes ""around their initialization""? If the initial depth map spans over multiple orders of magnitude, will the planes be uniformly sampled between the minimum and maximum disparity of the initial map?",,arg_request,1
"If yes, it seems that the initial depth map is not really needed, just its minimum and maximum value is needed, but then how come the method can be applied iteratively with respect to depth?",,arg_request,1
2. The Authors mention that depth maps are warped onto the virtual planes using differentiable bilinear interpolation.,,arg_fact,0
"Is there a mechanism to protect from interpolating across discontinuities? If no, were bleeding edge artifacts observed?",,arg_request,1
"3. In the introduction, the Authors point that prior methods have trouble dealing with textureless, reflective or transparent approaches, but it's not clear form the paper where it addresses these cases, and if yes, what is the mechanism for that.",,arg_evaluative,0
"Lastly, if the authors are not planning to release the code, the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.",,arg_request,1
"For example, ""our network learns a cost volume of size L × W × H using several 3D convolutional layers with kernel size 3 × 3 × 3""",,arg_other,0
"- more details about this network are needed, as well as the others in the paper.",,arg_request,1
"This paper proposed a general framework, DeepTwist, for model compression.",,arg_structuring,0
The so-called weight distortion procedure is added into the training every several epochs.,,arg_structuring,0
Three applications are shown to demonstrate the usage of the proposed approach.,,arg_structuring,0
"Overall, I think the novelty of the paper is very limited, as all the weight distortion algorithms in the paper can be formulated as the proximal function in proximal gradient descent.",,arg_evaluative,0
See http://www.stat.cmu.edu/~ryantibs/convexopt-S15/scribes/08-prox-grad-scribed.pdf for a reference.,,arg_evaluative,0
"Specifically, the proposed framework can be easily reformulated as a loss function plus a regularizer for proximal gradient.",,arg_fact,0
"Using gradient descent (GD), there will be two steps: (1) finding a new solution using GD, and (2) project the new solution using proximal function.",,arg_fact,0
"Now in deep learning, since SGD is used for optimization, several steps are need to locate reasonable solutions, i.e. the Distortion Step in the framework.",,arg_fact,0
Then proximal function can be applied directly after Distortion Step to project the solutions.,,arg_fact,0
"In this way, we can easily see that the proposed framework is a stochastic version of proximal gradient descent.",,arg_evaluative,0
"Since SGD is used for training, several minibatches are needed to achieve a relatively stable solution for projection using the proximal function, which is exactly the proposed framework in Fig. 1.",,arg_fact,0
"PS: After discussion, I think the motivation of the method is not clear to understand why the proposed method works.",,arg_evaluative,0
"The authors study the learning dynamics of deep neural networks, which is of fundamental importance but lacks understanding.",,arg_evaluative,0
"The authors study several dynamics like activation independence, gradient starvation, which gives new insights.",,arg_evaluative,0
"However, the assumption is too strong.",,arg_evaluative,0
There are two main results in the paper:,,arg_structuring,0
"1) Through learning, the neurons activates of one class.",,arg_structuring,0
"2) The classification error, with respect to the number of iterations of gradient descent, exhibits a sigmoidal shape.",,arg_structuring,0
"However, there are two strong assumptions: 1. the two data are perfectly separable by linear classifier.",,arg_structuring,0
"2.  H2 assumes ""at the beginning of training data points from different classes do not activate the same neurons"".",,arg_structuring,0
"This is a very strong initial assumption, I am not sure how likely this assumption would be satisfied.",,arg_evaluative,0
It sounds to me this assumption implicitly suggests that the algorithm is already ALMOST CONVERGENT.,,arg_fact,0
"If this assumption cannot be weakened, I don't think the paper can be accepted.",,arg_social,0
"After reading the authors' response, I'm revising my score upwards from 5 to 6.",,arg_other,0
"The authors propose a defense against adversarial examples, that is inspired by ""non local means filtering"".",,arg_structuring,0
"The underlying assumption seems to be that, at feature level, adversarial examples manifest as IID noise in feature maps, which can be ""filtered away"" by using features from other images.",,arg_structuring,0
"While this assumption seems plausible,  no analysis has been done to verify it in a systematic way.",,arg_evaluative,0
Some examples of verifying this are:,,arg_structuring,0
1. How does varying the number of nearest neighbors change the network behavior?,,arg_request,1
"2. At test time, a fixed number of images are used for denoising - how does the choice of these images change accuracy or adversarial robustness?",,arg_request,1
"3. Does just simple filtering of the feature map, say, by local averaging, perform equally well?",,arg_request,1
4. When do things start to break down? I imagine randomly replacing feature map values (i.e. with very poor nearest neighbors) will cause robustness and accuracy to go down - was this tested?,,arg_request,1
"Based on the paper of Athalye et. al., really the only method worth comparing to for adversarial defense, is adversarial training.",,arg_fact,0
It is hard to judge absolute adversarial robustness performance without a baseline of adversarial training.,,arg_fact,0
This paper proposed a fractional quantization method for deep net weights.,,arg_structuring,0
It adds XOR gates to produce quantized weight bits compared with existing quantization method.,,arg_structuring,0
It used tanh functions instead of a straight-through estimator for backpropagation during training.,,arg_structuring,0
"With both designs, the proposed method outperformed the existing methods and offered sub bit quantization options.",,arg_structuring,0
The use of XOR gates to improve quantization seems novel.,,arg_evaluative,0
The sub bit quantization achieved by this method should be interesting to the industrial.,,arg_evaluative,0
It significantly improved the quantization rate with slightly quality degradation.,,arg_evaluative,0
"With 1 bit quantization, it outperformed the state-of-the-art.",,arg_evaluative,0
The results seem thorough and convincing.,,arg_evaluative,0
This paper suggests a novel and compact neural network architecture which uses the information within bag-of-words features.,,arg_structuring,0
The proposed algorithm only uses the patch information independently and performs majority voting using independently classified patches.,,arg_structuring,0
"The proposed method provides the state-of-the-art prediction accuracy unexpectedly, and several additional experiments show the state-of-the-art neural networks mainly learn without association between information in different patches.",,arg_structuring,0
"The proposed algorithm is simple and does not provide completely new idea, but this paper has a clear contribution connecting the previous main idea of feature extraction, bag-of-words, and the prevailing blackbox algorithm, CNN.",,arg_evaluative,0
The results in the paper are worth to be shared in the community and need further investigated.,,arg_evaluative,0
"The presented experiments look fair and reasonable to show the importance of the independent patch information (without association between them), and the presented experimental results show some state-of-the-art methods also perform with independent patch information.",,arg_evaluative,0
Comparison with attention models is necessary to compare the important patches obtained from conventional networks.,,arg_request,1
"This work proposes a functional form for the relationship between <dataset size, model size> and generalization error, and performs an empirical study to validate it.",,arg_structuring,0
"First, it states 5 criteria that such a functional form must take, and proposes one such functional form containing 6 free coefficients that satisfy all these criteria.",,arg_structuring,0
"It then performs a rigorous empirical study consisting of 6 image datasets and 3 text datasets, each with 2 distinct architectures defined at several model scales, and trained with different dataset sizes.",,arg_structuring,0
"This process produces 42-49 data points for each <dataset, architecture> pair, and the 6 coefficients of the proposed functional form are fit to those data points, with < 2% mean deviation in accuracy.",,arg_structuring,0
"It then studies how this functional form performs at extrapolation, and finds that it still performs pretty well, with ~4.5% mean deviation in accuracy, but with additional caveats.",,arg_structuring,0
Decision: Accept.,,arg_evaluative,0
"This paper states 5 necessary criteria for any functional form for generalization error predictor that jointly considers dataset size and model size, then empirically verifies it with multiple datasets and architectures.",,arg_evaluative,0
"These criteria are well justified, and can be used by others to narrow down the search for functions that approximate the generalization error of NNs without access to the true data distribution, which is a significant contribution.",,arg_evaluative,0
"The empirical study is carefully done (e.g., taking care to subsample the dataset in a way that preserves the class distribution).",,arg_evaluative,0
I also liked that the paper is candid about its own limitations.,,arg_evaluative,0
"A weakness that one might perceive is that the coefficients of the proposed functional form still needs to be fit to 40-ish trained NNs for every dataset and training hyperparameters, but I do not think this should be held against this work, because a generalization error predictor (let alone its functional form) that works for multiple datasets and architecture without training is difficult, and the paper does include several proposals for how this can still be used in practice.",,arg_evaluative,0
"(Caveat: the use of the envelope function described in equation 5 (page 6) is not something I am familiar with, but seems reasonable.)",,arg_evaluative,0
Issues to address:,,arg_structuring,0
- Fitting 6 parameters to 42-49 data points raises concerns about overfitting.,,arg_evaluative,0
"Consider doing cross validation over those 42-49 data points, and report the mean of deviations computed on the test folds.",,arg_request,1
"The extrapolation section did provide evidence that there probably isn't /that/ much overfitting, but cross validation would directly address this concern.",,arg_fact,0
"- In addition, the paper provides the standard deviation for the mean deviations over 100 fits of the function as the measure of its uncertainty, but I suspect that the optimizer converging to different coefficients at different runs isn't the main source of uncertainty.",,arg_evaluative,0
A bigger source of uncertainty is likely due to there being a limited amount of data to fit the coefficients to.,,arg_evaluative,0
Taking the standard deviation over the deviations measured on different folds of the data would be better measure of uncertainty.,,arg_request,1
Minor issues:,,arg_structuring,0
"- Page 8: ""differntiable methods for NAS."" differentiable is misspelled.",,arg_request,1
"The paper proposes learning Restricted Boltzmann Machines for solving small computational tasks (e.g., 1-bit addition) and composing those RBMs to form a more complex computational module (e.g., 16-bit addition).",,arg_structuring,0
The claim is that such an approach can be more data efficient than learning a single network to directly learn the more complex module.,,arg_structuring,0
Results are shown for addition and factoring tasks.,,arg_structuring,0
- The paper is somewhat easy to follow and the figures are helpful. But the overall organization and flow of ideas can be improved significantly.,,arg_evaluative,0
"- The term ""combinatorial optimization"" is used in a confusing way -- addition would not usually be called a combinatorial optimization problem.",,arg_evaluative,0
- It would be good to understand what benefit does the stochasticity of RBMs provide.,,arg_request,1
How do deterministic neural networks perform on the addition and factoring tasks?,,arg_request,1
"The choice of RBMs is not motivated well and without any comparisons to alternatives, it comes across",,arg_evaluative,0
as arbitrary.,,none,0
- That learning simple functions and composing them to compute more complex functions would be more data efficient than directly learning the complex functions does not seem very surprising.,,arg_evaluative,0
"After all, the former approach gets a lot more knowledge about the target function built into it.",,arg_evaluative,0
"It's good that the paper empirically confirms the intuition, but doesn't feel like a significant contribution on its own.",,arg_evaluative,0
"- The paper would be stronger if it includes more complex tasks, e.g., TSP, and show that the same ideas can be applied to improve the learning a solver for such tasks.",,arg_request,1
"The current tasks and problem sizes are not very convincing, and the accuracy results are not very compelling.",,arg_evaluative,0
The paper proposes an approach to provide contrastive visual explanations for deep neural networks -- why the network assigned more confidence to some class A as opposed to some other class B. As opposed to the applicability of previous approaches to this problem -- the approach is designed to directly answer the contrastive explanations question rather adapting other visual saliency techniques for the same.,,arg_structuring,0
"Overall, while I find the proposed approach simple -- the paper needs to address some issues regarding the claims made and should provide more quantitative experimental results justifying the same.",,arg_request,1
"- Apart from some flaws in the claims made in the paper, the paper is easy to follow and understand.",,arg_evaluative,0
"- Assuming the availability of a latent model over the images of the input distribution, the proposed approach is directly applicable and faster.",,arg_evaluative,0
- The authors clearly highlight the problems associated with existing explanation modalities and approaches; ranging from ones applicable to only specific deep architectures to ones using backpropagation based heuristics.,,arg_evaluative,0
- The proposed approach to generate contrastive explanations is simple and is structured along the lines of methods utilizing probe images to explain decisions -- except for the added advantage that the provided explanations are instance-agnostic due to the assumption of a latent model over the input distribution.,,arg_structuring,0
Comments:,,arg_structuring,0
- One of the problems highlighted in the paper regarding existing explanation modalities is the use of another black-box to explain the decisions of an existing deep network (also somewhat of a black-box) which the authors claim their model does not suffer from.,,arg_structuring,0
The proposed approach provides explanations by operating in the latent space of a learned generative model of the input distribution.,,arg_structuring,0
The learned generator in itself is somewhat of a black-box itself -- there has been prior work indicating how much of the input distribution,,arg_fact,0
are GANs able to capture.,,none,0
"As such, conditioning on a generative model to propose such contrastive explanations is to some extent using another black-box (generator) to explain the decisions of an existing one.",,arg_fact,0
"Thus, the above claim made in the paper does not seem well-founded.",,arg_evaluative,0
"Furthermore, in experiments, the paper does not provide any quantitatively convincing results to suggest the generator in use is a good one.",,arg_evaluative,0
- While the authors suggest that a latent model over the input distribution needs to be trained only once and is applicable off-the-shelf for any further contrastive explanations regarding any network operating on the same dataset -- learning such a model of the input space is an overhead in itself.,,arg_evaluative,0
"In this light, experiments demonstrating comparisons between GANs and VAEs as the reference generative model for explanations would have made the paper stronger (as the proposed approach relies explicitly on how good the generative model is).",,arg_request,1
"- The paper proposes an interesting experiment to show that the proposed approach is somewhat capable of capturing slightly adversarial biases in the input domain (adding square to the top-left of images of class ‘8’). While I like this experiment, I feel this has not been explored to completion in the sense of experimenting with robustness with respect to structured as well as unstructured perturbations.",,arg_request,1
- Typographical Errors: Section 3.1 repeats the use of D for a discriminator as well as the input distribution.,,arg_request,1
Procedure 1 and Procedure 2 share the same titles -- which is slightly misleading.,,arg_request,1
"In addition, Procedure 1 is not referenced in the text which makes is hard to understand the utility of the same.",,arg_request,1
In Section 4.1,,arg_evaluative,0
", the use of Gradcam and Lime to generate counterfactual explanations is not very clear and makes it slightly hard to follow.",,none,0
"Citations used for Gradcam are wrong -- Sundarajan et al., 2016 should be changed to Selvaraju et al., 2017.",,arg_request,1
Experimental Issues:,,arg_structuring,0
- Experimental results are provided only on MNIST and Fashion-MNIST.,,arg_evaluative,0
Since the paper focuses explicitly on providing contrastive explanations for choosing a class A over another class B -- experiments on datasets which do not have real-images seem insufficient.,,arg_evaluative,0
Additional experiments on at least ImageNet would have made the paper stronger.,,arg_request,1
"Regarding contrastive explanations, experiments on datasets where distractor classes (y_probe) are present in addition to the class interest (y_true) seem important -- PASCAL VOC, COCO, etc.",,arg_request,1
"Specifically, since the explanations provided are visual saliency maps the paper would have been stronger if there were experiments suggesting -- what needs to change in a region of an image classified as a ‘cat’ to be classified as a ‘dog’ while there is an instance of the class - ‘dog’ present in the image itself.",,arg_request,1
"Also, section 7 in Gradcam (https://arxiv.org/pdf/1610.02391.pdf) provides a procedure to generate counter-factual explanations using Gradcam.",,arg_fact,0
Is there a particular reason the authors did not choose to adopt the above technique as a baseline?,,arg_request,1
"- Experimental results provided in the paper are only qualitative -- as such, I do not find the comparisons (and improvements) over the existing approaches convincing enough.",,arg_evaluative,0
"Since, there is no clear metric to evaluate contrastive explanations -- human studies to judge the class-discriminativeness (or trust) of the proposed approach would have made the paper stronger.",,arg_request,1
The authors adressed the issues raised/comments made in the review. In light of my comments below to the author responses -- I am not inclined towards increasing my rating and will stick to my original rating for the paper.,,arg_other,0
The paper has two distinct parts.,,arg_structuring,0
"In the first part (section 2) it studies the volume of preimage of a ReLU network’s activation at a certain layer as being singular, finite, or infinite.",,arg_structuring,0
This part is an extension of the work in the study of (Carlsson et al. 2017).,,arg_structuring,0
The second part (section 3) builds on the piecewise linearity of a ReLU network’s forward function.,,arg_structuring,0
"As a result, each point in the input space is in a polytope where the model acts linearly.",,arg_structuring,0
"In that respect, it studies the stability of the linearized model at a point in the input space.",,arg_structuring,0
The study involves looking at the singular values of the linear mapping.,,arg_structuring,0
"The findings of the paper are non-trivial and the implications potentially interesting. However, I have some concerns about the study.",,arg_evaluative,0
There is a key concern about the feasibility of the numerical analysis for the first part.,,arg_evaluative,0
"That is, a layer-by-layer study can have a computational problem where the preimage is finite at each layer but can become infinite by the mapping of the preceding layers.",,arg_evaluative,0
"In that regard, I would like the authors to comment on the worst-case computational complexity of the numerical analysis for determining the volume of a preimage through multiple layers.",,arg_request,1
"As for the second part, the authors mention the increase in the dimensionality of the latent space in the current deep networks.",,arg_fact,0
"However, this observation views convolutional networks as MLPs.",,arg_fact,0
"However, there is more structure in a convolutional layer’s mapping function.",,arg_fact,0
The structure is obtained by the shared and sparse rows of matrix A. I would like the authors to comment on how the studies will be affected by this property of the common networks.,,arg_request,1
"All in all, while there are some concerns and the contributions are not entirely novel, the reviewer believes the findings of the paper is generally non-trivial and shed more light on the inner workings of the ReLU networks and is thus a valuable contribution to the field.",,arg_evaluative,0
This paper propose to study the generalization properties of GANs through interpolation.,,arg_structuring,0
"They first propose to learn a linear (and non-linear) interpolation in the latent space for a specific type of image transformation for example zoom, translation, rotation, luminance, etc... They show that linear interpolation in GANs can produce really realistic images along the path and enable to control and transform generated images to some extent.",,arg_structuring,0
"They then propose to measure to what extent the generated images can be transformed without ""breaking"".",,arg_structuring,0
Finally they show that the quality of the interpolation can be improved by learning the interpolation and generator jointly.,,arg_structuring,0
I'm in favour of accepting this paper.,,arg_evaluative,0
The paper is well written and organized.,,arg_evaluative,0
The experiments and observations are very interesting and really illustrate the generalization capacity of GANs.,,arg_evaluative,0
Main argument:,,arg_structuring,0
- I think those observations are very valuable to the community and are a good way to get insight into the capabilities of GANs.,,arg_evaluative,0
This also give interesting informations about the different bias present and learnt in the dataset.,,arg_evaluative,0
This could also lead to very nice applications.,,arg_evaluative,0
- The interpolation with StyleGAN and BigGAN seem to give qualitatively very different results.,,arg_fact,0
"It would have been very interesting to study the quality of interpolations on more models and datasets, and compare their generalization capabilities as well as the bias present in the different datasets.",,arg_request,1
- Does training the generator and interpolation jointly improve the quality of the generator in general ? It would have been nice to run this method on more complicated dataset like CIFAR10 and see if this method increase the overall FID score.,,arg_request,1
Minor comments:,,arg_structuring,0
- In appendix A.2 the authors explain how the range of $\alpha$ is set for the different experiments.,,arg_fact,0
However it's not clear how is this range used in practice ? Do you sample uniformly $\alpha$ in this range to train the linear interpolation ?,,arg_request,1
Also how many steps are required to learn the linear interpolation ? How much the does it influence the quality of the interpolation ?,,arg_request,1
- There is a typo in equation 6,,arg_request,1
- In figure 6: What does the right figure represent ? especially what are the different colours ?,,arg_request,1
"This paper proposes a hybrid technique for rendering “control-variate” and class-conditional image in two steps, first by generating an approximate rendering of the image (“Y”) conditional on the control variate and then filling in the details with a conditional GAN dependent on a latent noise variable Z (although I note that the caption of Figure 2 which identifies “Z” as the identity makes this rather confusing).",,arg_structuring,0
"To ensure that Z is used to explain aspects of the model that are separate from the controlled variation, Z is combined in the refinement model at later steps (since otherwise the posterior over Z and Y conditional on X could induce entanglement between the variables).",,arg_structuring,0
"In the “supervised” setting where the control variates are observed, Y can be learned as a simple regression problem independent of the other parts of the model, and this two-stage refinement process is demonstrated (using inception scores) to generate convincing samples, including when C consists of up to 10 control variates.",,arg_structuring,0
"In the unsupervised setting, a beta-VAE is used to learn a disentangled representation of X as a proxy for C, but then the data is regenerated using a two step process.",,arg_structuring,0
"Readability suggestion: the paper starts with a very nice motivating example, but when the setup is provided, i.e., that (x,c) pairs are the input to the learner, the intended content of c is not immediately clear- control variates could assume anything from general context information to privileged information.",,arg_request,1
A similarly informative example would be great!,,arg_request,1
"Clarification regarding lemma 1: it seems that if the true posterior cannot be expressed by q, a gap will necessarily remain, even in the “limit” of perfect learning. Is this correct?",,arg_request,1
"Overall: this paper makes a convincing case that it can be used to generate higher quality images, but not that this improves the quality of the disentangled representations.",,arg_evaluative,0
"In fact, the separate training seems to make this unlikely.",,arg_evaluative,0
The paper presents an auto completion for UI layout design.,,arg_structuring,0
"The authors formulate the problem as partial tree completion, and investigate a range of variations of layout decoders based on Transformer.",,arg_structuring,0
The paper proposes two models: Pointer and Recursive Transformer.,,arg_structuring,0
The paper designs three sets of metrics to measure the quality of layout prediction based on the literature and the domain specifics of user interface interaction.,,arg_structuring,0
The writing quality is readable.,,arg_evaluative,0
The presentation is nice.,,arg_evaluative,0
The task of auto completion for UI layout design is relatively new.,,arg_evaluative,0
The paper misses the key baseline in Bayesian optimisation using tree structure [1] which can perform the prediction under the tree-structure dependencies.,,arg_request,1
"[1] Jenatton, Rodolphe, et al. ""Bayesian optimization with tree-structured dependencies."" Proceedings of the 34th International Conference on Machine Learning-Volume 70. JMLR. org, 2017.",,arg_other,0
NB: the reviewer has low confidence in evaluating this paper.,,arg_other,0
"This papers studies how to explore, in order to generate experience for faster learning of policies in context of RL.",,arg_structuring,0
"RL methods typically employ simple hand-tuned exploration schedules (such as epsilon greedy exploration, and changing the epsilon as training proceeds).",,arg_fact,0
This paper proposes a scheme for learning this schedule.,,arg_structuring,0
The paper does this by modeling this as a non-stationary multi-arm bandit problem.,,arg_structuring,0
"Different exploration settings (tuple of choice of exploration, and the exact hyper-parameter), are considered as different non-stationary multi-arm bandits (while also employing some factorization) and expected returns are maintained over training.",,arg_structuring,0
Arm (exploration strategy and hyper-parameter) is picked according to the return.,,arg_structuring,0
"The paper demonstrates results on the Atari suite of RL benchmarks, and shows results that demonstrate that their proposed search leads to faster learning.",,arg_structuring,0
Strength:,,arg_structuring,0
1. The paper tackles an interesting and important problem.,,arg_evaluative,0
"The proposed solution is simple, yet effective.",,arg_evaluative,0
Shortcomings:,,arg_structuring,0
1. The presentation is somewhat convoluted.,,arg_evaluative,0
"The paper motivates the problem that we need to pick out an exploration sequence that optimizes learning progress, but then approximates it as simply measuring the return.",,arg_evaluative,0
"Given there is no theoretical justification for the approximation, I believe the paper claims more than what it delivers and should change the presentation, so as not to claim that it is measuring and capturing learning progress to learn faster.",,arg_evaluative,0
2.,,none,0
"I am confused by Figure 4, and in general with the relative rank metrics.",,arg_evaluative,0
"Specifically, in Figure 4, is it that the proposed bandit approach not as good as picking a single hyper-parameter for the different settings (T=0.01, eps=0.01, omega=2.0)?",,none,0
"Similarly, for Figure 2, a singe fixed z, seems to do better than the bandit versions.",,none,0
Why doesn't the proposed bandit algorithm not pick out the best hyper-parameter?,,arg_request,1
How well,,none,0
would a simpler hyper-parameter search procedure (picking the best hyper-parameter after the first 2000 episodes)?,,arg_request,1
"3. This apart, I think that the experiment section is pretty hard to read, given all the metrics and methodology is in the Appendix.",,arg_evaluative,0
An alternate organization that presents all the main results in the main body in a self-contained manner will help.,,none,0
4. Comparison with past works.,,arg_evaluative,0
I believe there are other existing works that should be cited and compared to.,,none,0
"Using bandits to decide between different hyper-parameters is common (for example, see [A] for a service to do this with ML models), [B] uses improvements in accuracy as a way to pick between which question type to train on.",,none,0
Such past works should be cited and compared against.,,none,0
[A] https://ai.google/research/pubs/pub46180,,none,0
[B] Learning by Asking Questions,,none,0
"Ishan Misra, Ross Girshick, Rob Fergus, Martial Hebert, Abhinav Gupta and Laurens van der Maaten",,none,0
"This paper addresses the important / open problem of graph generation, and specifically in a conditional/transductive setting.",,arg_structuring,0
"Graph generations is a new topic, it is difficult, and has many important applications, for instance generating new molecules for drug development.",,arg_structuring,0
"As stated by the authors, this is a relatively open field: there are not many papers in this area, with most approaches today resorting to domain specific encodinings, or ""flattening"" of graphs into sequences to then allow for the use recurrence (like in MT); this which per se is an rather coarse approximation to graph topology representations, thus fully motivating the need for new solutions that take graph-structure into account.",,arg_structuring,0
"The setting / application of this method to graph synthesis of suspicious behaviours of network users, to detect intrusion, effectively a Zero-shot problem, is super interesting.",,arg_structuring,0
"The main architectural contribution of this paper are graph-deconvolutions, practically a graph-equivalent of CNN's depth-to-space - achieved by means of transposed structural matrix multiplication of the hidden GNN (graph-NN) activation - simple, reasonable and effective.",,arg_structuring,0
"While better than most of the baseline methods, the N^2 memory/computational complexity is not bad, but still too high to scale to very large graphs.",,arg_evaluative,0
"Results are provided on relatively new tasks so it's hard to compare fully to previous methods, but the authors do make an attempt to provide comparisons on synthetic graphs and intrusion detection data.",,arg_evaluative,0
The authors do published their code on GitHub with a link to the datasets as well.,,arg_evaluative,0
"As previously mentioned in public comments on this forum, some points in the paper are not very clear; specifically regarding the loss function, the definition of ""edge-to-edge"" convolutions and generally the architectural choice related to the conditional GAN discriminator.",,arg_evaluative,0
"Clarifications of these points, and more in general the philosophy behind the architectural choices made, would make this paper a much clearer accept.",,arg_request,1
Thank you!,,arg_social,0
"ps // next my previous public comments, in detail, repeated ...",,arg_structuring,0
--,,arg_structuring,0
"- the general architecture, and specifically the logic behind the edge-to-edge convolution, and generally the different blocks in fig.1 ""graph translator"".",,arg_request,1
"- how exactly do you do a L1 loss on graphs? I'd have to assume the topology of the graph is unchanged between Gy and T(Gx) ~ and then maybe take L1 of weight matrix? But then is this general enough ~ given your stated goal of modeling different topologies? Either ways, more explanation / and perhaps equations to clarify this loss would be very helpful.",,arg_request,1
"- why do you need a conditional GAN discriminator, if you already model similarity by L1?",,arg_request,1
"Typically one would use a GAN-D() to model ""proximity"" to the source-distribution, and then a similarity loss (L1 in your case) to model ""proximity"" to the actual input sample, in the case of trasductional domains.",,arg_fact,0
"Instead here you seem to suggest to use L1 and GAN to do basically the same thing, or with significant overlap anyways.",,arg_evaluative,0
This is confusing to me.,,arg_evaluative,0
Please explain the logic for this architectural choice.,,arg_request,1
"-  could you please explain the setting for the “gold standard” experiment. I'd have to assume, for instance, you train a GNN in a supervised way by using both source (non-suspicious) and target (suspicious) behaviour, and label accordingly? That said I am not 100% sure of this problem setting.",,arg_request,1
This paper deal with learning abstract MDPs for planning in tasks that require long-horizon due to sparse rewards.,,arg_structuring,0
This is an extremely important and timely topic in the RL community.,,arg_evaluative,0
The paper is generally clear and well written.,,arg_evaluative,0
The proposed algorithm seems reasonable and it is conceptually simple to understand.,,arg_evaluative,0
In the current experimental results presented it also seems to outperform the alternative baselines.,,arg_evaluative,0
"Nonetheless, the paper has few flaws that significantly impact the stated contributions and reduced my rating.",,arg_structuring,0
1) a stated contribution are theoretical guarantees about the performance of the algorithm.,,arg_fact,0
"this analysis is not currently included in the main body of the manuscript, but rather in the appendix, which I find rather annoying.",,arg_evaluative,0
"Moreover, said the analysis is in my opinion not sufficiently rigorous, with hand-wavy arguments, no formal proof and unclear terms (e.g. how do you define near-optimal?)",,arg_evaluative,0
.,,none,0
"Moreover, as observed by the authors this analysis currently rely on strong assumptions that might make it rather unrealistic.",,arg_evaluative,0
"Overall, if you want to claim theoretical guarantees you will have to significantly improve the manuscript.",,arg_request,1
"2) Related work, although extensive in terms of the number of references, do not help to place this work in the literature.",,arg_evaluative,0
Listing related work is no the same as describing similarities and differences compared to previous methods.,,arg_evaluative,0
"For example, a paper that obviously comes to mind is ""FeUdal Networks for Hierarchical Reinforcement Learning"".",,arg_evaluative,0
What are the differences to your approach?,,arg_request,1
"Also, please place the related work earlier on in the paper.",,arg_request,1
"Otherwise, it is impossible for a reader to correctly and objectively relate your proposed approach to previous literature.",,arg_evaluative,0
"3) In its current form, the experimental results are extremely cherry-picked, with a very small number of tasks evaluated, and for each task a single selected baseline used.",,arg_evaluative,0
This needs to be changed: a) you should run all the baselines for each of the current tasks b) you should also expand the experiments evaluated to include tasks where it is not obvious that a hierarchy would help/is necessary c) you should include more baselines.,,arg_request,1
"feudal RL should be one, Roderick et al 2017 should be another one (especially considering your discussion in Sec 8)",,none,0
Additional feedback:,,arg_structuring,0
- The paper is currently oriented towards discrete states. What can you say about continuous spaces?,,arg_request,1
- The use of random exploration for the discoverer is underwhelming. Have you tried different approaches? Would more advanced exploration techniques work or improve the performance?,,arg_request,1
- Using only 4 seeds seems too little to provide accurate standard deviations.,,arg_evaluative,0
Please run at least 10 experiments.,,arg_request,1
"- The use of RAM is a fairly serious limitation of your experimental setting in my view. You should include results also for the pixel space, even if negative.",,arg_request,1
"Otherwise, this choice is incomprehensible.",,arg_evaluative,0
This paper discusses the addition of a regularizer to a standard sparse coding/dictionary learning algorithm to encourage the atoms to be used with uniform frequency.,,arg_structuring,0
I do not think this work should be accepted to the conference for the following reasons:,,arg_structuring,0
1: The authors show no benefit of this scheme except perhaps faster convergence.,,arg_evaluative,0
"If faster training of dictionary learning models was a bottleneck in practical applications, this might be of interest, but it is not.",,arg_evaluative,0
SPAMS (http://spams-devel.gforge.inria.fr/) can train a model on image patches as the authors do here in a few tens of seconds on a modern computer.,,arg_fact,0
"On the other hand, the authors give no evidence, empirical or otherwise, that their method is useful on any downstream tasks.",,arg_evaluative,0
"In my view, they do not even show that the distribution of atom usage will be better with their algorithm after the learning has converged, as at least according to their learning curves, the baselines have not finished converging.",,arg_evaluative,0
It is not even clear that the final compression of the baselines would not be better.,,arg_evaluative,0
"Even if they did show these convincingly, it is not obvious to me that it is valuable; the authors need to *show* that uniform usage is desirable.",,arg_request,1
"2:    The authors should compare against several costs/algorithms (e.g. l_0 with OMP, l_1 with LARS, etc.), and across various N_0/sparsity penalties, and across several datasets.",,arg_request,1
"The empirical evaluation is quite weak- one sparsity setting, two baselines, one dataset",,arg_evaluative,0
.,,none,0
"Even without the ""train to convergence"" question above, I don't think the authors have demonstrated that their claims on the properties of their algorithms/formulations are generally true.",,arg_evaluative,0
Summary:,,arg_structuring,0
The authors propose quantize the weights of a neural network by enabling a fractional number of bits per weight.,,arg_structuring,0
They use a network of differentiable XOR gates that maps encrypted weights to higher-dimensional decrypted weights to decode the parameters on-the-fly and learn both the encrypted weights and the scaling factors involved in the XOR networks by gradient descent.,,arg_structuring,0
Strengths of the paper:,,arg_structuring,0
- The method allows for a fractional number of bits per weights and relies of well-known differentiable approximations of the sign function.,,arg_structuring,0
"Indeed, virtually any number of bits/weights can be attained by varying the ratio N_in/N_out.",,arg_structuring,0
- The papers displays good results on ImageNet for a ResNet-18.,,arg_evaluative,0
Weaknesses of the paper:,,arg_structuring,0
- Some arguments that are presented could deserve a bit more precision.,,arg_request,1
"For instance, quantizing to a fractional number of bits per weights per layer is in itself interesting.",,arg_evaluative,0
"However, if we were to quantize different layers of the same network with distinct integer  ratio of bits per weights (say 1 bit per weight for some particular layers and 2 bits per weight for the other layers), the average ratio would also be fractional (see for instance ""Hardware-aware Automated Quantization with Mixed Precision"", Wang et al., where the authors find the right (integer) number of bits/weights per layer using RL).",,arg_fact,0
"Similarly, using vector quantization does allow for on-chip low memory: we do not need to re-instantiate the compressed layer but we can compute the forward in the compressed domain (by splitting the activations into similar block sizes and computing dot products).",,arg_request,1
- More extensive and thorough experiments could improve the impact of the paper.,,arg_evaluative,0
"For instance, authors could compress the widely used (and more challenging) ResNet-50 architecture, or try other tasks such as image detection (Mask R-CNN).",,arg_request,1
"The table is missing results from: ""Hardware Automated Quantization"", Wang et al ; ""Trained Ternary Quantization"", Zhu et al ; ""Deep Compression"",  Han et al; ""Ternary weight networks"", Li et al (not an extensive list).",,arg_request,1
"- Similarly, providing some code and numbers for inference time would greatly strengthen the paper and the possible usage of this method by the community.",,arg_request,1
"Indeed, I wonder what the overhead of decrypting the weights on-the-fly is (although it only involves XOR operations and products)",,arg_request,1
"- Small typos: for instance, two points at the very end of section 5.",,arg_request,1
Justification fo rating:,,arg_structuring,0
The proposed method is well presented and illustrated.,,arg_evaluative,0
"However, I think the paper would need either (1) more thorough experimental results (see comments above, points 2 and 3 of weaknesses) or (2) more justifications for its existence (see comments above, point 1 of weaknesses).",,arg_request,1
This paper proposed a bio-inspired sparse coding algorithm where iterations,,arg_structuring,0
for dictionary updates take into account the past updates.,,none,0
It is argued,,arg_structuring,0
that time takes a crucial rule in learning.,,none,0
The paper is quite well written and contains an extensive literature review,,arg_evaluative,0
demonstrating a good understanding of previous literature in both ML/DL and biological,,none,0
vision.,,none,0
"The idea of using a ""non-linear gain normalization"" to adjust atom selection",,arg_evaluative,0
"in sparse coding is interesting and as far as I know novel, while providing",,none,0
interesting empirical results: The system learns in an unsupervised way faster.,,none,0
Misc:,,arg_structuring,0
- Using < > for latex brakets is not ideal.,,arg_request,1
"I would recommend: $\langle\,,\rangle$",,none,0
"- ""derivable"" I guess you mean ""differentiable""",,arg_request,1
- Oliphant and Hunter are cited for Numpy/scipy and matplotlib but the,,arg_request,1
reference to Pedregosa et al. for sklearn is missing.,,none,0
The paper addresses the problem of providing saliency-based visual explanations of deep models tasked at image classification.,,arg_structuring,0
"More specifically, instead of generating visualizations directly highlighting the image pixels that support the the decision of an image belonging to class A, it generates ""contrastive"" visualizations indicating the pixels that should be added or suppressed in order to support the decision of a image belonging to class A and not to class B.",,arg_structuring,0
"The method formulates the generation of these contrastive explanations through a generative adversarial network (GAN), where the discriminator D is the image classification model to be explained and the generator G is a generative model trained to produce images from the dataset used to train D.",,arg_structuring,0
Experiments on the MNIST and fashion-MNIST datasets compares the performance of the proposed method w.r.t. some methods from the literature.,,arg_structuring,0
Overall the manuscript is well written and its content is relatively easy to follow.,,arg_evaluative,0
The idea of generating contrastive explanations through a GAN-based formulation is well motivated and seems novel to me.,,arg_evaluative,0
My main concern with the manuscript are the following:,,arg_structuring,0
"i) The proposed method seems to be specifically designed for the generation of contrastive explanations, i.e. why the model predicted class A and not class B.",,arg_fact,0
"While the generation of this type of explanations is somewhat novel, from the text it seems that the proposed method may not be able to indicate what part of the image content drove the model to predict class A. Is this indeed the case?",,arg_request,1
"ii) Although the idea of generating contrastive explanations is quite interesting, it is not that novel.",,arg_evaluative,0
"See Kim et al., NIPS'16, Dhurandhar et al., arXiv:1802.07623.",,arg_structuring,0
"Moreover, regarding the presented results on the MNIST dataset (Sec 4.1) where some of the generated explanations highlight gaps to point differences between digit classes.",,arg_structuring,0
"The work from Samek et al., TNNLS'17 and  Oramas et al., arXiv:1712.06302 seem to display similar properties in their explanations without the need of explicit constractive pair-wise training/testing.",,arg_fact,0
The manuscript would benefit from positioning the proposed method w.r.t. these works.,,arg_request,1
"iii) Very related to the first point, in the evaluation section (Sec.4.1) the proposed method is compared against other methods in the literature.",,arg_structuring,0
"Three of these methods, i.e. Lime, GradCam, PDA, are not designed for producing contrastive explanations, so I am not sure to what extend this comparison is appropriate.",,arg_evaluative,0
"iv) Finally, the reported results are mostly qualitative.",,arg_evaluative,0
I find the set of provided qualitative examples quite reduced.,,arg_evaluative,0
"In this regard, I encourage the authors to update the supplementary material in order to show extended qualitative results of the explanations produced by their method.",,arg_request,1
"In addition, I recommend complementing the presented qualitative comparisons with quantitative evaluations following protocols proposed in existing work, e.g. a) occlusion analysis (Zeiler et al., ECCV 2014, Samek et al.,2017), a pointing experiment (Zhang et al., ECCV 2016), or c) a measurement of explanation accuracy by feature coverage",,arg_request,1
(Oramas et al. arXiv:1712.06302).,,none,0
Summary:,,arg_structuring,0
The authors propose a SfM model which integrates geometric consistency with a learned pose and depth network.,,arg_structuring,0
"An initial estimate of depth and pose are used to construct pose and depth cost volumes, which are then fed into a pose regression and depth refinement network, to produce a new set of cost volumes, and so on.",,arg_structuring,0
"In this manner, the pose and depth estimation are improved iteratively.",,arg_structuring,0
Strengths:,,arg_structuring,0
The proposed model is well motivated and shows strong performance and generalization ability on several datasets.,,arg_evaluative,0
There are convincing experiments to show the importance of the P-CV network.,,arg_evaluative,0
Weaknesses:,,arg_structuring,0
The authors claim that the LM optimization in BA-Net is memory inefficient and may lead to non-optimal solutions.,,arg_fact,0
It’s not clear to me that the proposed method can guarantee optimality any better.,,arg_request,1
"It’s also unclear if the proposed method is more memory efficient, since the authors only unroll 4 iterations of it.",,arg_evaluative,0
Other comments:,,arg_structuring,0
"It would be very interesting to see the test time behavior of the network when it is run with more iterations than it is trained with (say 10 or 20), especially since the depth error does not seem to have stopped decreasing at only 4 iterations.",,arg_request,1
It’s not made entirely clear whether the training backpropagates through the update/construction of the pose and depth cost volumes.,,arg_request,1
"In equation 5, “x” should be “i”.",,arg_request,1
"This paper proposed DSGAN which learns to generate unseen data from seen data distribution p_d and its somehow “broad” version p_{\hat d} (E.g., p_d convolved with Gaussian).",,arg_structuring,0
The “unseen data” is the one that appears in p_{\hat d} but not in p_d.,,arg_structuring,0
DSGAN is trained to generate such data.,,arg_structuring,0
"In particular, it uses samples from p_d as fake data and samples from p_{\hat d} as the real one.",,arg_structuring,0
"Although the idea seems to be interesting, the paper seems to be a bit incremental and is a simple application of existing GAN techniques.",,arg_evaluative,0
The paper shows two applications (semi-supervised learning and novelty detection) and it is not clear that the proposed method outperforms existing GAN methods in the classification accuracy in MNIST/SVHN/CIFAR10 (Table 1) and existing sampling methods (Table. 3).,,arg_evaluative,0
It seems that the sampled reconstruction results (Fig. 8) are not as good as VAE on CIFAR10.,,arg_evaluative,0
"I would also expect more ablation studies about how to pick p_{\had d}, which seems to be the key of this approach, in MNIST and CIFAR10.",,arg_request,1
"In terms of writing, the paper is a bit confusing in terms of motivations and notations.",,arg_evaluative,0
"Overall, the method looks incremental and experimental results are mixed on small datasets so I vote for rejection.",,arg_evaluative,0
Note that I am not an expert on GAN/VAE so I put low confidence here.,,arg_other,0
*** Update ***,,arg_structuring,0
"I'd like to thank the authors for answering my questions, and I am satisfied with their response. I have read the other reviews for this paper as well, and I am keeping my score.",,arg_social,0
"This paper proposes BERTScore, a method for automatic evaluation of text.",,arg_structuring,0
Their method uses BERT to produce contextualized word representations for the words in the reference and hypothesis.,,arg_structuring,0
"Then they compute the precision, recall, and F1 by greedily matching up words between the hypothesis and reference.",,arg_structuring,0
"To be more specific, for say recall they take each word in the reference and compute the cosine with all words in the hypothesis.",,arg_structuring,0
Then they add up the largest cosine similarity for each word and average them together.,,arg_structuring,0
Precision is defined similarly but with the roles of hypothesis and reference switched.,,arg_structuring,0
F1 is then the harmonic mean of these two scores.,,arg_structuring,0
They also experiment with using idf to weight importance.,,arg_structuring,0
"Their method is simple, but achieves very strong results and there are a ton of experiments in this paper (it is 41 pages).",,arg_evaluative,0
"The focus is largely on metrics for MT, but they also evaluate on image captioning.",,arg_fact,0
"The paper is also very thorough and many of the questions I had when reading it are answered (like effect of optimal matching, running time, etc.).",,arg_evaluative,0
"The latter (running time) being one of the downsides of the method if it was to be used for fine-tuning MT systems. 40 times slower than BLEU, but I think this increased cost would be worth it and could be engineered around.",,arg_evaluative,0
"Overall, I like the paper - it is simple and effective on its goal task of automatic evaluation for text generation.",,arg_evaluative,0
I think we are moving that way as a field and this paper proposes a useful method and is additionally a good study on the subject.,,arg_evaluative,0
A question I have is why the method doesn't perform well in certain cases.,,arg_request,1
"For instance, in Table 2 and 3 - some of the evaluations with tr and fi fall well below relative performance for other language pairs. Does this have to do with the quality of the representations in multilingual BERT? What is YiSi-1 doing, for instance for model selection of en-fi and en-tr that makes it have so much better performance?",,none,0
Edit: I also wonder if incorporating idf would be better if the values were computed by a larger corpus.,,arg_request,1
I think it would make the most sense to compute these from the training data for the underlying BERT models.,,arg_fact,0
"Since BERT itself is a function of this training data, it seems appropriate that these values would be as well (or perhaps at least a subset of this data).",,none,0
Missing citations:,,arg_structuring,0
"A citation to ""Beyond BLEU:Training Neural Machine Translation with Semantic Similarity"" from ACL 2019 should be incorporated into the related work.",,arg_request,1
They use semantic similarity to fine-tune NMT systems with their own embedding-based (semantic similarity) metric and they found some nice properties from training in this way.,,none,0
Have you tried BERTScore on sentence similarity tasks? It's possible BERTScore could have strong performance and some readers may wonder this.,,arg_request,1
"There are evaluations on PAWS for paraphrase detection which I appreciated, but that is a little different.",,none,0
"A citation to ""Deep Reinforcement Learning with Distributional Semantic Rewards for Abstractive Summarization"" from EMNLP 2019 should also be incorporated.",,arg_request,1
This paper is a big boon to BERT score showing that it is a very helpful metric for fine-tuning summarization systems.,,arg_evaluative,0
"They don't even need a cross-entropy term since BERTScore captures fluency so well. I'd like to see it for MT as well, but perhaps that is the next paper.",,none,0
Typos:,,arg_structuring,0
"The word ""language"" is misspelled twice in Appendix E.",,arg_request,1
"The authors introduce cryoDRGN, a VAE neural network architecture to reconstruct 3D protein structure from 2D cryo-EM images.",,arg_structuring,0
The paper offers for a good read and diagrams are informative.,,arg_evaluative,0
Below are comments for improvement and clarification.,,arg_structuring,0
> Consider explaining cryoSPARC in detail given that is the state-of-the-art technique and to which all the cryoDGRN results are compared.,,arg_request,1
> In Figure 4 and the related experiment,,arg_request,1
",  how are a) the cryoSPARK volumes related to cryoDRGN volumes, b) what do the clusters mean in cryoSPARK and how do they compare with the corresponding outputs of cryoDRGN",,none,0
"> What would runtime comparisons be for cryoSPARK and cryoDGRN, for an unsupervised heteregeneous reconstruction?",,arg_request,1
This paper proves a theoretical limitation of narrow-and-deep neural networks.,,arg_structuring,0
"It shows that, for any function that can be approximated by such networks, its level set (or decision boundary for binary classification) must be unbounded.",,arg_structuring,0
"The conclusion means that if some problem's decision boundary is a closed set, then it cannot be represented by such narrow networks.",,arg_structuring,0
The intuition is relatively simple.,,arg_structuring,0
"Under the assumptions of the paper, the neural network can always be approximated by a one-to-one mapping followed by a linear projection.",,arg_structuring,0
"The image of the one-to-one mapping is homeomorphic to R^n, so that it must be an open topological ball.",,arg_structuring,0
"The intersection of this open ball with a linear hyperplane must include the boundary of the ball, thus it extends to infinity in the original input space.",,arg_structuring,0
"The critical assumptions here, which guarantees the one-to-one property of the network, are: 1) the network is narrow, and 2) the activation function can be approximated by a one-to-one function.",,arg_structuring,0
The authors claim that 2) captures a large family of activation functions.,,arg_structuring,0
"However, it does exclude some popular activation families, such as the polynomial activation, which were proven effective in multiple areas.",,arg_evaluative,0
"As a concrete example, the simple function f(x1,x2) = x_1^2 + x_2^2 has bounded level sets, but it can be represented by a narrow 2-layer neural network with the quadratic activation.",,arg_fact,0
"Overall, I feel that the result is interesting but it depends on a strong assumption and doesn't capture all interesting cases.",,arg_evaluative,0
It is also not clear how this theoretical result can shed insight on the empirical study of neural networks.,,arg_request,1
"This paper proposes LEMONADE, a random search procedure for neural network architectures (specifically neural networks, not general hyperparameter optimization) that handles multiple objectives.",,arg_structuring,0
"Notably, this method is significantly more efficient more efficient than previous works on neural architecture search.",,arg_evaluative,0
The emphasis in this paper is very strange.,,arg_evaluative,0
"It devotes a lot of space to things that are not important, while glossing over the details of its own core contribution.",,none,0
"For example, Section 3 spends nearly a full page building up to a definition of an epsilon-approximate network morphism, but this definition is never used.",,none,0
I don't feel like my understanding of the paper would have suffered if all Section 3 had been replaced by its final paragraph.,,none,0
Meanwhile the actual method used in the paper is hidden in Appendices A.1.1-A.2.,,arg_evaluative,0
"Some of the experiments (eg. comparisons involving ShakeShake and ScheduledDropPath, Section 5.2) could also be moved to the appendix in order to make room for a description of LEMONADE in the main paper.",,arg_evaluative,0
"That said, those complaints are just about presentation and not about the method, which seems quite good once you take the time to dig it out of the appendix.",,arg_evaluative,0
I am a bit unclear about how comparisons are made to other methods that do not optimize for small numbers of parameters? Do you compare against the lowest error network found by LEMONADE? The closest match in # of parameters?,,arg_evaluative,0
Why is the second objective log(#params) instead of just #params when the introduction mentions explicitly that tuning the scales between different objectives is not needed in LEMONADE?,,arg_request,1
"It seems like LEMONADE would scale poorly to more than 2 objectives, since it effectively requires approximating an #objectves-1 dimensional surface with the population of parents.",,arg_evaluative,0
How could scaling be handled?,,arg_request,1
"This paper considers reinforcement learning tasks that have high-dimensional space, long-horizon time, sparse-rewards.",,arg_structuring,0
"In this setting, current reinforcement learning algorithms struggle to train agents so that they can achieve high rewards.",,arg_structuring,0
"To address this problem, the authors propose an abstract MDP algorithm.",,arg_structuring,0
"The algorithm consists of three parts: manager, worker, and discoverer.",,arg_structuring,0
"The manager controls the exploration scheduling, the worker updates the policy, and the discoverer purely explores the abstract states.",,arg_structuring,0
"Since there are too many state, the abstract MDP utilize the RAM state as the corresponding abstract state for each situation.",,arg_structuring,0
The main strong point of this paper is the experiment section.,,arg_evaluative,0
"The proposed algorithm outperforms all previous state of the art algorithms for Montezuma’s revenge, Pitfall!, and Private eye over a factor of 2.",,arg_evaluative,0
It is a minor weak point that the algorithm can work only when the abstract state is obtained by the RAM state.,,arg_evaluative,0
"In some RL tasks, it is not allowed to access the RAM state.",,arg_evaluative,0
================================,,arg_structuring,0
"I've read all other reviewers' comments and the response from authors, and decreased the score.",,arg_other,0
"Although this paper contains interesting idea and results, as other reviewers pointed out, it is very hard to compare with other algorithm.",,arg_evaluative,0
I agree to other reviewers.,,arg_other,0
The algorithm assumptions are strong.,,arg_evaluative,0
The paper presents a novel idea of generating discrete data such as graphs that is conditional on input data to control the graph structure that is being generated.,,arg_structuring,0
"Given an input graph, the proposed method infers a target graph by learning their underlying translation mapping by using new graph convolution and deconvolution",,arg_structuring,0
layers to learn the global and local translation mapping.,,none,0
The idea of learning generic shared common and latent implicit patterns across different graph structure is brilliant.,,arg_evaluative,0
Their method learns a distribution over graphs conditioned on the input graph whilst allowing the network to learn latent and implicit properties.,,arg_structuring,0
The authors claim that their method is applicable for large graphs.,,arg_structuring,0
"However, it seems the experiments do not seem to support this.",,arg_evaluative,0
It is not clear how the noise is introduced in the graphs.,,arg_evaluative,0
I would have expected to see some analysis and results on the translation quality over systematic noise applied to the input graph.,,arg_request,1
It is also not clear what are the assumptions made on the connectivity of the input graph and the target graph.,,arg_evaluative,0
Do we know how does the connectedness of the  input graph affect the translation quality in the case of strongly connected directed graphs? Or what happens if the target graph has a strong connectivity?,,arg_request,1
"Towards this, how does the computational complexity scale wrt to the connectedness?",,arg_request,1
A lot of clarity is required on the choice of evaluation metric; for example choice of distance measure ?,,arg_request,1
What is the L1 norm applied on?,,arg_request,1
I did not completely follow the arguments towards directed graph deconvolution operators.,,arg_evaluative,0
There is lack of clarity and the explanation seems lacking in parts in this particular section; especially since this is the key contribution of this work,,arg_evaluative,0
Typo:. The “Inf” in Tabel 1,,arg_request,1
This paper proposes the Cramer-Wold autoencoder.,,arg_structuring,0
The first contribution of the paper is to propose the Cramer-Wold distance between two distributions based on the Cramer-Wold Theorem.,,arg_structuring,0
"More specifically, in order to compute the Cramer-Wold distance, we first find the one dimensional projections of the distributions over random slices, and then compute the average L2 distances of the kernel density estimates of these projections over random slices.",,arg_structuring,0
The second contribution of the paper is to develop a generative autoencoder which uses the Cramer-Wold distance to match the latent distribution of the data to the prior distribution.,,arg_structuring,0
"While I found the derivation of the Cramer-Wold distance interesting, the final form of this distance (Eq. 2), to me, looks very similar to the MMD with a particular kernel.",,arg_evaluative,0
My main question is that: what is the main advantage of the Cramer-Wold distance to an MMD with a proper kernel?,,arg_request,1
"The paper points out that the main theoretical contribution is that in the case of the Gaussian distribution, the Cramer-Wold distance has a closed form.",,arg_fact,0
"However, I believe this is also the case in the MMD, since if one of the distributions is Gaussian or analytically known, then E[k(x,x')] in the MMD can be analytically computed.",,arg_evaluative,0
The paper further uses this closed form property of the Cramer-Wold distance to propose the Cramer-Wold autoencoder with Gaussian priors.,,arg_fact,0
"My question here is that how is this method better than the standard VAE, where we also have an analytic form for the ELBO when the prior is Gaussian, an no sampling is required.",,arg_request,1
"Indeed, in VAEs, the prior does not have to be Gaussian, and as long as the density of the prior can be evaluated, we can efficiently optimize the ELBO without sampling the prior; which I don't think is the case for the Cramer-Wold autoencoder.",,arg_evaluative,0
I believe the main advantages of methods such as WAE is that they can impose priors that do not have exact analytic forms.,,arg_fact,0
The authors proposed a new embedding for time - Time2Vec.,,arg_structuring,0
"Unlike previous research that is either proposing a new architecture or proposing expensive handcrafted features, this work proposes a model-agnostic learnable time embedding.",,arg_structuring,0
I would like to recommend an accept based on the following reasons:,,arg_structuring,0
* Modeling time is crucial for quite a few machine learning tasks.,,arg_fact,0
"With the two most desired properties, learnable and model-agnostic, this time embedding will be very useful in various applications.",,arg_evaluative,0
* The authors are good at story-telling and this makes the paper very readable and approachable.,,arg_evaluative,0
This increases the chance of the contribution made in this paper to be applied in real-world applications.,,arg_evaluative,0
* This work did clear and detailed analysis on both the empirical results and the probing experiments.,,arg_evaluative,0
"The paper proposes to combine several smaller, pretrained RBMs into a larger model as a way to solve combinatorial optimization problems.",,arg_structuring,0
"Results are presented on RBMs trained to implement binary addition, multiplication, and factorization, where the proposed approach is compared with the baseline of training a full model from scratch.",,arg_structuring,0
I found the paper confusing at times.,,arg_evaluative,0
"It is well-written from a syntactical and grammatical point of view, but some key concepts are stated without being explained, which gives the impression that the authors have a clear understanding of the material presented in the paper but communicate only part of the full picture to the reader.",,arg_evaluative,0
"For instance, there’s a brief exposition of the connection between Boltzmann machines and combinatorial optimization problems: the latter is mapped onto the former by expressing constraints as a fixed set of Boltzmann machine weights and biases, and low-energy states (i.e. more optimal solutions) are found by sampling from the model, which involves no training.",,none,0
What’s less clear to me is what kinds of combinatorial optimization problems can be mapped onto the RBM *training* problem.,,none,0
"The paper states that the problem of training ""large modules"" is ""equivalent to solving the optimization problem"", but does not explain how.",,arg_evaluative,0
"Similarly, the paper mentions that the ""general approach to solving these combinatorial optimization problems is to recognize the atomic unit necessary to solve the problem"", but at that point the reader has no concrete example of what combinatorial optimization problem would be mapped onto training and inference in RBMS.",,arg_evaluative,0
A concrete example is provided in the Experiments section: the authors propose to implement invertible (reversible?) boolean logic circuits by combining smaller pre-trained RBMs which implement certain logical operations into larger circuits.,,arg_fact,0
"I have two issues with the chosen example: 1) the connection with combinatorial optimization is not clear to me, and 2) it’s not very well explained.",,arg_evaluative,0
"As far as I understand, these reversible boolean logic operations are expressed as sampling a subset of the RBM’s inputs conditioned on another subset of its inputs.",,arg_fact,0
An example is presented in Figure 3 but is not expanded upon in the main text.,,arg_evaluative,0
I’d like the authors to validate my understanding:,,arg_social,0
"An RBM is trained to implement a complete binary adder circuit by having it model the joint distribution of the adder’s inputs and outputs [A, B, Cin, S, Cout] (A is the first input bit, B is the second input bit, Cin is the input carry bit, S is the output sum bit, and Cout is the output carry bit), where (I assume) the distribution over [A, B, Cin] is uniform, and where S and Cout follow deterministically from [A, B, Cin].",,arg_fact,0
"After training, the output of the circuit is computed from [A, B, Cin] by clamping [A, B, Cin] and sampling [S, Cout] given [A, B, Cin] using Gibbs sampling.",,arg_fact,0
"The alternative to this, which is examined in the paper, is to train individual XOR, AND, and OR gates in the same way and compose them into a complete binary adder circuit as prescribed by Section 3.",,arg_fact,0
"I think the paper has the potential to be a lot more transparent to the reader in explaining these concepts, which would avoid them spending quite a bit of time inferring meaning from figures.",,arg_evaluative,0
I’m also confused by the presentation of the results.,,arg_evaluative,0
"For instance, I don’t know what ""log"", ""FA1"", ""FA2"", etc. refer to in Figure 6.",,none,0
"Also, Figure 6 is referenced in the text in the context of binary multiplication (""[...] is able to outperform a multiplier created just by training, as can be seen in Figure 6""), but presents results for addition and factorization only.",,arg_evaluative,0
"The way I see it, implementing reversible boolean logic circuits using RBMs is an artificial problem, and the key idea of the paper -- which I find interesting -- is that in some cases it appears to be possible to combine RBMs trained for sub-problems into larger RBMs without needing to fine-tune the model.",,arg_fact,0
"I think there are interesting large-scale applications of this, such as building an autoregressive RBM for image generation by training a smaller RBM on a more restricted inpainting task.",,arg_fact,0
"The connection to combinatorial optimization, however, is much less clear to me.",,arg_evaluative,0
This paper presents an analysis of the inverse invariance of ReLU networks.,,arg_structuring,0
It makes the observation that one can describe the pre-image of an image point z = F(x) using linear algebra arguments.,,arg_structuring,0
They provide necessary conditions for the pre-image to be a singleton or a finite volume polytope.,,arg_structuring,0
They also provide upper-bounds on the singular values of a train network and measure those in standard CNNs.,,arg_structuring,0
"The paper is well-written but the structure is a bit disconnected; most notably, I didn't see clearly how Section 2 and 3 fit together.",,arg_evaluative,0
The proofs seem correct and rely mostly on elementary linear algebra argument; this simplicity makes the analysis quite interesting.,,arg_evaluative,0
"The argument about a different kind of adversarial examples is also very interesting; instead of looking for small perturbation that affect the mapping in drastic ways, find large perturbations that in invariant directions of the network.",,arg_evaluative,0
"However, the experiments are overall not very useful to the comprehension of the paper and not that illustrative.",,arg_evaluative,0
I have several questions for the authors:,,arg_structuring,0
"- the conditions presented in Theorem 4, seem hard to check in practice; what is the time complexity of this operation? I believe that checking if A is omnidirectional is equivalent to an LP but how do you solve the combinatorial size of doing that over all set of indices?",,arg_request,1
"- I understand the upper bounds on the singular values, but I am not sure how they relate to inverse stability.",,arg_evaluative,0
Maybe more explanation and quantitative analysis (e.g. relating the volume of the preimage of an epsilon ball around z to the singular values) could be helpful.,,arg_request,1
- Is there actionable consequences one could draw from your papers? The way the results are presented seem like they are only useful inspection after training; are your results able to derive methods to enforce conditions on the pre-images for example?,,arg_request,1
"In conclusion, this paper does an interesting and original analysis which can help us understand better the polytopes composing the input space.",,arg_evaluative,0
The experiments are not very convincing or illustrative of the theoretical results in my opinion.,,arg_evaluative,0
It is not clear how those observations can affect practical algorithms and this is something I hope the author can address.,,arg_request,1
"This paper proposes a novel transfer learning mechanism through credit assignment, in which an offline supervised reward prediction model is learned from previously-generated trajectories, and is used to reshape the reward of the target task.",,arg_structuring,0
"The paper introduces an interesting new direction in transfer learning for reinforcement learning, that is robust to the differences in the environtment dynamics.",,arg_structuring,0
I have the following questions/concerns.,,arg_structuring,0
"1. The authors insist that their fous is on transfer and not competing on credit assignment. If accurate credit assignment leads to better transfer, shouldn't achieving the best credit assignment model (thus competing in credit assignment) lead to better transfer results?",,arg_request,1
2. What effect does the window size for transforming states to observations have on the performance of SECRET?,,arg_request,1
"3. On a high-level, how does SECRET compare to transfer through relational deep reinforcement learning: https://arxiv.org/abs/1806.01830? Relational models use self-attention mechanisms to extract and exploit relations between entities in the scenes for better generalization and transfer.",,arg_request,1
"Although SECRET intentionally avoids using relations, I think a discussion around relational models for RL is warranted.",,arg_fact,0
I'm curious what happens if SECRET is allowed to exploit relations in the environment.,,arg_request,1
4. What happens if the reward model uses very few trajectories and is not able to predict good rewards? Does transfer through credit assignment become detrimental?,,arg_request,1
"In other words, in a real-world scenario, how I do know when to start using SECRET, or when am I better off learning from environment rewards alone? Especially given that SECRET requires 40000 trajectories in the source domain.",,none,0
5. Are the samples generated in the target domain for collecting attention weights included in the number of episodes when evaluating SECRET?,,arg_request,1
"For example, in Figure 4. I believe the number of episodes required to collect those target samples should be added to the number of episodes when using SECRET since the agent must interact with the environment in the target domain.",,none,0
"6. On a lighter note, I don't believe using a coffe-brewing machine has a 'universally invariant structure' of coffee-making. That's a luxurious way of making coffee :) In the developing world, we still need to boil water, pour coffee powder in it, etc., all without a coffee-brewing machine.",,arg_request,1
"This paper first gives a concise yet precise summary of maximizing one of variational lower bounds of mutual information, InfoNCE, then it provides an alternative view to explain case by case why word embedding Skip-gram, BERT, XLNet work in practice can be viewed by InfoNCE framework, thus we have a good understand for these methods.",,arg_structuring,0
Moreover it introduces a self-learning method  that maximizes the mutual information between a global sentence representation and n-grams in the sentence based on deep InfoMax framework instead.,,arg_structuring,0
"Experiments show that it is better then BERT and BERT-NCE. It's known that InfoNCE increases bias but reduce variance, the same is true for deep InfoMax. Do you observe this in your experiments? If so, please provide.",,arg_structuring,0
The paper is well-written and easy to follow.,,arg_evaluative,0
"The originality is relative low though, since it is mainly an application of  deep InfoMax to language modeling, not inventing a new algorithm and applying to language modeling.",,arg_evaluative,0
"In equations 1 and 2, should a, b be written in capital? Since they represent random variables.",,arg_request,1
The paper is well-written with a few figures to illustrate the ideas and components of the proposed method.,,arg_evaluative,0
"However, one of the main components in the proposed method is based on Tulsiani et al. CVPR'18.",,arg_evaluative,0
The remaining components of the proposed method are not very new.,,arg_evaluative,0
"Hence, I am not very sure whether the novelty of the paper is significant.",,arg_evaluative,0
"Nevertheless, the performance of the proposed method is fairly good outperforming all baseline methods.",,arg_evaluative,0
I also have a few questions:,,arg_structuring,0
"1. How did you get the instance boxes, union boxes, and binary masks in testing?",,arg_request,1
2. What are the training and inference time?,,arg_request,1
## Summary,,arg_structuring,0
The authors propose a method for encoding time features using a sine function with learned phase and frequency.,,arg_structuring,0
They apply this method to several synthetic and real-world datasets.,,arg_structuring,0
"Temporal and positional encoding is important to many applications, including NLP, sound understanding and time series modeling, so the topic is certainly of interest.",,arg_evaluative,0
"However, the method they propose offers very little that is new when compared to e.g. Vaswani (https://arxiv.org/pdf/1706.03762.pdf, section 3.5) (the authors acknowledge this work several times).",,arg_evaluative,0
"In addition, the authors compare to a baseline that seems to consist of passing time as a float.",,arg_fact,0
This seems like a very weak baseline---there are any number of other reasonable ways to encode this.,,arg_evaluative,0
"Due to the incremental nature of the improvement and the weak baseline, I don't think this paper should be accepted to ICLR.",,arg_social,0
## Specific Comments,,arg_structuring,0
"1. In Section 2, I find the sentence ""We follow a similar intuition but instead of decomposing a 1D signal of time into its components, we transform the time itself and feed its transformation into the model that is to consume the time information"" really unclear. Could you rephrase it?",,arg_request,1
"2. Often, positional encodings are used to encode ordering for a model architecture that is not inherently sequential.",,arg_fact,0
This is the case for the positional encodings in the transformer model.,,arg_fact,0
Did you try these encodings with non-recurrent architectures?,,arg_request,1
"3. In Section 5.2, did you mean 'fixing t2v(\tau)[n] = sin(2\pi n \tau / 16)'? i.e. I think it's missing a 'tau'",,arg_request,1
"4. In Section 5.2 ""Fixed frequencies and phase shifts"" you compare Time2Vec to a fixed set of frequencies.",,arg_fact,0
"Since positional encoding with Fourier transforms is well known, this seems like the relevant benchmark but it receives only a brief treatment.",,arg_evaluative,0
The authors compare these methods only on Event-MNIST and only for 16 frequencies.,,arg_fact,0
I would like to see this comparison expanded.,,arg_evaluative,0
"5. Could you clarify exactly how time is encoded for LSTM + T? Are you, in fact, just passing a float value? How is this encoded for each data set?",,arg_request,1
"For example, the ""times"" for Event-MNIST is always [0, 783] while the SOF data has timestamps.",,none,0
What is the encoding scheme for each?,,none,0
This paper introduces a particular learnable vector representation of time which is applicable across problems without the use of a hand-crafted time representation.,,arg_structuring,0
Their representation makes use of a feed-forward layer with sine activations which operates on time data.,,arg_structuring,0
"As it is a vector representation, it combines well with other deep neural network methods.",,arg_structuring,0
"They motivate their problem well, explaining why time data is important to a variety of problems and situate their solution as an orthogonal approach to many current solutions in the literature.",,arg_evaluative,0
They make reference to fourier analysis as motivation for their representation.,,arg_structuring,0
"Finally, they provide experimental results to support their claims using fabricated and real-world time series datasets, as well as ablation studies to support their design decisions.",,arg_structuring,0
"While I think this work has the potential to be a significant contribution, I rate this a weak reject because the theoretical motivation and analysis of the experimental results are lacking the depth of evidence I would expect for an ICLR paper.",,arg_evaluative,0
"If you provide a deeper discussion of the provable claims about the power of your model via Fourier analysis and provide a table of test accuracy/recall@K with/without your representation for more than one other state of the art algorithm for these datasets, I would be convinced to strong accept.",,arg_social,0
Specific comments:,,arg_structuring,0
* p.3 third paragraph: you repeat yourself in math notation a few times here.,,arg_evaluative,0
"Repeated equations usually indicate that there is something new happening, but all of these are just restatements of your theta sin(omega tau + phi) term.",,arg_evaluative,0
"I would introduce the notation for t2v(tau) upfront and use that to define a(tau, k)[j] and f_j",,arg_request,1
* p.3,,none,0
"A clearer explanation of the theory here would help, as I think Fourier's theorem nicely supports your claims.",,arg_evaluative,0
"* p.4 first paragraph you claim that this method responds well to data which exhibits seasonality, but none of your datasets deal with data that would exhibit seasonality.",,arg_evaluative,0
"There are plenty of simple real-world datasets available which show multi-scale periodic phenomena (activity or location data, weather data, travel data, etc.).",,arg_fact,0
"In fact, segmentation and recognition of wearable device activity would be a great application for this method.",,arg_fact,0
* p.4,,none,0
"third paragraph: Your claim of invariance to time rescaling is technically correct, but I am not convinced that a model can learn the correct omega values for an arbitrary rescaling (e.g. if the period is smaller than the time unit).",,arg_evaluative,0
"You show that this works for a rescaling from 2pi/7 to 2pi/14, but it would be nice if there was experimental confirmation of this property with frequency > 1.",,arg_request,1
* p.6 Showing accuracy/recall across training epochs is not sufficient evidence to show that this is a useful representation.,,arg_evaluative,0
There should be some kind of comparison with test set results from other state-of-the-art work on these datasets.,,arg_evaluative,0
"If adding your representation to the SOTA model improved test set performance (or at least sped up training without hurting test set performance), then that would be better evidence.",,none,0
"If LSTM+T is the SOTA, say so and restate the author's test performance",,none,0
compared to yours,,none,0
.,,none,0
"If this is what these graphs show, consider using a different visualization to make it clearer that you're improving the final performance, not just the training process.",,arg_evaluative,0
"* p.8 I think sine functions make optimization harder because they make the gradient function periodic with respect to the weights, creating infinitely many local extrema.",,arg_fact,0
"Historically this may have been an issue, but deep neural networks have so many local minima it might not matter.",,arg_fact,0
"Still, it would be good to show that trained performance doesn't depend on the initialization values more than a standard LSTM+T model.",,arg_request,1
"* You have an interesting corner case where your neural network parameters are interpretable: you can interpret the omega values from your model as frequencies and investigate their values to see which kinds of periodicity your model uses. You do something like this on p.7, but it would be neat to see a histogram like the one you have for EventMNIST for one of the real-world datasets to see if it learns the domain-relevant time knowledge you claim that it should learn.",,arg_request,1
"This paper introduces  a novel DPS(Deep Probabilistic Subsampling) framework for the task-adaptive  subsampling case, which attempts to resolve the issue of end-to-end optimization of an optimal subset of signal with jointly learning a sub-Nyquist sampling scheme and a predictive model for downstream tasks.",,arg_structuring,0
The parameterization is used to simplify the subsampling distribution and ensure an expressive yet tractable distribution.,,arg_structuring,0
"The new approach contribution is applied to  both reconstruction and classification tasks and demonstrated with a suite of experiments in a toy dataset, MINIST, and COFAR10.",,arg_structuring,0
"Overall, the paper requires significant improvement.",,arg_request,1
1. The approach is not well justified either by theory or practice.,,arg_evaluative,0
There is no experiment clearly shows convincing evidence of the correctness of the proposed approach or its utility compared to existing approaches (Xie & Ermon (2019); Kool et al. (2019); PlÂšotz & Roth (2018) ).,,arg_evaluative,0
2. The paper never clearly demonstrates the problem they are trying to solve (nor well differentiates it from the compressed sensing problem  or sample selection problem),,arg_evaluative,0
"The method is difficult to understand, missing many details and essential explanation, and generally does not support a significant contribution.",,arg_evaluative,0
3. The paper is not nicely written or rather easy to follow.,,arg_evaluative,0
The model is not well motivated and the optimization algorithm is also not well described.,,arg_evaluative,0
4. A theoretical analysis of the convergence of the optimization algorithm could be needed.,,arg_request,1
5. The paper is imprecise and unpolished and the presentation needs improvement.,,arg_evaluative,0
**There are so many missing details or questions to answer**,,arg_structuring,0
1. What is the Gumbel-max trick?,,arg_request,1
2. How to tune the parameters discussed in training details in the experiments?,,arg_request,1
3. Why to use experience replay for the linear experiments?,,arg_request,1
4. Are there evaluations on the utility of proposed compared to existing approaches?,,arg_request,1
5. Does the proposed approach work in real-world problems?,,arg_request,1
6. Was there any concrete theoretical guarantee to ensure the convergence of the algorithm.,,arg_request,1
[Post Review after discussion]: The uploaded version has significantly improved over the first submission. It is now acceptable.,,arg_social,0
The authors propose a network quantization approach with adaptive per layer bit-width.,,arg_structuring,0
The approach is based on a network architecture search (NAS) method.,,arg_structuring,0
The authors aim to solve the NAS problem through SGD.,,arg_structuring,0
"Therefore, they propose to first reprametrize the the discrete random variable determining if an edge is computed or not to make it differentiable and then use Gumbel Softmax function as a way to effectively control the variance of the obtained unbiased estimator.",,arg_structuring,0
This variance can indeed make the convergence of the procedure hard.,,arg_structuring,0
The procedure is then adapted to the problem of network quantization with different band-widths.,,arg_structuring,0
The proposed approach is interesting.,,arg_evaluative,0
The differerentiable NAS procedure is particularly important and can have an important impact.,,arg_evaluative,0
"The idea of having an adaptive per layer precision is also well motivated, and shows competitive (if not better) results empirically.",,arg_evaluative,0
Some additional experiments can make the paper stronger:,,arg_request,1
* Compare the result of the procedure to an exhaustive search in a setting where the latter is feasible (shallow architecture on an easy task with few possible bit widths),,arg_request,1
"* Compare the procedure to other state of the art NAS procedures (DARTS and ENAS) with the same search space adapted to the quantization problem, to empirically show that the proposed procedure is a compromise between these two methods as claimed by the authors.",,arg_request,1
This work explores the extent to which the natural image manifold is captured by generative adversarial networks (GANs) by performing walks in the latent space of pretrained models.,,arg_structuring,0
"To perform these walks, a transformation vector is learned by minimizing the distance between transformed images and the corresponding images generated from transformed latent vectors.",,arg_structuring,0
"It is found that when traversing the latent space of the GAN along the direction of the transformation vector, that the corresponding generated images initially exhibit the desired transform (such as zooming or changing X position), but soon reach a limit where further changes in the latent vector do not result in changes to the image.",,arg_structuring,0
"It is observed that this behaviour is likely due to bias in the dataset which the GAN is trained on, and that by exploring the limits of the generator, biases which exist in the original dataset can be revealed.",,arg_structuring,0
"In order to increase the extents to which images can be transformed, it is shown that GANs can be trained with an augmented dataset and using a loss function that encourages transformations to lie along linear paths.",,arg_structuring,0
"Overall, I would tend towards accepting this paper.",,arg_social,0
"Improving the amount of control that we have over generative models is desirable for image synthesis, and this paper does a great job of demonstrating the extent to which these models can be manipulated in terms of mimicking basic transforms.",,arg_evaluative,0
"Figures are very clean and informative, and experimental results are extensive.",,arg_evaluative,0
"I don't have much else to say about this paper, as I did not find anything in it that concerned me, and the paper answered all of my questions.",,arg_social,0
"In this paper, the authors propose a physical driven architecture of DeepSFM to infer the structures from motion.",,arg_structuring,0
Extensive experiments on various datasets show that the model achieves the state-of-the-art performance on both depth and pose estimation.,,arg_structuring,0
"In general, the paper is clearly written but I still have several concerns.",,arg_evaluative,0
1.	The paper is easy to follow but the authors are expected to clarify the rationality in integration of the loss function.,,arg_request,1
"How the parameter of \lambda_r, \lambda_t, and \lambda_r influence the performance.",,arg_request,1
It would be better if the authors could present some analysis.,,arg_request,1
2.	The experiments are rather insufficient.,,arg_evaluative,0
"The authors are expected to make more comprehensive analysis with the state-of-the-art methods, and also analyze why some alternative methods outperforms the proposed methods in table I and table II.",,arg_request,1
3.,,arg_request,1
The experiments in section 4.3 are also expected to be improved.,,none,0
It is difficult to draw a conclusion that the method is better than other ones based on such limited experiments.,,none,0
