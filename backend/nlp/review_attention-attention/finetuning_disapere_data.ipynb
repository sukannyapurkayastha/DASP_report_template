{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\carme\\miniconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizer\n",
    "from transformers import TFDistilBertForSequenceClassification\n",
    "# import tensorflow as tf\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   data  \\\n",
      "0     The present paper proposes a fast approximatio...   \n",
      "1     This is typically a bottleneck in deep learnin...   \n",
      "2     The approximation is a sparse two-layer mixtur...   \n",
      "3     The paper lacks rigor and the writing is of lo...   \n",
      "4                            See a list of typos below.   \n",
      "...                                                 ...   \n",
      "5211  It is difficult to judge the performance of th...   \n",
      "5212  3.\\tThe proposed method, which decomposes a pr...   \n",
      "5213  For example, representing a movie or news arti...   \n",
      "5214  In this way, the proposed method can be tested...   \n",
      "5215  4.\\tFor the final purpose, comparing problem s...   \n",
      "\n",
      "                         labels  \n",
      "0                          none  \n",
      "1                          none  \n",
      "2                          none  \n",
      "3                   asp_clarity  \n",
      "4                          none  \n",
      "...                         ...  \n",
      "5211              asp_substance  \n",
      "5212                       none  \n",
      "5213                       none  \n",
      "5214                       none  \n",
      "5215  asp_soundness-correctness  \n",
      "\n",
      "[5216 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Pfad zur .jsonl-Datei\n",
    "file_path = r'C:\\Users\\carme\\OneDrive - Appelt Steuerberatung\\Carmen Appelt\\Master\\Semester 3\\DASP\\data\\train_disapere.jsonl'  # Ersetzen Sie durch den tatsächlichen Pfad\n",
    "\n",
    "# JSON-Lines-Datei in ein DataFrame laden\n",
    "df = pd.read_json(file_path, lines=True)\n",
    "df = df[['text', 'aspect']]\n",
    "df = df.rename(columns={'aspect': 'labels'})\n",
    "df = df.rename(columns={'text': 'data'})\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verteilung der Labels:\n",
      "labels\n",
      "none                         2554\n",
      "asp_substance                 825\n",
      "asp_clarity                   566\n",
      "asp_soundness-correctness     536\n",
      "asp_originality               200\n",
      "asp_motivation-impact         174\n",
      "asp_meaningful-comparison     173\n",
      "asp_replicability             120\n",
      "arg_other                      68\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df[\"labels\"].unique()\n",
    "\n",
    "# Häufigkeit der Labels zählen\n",
    "label_counts = df['labels'].value_counts()\n",
    "\n",
    "print(\"Verteilung der Labels:\")\n",
    "print(label_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   encoded_cat                     labels\n",
      "0            0                  arg_other\n",
      "1            1                asp_clarity\n",
      "2            2  asp_meaningful-comparison\n",
      "3            3      asp_motivation-impact\n",
      "4            4            asp_originality\n",
      "5            5          asp_replicability\n",
      "6            6  asp_soundness-correctness\n",
      "7            7              asp_substance\n",
      "8            8                       none\n"
     ]
    }
   ],
   "source": [
    "mapping_df = pd.DataFrame({\n",
    "    'encoded_cat': range(len(df[\"labels\"].astype(\"category\").cat.categories)),\n",
    "    'labels': df[\"labels\"].astype(\"category\").cat.categories\n",
    "})\n",
    "\n",
    "print(mapping_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encode label for easy identification.\n",
    "df[\"encoded_cat\"] = df[\"labels\"].astype(\"category\").cat.codes\n",
    "\n",
    "data_texts = df[\"data\"].to_list() # Features (not tokenized yet)\n",
    "data_labels = df[\"encoded_cat\"].to_list() # Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Split (train 79%, val 20%, test 1%)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    " \n",
    "# Split Train and Validation data\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(data_texts, data_labels, test_size=0.2, random_state=0, shuffle=True)\n",
    " \n",
    "# Keep some data for inference (testing)\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(train_texts, train_labels, test_size=0.01, random_state=0, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4130\n",
      "0.20015337423312884\n",
      "0.008052147239263804\n",
      "['However, the experimental comparison is not fair, the description of the model (e.g. how Choquet is integrated into the model and help to learn “intermediate meaningful results”) is not clear, some claims are not true.', '- A sentence could be added to explain the truncation trick in the abstract directly since it is simple to understand and is key to the quality of the results.', '- The overall method seems to be not very principled, and requires a lot of \"tweaks and tunes\", with additional losses and regularizers, to work.', 'The main technical results of the paper are Theorems 5-8 which compute the statistics of the covariance of the activations and the gradients.', 'Hence, these overly simplified and unrealistic assumptions make the task too trivial.', \"- Currently there is no experimental evaluation of the claims, which would be valuable given that the setting doesn't immediately apply in the normal batch normalization setting.\", 'The probabilities are approximated with variational autoencoders.', 'Advances in Neural Information Processing Systems 29', 'However it is valuable to have a similar analysis closer to the batch normalization setting used by most practitioners.', 'are conducted to demonstrate the performance of the proposed model.', 'Furthermore, the claims of the model working for non-MCAR missingness are not substantiated by the experiments.', 'In addition, I would like to see some discussions whether this technique could be applied to off-policy learning as well.', 'Therefore, I do not feel confident in championing this paper.', 'The role of \\\\sigma seems very redundant given \\\\omega.', 'The result may be interpreted in the following form: if one chooses to use BN or other normalization, the paper gives a recommendation that only the learning rate of scale-variant parameters need to be set, which may have some practical advantages.', 'Two simple techniques are proposed.', '= Quality: The paper is well-written with a large set of experiments, making the case that exclusively using curiosity-based reward is very promising for the widely-used game RL testbeds.', 'Thus the proof is concluded by the linear (resp. sublinear) convergence of gradient descent (resp. stochastic GD) under PL assumption.', 'So what’s the key difference? Hierarchical learners can avoid this problem.', 'See Section 4.3.3 of that paper for more details.', 'Having this in mind note that the theoretical results on stochastic variant presented in the paper are wrong.', 'The problem is very interesting.', '-\\tSplitting noise z in multiple chunks, and injecting it in multiple layers of the generator', '[2]. Jiawei Zhang, Jin-shan Pan, Wei-Sheng Lai, Rynson W. H. Lau, Ming-Hsuan Yang: Learning Fully Convolutional Networks for Iterative Non-blind Deconvolution. CVPR 2017: 6969-6977', 'For this, they first split the entire state into two mutually exclusive sets of the context states and the states of interest.', '- One drawback is that this method requires the mask during training. How can it be adapted for scenarios where the mask is not present? In other words, we only see multiple modalities as input, but we are not sure which are noisy and which are not?', 'Still not convinced of the value of the work to the community. Will keep my score the same.', '* Equations (1, 2): z and \\\\phi are not consistently boldfaced', '3. Technical contribution: While the authors propose the first bounds for LSTMs and MGUs, most of the analysis seems to be a marginal contribution over the work of Bartlett et al. [3]', '2016', '- presumably, the sample complexity is ridiculous', 'This paper uses constrained Markov decision processes to solve a multi-objective problem that aims to find the correct trade-off between cost and return in continuous control.', 'Your team is highly competent your style is distinct.', '-\\tThe paper conducted detailed analysis, which invites more research on this task: despite the strong performance of many existing systems on NLI/RTE, there are larger gaps between the performance of these models and human performance on the proposed task.', 'The paper is clearly written and easy to follow.', 'Method:', 'The generative process of the toy data clearly states that there is no heteroscedastic noise to handle.', '- In the appendix, the statement \"Sarkar (2011) show that a similar statement as in Theorem 2 holds for a very general class of trees\" is confusing to me. The \"general class\", as far as I know, is actually *all* trees, weighted or unweighted.', '(BIM) is not equivalent to the projected gradient method since the direction chosen is the sign of the gradient and not the gradient itself (the first iteration is actually equivalent because we start at the center of the box but after both methods are no longer equivalent).', '- The setting of including unrelated classes in the unlabeled data resembles transfer learning setting. Could you explain why the ideas from transfer learning are not applicable in your case?', 'Clarity', 'The paper trains classification models to classify a labeling of a subset of images (assigned with label 1) from the rest of the images (assigned with a label 0).', 'It’s probably a bit unfair or misleading to claim neural networks suffering from ME bias.', 'arXiv preprint arXiv:1901.09005.', 'This paper studies the dynamics-based curiosity intrinsic reward where the agent is rewarded highly in states where the forward dynamic prediction errors are high in an embedding space (either due to complexity of the state or unfamiliarity).', 'In the case of continuous data x, is the reparameterization trick used?', '[1] L. Jiang, Z. Zhou, T. Leung, L. Li, and L. Fei-Fei. Mentornet: Learning data-driven curriculum for very deep neural networks on corrupted labels. In ICML, 2018.', 'The paper contributes a dataset (20,000 commonsense narratives and 200,000 explanatory hypotheses).', '1. For optimizing a general smooth function with all parameters forming a single scale-invariant vector.', '7. ``Our experiments show that our networks can remember a large number of images and distinguish them from unseen images’’ -- this does not seem to be true, since the model is trained on both n as well as N -n ``unseen’’ images which it labels as the negative class, thus the negative class is also seen by the memorization model.', '-- Consider using a standard fonts for the equations.', 'The main problems come from the experiments, which I would ask for more things.', 'Given that object manipulation is the specific application of interest, the comparison with DIAYN and the combined objective with DIAYN is interesting but little motivation or discussion has been provided in the paper.', 'Since the paper relies so heavily on Lee et al., the authors should make an effort to summarize the approach in a mathematically precise way.', 'More specifically, the data efficiency (factor improvement in data to achieve some accuracy) of the proposed method over using a deterministic ensemble is around just 10% or so.', 'Such task-level modules include object/attribute prediction, image captioning, relationship detection, object counting, and finally VQA model.', 'The experimental validation is performed on continuous control learning tasks, showing the benefits of the proposed.', 'However, as this is to me mostly a detailed empirical investigation and presentation of high-performance GANs on large scales, I would be likely to share this with colleagues who want to tackle similar problems.', 'Extensive experiments have been conducted, showing that the proposed approach', '* good results and good analysis of the model', 'Is there a reason why the authors do not introduce their objective by following the variational framework?', 'When the rewrite is possible, the model also predicts the embedding of the resulting formula.', 'I vote for weak reject as (1) the paper makes a strong assumption about the availability of both the robot and object states which is not realistic in typical robotic manipulation applications and (2) the objective in the paper will not work if there is no notion of an “object” or object-state e.g.: this algorithm will not learn skills for a robot trying to control itself; hence, it is not truly a general purpose skill discovery algorithm but rather a skill discovery algorithm specifically meant for robot-object manipulation tasks.', 'The paper proposed to incorporate a manifold regularization penalty to the GAN to adapt to semi-supervised learning.', 'At the same time, they should compare with VAT [7].', 'Weakness', 'The authors show through experiments how their attack is more persistent than centralised backdoor attack.', 'This matters, because the notion of equivariance really only makes sense for a group.', 'Therefore, it is an overclaim that the KL-divergence bound (2) provides an immediate justification for AGZ’s core learning algorithm.', 'Nevertheless, its presentation and Figure 1 makes this elegant idea seems over-complicated.', \"Either case, I don't think we can have the inequality in eq. (5).\", 'However, this is not the case already in some of the prior work that uses random directions for estimating gradients (e.g., the cited paper by Ilyas et al.)', 'The description of consistency regularisation methods in section 2.2 is not very clear and I would like to get better understanding of temporal ensembling and SNTG methods here as they play an important role in the experiments.', 'The approach taken by the paper is to solve a constrained optimization problem, where the constraint imposes a (potentially state-dependent) lower bound on the reward.', '3) to propagate the relational information across all the nodes, the model performs L layers of LSTM for each entity that attend on other nodes via attention (where the weights come from the adjacency matrix).', '---------------', '1.\\tFrom the generative model point of view, could the authors elaborate on the comparison against cGAN (aside from the descriptions in Appendix 2)? It is quoted “cGAN…often lack satisfactory diversity in practice”.', 'This paper shows that AGZ’s optimal policy is a Nash equilibrium, the KL divergence bounds distance to optimal reward, and the two-player zero-sum game could be applied to sequential decision making by introducing the concept of robust MDP.', '- how to tune lambda? it is an important hyper-parameter, but it is set without a good principle, e.g., \"For SGD-APO, we used lambda = 0.001, while for SGDm-APO, we used lambda = 0.01\", \"while for RMSprop-APO, the best lambda was 0.0001', 'I will leave my score as is.', 'The results are not strong. And, unfortunately, the model contribution currently is too modest.', 'Cons:', 'The experimental validation of the proposed approach can also be further improved, see more specific comments below.', 'Specifically, the dynamics of W_U, and W_L, according to lemma1, is (epsilon2-epsilon1)*w(P, G) while the dynamics of W_s is 2*epsilon1 * w(P, G), and 2epsilon1 > epsilon2 - epsilon1 (according to the assumption in lemma 1).', 'In addition, the paper would also need to show that such a model does not generalize to a validation set of images.', 'it becomes harder and harder to memorize the positive training', '3. In the paper, the authors argued that clip-level feature learning is limited as it is hard to learn long-range spatio-temporal dependency.', 'A Bernoulli(p) random variable is discrete, yet it is reparametrizable as [Z>p] with Z following standard logistic distribution, whose density and cdf is smooth.', 'Summary:', '1. A comprehensive empirical comparison of deep learning optimizers, with their performance compared under different amount of hyper-parameter tuning (they perform hyper-parameter tuning using random search).', 'In this paper, the authors address representation learning in non-Euclidean spaces.', 'The paper proposes to modify the \"Dual Learning\" approach to supervised (and unsupervised) translation problems by making use of additional pretrained mappings for both directions (i.e. primal and dual).', 'In some ways the presented work is a form of meta-learning or *meta-mapping* as the authors refer to it.', 'But there are no comparisons between the proposed training method and previous related works.', 'What happens if you instead densely sample from a 3D box (similar to the implicit shape models)?', 'In such a situation, both the value and the dynamics of W_s will be very close to that of W_U. Does it mean that W_L is not so important as W_U?', 'Section 4:', 'Therefore, another baseline to check is to conduct centralized attacks with the same number of times as DBA, but each update includes 1/4 number of poisoning samples, so that the total number of poisoning samples included to compute the gradient update still stays the same.', 'The main novelty in Dreamer is to learn a policy function from latent representation-and-transition models in an end-to-end manner.', \"Doesn't the classification loss have a dependency on the input condition?\", 'In general, the proposed approach of using the average saliency map as attention mask for learning appears to be reasonable.', 'This distinction is potentially confusing to a reader since the gradient-based methods can be seen as optimization algorithms, and the optimization-based methods rely on gradients.', 'Because of this, it seems like a precise answer to what makes a good single training image remains unknown.', \"2. What's the training time of the proposed method compared with vanilla adversarial training?\", 'The experimental result itself is quite comprehensive.', 'Concerning the experimental section:', 'This is just channel-wise normalization with some extra parameters - no need to call it this way (even if it\\'s implemented with the same function) as there is no \"batch\".', 'The way the author handles the missingness is by introducing an auxiliary binary random variable (the mask) for it.', 'This horizon length is still short compared to the entire horizon length of many MDPs (e.g., 1000).', 'http://hemanthdv.org/OfficeHome-Dataset/', 'The experimental improvements on omniglot seem quite substantial.', 'Specifically it tests on Omniglot with natural image background and cliparts to real images.', 'i.e., omniglot <-> omniglot-M. I wonder whether the proposed approach can still work in large domain shift such as omniglot to fashion-mnist etc.', 'The paper does not talk about how these skills can be used as primitive actions by a higher level controller (in a hierarchical RL setup), which would help in demonstrating the usefulness of these skills - e.g.: are these skills sequentially composable such that they can solve a complex task?', 'Now may be the time to move you to understanding what structures get learned in latent space, are the in fact compact, diverse?', 'The merits of this paper lie in the following aspects: (1) It is the first to learn label propagation for transductive few-shot learning.', 'Scalars?', 'The paper is well written and the key elements are in the body.', '=> Baselines: The comparison provided in the paper is weak.', '.', 'The convergence analysis is on Z, not on parameters x and hyper-parameters theta.', 'Looking at Table 3, which is on ResNets, I will make this point clear.', 'I think it is ok to say that meta dropout tries to optimize a lower bound of log p(Y|X;\\\\theta,\\\\phi^*), but meta dropout does not regularize the variational framework because there is no variational inference framework.', 'The paper introduces a generative model for video prediction.', '#9) I think that MNIST is almost a toy experiment, since the crucial component of the proposed method is the prior modeling with the GAN.', 'This paper can be situated as a new contribution combining these two strands of research.', 'An example of lack of mathematical rigor is equation 4 in which the same variable name is used to describe the weights before and after pruning, as if it was computer code instead of an equation.', 'Comparisons against these more standard inference algorithms is essential for understanding what advantages are afforded by the objective proposed in the paper.', 'The paper does a good job of motivating the additional regularization penalty and their approximation to it with a series of experiments and intuitive explanations.', 'particular dataset has been used to train a ConvNet.', '- Are the curvatures the same for each layer for the GCNs?', 'I do not have major comments about the paper itself, although I did not check the technical details super carefully.', 'For example, the number of labeled examples in Table 1 is fairly large and inconsistent (4K, 1K, 10K for the 3 organic datasets).', '* Generative modelling of partially observed data is a very important topic that would benefit from fresh ideas and new approaches', 'The idea is that the prior distribution may explain structure that the embeddings would not have to capture.', 'The authors should more clearly define what the private, public and evaluation sets are, right from the beginning.', 'The paper proposes a few variations on the RGCN model with adding attention to either within relations or across relations.', 'A small selection is given below, although a literature search will reveal many more papers.', 'The authors present an architecture search method where connections are removed with sparse regularization.', '3. How can the proposed method be generalized to non-image data?', 'I think Figure 5 is the most interesting figure in the paper.', 'In summary, the quality of the paper is poor and the originality of the work is low.', '- Table 1: Not sure why there is only one model that employs beam search (with beam size = 2) among all the comparisons. It looks strange.', 'I think the paper is solid as-is; positive results in this comparison would take it to the next level.', 'After reading author feedback', 'Is this different from first learning a image translation mapping (using the unlabelled data in the target domain), and then applying existing meta-learning models/algorithms to the labelled data in the target domain?', 'The objective of this work is to provide a method for better uncertainty estimates from deep learning models.', 'Summary:', 'This has been done in particular in hyperbolic space, but some datasets benefit from more complex spaces.', 'This paper proposes a method to address the interesting task, i.e. controllable human activity synthesis, by conditioning on the previous frames and the input control signal.', '(5) Due to the mean policy approximation, does the mean policy depend on \\\\phi?', 'The authors did provide literature in classification that had similar findings, but it would be beneficial for the authors to explore reasons that random features perform well in reinforcement learning.', 'Minor Suggestions:', 'On page 4, State update function, what is the meaning of variable \"Epsilon\" in the equation?', 'In particular, the linear probing accuracy appears to be often monotonic as a function of the depth of the layer it is computed from.', 'It makes sense, and I expect the proposed model may benefit from its design for long-range spatio-temporal feature learning.', 'To this end, it proposes constructing a knowledge graph using recurrent updates over the sentences of the text, and using the graph representation to condition a reading comprehension module.', 'The author should comment on this.', 'results have been achieved, I think there are several limitations regarding the novelty and', '* Is the acronym ME pronounced like the word “me” or is it spelled out “M-E”? If the latter, then all cases of “a ME bias” should be corrected to “an ME bias”.', 'For these reasons I lean towards rejecting this paper for now, but would love to see it refined for a future machine learning conference.', 'None of these problems exist with the presented approach.', \"The word non-trivial really doesn't add anything here.\", 'Model-based RL algorithms can work in real-time (e.g. http://proceedings.mlr.press/v78/drews17a/drews17a.pdf) and have been shown to have same asymptotic performance of MB controllers for simple robot control (e.g. https://arxiv.org/abs/1805.12114)', 'References', 'material presented in section 3 and filling this out with more', 'Requests for clarification/additional information:', 'The labels of figures are hard to read.', 'Also I did not verify the proofs the paper appears technically correct and technically clear.', '\"each 10 sub classes are belonged to one\"', 'This paper focuses on the second part, with a different model.', 'Quality: A simple approach accompanied with a theoretical justification and large number of experimental results.', 'Overall, I find the paper a promising contribution. But until the authors provide a more thorough experimental evaluation, I hesitate to recommend acceptance.', '\"', 'This needs to be clear in the paper (add clear arguments and related references).', 'What is the relationship between $\\\\theta$ and $\\\\tilde\\\\theta$ exactly?', 'This manuscript studies mutual-information estimation, in particular variational lower bounds, and focuses on reducing their sample complexity.', 'Cons and comments:', 'Overall, this paper was well written with useful illustrations and clear motivations.', 'The conclusion is that, indeed, deep convnets can learn to make such a decision. As could be guessed from intuition, the larger the capacity of the network and the smaller the size of the sets, the higher the accuracy.', 'Brief summary:', 'Though the Bayesian inference using GAN is a natural idea, learning algorithms proposed in this paper are simple and are not intensively developed.', 'I think this function composition angle should be deemphasized in the title/abstract, but I think the paper stands', 'I have some concerns regarding the presentation of the main objective and the lack of justification in certain parts of the methodology.', 'There are some general discussions on how to adapt gyrovector space theory into spherical spaces.', '[Motivation]', 'and', 'They provide results confirming that this measure is able to identify neurons that are important for specific tasks.', '[1] Rocktäschel, Tim, and Sebastian Riedel. \"End-to-end differentiable proving.\" Advances in Neural Information Processing Systems. 2017.', 'This is ultimately due to the fact that the set of alt-az rotations is not the same as the set of points on the sphere, topologically speaking.', '- My biggest issue is that there is no clear evaluation of the runtime benefit of the second Viterbi decompressor.', 'Since the input visual feature is the same, the only difference is the proposed model has additional label embedding as input.', '[1] Deep Variational Information Bottleneck', '3) What is the main difference on the combination of assumptions on Theorems 3.2, 3.2 and 3.4. Which one is stronger. Is there a reason for the existence of Theorem 3.3?', 'Back to introduction section, the goal of this paper (as claimed in the beginning of second paragraph) is to clarify the debate.', 'The introduced technique is interesting.', 'From a high level perspective, this is an interesting result that ostensibly will lead to a fair amount of discussion within the RL community (and already has based on earlier versions of this work).', 'I would have also liked to see a comparison to these methods in the the classification results.', 'I have gone through the other reviews as well as the author response.', 'This is clearly an intractable problem.', 'The second result shows strong convergence when D is set as the Bregman divergence.', 'Originality: The work is moderately original.', 'The experiments are also good.', 'I think this point should be discussed in the paper.', '3. For the LSTM in the graph update, why does it have only one input? Shouldn’t it have two inputs, one for previous hidden state and the other for input?', '[1] Kondor, Trivedi, \"On the Generalization of Equivariance and Convolution in Neural Networks to the Action of Compact Groups\"', 'Moreover, the main difference between math problems and other problems is that there are math expressions; I do not think that using words/concept labels only is enough without touching on the math expressions.', 'However, to compare different classifiers, the proposed algorithm still needs humans to annotate the selected dataset, which is very expensive compared with traditional methods.', 'Authors observe a list of issues with the DNC model: issues with deallocation scheme, issues with the blurring of forward and backward addressing, and issues in content-based addressing.', 'The paper is well motivated (fast and accurate posterior inference) and the construction of the solutions (invertible architecture, appending vectors to input and output, choice of cost function) well described.', '3. The model achieved better performances in challenging control tasks compared to previous latent space planning methods, such as PlaNet.', '-  Instead of relying on the KL regularizer, a standard approach to learning the model in Eq 3 would be to use well understood MCMC or variational methods that explicitly retain uncertainty in \\\\theta and induce predictive uncertainties.  Were they explored and found to be not effective?', 'Other comments:', 'The idea of regarding the history as a tree looks very promising.', 'First of all, the setup for the AE and VAE is not specified.', 'Finally, the style (font) of the paper does not adhere to the ICLR style template, and must be changed.', 'However, one explanation of why DBA is more persistent in this case is because the adversarial parameter updates happen 4x times compared to the centralized attack.', 'The iterative architecture is similar to the routing in CapsNet (Hinton 2018) in terms of weight sharing between successive layers.', 'The authors run a set of experiment to show the usefulness of SPoSE.', 'Very nice to have included negative results and detailed parameter sweeps.', '--- After rebuttal ---', 'Thus, the trained model will also only be able to handle one test domain, not much different than regular meta-learning models.', 'This procedure works for all architectures and objectives.', 'These two models are trained using the gradient-based meta-learning technique (for instance, MAML).', 'There is another symbol G_g which is a constant.', 'Specifically the paper builds on top of the idea of the ensemble neural networks (NNs) and introduces a stochastic neural network for handling the mixing components.', \"Let's just consider the distortion objective on x (of course it also applies to y without loss of generality).\", 'To prevent this behavior, the paper proposes to combine multiple Bayesian neural networks, independently trained with Dropout MC, to an ensemble.', 'It would be good to see more analysis on the axioms 1 through to 4 for the sake of completeness in the light of partial axiomatization of conductance.', 'I still feel that more learning (as inGraphRNN) to build a fuller generative model of the graph would be interesting, but the authors make a strong case for the usefulness and practicality of their approach.', 'In [1], the authors give two algorithms: one with a better sample complexity than the algorithm presented here, but which has some systematic, somewhat large, error floor which it cannot exceed, and another which can obtain similar rates of convergence to the exact solution, but which requires polynomial sample complexity (the explicit bound is not stated in the paper).', 'The major concern I have is that the ensemble of MC-Dropout models is not an approximation of the posterior anymore.', 'Update After rebuttal:', '3.  Or simply to a well tuned \\\\lambda, chosen on a per dataset basis? From the text it appears that \\\\lambda is manually selected to trade off accuracy against uncertainty on OOD data.', '- doesn’t answer the one question regarding observation representation that it set out to evaluate', 'In particular, I think that the proposed algorithm has a more-than-superficial resemblance to stochastic LBFGS: the main difference is that LBFGS approximates the inverse Hessian, instead of (GG^T)^(-½).', 'So this work has to be supported with more detailed experimental results to express the potential of this approach fully.', 'The generative task is a better option.', '1. Where is L_da in Figure 2? In Figure 2, what’s the unlabelled data from which testing tasks are drawn? Is it from meta-test data training set?', 'Because of the efficacy of such \"worst-case\" comparison, the needed set size is very small and thus minimizes the human annotation workload.', \"On the plus side, it's great that the authors added the nice development for the spherical operations, since that will come in handy for many models.\", '[A]: Blier, Léonard, and Yann Ollivier. 2018. ``The Description Length of Deep Learning Models.’’ arXiv [cs.LG].', 'A Wild Bootstrap for Degenerate Kernel Tests', 'It consists of math problems in various categories such as algebra, arithmetic, calculus, etc.', \"I'm not sure I understand the PCA figures. Can you please explain how the first principal component was used to generate them?\", 'I encourage the authors to clarify these points in an updated version of their paper.', 'I think that is work is clearly very interesting.', 'However, the theoretical justification and experimental results are not.', 'This would also help to explain the difference of performance/ behavior  w.r.t. these models (Fig. 5).', 'It isn’t clear why the particular proposals are necessary or to which of the proposed extensions the inflated OOD uncertainties can be attributed:', 'In the end, no solution that can ensure quality and stability is found, except having prohibitively large amounts of data (~300M images).', 'The upper bound can be simplified as the quartic form (Eq. 4) and can be optimized with the Lagrangian form.', 'Summary', '*Decision*', 'How influential is that extra term to the uncertainty quality that you obtain in the end?', 'Comparing to non-pretraining GNN, the improvements are significant for most cases.', '- The authors note that previous datasets are often specific to one type of problem (i.e., single variable equation solving). Why not then combine multiple types of extant problem sets?', 'This paper proposes meta dropout, which leverages adaptive dropout training for regularizing gradient based meta learning models, e.g., MAML and MetaSGD.', 'If it is (i) we care about, then it would be useful to quantify the advantage gained either empirically or analytically.', 'The results are mainly shown on VQA 2.0 set, with a good amount of analysis.', 'To infer latent variables from partial-observation data, they introduce the selective proposal distribution that switches encoders depending on whether each input modality is observed.', '- How much does the image matter for the single-image data set?', 'Measures as the one proposed in this paper are a very important building block towards this.', 'Another modeling choice that is not clear is regarding how the model processes the text -- reading prefixes of the paragraph, rather than one sentence at a time. What happens if the model is changed to be read one sentence at a time?', 'I would imagine that playing with the hyperparameters would allow one to traverse the trade-off between realism and diversity.', 'Such an approach is named Interpretable COntinual Learning (ICL)', \"This paper proposes an efficient zero-confidence attack algorithm, MARGINATTACK, which uses the modified Rosen's algorithm to optimize the same objective as CW attack.\", 'Also, the idea is pretty clean, and the derivations are simple and clear.', 'In this case, KFLA always optimises a “correct” Bayesian model for every value of the hyperparameter whereas MNF and noisy K-FAC do not.', '- the notation q appeared in the middle part of page 3 before the definition of q is shown in the last paragraph of p.3.', '[Cons]', 'Next, the problem is formalized as a partially-observable MDP with a discrete action set, and PPO and SimPLe are introduced.', '? why is this a trade-off?', 'Given the ties between Hebbian synapses and attention (see Ba et al), an important control here could be an LSTM with Bahdanau (2014) style attention.', 'A newly developed cue-reward association task shows the clear limitations of basic plasticity and how modulation can resolve this.', 'This is a good paper and I higly recommend acceptance.', 'parameters in the network and data augmentation.', '- The general methodology of generating questions and ensuring that no question is too rare or too frequent and the test set is sufficiently different---these are important questions and I commend the authors for providing a strong methodology.', '1. The paper should also talk about the details of ARNet and discuss the difference, as I assume they are the most related work', 'Instead, it is a low-rank approximation to the full matrix.', 'Minor comments /questions not affecting review:', 'The paper considers adaptive regularization, which has been popular in neural network learning.', 'The paper demonstrates the method on the dSprites dataset, showing that it can effectively learn linear canonicalizations, and that multiple of these canonicalizations can be applied to the same image representation.', 'Although their network was resilient toward white box attacks they suffered from black box attacks.', 'It is shown experimentally that one can detect (even partial) leakage with such a technique.', 'Taking the perspective from signal processing, this paper proposes a pooling operation called frequency pooling (F-pooling).', '- For predicting a deformation R^3 -> R^3 function composition sort of makes sense, but how generalizable', 'However, this contribution seems to be orthogonal to the sparsity regularization, which, I suppose, is the main point of the paper.', 'the Berrett  and Samworth test.', 'The motivation for doing this is that this structure is typically hard to model for network embeddings.', 'This paper addresses machine reading tasks involving tracking the states of entities over text.', 'This paper proposed Compound Density Networks (CDNs), a neural network architecture that parametrises conditional distributions as infinite mixtures, thus generalising the traditional finite mixture density networks (MDNs).', 'Intuitively, it should provide similar results to the success-failure curve.', 'A more interesting question to study is how this gap affect the iterative algorithm, and whether/how the error in the imperfect target policy accumulates/diminishes so that iteratively minimizing KL-divergence with imperfect \\\\pi* (by MCTS) could still lead to optimal policy (Nash equilibrium).', 'ICLR 2018', 'Whether to accept it or not depends on what standard ICLR has towards such papers (ones that do not propose a new model/new theory).', 'Then, The authors proposed how to incorporate GAN to the Bayesian inference.', '- Similarly to the problem of sparse reward, if the robot and the object are far apart and it is difficult to reach the object with random exploration, it would also be difficult to train the mutual information discriminator. How was the discriminator trained? How many time steps were used to train MI discriminator?', 'However, it is not obvious that how to move from line 3 to line 4 at Eq 15.', 'In general I find the paper novel (to the best of my somewhat limited knowledge), interesting and well written.', 'Given these two categories of states, the proposed algorithm maximizes a lower bound on the mutual information between the two categories of states such that a policy is learnt that is able to manipulate the object with the robot meaningfully.', 'I do acknowledge that the authors did a reasonable job of trying to clear this up in section 3.2 and section G of the appendix.', 'Minor comments:', \"- The role of the baseline x' should be better explained when it is presented (first paragraph of section 3).\", '1. The observation that gradients explode in spite of BN is quite counter-intuitive. Can you give an intuitive explanation of why this occurs?', 'Perhaps an example in linear algebra is needed.', 'Pros:', 'Paper summary.', 'In statistics:', 'High-level', 'The recent work of Schott et al (which the authors compare results to) proposed the use of Bayes rule inversion as a more robust mechanism for classification under different types of adversarial attacks.', 'I would suggest comparing with CW attack under different sets of hyper-parameters.', 'Overall, this is a reasonable paper but experimental section needs much more attention.', '6. Provide a summary judgment if the work is significant and of interest to the community.', 'proportionality must depend very much on the number of layers in the', 'It would make the paper much stronger if the authors perform quantitative + qualitative evaluation on the MF training of RBMs first.', 'For example, the authors should at least say that the \"D\" in Figure 2. stands for delay, and the underline in Figure 4. indicates the bits that are not pruned.', 'These points remain me puzzled regarding either practical or theoretical application of the result. It would be great if authors could elaborate.', 'For hyper-parameter search (using hyperopt), the manuscript should make it explicit what metric is optimized.', 'NIPS 2017', 'when … networks are sufficiently complex” or do they actually mean when the “when the problem is simple enough”?', 'So the two results coincide for the exchangeable case. Might be worth pointing this out.', 'First of all, the proposed SST algorithm alone only performs better than baselines in 1 case, equal to them in 1 case and worse in 1 (table 3).', 'More precisely, for the digit datasets, the reviewer was interested to see how the proposed MDL performs on jointly adapting SVHN, MNIST, MNIST-M, and USPS or jointly adapting DSLR, Amazon, and Webcam for OFFICE dataset.', 'The paper attempts to clarify the debate on large-batch neural network training, particularly on the relationship between learning rate, batch sizes and test performance.', 'From my understanding, it seems this method is also to be compatible with other methods of approximate Bayesian Computation (ABC).', 'It argues that early layers of a convolutional neural network can be effectively learned from a single source image, with data augmentation.', 'I would expect that you would like to impose additional structure to the weights.', 'This form of noise injection at various levels of the generator is also close in spirit to what BigGAN employs, except that in the case of BigGAN different parts of the noise vector are used to influence different parts of the generator.', 'RNN-based approaches are with better “complexity” comparing to your sum baseline and “Deepset” approach.', 'Cons:', 'I would like to thank the authors for their reply and for the further clarification.', 'Strengths: The paper provides an elegant solution for tracking relationship between entities as time (sentence) progresses.', 'The authors formulate a VAE-GAN model and successfully implement it.', 'The authors’ add a manifold regularization penalty to GAN discriminator’s loss function.', 'Please comment on the choice, and its impact on the behavior of the model.', 'I am curious to know if authors observe similar convergence gains in bAbI tasks as well. Can you please provide the mean learning curve for bAbI task for DNC vs proposed modifications?', 'Is \\\\eta_{\\\\mathcal{H}} is a better estimation that previous adaptability term in multi-source DA? What is the role of \\\\eta_{\\\\mathcal{H}} in the algorithm design? How to control it in empirical algorithm?', 'In particular, I believe that any algorithms you compare against, you should optimize both G and theta, since optimizing purely the hardware is unfair.', 'set of images was used for training or not (or similar tasks).', 'Consequently, the proposed regularization technique takes a step of tunable size in the direction of the manifold gradient, which has the effect of smoothing along the direction of the gradient while ignoring its norm.', 'Also, what the numbers in Table 4 show? Which is the best value that can be achieved?', '-\\tThe proposed task is interesting and well motivated.', 'It is more or less like a reformulation of the AGZ setting in the MDP problem. And it is commonly known that two-player zero-sum game is closely related to minimax robust control. Therefore, it cannot be called as “generalizing AlphaGo Zero” as stated in the title of the paper.', 'Then the memory footprint (in terms of messages) is reduced by rate k/n.', '+ Results on confidence as a signature of a dataset are interesting.', 'Concerns or suggestions:', 'The paper studies the Bayesian inferences with the generative adversarial network (GAN).', '+ demonstrates that curiosity-based reward works in simpler game environments', 'Kacper Chwialkowski, Dino Sejdinovic, Arthur Gretton', 'The paper is well-written.', 'Review for MANIFOLD REGULARIZATION WITH GANS FOR SEMISUPERVISED LEARNING', 'whether an input to a trained ConvNet has actually been used to train the', 'This work is focused on learning 3D object representations (decoders) that can be computed more efficiently than existing methods.', 'I enjoyed the paper; it addresses the interesting setting of an extremely small data set which complements the large number of studies on scaling up self-supervised learning algorithms.', 'See for example [A, B].', 'In addition to report the median scores, standard deviations should be reported.', 'Thank you for your detailed answers to my questions.', 'Experimental construction and analysis also seems sound.', \"While the paper shows experimentally that they aren't as successful as the RFs or IDFs, there's no further discussion on the reasons for poor performance.\", '= Significance: Primarily, this ‘finishes’ [Parthak, et al., ICML17] to its logical conclusion for game-based environments and should spur interesting conversations and further research.', 'I’m curious to hear the authors’ thoughts in this.', 'The paper does not provide very significant evidence that this method is useful.', 'Also there are a few unclear parts which I discuss in questions below.', '+2 Does not require additional computation during inference, unlike ensembling.', 'The primary difficulty in reviewing this paper is the poor presentation of the paper.', 'The model-based method achieves better validation error than the other baselines that use actual data.', '(1)\\tThe idea of decomposing problems into concepts is interesting and also makes sense.', 'The paper studies the problem of question generation from sparql queries.', 'They demonstrate performance on a clique-finding problem.', 'A Kernel Statistical Test of Independence', 'References:', 'They provide some theoretical motivation and empirical evidence that it helps avoiding memory aliasing.', 'Also, the theorem only applies to the iid noise, while most natural noise patterns have structure (e.g. JPEG artifacts, broken pixels, etc) and thus can probably be better approximated with deep models.', 'because our fMRI study is trying to show that dependency testing works \"', '* The oracle-augmented datasteam model needs to be contextualized better.', 'Overall I like the motivation, provided background information and simplicity of the approach.', 'The presented analysis seems to neglect the error term corresponding to the value function.', 'In addition, the paper proposes practical recipes for characterizing collapse in GANs.', '4) All the results heavily depend on the PL condition.', '2.\\tThe theoretical guarantee (with some assumptions) of the true posterior might be beneficial in practice for relatively low-dimensional or less complex tasks.', 'Here, the straightforward regression term means directly regress the output mask to the target densepose mask. Will the proposed mask term perform better?', 'Weaknesses:', '- Was the auxiliary tower used during the training of the shared weights W?', \"- For MISC, the additional assumption is required: the agent should know that which parts of the states are its own controllable state and object's state respectively.\", 'The idea of improved bounds for streaming algorithms using machine learning oracle seems to be very appealing to me.', 'Another contribution is on using negative pre-training to deal with an imbalanced training dataset.', \"In the first mentioning of Graph Neural Networks, the authors only cited Kipf & Welling's GCN.\", 'The authors introduce the idea of distributed backdoor attacks in the FL framework, in which the dishonest participants in FL add local triggers to their training data to influence the global model to classify triggered images in a desired way.', 'Overall, I do think the paper has makes a contribution in experimentally showing a setting where tree-structured NMNs can show better systematic generalization than other visual reasoning approaches.', 'Authors aim at theoretically showing the difficulties of using existing generative models to learn distributions of point clouds, and propose a variant that supposedly solves the issues.', 'What instead is missing is an answer to the question: Is it worth using a neural graph? what are the advantages and disadvantages compared to previous approaches?', 'The F-pooling is then implemented with matrix multiplications and tested with recent convolutional neural networks for image classifiation on CIFAR-100 and a subset of ImageNet dataset.', 'Even the ‘focused experiments’ can be explained with the intuitive narrative that in the state/action space, there is always more uncertainty the farther one goes from the starting point and this is more of a result of massive computation being applied primarily to problems that are designed to provide some level of novelly (the Roboschool examples are a bit more interesting, but also less conclusive).', 'Table 8 rises some concerns.', 'While another trivial trick is to evaluate \\\\alpha by the domain closeness between each source domain with the target domain.', 'The additional parameters of this paper come from having parameters associated with the diagonal (intuitively: self edges get treated differently to other edges) and having parameters for the transpose of the matrix (intuitively: incoming edges are different to outgoing edges).', 'Questions:', 'Actually, imbalance data sets are common in machine learning problems and there are many related works.', 'The paper is well written and the proposed method well motivate and intuitive.', 'Neal) that for MLPs what determines the complexity of the typical', '- I think overall this is a good paper, with clear organization, detailed description of the approach, solid analysis of the approach and cool visualization.', 'based on the definition of conductance and attribution.', 'You should also show the performance of regular SSL methods in the setup on Table 4.', '- This investigation gives a significant amount of insights on GAN stability and performance at large scales, which should be useful for anyone working with GANs on complex datasets (and that have access to great computational resources).', 'The experiments show that the method is quite effective.', 'Main argument:', 'Second, many typos and grammar errors need to fix, e.g., \"the proposed SST is suitable for lifelong learning which make use...\", \"the error 21.44% was lower than\" 18.97?', 'I suggest weak accept but I am open to reconsider in case that my above concerns are answered.', '* Page 7, first paragraph of section 5.: \"is ran\" --> \"is run\"', 'Very minor details:', 'Clarity -  The paper is very clear and well written.', 'We have two questions in the following.', 'Though, I still think the contribution is incremental, since back-propagating gradients through values and dynamics has been studied in prior works (albeit with less empirical successes compared to Dreamer).', '[C] K. Saenko, B. Kulis, M. Fritz, and T. Darrell. Adapting visual category models to new domains. In European Conference on Computer Vision (ECCV), 2010.', 'Experimental results suggest that the proposed layer can outperform existing methods in supervised learning with graphs.', '(1) the settings and the analysis of adversarial robustness experiment can be discussed in details.', 'As for the experimental results, the paper only provides results on the sentimental analysis results and digit datasets, which are small benchmarks.', '- The model G_theta does not appear in the training objective function (4), how is this module trained precisely?', 'To conclude, while I find the studied problem quite interesting and intuitions behind the method very reasonable, the current methodology is not very principled and the experiment evaluation did not convince me that such an elaborate strategy is needed.', 'With regards to the fMRI experiments, good baselines are missing: DEMINE is compared to Pearson correlation.', 'For example, Narayanaswamy et al. [1] also propose to utilize labels to VAE.', 'As the authors themselves remarked, \"it can be seen that our models sometimes outperform the two Euclidean GCN\".', 'The paper proposes an approach to learn nonlinear causal relationship from time series data that is based on empirical risk minimization regularized by mutual information.', 'Indicators can be seen as a measure of importance.', 'The comparison of different observation representations doesn’t include any analytical component, the empirical component is primarily inconclusive, and the position statements are fairly non-controversial (and not really conclusively supported).', 'These goal are', 'Moreover, the performance reported in this paper seems to be much inferior to the state-of-the-art results reported in the literature.', 'It seems there is no conclusion to take away from the experiments in section 5 (convolutions).', 'Soundness:', 'The motivation is that meta-learning the noise allows to learn how to best perturb examples in order to improve generlization.', 'Summary', 'Honestly, I cannot confirm this claim (not sure if I have seen all NAS papers out there), but if it is the case, then it is impressive.', 'This paper discusses the optimization of robot structures, combined with their controllers.', 'It is prefer to provide detailed description or pseudocode for this step.', 'For example, if the rule-based concept extractor can already extract concepts very well, the “problem retrieval” should be solved by searching with the concepts as queries.', 'Since this is the core contribution, this should also be the main comparison.', 'Summary:', '2. \\\\lambda is trained using Accelerated Proximal Gradient (APG, Huang and Wang 2018).', 'In addition, two preliminary convergence theorems were provided for two extensions of HGD: (i) a stochastic variant of HGD and (ii)  Consensus Optimization Algorithm (CO) (by establishing connections of CO and HGD).', '* It would be interesting to see a more detailed analysis of the predictions of the image classifiers in section 4.2.', 'Overall, I think this paper is a nice sanity check on recent self-supervision methods.', 'In the experiments, the authors compare the proposed method to only the one proposed by Arora et al. 2015.', 'The statements should be put in perspective with others.', 'It seems that the authors are not aware of this difference.', 'Cons:', 'It is related to existing works such as neural logic induction[1] and planning in latent space[2].', '+ The neural message passing architecture is adapted to the problem, handling features not present in the standard GNN literature (hyperedges, ...)', 'Weakness:', '3,5', 'For example, for baseline 1, it is very hard to understand why would we want to use such an unusual baseline, and why it is called a \"random baseline\".', 'Hence, the authors proposed an alternative approach to comparing different classification models by the notion of inter-model discrepancy.', 'Compared to the previous result (Zhang et al., 2018) for RNNs, this paper refines the generalization bounds for vanilla RNNs in all cases and fills the blank for RNN variants like MGU and LSTM.', 'The authors propose a discriminative variational autoencoder (DiVA) to solve this problem under the generative replay framework.', 'with', 'A more extensive discussion will help convince the readers that the proposed dataset is indeed the largest and most diverse.', 'When it comes to experiments, constant epoch budget is also fairly well understood and the behavior in Figure 1 is not really surprising (as the eventual training performance gets worse with large batches).', 'The usefulness of the learned features is evaluated with linear probes at each layer for either ImageNet or CiFAR image classification.', '-1: Potential Issues with the Maths.', 'It is well known from the literature that combining multiply neural networks to an ensemble leads to better performance and uncertainty estimates.', 'Weaknesses:', '= Clarity: The paper is very clearly written. (7/10)', '- The word in the title should be “Convolutional”, right?', '\"Lipschitz-Margin Training: Scalable Certification of Perturbation Invariance for Deep Neural Networks\"', 'Sometimes these are capitalized, but the use is inconsistent throughout the paper.', 'Comments / Questions', 'The authors verify the effectiveness of the proposed method on the performance of different tasks and modules.', '--The notation for the proposed parameters theta, theta’, phi, phi’ are not consistent with the notation in the intro section, where phi was used for the encoder and theta for the decoder.', \"- One thing that I didn't see discussed by the authors is that there are subtle difference between hyperbolic and spherical spaces.\", 'http://ttic.uchicago.edu/~smaji/papers/similarity-cvpr14.pdf;', '- The inverse kinematics is an interesting illustration of the potential advantage of this method over conditional VAE and how close it is to ABC which can be reasonably computed for this problem.', 'Some important equations in the paper should be numbered.', 'I would suggest tuning these values for each method independently.', 'It is easy to follow and contains a lot of empirical results.', 'So how importance of each component to the whole framework? I would ask for the ablation study/additional experiments of using each component.', 'This paper proposes a neural architecture search method based on a direct sparse optimization, where the proposed method provides a novel model pruning view to the neural architecture search problem.', 'The idea is similar to Zhao et al (2019), which introduces a weighted parameter \\\\alpha to combine the source domains together.', 'The authors propose a clean -if improper- prior on networks and proceed to perform maximum likelihood inference on it.', \"Some details weren't clear to me.\", '1. The paper uses some nice ideas to reduce the variance of the MI estimates and to be able to work with smaller dataset sizes.', 'This might be because the Bayes and MAT attacks are too simplistic.', '* Page 4, bottom: use Bernoulli distribution -> use factorized/independent Bernoulli distribution', 'The idea is intuitive when the upper bound is small.', 'The results on several video sequences look nice with more natural boundaries, object, and backgrounds', 'There should be sufficient details for a reader to implement this model, thought there are some minor details missing regarding the experimental setup, which will be addressed below.', 'It seems to me that the proposed framework also falls in this category, with a treatment from deep learning.', 'You mention that \"APO converges quickly from different starting points on the Rosenbrock surface\" but 10000 iterations is not quick at all for the 2-dimensional Rosenbrock, it is extremely slow compared to 100-200 function evaluations needed for Nelder-Mead to solve it. I guess you mean', 'So for example, even if a linear classifier on top of Scattering features does poorly, if downstream fine-tuning results in the same performance as another pre-training method, then Scattering is a perfectly fine approach for initial features.', 'This paper tackles the question generation problem from a logical form and proposes an addition called Scratchpad Encoder to the standard seq2seq framework.', 'Is it possible that the output acoustic model is simply better-matched to continuous rather than discrete input (direct vq-wav2vec gives discrete while BERT gives continuous)? Would it make sense to train the wav2vec acoustic model on top of the vqvae codebook entries (e) instead of directly on the symbols?', '- make the bib uniform: remove unnecessary doi - url - cvpr page numbers', 'The motivation for the work (avoiding high amplitude/frequency control inputs) is certainly now new; this has always been a concern of control theorists and roboticists (e.g., when considering minimum-time optimal control problems, or control schemes such as sliding mode control).', '6.', \"- Figure 3: wouldn't the plot of accuracy vs amount of data be more suitable here?\", 'The second level of sparsity comes from every expert only outputting a limited set of output categories.', 'Two performance metrics are introduced to assess the auto-encoding ability of the model: to what extent does the encoder-decoder pipeline result in point clouds similar to the shape from which the input point-cloud is generated.', '- (Minor) $\\\\alpha$ not alway have the same unit: Thm 3.2 it is proportional to a strong convexity and in Lemma 4.7 it is proportional to a strong convexity squared (actually the PL of the squared norm of the gradient).', 'the relational graph networks (Schlichtkrull et al. 2018).', 'Experiment on VQA shows the proposed model benefits from utilizing different modules.', 'This paper proposes to impute multimodal data when certain modalities are present.', 'In general, I find both the proposed model and optimization algorithm interesting.', 'This paper shows some promise when graph network-based controllers augmented with evolutionary algorithms.', 'The next submission of the paper could choose one or few of these pieces as target research problems and develop a thoroughly analyzed novel technical solution for them.', 'Furthermore, the authors appear to imply that increased FSM values for an old task after training on a new task indicate catastrophic forgetting.', '- \"deferent augmentation modes\" -> different', 'g(f(1 - epsilon)) = 0, why would it be 1- epsilon?', '* The proposal looks theoretically solid', 'Does such approximation guarantee the policy improvement?', 'Due to several shortcomings of the paper, most important of which is on presentation of the paper, this manuscript requires a significant revision by the authors to reach the necessary standards for publication, moreover it would be helpful to clarify the modeling choices and consequences of these choices more clearly.', 'SGD is observed to be significantly faster than batch gradient when far from convergence (experimental evidence), and', 'It is a pity that the code is not provided.', '2. The authors perform numerical experiments to demonstrate the effectiveness of their framework in benchmark datasets. And their experimental result support their previous claims.', '- Providing the same computational budget seem rather arbitrary at the moment, and it heavily depends from implementation. How many evaluations do you perform for each method? why not having the same budget of experiments?', 'Thus, the theoretical contribution of this paper is limited.', 'of a (deep) neural network', '2. To evaluate A-S setting, I understand that it may be tricky to enable a fair comparison between the centralized attack and DBA.', 'Additionally, how much variance is there in the imagined trajectories from a certain starting state? In other words, are the endpoints of most imagined trajectories similar or very different?', 'My second concern is about the theoretical contribution.', 'I found the motivation and solution are reasonable (despite some questions pending more elaboration), and results also look promising, thus give it a weak accept (conditional on the answers though).', '11. Baseline 2 is actually referred to as \"usage baseline\" but this name is not introduced in the itemized part.', 'This weakens the feasibility of the proposed method.', 'I vote for rejecting this submission for the following concerns.', '2) At the end of Page 4, the authors show that the update of the generator only depends on the improvement of the classifier after using the auxiliary label for training.', 'It is also the case that if the variables have time dependence, then appropriate corrections must be made for the test threshold, to avoid excessive false positives.', 'The properties of BN used as motivation for the study, are observed non-asymptotically with constant or empirically decreased learning rate schedules for a limited number of iterations.', 'The refenence to  Bengio 2018 is incomplete: what do you refer to precisely?', 'The problem of image classification is considered only, while authors claimed the method can be easily applied to other problems as well.', 'Is there any explanation for this?', 'They also show that current end-to-end approaches for inducing model layout or learning model parametrization fail to generalize better than generic models.', 'This paper proposed a general method for image restoration based on GAN.', 'One such example would be to have a network for each body part and share parameters across each body part.', '2) The paper is applicable to many practical scenarios since the data from the real-world application is typically collected from multiple sources.', '3. Mentioning \"merging \\\\sigma and \\\\omega, is left for future work\" is confusing before formally introducing \\\\sigma and \\\\omega.', 'I am curious about why that might be.', 'The paper is very well written, and easily understandable.', 'The authors define a new discrimination task to discriminate, within each domain, labeled samples from unlabeled ones that most likely belong to extra classes (classes with no labeled or unlabeled samples in the domain).', 'This paper aims to estimate time-delayed, nonlinear causal influences from time series under the causal sufficiency assumption.', 'Therefore, although I would like to see an extended version of this paper at the conference, without further experiments and analysis I see the current version rather as an interesting workshop contribution.', 'These are important problems in streaming data analysis.', 'The current work addresses a meta-learning approach to automatically generate auxiliary tasks suited to the principal task, without human knowledge.', 'So overall I have the feeling that the authors have not succeeded to evaluate the model’s power with these two experiments and we cannot draw any strong conclusions regarding the benefit of the proposed mixing approach.', 'These are my major concerns.', 'The authors propose a notion of conductance to attribute the deep neural network’s prediction to its hidden units.', 'Strengths:', 'Pros:', 'Modulo a few pointers, the work is well-contextualized and makes reasonable assumptions in conducting its experiments.', 'I did appreciate also the novel path-based evaluation of shape accuracy in the Appendix, although it would have been helpful to see more discussion of this in the main paper.', 'Based on the theoretical results and the algorithm, the paper introduces Domain AggRegation Network (DARN), which contains a base network for feature extraction, h_y to minimize the task loss and h_d to evaluate the discrepancy between each source domain and target domain.', 'In the past, many works have argued for encouraging the certain statistical behavior of these representations (e.g., sparsity, low correlation etc) in order to have better classification accuracy.', 'Defending against black box attacks is considerably easier than defending against white-box attacks.', 'i) The proposed architecture is mainly adopted from the graph attention networks (Veličković', 'I feel the approach to implicitly assume that the classifiers to be compared are already \"reasonably accurate\"; since if not, both classifiers might be easily falsified by certain trivial examples, making the \"disagreed examples\" not as meaningful.', '- q_theta was introduced in Eq. (8) before it is defined in Eq. (11).', 'The test set is typically small compared to the training set, so it is no surprise that the compression rate can be as high as 90%.', 'Why do we need to use deep learning here?', \"The paper is more clear after authors' clarification.\", 'The model is trained by directly supervising for the correct span by the MRC model at each time step, which is possible because the data provides strong supervision for each sentence (not just the answer at the end).', 'As a result, they explore and discard Spectral Normalization of the generator as a way to prevent collapse and show that a severe tradeoff between stability and quality can be controlled when using zero-centered gradient penalties in the Discriminator.', 'However, there are some key issues with the paper that are not clear.', 'It seems to me just a combination of several mature techniques.', 'Also, it is not clear why those are called axioms since they are not use to build anything else.', 'Also, from the three Tables in the experimental part, the improvement of F-pooling over AA-pooling (developed by the main reference of this work) does not seem to be significant or consistent.', '- In Equation (6), the posterior distribution should be P(X|G) since X is the latent variable to be inferred, right', 'The paper proposes an approach to construct surrogate objectives for the effective application of policy gradient methods to combinatorial optimization without known neighborhood structure.', '- figures 2 & 3 should be a lot larger in order to be readable', 'The authors stated that \"F-pooling remarkably increases accuracy and robustness w.r.t. shifts of moderns CNNs\"; however, in Table 1-3, the winning margin of accuracy is actually quite small (<2%), and the consistency (<3.5% compared to the second best baseline except resnet-18 on CIFAR 100 has large improvement ~7-8%).', 'It would be interesting to see how these two algorithms stack up.', 'They use a penalized formulation instead of a constrained one.', '- Second paragraph of remark 1 is not clear.', 'It would also be good if besides the mean error rates, they reported best runs chosen by performance on the validation task, and number of the tasks solve (with < 5% error) as it is standard in this dataset.', 'These studies should be mentioned.', 'I lean towards accept, as this is a realistic attack model, and as such can further stimulate research into the robustification of FL model aggregation algorithms.', '= Summary', '- While the results on SQOOP dataset are interesting, it would have been very exciting to see results on other synthetic datasets.', 'More specifically, it evaluates three aspects:', 'Training loss plots would be more clear in the log scale.', 'The approach proposed is a two-stage process: 1. train a quantized version of wav2vec [my understanding is that wav2vec is the same thing as CPC for Audio except for using a binary cross-entropy loss instead of InfoNCE softmax-cross entropy loss].', 'The introduction contains a few statements that may paint an incomplete or confusing picture of the current literature in adversarial attacks on neural networks:', 'The approximation is a sparse two-layer mixture of experts.', '[3] B. Han, Q. Yao, X. Yu, G. Niu, M. Xu, W. Hu, I. Tsang, M. Sugiyama. Co-teaching: Robust training of deep neural networks with extremely noisy labels. In NIPS, 2018.', 'However I do not want to discourage publishing of negative results, and I definitely wish to see this investigation in the paper, but I merely question the positioning of such information.', 'In the meta-learning language, the author attempts to learn a good representation of graphs based on different graph classification tasks generated by a task distribution.', '- I do not understand the problem explained in fourth paragraph of section 4.1.', 'The method requires a class hierarchy in advance to define the binary mask applied to the output layer for auxiliary class prediction.', 'First, I consider the tabular features as multi-feature data and less to be the multimodal data.', 'I do not understand how the model is trained to solve multiple tasks.', 'Finally, in the fine-tuning stage, the feature extractor is fixed, and the classifier is trained based on the novel class.', 'One of the main advantage is that it can select a sample set from an arbitrarily large unlabeled images.', 'Overall, the idea is simple, the explanation is clear and experimentation is extensive. I would like to see more commentary on why this method might have long-term impact (or not).', 'I think the paper is particularly clearly written, and I would vote for it being accepted as it has implications beyond the DNC.', 'In the generator, the exploding of the top 3 singular values of each weight matrix seem to indicate collapse.', 'In the work cited by Chwialkowski et al, failure to account for the dependence between samples results in excessive false positives.', '# Conclusion', 'The experiments verify the proven effect and make the paper more substantial.', 'Furthermore, the baseline models did not use other almost standard regularisation techniques (weight decay, dropout, batch-norm).', 'The authors propose to do so by replacing  the curvature while flipping the sign in the standard Poincare model.', '- Sec 4.1:', '- Need more motivation for faster white-box attack.', 'The strongest part of the analysis of conductance is that conductance naturally couples  the path at the base features with that of the hidden layer.', 'After reading the updated paper I still believe that 6 is the right score for this paper.', 'In just considering the empirical results, they clearly entail a fair amount of effort and just a dump of the code and experiments on the community will likely lead to new findings (even if they are that game simulators are weaker testbeds than previously thought).', 'In general, I feel the paper is interesting but would benefit from a major revision which makes the message of the paper more clear, and addresses these and other issues raised in the review phase. Thus I am holding my current rating.', 'some comments would be helpful to the readers.', '1.\\tLack of technical novelty.', 'The authors present a empirical investigation of methods for scaling GANs to complex datasets, such as ImageNet, for class-conditioned image generation.', 'In my view, learning without labels is an important problem, and it is interesting what can be learned from a single image and simple data augmentation strategies.', 'Tab. 4 shows to some extend the influence of k, but I would like to see a more extensive evaluation.', '---', 'Considering that the ultimate goal is classification, the information to be maximally preserved through each operation through the layers shall be the information that relates to the class label y. In light of this, some justification and explanation shall be provided for using this criterion for optimality.', 'The idea of selective sampling for self-training is promising and the investigated questions are interesting.', 'Also, does this imply that for both forward and inverse process those zero-padded entries always come out to be zero?', 'This is certainly a scientific decision, i.e., the authors are determining which models to use in order to determine the possible insights they will derive.', '3. https://arxiv.org/abs/1905.11449', 'While the human achieves the best performance at the end of the run, the proposed method appears to learn more quickly than the others and finishes with performance comparable to the model-free baseline.', '2. The case of data-independent normalization (such as weight normalization).', 'Over the last few years, researchers in the speech community have invested significant effort in learning better speech representations, and this is not discussed.', 'The paper proposed a novel image classifier comparison approach that went beyond one fixed testing set for all.', 'Overally, I think this paper is well motivated and experiments on few shot learning are impressive.', 'After all, newton method and natural gradients method are not used in experiments.', 'In general, I agree with R2 that the paper generally has some potentially interesting ideas and results but the manner in which the current draft is organized and presented makes it hard to grasp them and there is a lack of coherent message about what the paper is about.', 'For instance, when the authors observe that the structure of the fixed point is such that activations are of identical norm equally spread apart in terms of angle, this is quite far from practice.', 'This is a non-differentiable process and relies on maintaining a large pool of candidates out of which best ones are chosen with the highest fitness.', 'More analysis should be provided on why (Kumar et al 2017) perform so well on SVHN.', 'combination is a good attempt to incorporate both node features and edge features but the', \"It consists of a translation of a program's type constraints and defined objects into a (hyper)graph, and a specialised neural message passing architecture to learn from the generated graphs.\", '* Fig. 3 needs more explanation.', 'This triggers my curiosity of which difficulty is it? It would be nice to see this point more elaborated, and to see ablation study experiments.', 'Overall, this paper is well written and show significant improvements especially for image generation.', 'I will keep my score the same but I highly encourage the authors to add some clarification related to my last comment on the globally bounded gradient.', 'Considering the selection based on highest-confidence, the in or out of class unlabeled data in most cases does not matter.', 'Can you clarify how you view the relationship between the approaches mentioned above?', 'a) The uncertainties produced by CDN in Figure 2 seems strange.', '- What do you mean in section 3.3 by \"if one class dominates the dataset, the model tends to overfit\"?', 'This is typically a bottleneck in deep learning architectures.', 'The effect of the imperfection in the target policy is not taken into account in the paper.', 'One of the main area which is missing in the paper is the comparison to two other class of RL methods: count-based exploration and novelty search.', 'If the authors claim that the proposed MaxConfidence attack method is more powerful than the MaxLoss based attacks, they should provide more comparisons between these methods.', 'This also makes me wonder if this paper is more suitable for a journal - both in terms of the extensive supplementary material (e.g., curvature sampling algorithm and other details can be found only in supplementary), as well as the more rigorous review process that a journal paper goes through.', \"5. In fact, I don't know why \\\\omega needs to output p. It's never mentioned in the experiment section.\", 'The authors propose a heuristic for learning from both data sets through minimization of a joint loss function.', 'This is an interesting paper overall, so I am looking forward for further discussions.', 'Then a vector of length n is represented by inputing a message of length k and receiving n encoded bits.', 'In examining reward curves (generally extrinsic during testing), ‘curiosity-based’ reward generally works with the representation effectiveness varying across different testbeds.', 'The manuscript proposes a multi-domain adversarial learning (MDL) method called MULANN, to leverage multiple datasets with overlapping but distinct class sets, in a semi-supervised setting.', '- In the proof you wrongfully use the term telescope sum twice, there is nothing telescopic about the sum it is just bound by the max value times the length.', 'I would strongly encourage the authors to incorporate the baseline \"(1)\" as proposed by R3 in a future version of the paper as I agree with them that this is a relevant baseline.', 'Overall recommendation:', 'They then demonstrate that because the local triggers cause smaller corruptions in the model coefficients, these distributed attacks survive robust FL training algorithms (namely FoolsGold, and a recent robust regression based method) more often than centralized attacks.', '- Figures 1-4 are difficult to interpreted on a printed version.', 'However, there are still details and concerns.', '(2) Eq. (3): is there a superscription \"(j)\" on z_canon in decoder?', 'The experiments are carefully described and presented, and the paper is well-written.', 'This is due to it having relatively few parameters and to it having a strong inductive bias.', 'Considering the optimization problem involved in the learning process, it is hard to judge whether the effect of such a procedure from the optimization perspective.', 'The paper proposes Dreamer, a model-based RL method for high-dimensional inputs such as images.', 'The paper also uses the results from [Neyshabur et. al., ICLR 2018], where the original paper provided generalization bounds that depends on spectral norm of each layer.', 'Note that, recently [1] seem to propose a result on extragradient in the same vein (i.e. without strong convexity or linearity).', '- The notation in section 3 (before 3.1) is rather sloppy.', 'I expect the authors to comment on that.', 'Also, the definition of mutual information used in this paper uses the inferred distribution q (e.g., in eq. 2), which is somewhat unusual.', 'e.g. \"In specific\" --> \"Specifically\" in the abstract, \"computational budge\" -> \"budget\" (page 6) etc.', '- In Appendix D, the Figures could be slightly clarified by using a colored heatmap to color the curve, with colors corresponding to the threshold values.', '- In Theorem 4.7 an expectation on g(x_a) is missing', 'The benefit of using a deep network is to exploit its optimization capability and the parallelism on GPUs.', '(2) Empirically, it seems TPN achieved very small improvements over the very baseline label propagation.', 'Instead, for a pair of classifiers to be compared, it advocated to sample their \"most disagreed\" test set from a large corpus of unlabeled images.', 'consider these connections. Or at least comment on how these factors', 'The authors are encouraged to conduct 1 NLP dataset.', 'While some results are even better than the last two rows.', 'and could be very useful for future research.', '1. The focus of this paper is the hyper-parameter, please focus and explain more on the usage with hyper-parameters.', 'The paper assumes that the state space can be divided into two parts - the state of the robot (“context states”) which is controllable via actions and the state of an object (“states of interest”) which must be manipulated by the robot.', 'Choquet Integral is used to deal with set input.', 'Indeed, your DIAYN baseline with skill length set to 1 and the number of skills equal to the number of actions (or same parameterization in the case of continuous actions) should recover this approach.', 'Proof reading', 'The proposed method is evaluated on CIFAR-10 and ImageNet dataset.', 'Cons:', 'Qinyi ZhangEmail Sarah Filippi, Arthur Gretton, Dino Sejdinovic', '- \"there are constraints per which state can transition\"', 'The paper lacks rigor and the writing is of low quality, both in its clarity and its grammar.', 'Optimization on lambda reduces burdensome hyperparameter selection, but a new hyperparameter beta is introduced.', '- What is the difference between the result of Theorem 4.3 and the result from (Lacoste-Julien 2016)?', 'For adversarial attacks, the author should compare with data type from [10], and list L-FBGS [11] as a basic baseline.', 'Thus all attacks make some kind of approximation, including this paper.', 'These transformations are called canonicalizations in the paper.', 'Instead, meaningful representations are learned through gathered triplet comparisons of these IDs, e.g. is instance A more similar to instance B or instance C? Similar existing techniques fall in the realm of learning ordinal embeddings, but this technique demonstrates speed-ups that allow it to scale to large real world datasets.', '[Comments]', 'Weaknesses:', '* The use of the name \"batch-norm\" for the layer wise normalization is both wrong and misleading.', '===========================================================================', 'Unfortunately the paper falls short in two main areas:', 'The core strength of this paper is in the results that are achieved on standard speech recognition benchmarks.', '- It would be better to compare with a naive approach that uses domain discrepancy between each source and target as (fixed) coefficient since this approach such as Mansour (2009), Ben-David(2007, 2010) and Kuroki et al (2019) which explicitly consider the hypothesis class has theoretical justification in the form of generalization error bound in the target domain.', '* \"data  tripets\" on page 2', 'see paragraph after equation 8 of the Deep BM paper: http://proceedings.mlr.press/v5/salakhutdinov09a/salakhutdinov09a.pdf', '* Section 5.3 (Fig. 6) is the part most relevant to the generalisation problem.', 'Post rebuttal update: I think the authors have addressed my main concern points and I am updating my score accordingly.', '* The problem is interesting and well explained', 'The fact that standard neural architectures are extended and adapted to the task, and the way domain knowledge is used to design the graph representation makes this interesting even to people outside the task-specific audience, and hence I strongly recommend acceptance.', 'The paper also misses relevant citations of similar questions from the field of (probabilistic) matrix factorization and relational learning.', 'Minor comments:', '1. The title of the paper is \"visual reasoning by progressive module networks.\" The title may be a little overstated since the major task is focused on visual question answering (VQA).', 'domain adaptation. In NeurIPS, 2007.', 'The author can perhaps consider the datasets used by Tsai et al. There are seven datasets, and they can all be modified to the setting of partially-observable multimodal data.', 'when comparing the proposed approach with another methods why not use more complex datasets (like those used in section 4.3)', 'The trick they use is adding “synthetic” missing features in addition to the real ones and only train on those.', 'Main remarks:', 'References.', 'The experiment results are very through and overall promising.', 'On page 4 the mapping network is described as a function that maps c-dimensional points to 3D points.', 'This paper proposes using spectral normalization (SN) as a regularization for adversarial training, which is based on [Miyato et. al., ICLR 2018], where the original paper used SN for GAN training.', '*Summary*', 'Such problems arises with many probabilistic models with noises or latent variables.', 'One could understand the use of \"selection network\" as a way to automatically select a threshold of what to consider confident, however, in this case, the prediction of \"selection network\" should be thresholded at 0.5 (correct prediction or not), but the experiments use complex thresholds.', 'How do you guarantee that the representation learned by the neural network still obeys the property of Choquet integral? What is your loss or your algorithm?', 'Weaknesses:', 'Overall, the paper is well organized and logically clear.', '--In the main section describing the approach (Section 4), the authors start with a claim that Equation 1 and 2 are equal; I don’t believe 1 and 2 are equal.', 'After pre-training, the authors use the learned representations for speech recognition.', 'Assume GO gradient needs to evaluate f N times, how does the performance compared with the REINFORCE gradient with N Monte-Carlo samples?', 'This paper proposed another variant of Langevin dynamics, called “Stein self-repulsive dynamics,” which simultaneously decreases the auto-correlation of Langevin dynamics and eliminates the need for running parallel chains in SVGD.', '3. The paper is easy to follow.', 'Major Comments:', 'However, the idea is very related to Yeh et al.’s work which has already published but not mentioned at all.', 'On the one hand, I enjoyed the angle the authors tackled proving that the network architecture is underparameterized enough to be a good model for denoising.', 'in practice. For most of newly proposed graph embedding algorithms, it is hard to convince', 'in Sect. 6.', \"If a layer l satisfies l R = R l  (for R a alt-az rotation), then it automatically satisfies l RR' = RR' l, which means l is equivariant to the whole group generated by the set of alt-az rotations.\", '- The tables will look a lot nicer if booktab is used in LaTeX', '- The authors divide dataset construction into crowdsourcing and synthetic.', 'In the experiments, both the setting and the experimental results show that the proposed W_s will be very close to W_U. As a result, the improvement caused by the proposed method is incremental compared with its variants.', 'It would be helpful to note in the description of Table 3 what is better (higher/lower).', 'I think it is necessary to make it clearer how s_{post_read} and attn_copy are computed with the updated {h^i_t} and what u^i is expected to encode.', \"What is c? Isn't it always 3, or how else is it possible to composite the mapping network?\", 'The text is well written in general, but the structure could be improved.', 'https://arxiv.org/abs/1711.06642', '- in section 4.3 how is the reconstruction built (Figure 3b)?', '3) The overall performance of the proposed SST in the experiments is not convincing and not promising.', 'They provide a structured and deterministic function G that maps a set of parameters C to an image X = G(C).', 'Paper summary - This paper extends the differentiable plasticity framework of Miconi et al. (2018) by dynamically modulating the plasticity learning rate.', 'The first one considers that a sample is part of the training set if it correctly classified.', '1. Typo: Third paragraph in section 1, \"...which is makes use of ...\".', 'As this modulation only depends on the noise vector, this technique does not require additional annotations.', 'For example, the original GNN, ChebNet, etc. that leads to GCN can be mentioned.', ', then also  \\\\nabla L(w)  is not perpendicular to w.', 'In other words, there is no setting of parameters of the original network that would make its forward pass equivalent to that of BN network (2) for all batches.', '- Other algorithms in the comparison achieve a better distortion (smaller perturbation).', 'This paper', 'I am relative positive for this work.', 'It reads very much like a STOC theory paper, and a lot of the key ML details that would be relevant to audience at this conference seem to have been shoved under the rug in a way.', 'Perhaps more important than the rate of convergence, is the guarantee that the method will not diverge (and will not get stuck in a non-local minimum).', 'Additionally, robustness to outliers is achieved by the minimum covariance determinant estimator for the LDA covariance matrix.', 'Assuming that a second is represented with 15-30 frames, this corresponds to 10-20 sec and 1-4 sec, respectively.', '* What does the decoder produce when taking a trained C on a given image and changing the source noise tensor?', 'Similarly \"offers multi-level feature extraction capabilities\" is almost meaningless since all DL methods can be said to do so.', 'I’m not sure I understand what we are supposed to learn from the astrophysics experiments.', 'The main idea is then to use an additional GNN to encode the context and to learn simultaneously the main GNN and the context GNN via negative sampling.', '- In Section 4.2, in the second paragraph, you refer to Appendix F and describe “sharp upward jump at collapse” in D’s loss.', '6. For ProPara task 2, what threshold did you choose to obtain the P/R/F1 score?', 'E.g. \"what job did jefferson have\" is semntically related to his role in the declaration of independence but rather different.', 'Strengths', 'Details:', 'Uniform noise is the least realistic assumption for label noise.', 'This is a strong point reagarding this work, as it seems that no such unification has been attempted in the past (although simply replacing the curvature in the Poincare model seems a bit too straightforward to not have been attempted, it seems to be the case).', '[3] Bayesian Optimization with Robust Bayesian Neural Networks', '2. Though seemingly very important to the architecture, the purpose of constructing the super-graph g^{sup} in the training of C^{CAT} seems to be unclear to me.', 'build upon each other to help form a clearer picture of memorization', \"--> isn't this the same as using the hinge loss to bound the zero-one loss?\", 'The authors also compare two aggregation rules for federated learning schemes, (Fung et al 2018 & Pillutla et al 2019), suggesting that both rules are bypassed by the proposed distributed backdoor attack.', 'It is more system engineering than science.', 'I think that the dataset generation process is well-thought-out.', '2.\\tData size is too small, and the baselines', 'The reason I ask is that my experience is that the hyperboloid is typically easier to work', 'The theory tells the same as in case 2 above but with an additional price of optimizing a different function.', 'Several models including LSTM, LSTM + Attention, Transformer are evaluated on the proposed dataset.', 'In later sections they use theta and theta’ for encoder/decoder resp.', '= Recommendation', 'I read the rebuttal. the authors clarified and answered the questions. I would like to raise the score.', '4. Authors have done a good comparison in the context of deep nets.', 'This is particularly important when the application is to test for independence, as in the fMRI experiments.', 'The key motivation is to make the pooling operation shift-equivalent and anti-aliasing.', 'Thanks in advance. I will re-adjust the review rating following your reply.', '- In active learning, the proposed method should be compared with other methods such as Bayesian DNN using dropout, etc.', '*', 'Typos, grammar and style:', 'I would like the method to be applied on other domains such as continuous non-convex optimization and reinforcement learning.', 'The paper investigates the Frank-Wolfe (FW) algorithm for constructing adversarial examples both in a white-box and black-box setting.', 'Specifically, the author mentioned Tsai et al. assumed factorized latent variables from the multimodal data, while Tsai et al. actually assumed the generation of multimodal data consists of disentangled modality-specific and multimodal factors.', '4. Some relevant related work is discussed and this seems like a novel', 'I am a making a recommendation for reject for this paper with the main reason being that I believe the primary derivations for their method appear flawed.', '- The paper does not have a significant novel contribution, but rather extends GANs (improved-GAN mostly) with a manifold regularization, which has been explored in many other works Kumar et al. (2017) and Qi et al. (2018).', 'This is in contrast to what is observed for AlexNet in Tables 2 and 3, where the conv5 accuracy is lower than the conv4.', 'Experimental results of different GNN architectures w/o different PT for different tasks are provided.', 'On the other hand, the proposed algorithm requires 100x more forward passes when computing the uncertainty (which may be significant, unclear without runtime experiments).', 'It would be helpful to have a brief paragraph describing this architecture, for readers not familiar with the referenced paper.', 'As a concrete experiment to determine the importance, what would be the accuracy and computational comparison of ensembling 4+ models without MC-dropout vs. 3 ensembled models with MC-dropout?', '- First line page 4 you mention AF, without introducing the acronym ever before.', 'Both dynamic modulation variations strikingly outperform non-plastic and plastic non-modulated recurrent networks on a cue-reward association task with high-dimensional cues.', '--------------', '- action detection or localization (e.g., in benchmarks JHMDB or UCF101-24).', 'They stated that another approximation of the manifold gradient, i.e. adding Gaussian noise \\\\delta to z directly (||f(z) - f(g(z+\\\\delta))||_F) has some drawbacks when the magnitude of noise is too large or too small.', 'A good prior of one optimizer could significantly affect the HPO cost or increase the tunability, i.e., the better understanding the optimizer, the less tuning cost.', '[-] It seems to me the motivation is similar to that of Sparsely-Gated MoE (Shazeer et al. 2017), but it is not clear how the proposed two-hierarchy method is superior to the Sparsely-Gated MoE. It would be helpful the paper discuss more about this.', 'e.g. network architecture, hyper-parameters (e.g. I_tran^max), and how they were searched.', '[11] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow, and R. Fergus. Intriguing properties of neural networks. In ICLR, 2013.', 'The paper presents a video classification framework that employs 4D convolution to capture longer term temporal structure than the popular 3D convolution schemes.', '* Related Work', 'The samples from MNIST in Figure 3 are indeed very blurry, supporting this.', 'In addition, in their response they highlight the non-increasing nature of function H over the course of the algorithm which is important for their argument.', 'One more unclear but important point: is Table 3 obtained by white-box attacks on the Resnet/Denset but oblivious of the MCD? Is so, I don’t think such an experiment tells the whole story: as the the MCD would arguably also be deployed for classification, the attacker would also target it.', 'In sum, the paper has a very good application but not good enough as a research paper.', 'parameters in the network.', 'Experiments on a large set of continuous control tasks show that Dreamer outperforms existing model-based and model-free methods.', 'The paper presents algorithms for solving computational problems in a datastream model augmented with an oracle learned from data.', \"But it's not clear to me why testing more sophisticated models that are tailored for math questions would *not* be useful.\", 'Online tuning of  hyperparameters is an important functionality and I hope your paper will make it more straightforward to use it in practice.', 'That does not seem like a small enough percentage to claim that these are “unseen” images.', '- the proposed approach seems to be effective according to the experiments', 'Also, the title of the paper is about problem retrieval but the experiments are about similarity comparison, there seems a gap.', 'The paper examines an architectural feature in GAN generators -- self-modulation -- and presents empirical evidence supporting the claim that it helps improve modeling performance.', \"- It seems that the MI discriminator learns to estimate the 'proximity' between the robot and the object.\", 'The proposed method decreases the auto-correlation of Langevin dynamics, so the proposed method increases the sample efficiency.', 'The only set of experiments are comparisons on first 500 MNIST test images.', '- in the abstract you say that \"NGE is the first algorithm that can automatically discover complex robotic graph structures\".', 'Also, an optimization algorithm is presented that solves the proposed constrained optimization problem.', '-This paper is well-motivated.', 'A general overview of related work in these directions are needed.', '#2) After Eq. 6 the \"nonnegative\" should be \"nonzero\".', 'Other domain transfer settings such as synthetic rendered vs. real (e.g. visDA challenge) could have been considered.', 'For tutoring applications, the most important thing is to select a problem that can help students improve; even if you can indeed select a problem that is the most similar to another problem, is it the best one to show a student? There are no evaluations on real students in the paper.', 'Also, how do you select the number of factors of each type?', 'I am reluctant to give a higher score due to its incremental contribution.', 'Also, it took me a long time to figure out that ‘i’ is used to index each entity (it is mentioned later).', 'Cons/comments:', 'The paper proposes a recurrent knowledge graph (bipartite graph between entities and location nodes) construction & updating mechanism for entity state tracking datasets such as (two) ProPara tasks and Recipes.', 'Minor Points', 'Let me try to make it clear with the following example.', 'Clarity: The simple approach is clearly described.', 'It does not seem sensible to drop X^{j}_{t-1} + \\\\eta_X \\\\cdot \\\\epsilon_X and attain a smaller value of the cost function at the same time.', 'I think that the other reviewers make a number of valid points, especially with regards to the theoretical analysis of the paper.', 'However, it isn’t entirely clear if the primary contribution is showing that ‘curiosity reward’ is a potentially promising approach or if game environments aren’t particularly good testbeds for practical RL algorithms — given the lack of significant results on more realistic domains, my intuition leans toward the later (the ant robot is interesting, but one can come up with ‘simulator artifact’ based explanations).', 'Specifically, what if the few-shot learning component is removed, and the network is trained with standard domain adaptation.', '2. The learned actor-critic model replaced online planning, where actions can be evaluated in a more efficient manner.', 'arXiv.', 'Nature, 1996.', '- The proposed method does not outperform the existing state-of-the-art methods in terms of classification accuracy.', 'Finally, the authors show that adding dynamic plasticity to a small LSTM without dropout improves performance on Penn Treebank.', 'This is done by using a Lagrangian relaxation that learns the parameters of a control policy that satisfies the desired constraints (and also learns the Lagrange multipliers).', 'Lemma 2.4, Point 1: The proof is confusing.', 'Such a perspective is interesting, but needs to be further developed and explained.', 'It is inspired by implicit shape models, like presented in Park et al. and Mescheder et al., that given a latent code project 3D positions to signed distance, or occupancy values, respectively.', 'Specifically, the bottleneck is created by having the data depend on a noisy version of the parameters, rather than the true parameters and invoking the information processing inequality.', 'Consider the one variable vector case.', 'This paper proposes an anisotropic spherical convolution that produces 2D spherical feature maps.', 'This is an important research area and relevant for ICLR.', 'Summary:', '\"the necessary and sufficient condition of dependency, is more general and is complementary to other techniques that make stronger assumptions about the data. \"', 'In particular, the method tries to find the latent point for which the GAN generates the image, which if gets degraded will match the given degraded image.', 'The goal is typically to recover the dictionary A, from which one can also recover the x under suitable conditions on A. The paper shows that there is an alternating optimization-based algorithm for this problem that under standard assumptions provably converges exactly to the true dictionary and the true coefficients x (up to some negligible bias).', '(4) Are \\\\theta and \\\\phi jointly and simultaneously optimized at Eq 12?', 'Thanks for the results, but I have several questions.', 'From the supplementary, it seems Epsilon means the environment?', 'I read the paper and understand it, for the most part.', 'this limitation in latenby?', '1. Cakewalk is *very* closely related to the cross-entropy method.', 'In the attentional LSTM, the use of “parse LSTM” is also not a standard approach in seq2seq models and doesn’t seem to work well in the experiment (similar result to “Simple LSTM\").', '- The Mixture of Gaussians experiment is a good illustration of how the choice of cost functions influences the solution.', 'Optimizing compression rates should be done on the training set with a separate development set.', '2. Compared to AA-pooling, it seems that F-pooling has a better theoretical guarantee (i.e. the optimal anti-aliasing down sampling operation given U).', '[3] Gidaris, Spyros, Praveer Singh, and Nikos Komodakis. \"Unsupervised representation learning by predicting image rotations.\" arXiv preprint arXiv:1803.07728 (2018).', 'Dealing with highly stochastic environments seems a potential fatal flaw of the assumptions of this method.', 'This is the improvement with the weakest possible baseline, i.e. no method to defend for noise!', '1) Besides an comparison to the work by Lakshminarayanan et. al, I would also like to have seen a comparison to other existing Bayesian neural network approaches such as stochastic gradient Markov-Chain Monte-Carlo methods.', 'Question: In your experiments, you set the learning rate to be really low. What happens if you set it to be arbitrarily high? Can you algorithm recover good learning rates?', 'Sec 7.1', '[2] MnasNet: Platform-Aware Neural Architecture Search for Mobile. https://arxiv.org/pdf/1807.11626.pdf', 'This paper is well set-up to target the interesting problem of degraded generalisation after adversarial training.', '- Small typo in the “Layout induction” paragraph, line 6 on Page 7:', 'One minor comment: for images in \"Case III\", the authors considered them \"contribute little to performance comparison between the two classifiers\" and therefore did not source labels for them.', 'The authors further proposed three weighting schemes to emphasize the tunability of different stage of HPO.', 'Second, it evaluates whether we can detect that a group of samples of a dataset was used to train a model.', 'This seems clunky, I would have been more impressed with a prior structure that ties in closer with the embeddings and requires less hand-engineering.', 'Does the model manage to capture that some shapes have holes, or consists of a closed 2D surface (ball) vs an open surface (disk),', 'Were samples from those networks better without using truncation? Why would this be?', 'Each chemical compound is labeled', 'I am not sure how this will generalize to a larger number of problem spanning many different domains.', '- The proposed approach is a fairly specific form of self-modulation.', '- Pg. 6: ...thedse value of 90...\"', 'pre-trained networks and determining whether by analyzing', 'You mention that negative mining should improve over this strategy. What does negative mining correspond to in this context? Are there bad rewrites better than others?', 'Time dependence', 'At the same time, this work can be further enhanced at the following aspects:', 'First this paper proposes a new theory for this domain that extends generalized discrepancy theory to multi-source setting.', 'The evaluation of the proposed approach on the four used datasets appears to be reasonable and well done.', 'The idea to extend the use of VAE-GANs to the video prediction setting is a pretty natural one and not especially novel.', 'However, what the authors implicitly did was to perform variational inference for maximising their likelihood by introducing a variational distribution q(\\\\theta) = p(\\\\theta | g(x_n; \\\\psi).', ',', '- in the introduction you mention that automatic robot design had limited success.', 'Also, to demonstrate the superiority of the proposed method an appropriate comparison against previous work is needed.', 'Strengths: Overall I think the paper is well-written, and proposes simple adaptions to the DNC architecture which are theoretically grounded and could be effective for improving general performance.', 'I’m glad to see that the authors considered adding momentum (to adapt ADAM to this setting), and their experiments show a convincing benefit in terms of performance *per iteration*. Interestingly, they also show that the models found by their method also don’t generalize poorly, which is noteworthy and slightly surprising.', 'It asks an interesting question: can we learn a model of the training dynamics to avoid actually having to do the training?', 'On the experimental side, the contribution is to test the scenario where datapoints from irrelevant classes are included in the unlabeled dataset.', '2) If \\\\eta_2 is non-zero, then x^1_{t-2} will be useful for the purpose of predicting x^3_{t}. (Note that if \\\\eta_2 is zero, then x^1_{t-2} is not useful for predicting x^3_{t).) From the d-separation perspective, this is because x^1_{t-2} and x^3_{t} are not d-separated by x^2_{t-1} + \\\\eta_2 \\\\cdot \\\\epsilon_2, although they are d-separated by x^2_{t-1}. Then the causal Markov condition tells use that x^1_{t-2} and x^3_{t} are not independent conditional on x^2_{t-1} + \\\\eta_2 \\\\cdot \\\\epsilon_2, which means that x^1_{t-2} is useful for predicting x^3_{t}.', '2.', '- The randomized weight is not very practical. Though it may be the standard approach of mean field,', 'All in all, this paper was well structured and extensively detailed wrt how it engineered this solution (and why).', \"I appreciate the authors' great effort to address my concerns! I think the evaluation in the current version of the paper is pretty comprehensive and provides a valuable study, and I am happy to raise my score accordingly.\", 'More papers should present this, and those that do should do it more systematically.', 'I am concerned though why the authors didn’t compare to adaptive optimizers such as ADAM and ADAGRAD and how the performance compares with population based training techniques.', 'The authors also found that random features embedding, somewhat surprisingly, performs well in the tasks.', 'The authors propose to meta-learn, using MAML, the mean of an elementwise, input-dependent, multiplicative noise to improve generalization in few-shot learning.', 'This paper proposes two new architectures for processing set-structured data: An RNN with an accumulator on its output, and an RNN with gating followed by an accumulator on its output.', 'I feel the baseline in domain adaptation area is a bit limited.', 'Even though the authors answer positively to each of their four questions in the experiments section', '- It is claimed that Theorem 3.4 gives the first linear convergence rate for minmax that does not require strong-convex or linearity.', 'The primary idea behind the proof techniques is to show that the objective (Hamiltonian) satisfies the PL condition.', '.', '-', '3. can you provide the training memory, inference speed, and total training time?', '- \"Different notations of convolutions\" -> notions', 'The paper is well written, ideas are well motivated/justified and results are very compelling.', 'To my knowledge, this part is novel and interesting.', 'Thus I believe that it would be better if you consider the same hyper parameter on all of these methods, e.g. the precision of the Gaussian prior.', 'If Fig. 4 is following Fig. 3 in considering $p(c(\\\\gamma(T), \\\\pi(P)))$, then Fig. 4 should plot the performance of, e.g., $p(c(e\\'(c\\'(\\\\gamma\\'(T_{i-1}), \\\\pi\\'(P_{i-1}))), \\\\pi(P_i)))$ (i.e., $p$ applied to approximate embedding of $T_i$ and (\"true\") embedding of $P_i$).', 'Questions/Clarity:', 'Strengths:', '2.1 Baselines: For noisy labels, the authors should add MentorNet [1] as a baseline https://github.com/google/mentornet From my own experience, this baseline is very strong.', 'I found this to be an interesting approach.', '+ The evaluation results show that the algorithm is efficient.', 'The basic idea is to combine the graph attention networks (Veličković et', 'References would be in order. Similarly, hexagonal convolution has a history in DL and outside.', 'Similar to [1] one can use MCMC with importance sampling on auxiliary variables to infer the hidden diffusion given the observed cascades in continuous-time independent cascade model.', 'This paper develops a framework for evaluating the ability of neural models on answering free-form mathematical problems.', 'Extensive experiments with varying losses, architectures, hyperparameter settings are conducted to show self-modulation improves baseline GAN performance.', 'This is an interesting paper with a new approach to learn a sparse, positive (and hence interpretable) semantic space that maximizes human similarity judgements, by training to specifically maximize the prediction of human similarity judgements.', 'In active learning, there is generally a trade-off between data efficiency and computational cost.', 'However, I don’t know how effective this is in practice.', \"A concern I have is that the manuscript as it stands is positioned somewhere between two distinct fields (sparse learning/feature selection, and causal inference for counterfactual estimation/decision making), but doesn't entirely illustrate its relationship to either.\", 'Why these methods are not considered in the beginning? In my opinion, transformer is good for modeling long term dependency and concurrent predictions which is not necessarily the case for learning curves.', 'It seems to me that the tasks are organized as unbalanced instances of each classes, and asking the common NNs to learn this  task. Of course, this common NNs can not address it.', 'In addition, the paper further proposed negative pre-training for training with imbalanced data sets to decrease false negatives and positives.', '- maze navigation shows incremental benefits over non-modulated plasticity', 'This paper builds upon the assumption that GANs successfully approximate the data manifold, and uses this assumption to regularize semi-supervised learning process.', 'Following the suggested rubric:', 'The paper gives an example of Gaussian mean field inference.', 'This is an application-driven paper with nice practical results.', 'Hence, the effectiveness and advantage of the proposed methods are not clear.', '- Footnote 2 on page 5 it difficult to read.', '- please define P and G, the elements of the divergence D(P||G) that appears in the first paragraph of section 3.', 'They also explain typicality judgements and cluster semantic', \"- it's better to show time v.s. testing accuracy as well.\", 'The FW algorithm is a classical method in optimization, but (to the best of my knowledge) has not yet been evaluated yet for constructing adversarial examples.', '1. Interesting technique to take advantage of neural networks to efficiently learn ordinal embeddings from a set of relationships without a low-level feature representation, but I believe the experiments could be improved.', 'However, I think since this is few-shot learning with domain adaptation, there is no domain adaptation baselines being mentioned in comparison.', 'and for $p_0(tree) = 0.1$ and when we use the Find module', 'The proposed model-based method is compared against a human and a model-free baseline training a Wide ResNet on CIFAR-10.', 'This paper leverages graph structure (e.g., context neighbors) and supervised labels/attributes (e.g., node attributes, graph labels) for PT.', '= Strong/Weak Points', 'Overall, I think this paper is below the borderline of acceptance due to insufficient comparison with Sparsely-Gated MoE.', 'However, this does not take into account the time already spent on pre-training. Perhaps the authors can include some results as to the total time taken as well as amortized total time over a number of different downstream tasks.', \"Third, it is rather surprising that the authors didn't mention anything about the traditional causal discovery methods based on conditional independence relations in the data, known as constraint-based methods, such as the PC algorithm (Spirtes et al., 1993), IC algorithm (Pearl, 2000), and FCI (Spirtes et al., 1993).\", 'They evaluate their approaches on two domains, biology and chemistry on a number of downstream tasks.', 'This should be made crystal-clear in the paper.', 'More experiments on the diversity would help make the paper more convincing.', '# Weaknesses', 'Update: After reading the other reviews and the responses by the authors, I lowered my score from 6 to 3.', '* Page 2: “This algorithm needs complete data during training cannot learn from partially-observed data only.”', 'Authors use the RBM 8x8 feature representation as a fixed convolutional layer and train a CNN on top of it.', 'and within subjects was a nice addition.', 'If this solution can be proven to improve a valuable metric (e.g. accuracy, interpretability, theoretical understanding, or computational efficiency) of a setup, it is then worthwhile being published.', 'The same initialization can be achieved by performing one training pass with BN with 0 learning rate and then removing it, see e.g. Gitman, I. and Ginsburg, B. (2017).', 'While the models are evaluated on all possible object pairs, they are trained on a smaller subset.', '1. While I understand that vector quantization makes the use of NLP-style BERT-training possible (as the inputs to NLP models are discrete tokens),  there are potential disadvantages as well.', 'In particular, evaluation for the classification task should be compatible with the proposed model, which would give a much better picture of the learned representations.', 'Pros:', '1) as an experimental study, it would be valuable to compare the performance of curiosity-based learning versus learning based on well-defined extrinsic rewards.', 'The paper proposes a novel training method for variational autoencoders that allows using partially-observed data with multiple modalities.', 'Detailed Comments', 'Hence the theoretical sample complexities contributed are not comparable to those of MIME.', 'These first two elements result in an Inception score (IS) boost from 52 to 93.', 'On the positive side, the paper is well written and structured.', '# Other comments', \"I believe this would be a much more appropriate baseline, and I'd be curious to hear the intuition for why I(s_c ; s_i) should be superior.\", '- Section 3.3: What exactly does \"mode collapse\" refer to in this context? Would this be using only one codebook entry, for instance?', 'activity. It generates novel image sequences of that person, according', 'PS: I am downgrading my confidence in my evaluation.', 'The final prediction uses all the states and question to infer the final answers.', 'The only new result seem to be Theorem 4.7 which is a natural extension to theorem 4.3 to zeroth-order methods.', 'Minor Comments:', 'I also remain unconvinced by the response to my issue with the claim “Our experiments show that our networks can remember a large number of images and distinguish them from unseen images”, where the negative images are also seen by the memorization model, so they are not unseen.', 'The training method for imbalanced datasets is impressive.', 'If you can also compare against one or two algorithms of your choice from the recent literature it would also give more value to the comparison.', 'Minor:', 'The paper assumes that the latent canonicalizers are predefined for each task and that it is possible to obtain the ground-truth image of different latent canonicalizations.', '- augmenting the width of the networks by 50%', 'The paper builds upon Deep Image Prior (DIP) - work which shows that one can optimize a neural generator to fit a single image without learning on any dataset, and the output of the generator (which approximates the image) can be used for denoising / super resolution / etc.', '==========', 'Also, the ensemble of MC-Dropout models does not have the theoretic support from the Bayesian perspective.', '[2] Gao Huang and Yixuan Li and Geoff Pleiss and Zhuang Liu and John E. Hopcroft and Kilian Q. Weinberger', 'Rather than \"skill discovery\", I suggest the authors position MISC relative to earlier work on empowerment, wherein a single policy was used to maximize mutual information of the form I(a; s_t | s_{t-1}).', 'Decision: despite the seeming unfair comparison, this task is novel. I vote for weak accept.', 'Slight improvements can also be seen on a simple maze navigation task as well as on a basic language modeling dataset.', 'The following is the concern:', 'The reviewer votes for rejection as the method has limited novelty.', '- On page 2 it says “We approach this limitation in latenby”, which I assume is a typo?', '-', 'Any response would be highly appreciated.', 'analysis and experiments to perhaps explore the issue of the', 'All the models show very high training accuracy, even if they do not show systematic generalization.', \"[2] Smith et al, Don't Decay the Learning Rate, Increase the Batch Size, https://arxiv.org/abs/1711.00489\", '- It is extremely hard to follow what exactly is going on; I believe a few illustrative examples would help make the paper much clearer; in fact the idea is not that abstract.', 'The authors consider the setting of a RL agent that exclusively receives intrinsic reward during training that is intended to model curiosity; technically, ‘curiosity’ is quantified by the ability of the agent to predict its own forward dynamics [Pathak, et al., ICML17].', 'The introduction provides useful insights, motivates the work convincingly, and provides interesting connections to past work.', '--In the second line of Equation 5, the KL term appears to be measuring a distance between distributions on two different variables; z|c and c|z.', 'There are tons of algorithms for image inpainting, denoising, and super-resolution, but the proposed method was not compared with them.', 'For instance, Lakshminarayanan et al.[1] showed that Dropout MC can produce overconfident wrong prediction and, by simply averaging prediction over multiple models, one achieves better performance and confidence scores.', 'This paper proposes a new method to compare existing classifiers, which does not use fixed test set and adaptively sample it from an arbitrarily large corpus of unlabeled images.', '* Contributions (1) and (2) should be merged together.', 'This is because, for dependent data, the effective sample size is reduced, and the tests must be made more conservative to correct for this effect.', 'I request the authors to address the comments raised above.', 'The presentation is very easy to follow.', 'This results in the class collapsing problem observed by the authors.', 'Update: thank you for your rebuttal.', 'The bound obtained in Theorem 2 comes rather easily from the bounded assumption on the non-linearity and is this not very interesting.', 'Authors report how wider networks perform best, and how deeper networks degrade performance.', 'Summary:', 'CDNs can then be straightforwardly optimised with SGD for a particular task by using the reparametrization trick.', 'As a final point; the hyper parameters that were tuned for the MNF, noisy K-FAC and KFLA baselines are not on common ground.', '5. On relational detection task, is there a way to compare with the STOA method on some specific data split? This will leads to much more convincing results.', '- C_R in Eq(5) is not introduced.', 'The authors show that under this model, there exist algorithms that have significantly better time and space complexity than the current best known algorithms that do not use an oracle.', 'If the novelty is in applying to continual learning and new datasets, it is not clear that this is sufficient.', 'Deterministic models and certain loss functions such as Mean Squared Error (MSE) will produce', 'The paper is comprehensive and includes mathematical details.', '#', '-\\tProbably no need to mention that, but results are quite impressive', 'For example, the answer being a special \"this is impossible\" character for \"factorise x^2 - 5\" (if your training set does not use \\\\sqrt, of course).', \"Then, how about the opposite situation? What if the task requires that the robot should 'avoid' the object of interest? Does MISC still work? Is it helpful for the improvement of sample efficiency?\", \"Objectively saying that the author's method is better than CycleGAN is difficult. How does their ensemble method compare to just their single-agent dual method? Is there a noticeable difference there?\", 'The authors cite such models in the appendix (Melor et al), but claim that “much larger models” are needed, potentially with other mechanisms, such as dropout.', '3. On the object counting task, the query transmitter needs to produce a query for a relationship module.', 'Do the same model is trained for multiple tasks? Is each of the tasks trained sequentially or simultaneously?', '* similarly to the above, if the configurations are always sampled from the same 100, confidence intervals in the graphs become less reliable as the budget increases.', 'It has two networks, Pose2Pose, and Pose2Frame.', 'It is my impression that, if the authors elaborate on the experimental protocol and implementation details, this paper would be a good fit for the venue.', 'Besides, the current paper only verifies on vision datasets.', 'This paper formulates a new inference method called DDGC for noise labels and adversarial attacks.', '- I would also encourage the authors to come up with a more descriptive name for the approach.', 'It is a good idea to conduct an ensemble based on the confidence scores of trained models in iterations, although the authors did not mention any theoretical explanation or guarantee behind this.', 'The approaches are demonstrated on synthetic data as well as fMRI data, to detect significant inter-subject dependencies in time-series of neural responses.', 'Some clarity issues.', 'Have authors ever tried Sinkhorn iteration?', 'This paper investigates the effect of the batch normalization in DNN learning.', 'This paper is very well written, although very dense and not easy to follows, as many methods are referenced and assume that the reviewer is highly familiar with the related works.', '- The paper is clearly written and easy to follow. It gives some intuitive explanations of why their method works.', 'Significance', 'Also, a \"1x\" label seems to be missing in for the full softmax, so that the reference is clearly specified.', 'While the comparison with HSIC is a helpful one, it is also required to compare with the competing method closest to yours, i.e.', 'Furthermore, the cue-reward experiment seems to be a well designed show case for neuro-modulation.', 'Strengths:', '- important features for the new task should be in similar locations as important features of the old task (for example, one would expect that the proposed approach would negatively affect learning the new task if the important features of the old task were all located in the bottom of the image, while all important features for the new task are in the top)', 'I wonder how feasible it is to find a proxy metric that corresponds to the performance on downstream tasks which is expensive to compute.', \"The paper first adapts the Transformer model to be suitable to this prediction task by introducing a discretization scheme that prevents the transformer decoder's predictions from collapsing to a single curve.\", 'Besides visible artifacts, what does it mean? Why does a smoother G reduces those artifacts?', 'Second, the synthetic image pairs are not multimodal in nature.', 'The manuscript proposes a modification of generators in GANs which improves performance under two popular metrics for multiple architectures, loss, benchmarks, regularizers, and hyperparameter settings.', '- The search space of the proposed method, such as the number of operations in the convolution block, is limited.', 'In this paper, the authors show how to maintain and update this pxp matrix by storing only smaller matrices of size pxr and rxr, and performing 1. an SVD of a small matrix of size rxr', 'However, the derivations about \\\\phi are missing.', '- One useful thing to point out in B.3.3 is that in general, it need not be a diffeomorphism for all of M for any manifold, which leads to non-uniqueness.', 'Typos:', 'This paper is well written, and the method proposed in this paper is nice.', 'Several experiments', 'It would be good to mention this in the introduction or the conclusions.', '2. The distribution proposed in section 3.2 assumes independence between the elements $x_j$. This seems problematic for some relatively simple problems.', 'regularization technique applied is very significant in the', 'I have following comments/questions', 'In the cue-award association task the retroactive and simple modulation networks perform well, while the non-modulated and non-plastics fail.', 'The authors show that parameter noise exploration is a particular case of the proposed policy.', \"I don't have a good sense of whether this is a reasonable theoretical model to explore and a lot of very basic questions remain unanswered for me.\", 'I believe the primary claim of this paper is neither surprising nor novel.', 'They also introduce a bound on the average- and worst-domain risk in MDL, obtained using the H-divergence.', 'I would appreciate if the authors can survey and compare more baselines in their paper instead of listing some basic ones.', 'Using neural network if an interesting choice for capturing the influence probability and its timing.', '- The difference between experiments of Figure 5 and 6 should be made more clear.', 'In terms of applicability, it seems that many cases where discrete latent variables would be really interesting are not covered (e.g. sigmoid belief networks); the paper demonstrates experiments with discrete images (binary or 4-bit) not particularly motivated in my opinion.', 'The authors seems interchangeably using “runs” and “iterations”, which makes the concept more confusable.', 'Experiments present results on toy data, MNIST/not MNIST as well as on adversarial perturbations of MNIST and CIFAR 10 datasets.', 'This casts doubt on the usefulness of these characteristics in explaining the performance of the network.', 'Is there any theoretical motivation or guarantee for this assumption as well?', '2. Missed citation: MnasNet [2] also incorporates the cost of architectures in their search process.', '- How general is the proposed approach? How likely is it to generalize to other approaches such as Jigsaw (Doersch et al., 2015) and Exemplar (Dosovitskiy et al., 2016)? It would be good to comment on this.', 'The authors position their work well within the existing approaches in the community and generalizes the efficient use of measuring hidden activation wrt to specific input or set of inputs.', 'This paper proposes a combination of existing techniques, not just ensembling neural networks and not just doing MC dropout, but doing both.', 'It is shown experimentally that one can make such a decision with moderate accuracy.', 'such as VGG and ResNet.', 'This paper develops a mean field theory for batch normalization (BN) in fully-connected networks with randomly initialized weights.', 'Also, Huand et al. [2] showed that by taking different snapshots of the same network at different timesteps performance improves.', 'The paper deals with the problem of recovering an exact solution for both the dictionary and the activation coefficients.', 'In what concerns the optimization, the method achieves a better objective value much faster, confirming that it is a lower variance gradient estimator.', 'The main problem is that directly predicting the context is intractable because of combinatorial explosion.', '#5) Before Eq. 8 the matrix V is a function of z and should be presented as such in the equations.', '- in section 4.4', 'The descent lemma used by the author is not valid for the stochastic result.', 'As more body parts are added, more such network modules can be added.', 'Scalable Bayesian Optimization Using Deep Neural Networks', 'For example, it is curious to see how denoising Auto encoders would perform.', 'couple of caveats.', '3. The graph neural networks used in the model are not described in the paper, only a reference to Paliwal et al (2019) is given.', 'It is very valuable that the experimental results include many recently proposed methods.', 'The paper is a reasonable dataset/analysis paper.', '+ Well-written', 'This paper makes an observation that most of the neural network architectures do not learn the mutual exclusivity (ME) bias: if an object has one label, then it does not need another.', 'This will help to make this paper more self-contained.', 'The paper tackles the problem of semi-supervised classification using GAN-based models.', 'Since the paper manages to use very few parameters (BTW, how many parameters in total do you have? Can you please add this number to the text?), it would be cool to see if second order methods like LBFGS can be applied here.', '- The conclusions of the paper regarding the generalization ability of neural modular networks is timely given the widespread interest in these class of algorithms.', 'Therefore, the technical contribution of this paper is moderate.', 'The authors propose that the encoder directly predict the weights and biases of a decoder network that, since it is specific to the particular object being reconstructed, can be much smaller and thus much cheaper to compute.', 'However, which independence assumptions to use is not in scope for our paper,', 'Comparing to the other work, what are strengths of this work? In addition, have the authors compared the performances of their work and [Z Hu, arXiv:1905.13728] using the same data?', \"The function composition doesn't capture that.\", 'The paper is mainly empirical, although the authors compute two diagnostic statistics to show the effect of the self-modulation method.', '-- Based on Table 1, why did not evaluate the proposed model with beam-search?', 'Is there any way to help the models find the \"right\" (or better) solutions - e.g. adding regularization, or changing the model size?', 'The new model has been tested on the WebQuestionsSP and the WikiSQL datasets, with both automatic and human evaluation, compared to the baselines with copy and coverage mechanisms.', \"This reduces the paper to an empirical exercise rather than a true understanding of their method's advantages and limitations.\", 'However, instead of a latent vector, the proposed method directly outputs the network parameters of a second (mapping) network that displaces 3D points from a given canonical object, i.e., a unit sphere.', 'I think it is not extremely surprising that using the proposed strategy allows to learn low level features as captured by the first few layers, but I think it is worth studying and quantifying.', 'The method seems technically sound and achieves good results.', 'Though this may be true, these models still undermine the claim that “neuromodulated plastic LSTMs...outperform standard LSTMs on a benchmark language modeling task”.', 'There are some interesting ties to biological work -- in particular, to retroactive plasticity phenomena.', 'This is all the more essential when the estimate has required optimisation over parameters.', '7. Typo: Page 5 last paragraph, \"... negative instances for for each ...\".', '- \"to speed up and trade off between evaluating fitness and evolving new species\" Unclear sentence. speed up what', '- In remark 4.8 in the end option I and II are inverted by mistake', 'This paper presents a method for single image 3D reconstruction.', '===', '3) The paper only conducts comparison experiments with fixed-alpha baselines.', 'The premise of the paper is very interesting and the overall problem is definitely of high interest and high potential impact.', 'Thus while I don’t think their algorithm is super novel, it is significant and thus novel enough.', 'Experiments are performaed on 2 simple toy datasets and a simple language modeling task.', 'The authors also discuss how to adapt the proposed algorithm with Adam style updates that incorporate momentum.', 'Including the comparison with safe reinforcement learning algorithms is more convincing.', 'Different terms should be use, even if the context makes it possible to infer which one is being referred to.', 'Although the setting (informative temporal data) is relatively restricted with respect to the general problem of causal inference, this is not unreasonable given the proposed direction of application to automated reasoning in machine learning.', 'For example, in the associative recall and key-value retrieval only DNC and DNC + masking are studied.', 'Note that this step does not involve \\\\lambda.', 'The paper is well-written and the experimental setup seems to be carefully carried out.', '1. to use this WE from the current task to generate a silence map (by smoothing the WE) for the next task.', '3) Uses several visualizations to show experimental results.', 'In section 3 is', 'The idea of the proposed method is natural, which is incorporating the functionality of SVGD to reduce the auto-correlation of Langevin dynamics.', '- Even though commonly used evaluations metrics for GANs are still not fully adequate, the authors obtain quantitative performance significantly beyond previous work, which seems indeed correlated with remarkable visual results.', '-', 'The authors show that, combined with a proper initialization, this has exact recovery guaranties.', 'Overall, the authors did a comprehensive study on large-batch training with the support of extensive experiments.', '1. Missing comparison with parameter counting bounds: there has been a long line of research on generalization bounds for RNNs by obtaining bounds on the VC dimension of the function class [1, 2] which provide generalization bounds for various non-linearities.', 'Currently I lean towards accepting this paper for publication, despite a few issues.', '== Experiments', 'What is G_t in Theorem 2.5. It should be defined in the theorem itself.', '- In the outputs shown in Table 3, the questions generated by the scratchpad encoder often seem to be too general compared to the gold standard, or incorrect.', 'As noted above, some NLP-like ideas (such as prediction of future speech segments, stemming from text-based language modelling) have already been considered within the speech community.', 'The conductance is the flow of attribution via the hidden unit(s) in consideration.', 'Summary', 'How do we take a limit of M -> ∞ ? Does k also go ∞?', 'The core contribution of the paper is to estimate the network parameters conditioned on the input (i.e., the RGB image).', '6, the experimental design of Sec. 4.2 is also a bit unfair.', 'It looks like section 2.1 wants to show the connections between eq. (2) and other popularly used inference methods.', 'The paper is written clearly, and the experiments seem solid.', 'It doesn’t seem like a surprising discovery that maximizing the mutual information between the robot state and object state will lead to skills that actually make the robot move the object.', 'That is, authors assumed that they have a degradation function F and all the inference process is just based on this known function.', 'This work thus shows a modern GAN architecture for complex datasets that could be a strong basis for future work.', 'There are experimental results to demonstrate the efficiency of the algorithms.', \"However, I don't understand the use of $\\\\alpha$ here.\", 'The metric is defined as a weighted average of the performance after tuning with i random trials, with i that goes from 1 to K.', 'The contributions are i) a publicly available dataset, and ii) an evaluation of two existing model families, recurrent networks and the Transformer.', 'The mask term in Eq. (7) seems to work well for the foreground (body+object) and the shallow regions.', 'One issue is perhaps, very little in terms of related work.', 'Therefore, it is a little misleading to still call it Bayesian active learning.', '- Architecture choice unclear: Why are $\\\\sigma$ and $\\\\omega$ separate networks.', 'The result is particularly interesting because the same question was asked about exchangeable matrices (instead of *jointly* exchangeable matrices) by Hartford et al. [2018] which lead to a model with 4 bases instead of the 15 bases in this model, so the additional assumption of joint exchangeability (i.e. that any permutations applied to rows of a matrix must also be applied to columns - or equivalently, the indices of the rows and columns of a matrix refer to the same items / nodes) gives far more flexibility but without losing anything with respect to the Hartford et al result (because it can be recovered using a bipartite graph construction - described below).', 'The fact to prove linear convergence rate without strong-convex-concavity is quite surprising. And this paper brings nice tools to analyse HGD.', 'This could significantly increase the tunability of SGDM.', 'The parameter of the convolution are fitted to each target image, where the source noise tensor is fixed.', 'Therefore the novel contribution of this paper over Lee 2018 is not clearly outlined.', 'However, the selected experiments on the cubic regression toy data (Section 5.1) and the out-of-distribution classification (Section 5.2) are clear examples of system’s noise, i.e. epistemic uncertainty.', 'The unlabeled set is not \"unlabeled\" in essence. If my understanding was correct, it cannot contain open-set images which do not belong to any of the classes of interest.', 'The idea is nice.', 'Another concern is that the evaluation of domain adaptation does not have much varieties.', \"Even on synthetic data, when do or don't we see generalization (systematic or otherwise) from NMNs/MAC/FiLM?\", 'Typos', 'My major concern is the hyperparameter distributions for each optimizer highly requires prior knowledge.', 'The observation is that in many real-world applications, we want to exploit multiple source datasets of similar tasks to learn a model for a different but related target datasets.', \"In don't see why there is a need to work on any $y$. If it is true\", 'It did still leave me wanting more with respect to the practical significance though.', '- thorough experimentation', 'Since no closed-form expression for the optimal v can be derived, the authors propose to use binary search to find it.', 'Each one of these topics is worthy of', 'The submitted code and videos result in a high-quality presentation and trustworthiness of the results.', 'The \"De Sa\" et al 2018 arxiv citation is really Sala et al and is an ICML \\'18 paper.', 'However, I found the contribution of the actor-critic model is insufficient and requires additional experimental validation (see below).', '+ Experiments compare with relevant baselines and consider interesting ablations, studying the effect of the GNN extensions in detail.', 'The concern raised by reviewer 3 is very important.', 'Perhaps the results would be a little more convincing if additional common word embeddings were also tested.', 'It would be good to have some non-NN results too.', 'As mentioned earlier, the actual conclusion in Section 4.2 is that minimizing the KL-divergence between the parametric policy and the optimal policy by SGD will converge to the optimal policy, which is straightforward and is not what AlphaGo Zero does.', 'Originality :', 'Though fundamental understanding can happen asynchronously, I reserve my concern that such empirical method is not substantial enough to motivate acceptance in ICLR, especially considering that in (only) 124/144 (86%) of the studied settings, the results are improved. And there is no analysis of the failure settings.', '1) How to solve the constrained problem (8) is unclear.', '- A short explanation of what is tested in section 6 should be given at last paragraph of section 4.1.', 'It is difficult to understand the goal of Section 4.2.', 'Without normalization, we have to tune learning rate to achieve the optimal convergence.', 'The learned features could in principle be employed to construct advanced learners, .e.g., hierarchical Bayesian.', '1.\\tExtensive analyses of the possibility of modeling posterior distributions with an INN have been shown.', 'You have the phrase \"allowing to imagine thousands of trajectories in parallel\". I would like some elaboration on this. I think you have ideas of what is happening in the latent space that I am not following.', 'The paper needs more discussion and experiments to explain how and why to use this approach.', 'tackled empirically with multiple sets of experiments on largescale', 'language or the presentation of the material but more because there', '4. https://arxiv.org/abs/1904.07556', 'Although the proposed measure is close to what is proposed by Datta et al., this paper makes the distinction clear and benchmarks its results properly against it.', 'Only two domains shifts are evaluated in the paper, specifically Omniglot + BSD500 and Office-Home.', 'The paper proposed variational selective autoencoders (VSAE) to learn from partially-observed multimodal data.', 'Gretton, Arthur and Fukumizu, Kenji and Choon H. Teo and Song, Le and Sch\\\\\"{o}lkopf, Bernhard and Alex J. Smola', '- in section 2.2, please explain more how gradients w.r.t hyper-parameters are computed.', 'The experimental settings, e.g. how the train:test datasets are split, and hyperparameter settings, are not clearly given.', 'The authors evaluate on both synthetic high-dimensional and challenging low-dimensional multimodal datasets and show improvement over the state-of-the-art data imputation models.', 'The plot by itself, as I understood, quantifies the model’s uncertainty in in- and out-of sample prediction.', 'The description of the generator in the appendix is difficult to follow.', 'CONS:', '**********after rebuttal', 'The first is regarding the structure of the paper.', 'As authors acknowledge in Section 6', \"You have argued that your method is sensible to try (cog. sci motivations), and shown that one instance works, but what can we expect in a more mathematical or general sense? Can any sizes of encoder and mapping network fit together? How does the number of mapping layers effect performance? Won't we eventually expect vanishing/exploding gradients with particular activation and can one address this in some way?\", 'Since each domain may have different number of classes, it is not clear how the number of classes (L) is set in the classification module (maximum number of classes in all domain?).', 'Although the results are favorable to the conductance metric, it is not clear how they precisely confirm the problem of incorrect signs presented in the caricature examples.', '[2] Smooth Neighbors on Teacher Graphs for Semi-supervised Learning, CVPR 2018', '- p6par1: \"much cheaper then computing\" -> than', 'it is indeed better without some significant improvements (at least 2% absolute accuracy more).', '- In a non-convex-concave setting, Hamiltonian gradient descent is attracted the any stationary point, even “local maxima” (or the equivalent in the minmax setting).', 'The paper first provides a generalization bounds for adversarial training, showing that the error bound depends on Lipschitz constant.', 'Balaji Lakshminarayanan, Alexander Pritzel, Charles Blundel', 'Meanwhile, Dreamer learns the remaining components, namely a value function, a latent transition model, and a latent representation model, based on existing methods (the world models and PlaNet).', 'Section 5:', '1. This work can make it clearer in principle how anti-aliasing contributes to improving the classification performance and robustness.', '.', 'Moreover, comparison with some of the DA baselines (ADDA[1], DSN[2]) is missing.', 'Furthermore, it proposes a new metric for the goodness of saliency maps.', 'It seems that both are using meta-learning with domain adaptation technique.', '- The interchangeable use of the term \"conductance of neuron j\" for equations 2 and 3 is confusing.', 'The main comparison with prior work', 'Spectrum pooling has been used in the community of computer vision and machine learning.', 'References:', 'Perhaps the authors could clarify on what a confounding \"time-locked scheduling strategy\" would look like in this task?', 'How does the transformer based method comparing to others?', '- The technical contribution of the proposed method is not high, because the architecture space of neural network is similar to the prior works.', 'This paper proposes a suite of tricks for training large-scale GANs, and obtaining state-of-the-art results for high-resolution images.', 'The idea of having a separate class for out-distribution is a very interesting idea but unfortunately previously explored.', 'The architecture is novel, and the classification of graph based on spectral embedding together with the Wasserstein metric is novel to me.', 'There are potentially interesting ideas in this paper.', 'Some comments:', '=== Pros ===', 'Gábor J. Székely, Maria L. Rizzo, and Nail K. Bakirov', 'First, though these are 1x1 convolutions, because of the up-sampling operation and the layer wise normalizations the influence of each operation goes beyond the 1x1 support.', 'Furthermore, the usage of the evaluation method unclear as well, it seems to be designed for evaluating the effectiveness of different adversarial attacks in Figure 2.', '(In both cases the equivariance group of data is a strict subgroup of the equivariance of the layer.)', 'Information similar (although not identical) to that summarized in table 5 could be captured by substituting the 3 metrics with the best performance after tuning for 4, 16 and 64 iterations respectively (just as examples).', 'The approach is novel and well motivated.', '4.\\tFor the final purpose, comparing problem similarity, I am wondering what the result will be if we train a supervised model based problem-problem similarity labels?', 'Moreover, the same limitations that apply to other algorithms to make them successful, in my opinion, apply to your proposed algorithm (e.g., difficulty to move from simulated to real-world).', 'To elaborate, the problem is known to be NP-hard in the worst case, while the data sets used in the paper seem to have certain nice properties.', 'The detailed analysis of the training of DNN with the batch normalization is quite interesting.', 'I think having such a curve will help sell the paper as giving the practitioner the freedom to select their own preferred trade-off.', 'Algorithm 1 does not include mitosis, which may have an effect on the resulting approximation.', '-\\tDetailed analysis for detecting collapse and improving stability in large-scale GAN', 'I do not understand why making use of labels is important for solving the catastrophic forgetting problem and how the labels are useful in the generative replay process.', 'While the use of a GAN in general will make the results less blurry and visually appealing, it does not necessarily mean that the samples it generates are going to be plausible or better.', '- It would be also better to show the coefficient of existing methods that have no theoretical justification.', 'Even though this is mostly an empirical investigation, I think some more efforts should be put in understanding and explaining why some of those behaviors are shown, as I think it can bootstrap future work more easily.', 'The authors assume access to a large unlabelled set in test (target) domain, and a large labelled (few-shot) set in the source domain.', 'Summary:', '4. Another curious question is whether the features would still provide as much improvement when a stronger ASR system than AutoSeg (e.g., Lattice-free MMI) is used.', '+ The theory in this paper improve bounds for multi-source DA in previous paper.', 'I do expect the paper be a strong submission after a significant effort in presentation and experimental designs.', 'Presumably if you get a more diverse pool of agents, that should improve things.', 'The paper proposes an approach called structured value-based planning or learning, where the Q matrix or the Q function is estimated from incomplete observations based on the prior that it is low-rank.', 'After rebuttal:', 'Strength:', '3. Can the statistics of activations be controlled using activation functions or operations which break the symmetry?', 'The generation process of the dataset is well thought out.', \"Perhaps it's helpful to briefly introduce it in addition to citing Bartlett et al. 2017.\", 'The result showed some interesting insights about the evaluated models.', 'The proposed method is also able to be used in the regression task since it is based on the disc which can be estimated in the regression task.', 'In this case, if future readers limit themselves to the main text, I think it can have more value to present some content form Appendix B and C than to have more than a full page on stability investigations and attempted tricks that turned out not to be used to reach maximal performance.', 'Second, by modifying the de-allocation system by also multiplying the memory contents by a retention vector before an update', 'It then shows some experimental evidence suggesting that this is possible.', 'In fact, this is a very difficult part of learning disentangled representation.', 'Hopefully, such a benchmark will inspire more researchers to explore this setting, and perhaps propose simpler, more principled approaches to perform this task.', 'Could you please comment on how this can/will be fixed?', 'Therefore, the paper cannot be qualified for ``meta domain adaptation’’ and has very limited novelty in terms of its contribution to meta-learning; however, the combination of domain adaptation and few-shot learning is fair.', '2.', 'algorithms. In COLT, 2009.', 'In the modeling section, authors use p(I|D) as q^D(.) in the Eq. 12, where p(I|D) is the conditional probability that a particular node infected an observed infected node first.', 'Novelty: As far as I know, this work is among the earliest works to think about GNN pre-training.', 'The main motivation to use PARCUS is that it works better in a low-resource setting than recent state-of-the-art models for the high-resource case.', 'The “ground truth” values for the unobserved modalities are provided by sampling from the corresponding latent variables from the prior distribution once at some point of training.', 'Summary:', 'This question comes from my observation that the nature of the losses, especially for L_y vs. L_y,L_x (i.e., SL vs. USL) seem to be different.', 'Why not compare with Sparsely-Gated MoE?', '2. The learning of the proposed model relies on meta-data description such that the learning is supervised. Can the method be applicable to situations where no meta-data and no class labels are available?', 'body. The generated video can have an arbitrary background, and effectively', 'I would imagine that even if an embedding X is a bit noisy, because not exactly equal to gamma(P) where P is the expression it represents, you could consider doing the propagation with gamma(G(X)).', '[A] Mutual Information Neural Estimation, ICML 2018', '-', 'Experiments show that the network can be applied multiple steps in a row, while operating only in the embedding space.', '5) The experiments only test the proposed method on CIFAR100 and CIFAR10, which has at most 100 fine classes.', 'Furthermore, and more importantly is the weight sharing scheme induced by this - using convolutions is a very natural choice for natural images (no pun intended) due to the translation invariant statistics of natural images.', 'Questions:', 'The most similar paper at the same period is [Z Hu, arXiv:1905.13728]', 'however, you do not provide any evidence for it, so you should avoid making such claims.', 'why the fact that the classifier is able to predict which dataset the image ‘m’ corresponds to is useful or practical, as this seems to be a property of the set ‘m’ rather than the property of the trained classification model (f_\\\\theta)', '- While the actual arguments are somewhat technical as is expected from such a paper, the motivation and general strategy is very easy to follow and insightful.', 'When you say that full-matrix computation \"requires taking the inverse square root\", I assume you know that is not really correct?', 'The authors should address that in their revision.', 'The loss is solved using feed-forward neural network with the input to the network being the ids of the items encoded in binary codes.', 'For this, the authors use the ideas of the standard dropout for deep networks.', 'For example, Page 6. “is pre-trained on a pure set of negative samples”— what is the objective function? How to train on only negative samples?', 'The empirical evaluation is limited in considering only one task (clique finding), and the results seem to be quite sensitive to the chosen optimizer.', 'Additionally, the idea is nicely presented in the paper.', 'The paper design a low variance gradient for distributions associated with continuous or discrete random variables.', 'Typos:', 'I especially like how Section 2 synthesizes existing work into model categories which make it easier to think about their relationships.', 'Recently, Tatarchenko et al. showed in \"What Do Single-view 3D Reconstruction Networks Learn?\" shortcomings of this evaluation scheme and proposed alternatives.', 'Also, the selection of the methods to be compared is appropriate.', '4. [Experiments.] The author presented a multimodal representation learning framework for partially-observable multimodal data, while the experiments cannot corraborrate the claim.', 'This paper proposes a Self-Modulation framework for the generator network in GANs, where middle layers are directly modulated as a function of the generator input z.', 'labelling of $n$ inputs in $d$-dimensional space.', 'There are some preliminary results in the appendix but it will be useful for the reader if there are are some plots showing the benefit of the method in comparison with existing methods that guarantee convergence (which method is faster?).', 'However, the effort to implement it successfully is commendable and will, I think, serve as a good reference for future work on video prediction.', 'The system first used a rule-based method to extract the concepts for problems and then learned the concept embeddings and used them for problem representation.', 'Or a specific useful application where spherical methods in general outperform other approaches?', 'I also find weird the way that the authors arrive to their final objective in Equation (5).', '= Originality: The algorithmic approach is a combination of [Parthak, et al., ICML17] and [Schulman, et al. 2017] (with some experiments using [Kingma & Welling, 2013]).', '- Section 3.3: “Different from pruning, which the search space is usually quite limited”. “which” should be “whose”?', 'The advantage of INN, however, is not crystal clear to me versus other generative methods such as GAN and VAE.', 'It seems that the proposed algorithm is not very original because its two parts, namely prediction (coefficient estimation) and learning (dictionary update) have been widely used in the literature, using respectively a IHT and a gradient descent.', \"In particular I do not think the satement 'all the pre-training methods are at most linear with respect to the number of edges' made in appendix F is correct.\", 'In particular, the latent variable z is optimized based on the MAP framework.', '- Section 1: As motivation for this work, it is stated that \"we aim to make well performing NLP algorithms more widely applicable\".', 'Human baseline does not need any training!', '- An experimental study of the effect of the mixing parameter “s” would be useful to include.', 'It would be interesting to see how deep networks do for the hard cases.', 'The more interesting question of how to generalize to unseen items (how would that be possible given that items have no representation at all) is not discussed at all and seems not to be realizable, which makes the starting point of such methods (items have no representation) questionable.', 'The proofs provided by the authors assume that convexity and many assumptions, which makes it not very useful for the real world case.', '[1] AUEB, Michalis Titsias RC, and Miguel Lázaro-Gredilla. \"Local expectation gradients for black box variational inference.\" In Advances in neural information processing systems, pp. 2638-2646. 2015.', 'The canonicalizations are trained such that images for arbitrary values of the corresponding factor of variation are transformed into images with a fixed, canonical, value for that factor.', '- Given the small size of the dataset, I would propose experimenting with non-neural approaches as well, which are also quite common in NLG.', 'This could potentially prevent a lot of unnecessary computation and also lead to better-performing models.', 'The author should also add more explanation here,', 'This approach translates to improved quantitative evaluation measures,', 'I like this work, it addresses a real problem in a number of models for 3D representation learning (similar models are also used for e.g. cryo-EM reconstruction).', 'In differential geometry, the \"cut locus\" is the region beyond which there is this non-uniqueness.', '5. In the discrete VAE experiment, upon brief checking the results in Grathwohl(2017), it shows validation ELBO for MNIST as (114.32,111.12), OMNIGLOT as (122.11,128.20) from which two cases are better than GO.', 'EDIT: the concerns were mostly addressed in the revision.', 'I know these are not the concerns of this paper, but I would be really grateful if you could provide some intuitive answers!', 'The goal is to learn a classification algorithm over the novel class based on the sample from the base class and novel class.', '-- Concerns regarding the clarity of presentation and interpretation of the results.', 'The long history of successful hand-designed descriptors in computer vision, such as SIFT [Lowe, 1999] and HOG [Dalal and Triggs, 2005], suggest that one can design (with no data at all) features reminiscent of those learned in the first couple layers of a convolutional neural network (local image gradients, followed by characterization of those gradients over larger local windows).', 'I have only two major concerns.', '- Missed opportunity of better analysis of which theorem/rewrite rule properties are more likely to fail', 'There are many typos (see below).', 'It would be good to include comparisons against VI with \\\\lambda = 1.', 'The paper can benefit from a proofreading.', 'The authors further propose a orthogonalization-based regularization to mitigate this problem.', '- I don\\'t understand why the authors say that their space \"interpolates smoothly\" just because the limit in the curvature is the same from the left and right side.', 'Additionally, the Figures 2,3 are very intuitive and nicely explain the theory.', 'The authors focus on a setup where both target and external training data come from the same distribution but differ in class labels, where each class in the target data is a set of finer-grained classes in the auxiliary data.', '- \"inspite\" -> in spite', 'Summary:', 'In particular, the example in section 3.1 says that a noisy information bottleneck is introduced, but then says that the modified and unmodified models have \"training algorithms that are exactly equivalent.\" I think this example needs to be clarified.', 'My question is if we want to integrate the proposed method into theorem provers, after multiple steps of math reasoning, how would us know the goal has been proved? Is it possible that we can train a decoder that maps back from the latent space to the formula space?', 'Moreover, the authors pointed out that near-linear activation function can improve such gradient explosion.', 'The results and discussions in the main part of the paper are too light in my opinion; the average model accuracy across modules is not an interesting metric at all, although it does show that the Transformer performs better than recurrent networks.', 'Is this the most suitable shape to sample from? How do you draw samples from the sphere (Similarly, how are the points sampled for the training objects)?', 'This 1996 paper clearly established that it is possible to learn such filters from a small number of images.', 'Is there a rationale in this setting or it is just a more tricky method to fix the weight norm to some constant, e.g. ||w||=1?', '- The idea is theoretically sound and interesting.', 'The negatives are that the paper does not really show this modified DNC can solve a task that the original DNC could not. As the authors also admit, there have been other DNC improvements that have had more dramatic improvements on bAbI.', 'In particular, the derived criterion is comparable to other sparsity-inducing penalities on variable inclusion in machine learning models; although it has motivation in causality it is not exclusively derived from this position, so one might wonder how alternative sparsity penalities might perform on the same challenge.', '###', 'Thus, it is hard to say whether the results are applicable in practice.', '- clipping-trick is a neat observation', '- The evaluation of the proposed method is not complete.', 'In the ordinary feature selection regime one is concerned simply with improving the predictive capacity of models: e.g. a non-linear model might be fit using just the causal variables that might out-perform both a linear model and a non-linear model fit using all variables.', 'according to whether it has mutagenic effect on the gram- negative bacterium Salmonnella', 'It remains to see how the method performs with more acoustically-challenging speech data, and how universally useful the learned features are (as is the case for BERT in NLP).', 'Specifically, in the work of Zhang et al. (2018), the complexity term quadratically depends on the layer (or say, current sequence length, denoted by t in original paper), making it less instructive.', 'Moreover, authors should conduct experiments on state-of-the-art benchmarks, including natural images.', 'I also agree with the authors that this line of work (dynamic KG construction and modification) is an important area of research.', 'Yet, in Fig.1 some difference is observed between the methods, why is that so?', '### Novelty ##', '- I am in general curious to see if it will be beneficial to fine-tune the modules themselves can further improve performance. It maybe hard to do it entirely end-to-end, but maybe it is fine to fine-tune just a few top layers (like what Jiang et al did)?', '- the authors state that they use \"the power of DNNs\" while they are experimenting with a neural network with only 4 layers. While there is no clear line between shallow and deep neural networks, I would argue that a 4 layer NN is rather shallow.', 'Summary:', 'If this method is to be useful, understanding how these spectral properties change during training for different types of networks is essential.', 'Subsection 4.2:', 'I read both papers and found they have similar idea about PT although they have different designs.', 'Why is non-plastic rnn left out of Figure 2b?', '=== After rebuttal =', 'Consequently the regularization hyperparameters have to be set differently.', 'It would be better to compare with them.', '1) Some of the benchmark datasets for the proposed task as well as some well-known methods (see Battaglia et al’18 and references in there) are missing.', 'The experiments well support the conclusions made in the paper.', 'A naive approach to estimate coefficient with single-source domain discrepancy measures such as [1]Mansour (2009), [2,3] Ben-David(2007, 2010), [4] Kuroki et al (2019), and W1-distance is not considered.', '-typo “implmented”', \"- I didn't understand the motivation for testing only very general-purpose models (this is described in Section 3).\", '- Experimental evaluation of auto-encoding using a variable number of input points is interesting to add: ie how do the two evaluation measures evolve as a function of the number of points in the input point cloud?', '- The authors should make a distinction on what kinds of attack is considered: white box, black box or grey box.', 'I am not sure if this is the first work in this direction of proving bounds assuming an oracle or if there is some background work that the authors could provide to put this into context.', '- There are a couple of recent papers that also consider hyperbolic GCNs, and in fact use  similar ideas for the aggregation and update steps (i.e., same lift to hyperbolic space).', 'judgements', 'Summary', 'Pros:', 'Above on, this paper did a meaningful work.', 'The latter task is generalized to detecting if a', 'Permutation gives a guarantee of the correct level.', 'I will therefore keep my rating.', '#', 'there a training task which would allow one to more explicitly', 'Th 1 gives the number of validation samples required to bound error between the mutual information estimate at finite samples and asymptotically for a function T parametrized by \\\\tilda{theta}. This control is of a very different nature from the control established by MIME which controls the error to the actual best possible variational bound.', 'The submission clearly improves the state-of-the-art, experimentally demonstrates the method on several problems comparing with the alternative techniques.', 'Section 7-8:', '#Cons#', 'Office-Home: Deep Hashing Network for Unsupervised Domain Adaptation, CVPR 2017.', 'Although the idea behind this paper is fairly simple, the paper is very difficult to understand.', \"The overall idea and motivation looks very similar to the coverage-enhanced models where the decoder also actively “writes” a message (“coverage”) to the encoder's hidden states.\", 'I think it should be vice versa, N >> n', 'This work is clearly the work of a large team.', 'It is thus unclear whether the advantage can be maintained after applying these standard regularsisers.', 'The theoretical contribution is very limited.', 'it would like that the authors provide more intuition why these improvements occur and also outline the limitations of their approach.', 'which defines the capacity of a single layer network to memorize the', '##', '--What does a \"heavy classifier\" imply concretely?', 'Besides, the results on the sentimental analysis are comparable with the compared baselines.', 'What does “keeping notes” mean?', 'The model achieves the state of the art in the two tasks of ProPara and Recipes dataset.', 'In order to correctly estimate causal relations from data, both cases must be considered.', 'It is also not clear how the loss function proposed differs from that of the CDVAE, etc.', 'The method is very simple: first, transform the input 1D/2D signal into the spectrum domain based on discrete Fourier transform (DFT), then cut the high-frequencies, then transform back to the time domain using the inverse DFT.', '* The experiments do not demonstrate that the model learns a meaningful *conditional* distribution for the missing modalities, since the provided figures show just one sample per conditioning image.', 'Also, it is not clear from the discussion on z, whether sampling is performed once for each video of for each frame.', 'The proposed sampling distributions assumes independence between the random variables over which the authors optimize — I find it surprising that this leads to good empirical results', 'Experiments on adversarial robustness can be further improved.', 'A more predictive generative model that makes less hard assumptions on graph data would be interesting.', 'It is in the end plugged into a continual learning algorithm which also performs domain transformation.', 'The paper states two major contributions (the last paragraph of Introduction), one is the new model Scratchpad Encoder, and the other is “possible to generate a large high quality (SPARQL query, local form) dataset”.', 'In this paper, the authors presented a large experimental study of curiosity-driven reinforcement learning on various tasks.', '3. What are the results when using the whole training set of Recipes ?', '(*)', 'The experiments presented in the paper include a set of simulation experiments and a real-world task.', 'Particularly, the authors mixed their observations up with the results of published works, making it hard to identify the contributions of this paper.', 'The order is reversed when inverting.', 'Regarding the presentation, I found odd having some experimental results (page 5) before the Section on experience even have started.', 'There are two conflicting qualities that need to be optimized--performance and compression rate.', 'This model is trained, presumably, in a conventional GAN style using the global latent variable representations inferred across the different training point clouds.', 'What about the performance of GO gradient in the 2 stochastic layer setting in Grathwohl(2017)?', 'It is also not clear to me why these problems are important.', 'The authors seek to make it practical to use the full-matrix version of Adagrad’s adaptive preconditioner (usually one uses the diagonal version), by storing the r most recently-seen gradient vectors in a matrix G, and then showing that (GG^T)^(-½) can be calculated fairly efficiently (at the cost of one r*r matrix inversion, and two matrix multiplications by an r*d matrix).', 'This paper provide a method to produce adversarial attack using a Frank-Wolfe inspired method.', 'The authors manually design the task hierarchy and propose a progressive module network to recursive calls the lower modules and gather the information by soft-attention.', '[1] Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles', 'I do not have any major concerns about the paper.', 'Weaknesses: The dataset created here is entirely synthetic, and the paper only includes one single small real-world case; it seems like it would be easy to generate a larger and more varied real world dataset as well (possibly from the large literature of extant solved problems in workbooks).', 'Also pervasive is the use of the asterisk to denote multiplication, again as if it was code and not math.', '- perplexity improvements of less than 1.3 points over plasticity alone (which is the actual baseline for this paper) can hardy be called \"significant\". Even though they might be statistically significant (meaning nothing more than the two models being statistically different), minor architectural changes can lead to such improvements.', 'I wonder if the authors ever looked at how much the size of the latent vector determines the performance of the system?', '---------------------', 'While, the paper is a plaisant read, I find difficult to access its importance and the applicability of the ideas presented beyond the analogy with the capacity computation. Perhaps other referee will have a clearer opinion.', 'Specifically, there are two datasets which are more complex and uses templated language to generate synthetic datasets similar to this paper:', '- Although the proof does not seem to use the axioms as a building block, which is fortunate since it would make it a circular argument otherwise, the text suggests so: \"Given these three axioms, we can show that:\".', '- Space should be added between figures to better divide the captions', '- The paper addresses an important scenario which has not been addressed to this point: namely, meta-learning without the assumption that the train and test sets are drawn from the same domain/distribution.', 'They start from Equation (4) which is incorrectly denoted as the log-marginal distribution while it is the same conditional distribution introduced in Equation (3) with the extra summation for all the available data points.', 'Page 7, 2nd line from the bottom, FSGM->FGSM', \"- I don't see a discussion about the downsides of the method (for example, the large number of triplet comparison examples needed for training; and possible methods to overcome this problem).\", 'This paper proposed a new pooling method (Frequency pooling) which is strict shift equivalent and anti-aliasing in theory.', 'The authors mention gene regulatory networks , neuroscience etc as key applications.', '[1] Smith and Le, A Bayesian Perspective on Generalization and Stochastic Gradient Descent, https://arxiv.org/abs/1710.06451', '- in section 4.3, there is no guarantee that the intersection between the training set and test set is empty.', 'I have two main concerns about this paper.', 'You say that second order methods are outside the scope, but you say that your method is particularly relevant for ill-conditioned problems.', '(3) FGSM is a quite weak adversarial attack method, which makes evaluating adversarial robustness on FGSM may be misleading.', 'Writing: The writing is good and easy to follow.', 'The author should explain this.', 'I also suggest the paper discusses e-SNLI a bit more.', '-- It is not clear how the  per-example scalar sigma-i is learned. (for Eq 2)', '- The experiments are conducted thoroughly in the CIFAR-10 and ImageNet.', 'The explanation is the COCO dataset has a fixed set of 80 object categories and does not benefit from training the diverse data.', \"I am puzzled and looking forward to answers to the above questions. I don't yet understand what is the thing that makes this approach appear to work, or why you were able to drop the Bayes inference inversion altogether as done by Schott.\", 'It would be interesting to see what happens if one uses the same architecture on a number of problems, changing only the convolution in each case.', 'It leaves it unclear to the reader when someone should choose to utilize a spherical method or when the proposed method would then be preferred compared to other spherical methods.', '6. The rationale of the two tower design (why not combine two) is not clearly explained.', 'heteroaromatic nitro compounds with 7 discrete node labels.', 'perturbed versions of the input images at test time.', 'NeurIPS 2014', 'How is this a reasonable assumption?', 'Strengths:', 'Without an empirical example in a realistic setting, it is hard to judge whether the benefits of introducing an ME bias for better classification of new inputs belonging to new classes can outweigh the negatives via a potential increase in misclassification of those belonging to old classes.', 'They observe that the behaviors seem to arise in different batch sizes', 'Actually, this is the standard practice in some learning tasks, e.g., zero-shot learning.', 'I think having this in mind, showing the convergence of Theorems 3.2-3.4 is somehow trivial.', 'The new idea in the paper is better streaming algorithms under the assumption that there is a “heavy hitters” oracle that returns data items that have a lot of representation in the stream.', 'The mutual information at the minimizer of the objective function  is used as causal measure.', '- For semi-supervised classification, the paper did not report the best results in other baselines.', 'The technique basically fills out the fourth entry in a Punnett square.', 'The reviewer recommends the authors to read some literature review of the topic of GCN and re-organize the references, and use search engines to have a better view on the state of the art of (hyperbolic) geometric theories and graph convolutional networks.', 'Regarding the experimental evaluation of the model rather confusing.', 'I believe that for the specific uncertainty evaluation tasks this paper considers the latter is more appropriate.', 'The paper is relatively well-written, although the description of the neural models can be improved.', 'It is not clear how MULANN can work in this situation and how its performance vary with the noisy signals conveyed in those false pseudolabeled samples.', '5)In session 3.1-3.3, for completeness. I think it would be helpful to explicitly compare the equations from the original DNC paper with the new proposed ones.', '- As the authors point out, the idea of using a batch-normalization like strategy to set an adaptive learning rate has already been explored in the WNGrad paper.', 'The idea of using a constrained formulation is not novel either (constrained MDPs have been thoroughly studied since Altman (1999)).', 'There are many acronyms that are never defined: MINE, TCPC', '#8) In the experiments I think that at least one example of F and p(Omega) should be presented.', 'The maths is not clear, in particular the gradient derivation in equation (8).', 'On a high level, the main difficulty of abduction is to search in the exponentially large space of hypothesis.', 'Therefore, it is plausible that the mutual information regularization can consistently explain the performance of an NN.', '- The hyperparameter selection regime (and the experiments used to find them) is not described', 'The paper not only claims \\'large scale representation learning\\' but also utilizing the described idea to use neural networks to \"directly, approximately solve non-convex NP-hard optimization problems that arise naturally in unsupervised learning problems.\" Both claims are not really shown in the paper: (i) The experiments are not large scale and (ii)  it becomes not clear how any substantiate insight with respect to NP-hard problems can be gained here apart from the fact that it tackles a ML problem, which many seem to be computationally hard problems.', 'A quick glance at Table 1 suggests that the bounds obtained through Theorem 3 are exponential in t and are mostly vacuous.', '(mean error DNC: 16.7 \\\\pm 7.6, DNC-MD (this paper) 9.5 \\\\pm 1.6, sparse DNC 6.4 \\\\pm 2.5).', 'More emphasis on the importance of the axioms (desirable properties) should be made.', 'hence, there is no connection between the theoretical results on MDL generalization bound and the proposed method MULANN.', 'The main contributions include 1) a discretization on the learning curves such that transformer can be applied to predict the them; 2) an empirical evaluations using the predicted learning curves to train the policy.', 'The BERT features are used as inputs to ASR systems, rather than the usual log-mel features.', 'Significance - The results show that meta-learning by gradient descent to modulate the plasticity learning rate is a promising direction -- a significant contribution in my view.', '#1) I do not understand exactly what the \"general method\" means. Does it mean that you propose a method, where you can just change the F, such that to solve a different degradation problem? So you provide the general framework where somebody has to specify only the F?', '* The upsampling analysis is interesting but it is only done on synthetic data - will the result hold for natural images as well? should be easy to try and will allow a better understanding of this choice.', '* Revision', '2. How is the performance of a simpler baseline such as combining a subset of new domain as training set to train MAML or PN (probably in 5-shot, 5-class case)?', 'This study primarily centers around an initially somewhat surprising result that non-trivial policies can be learned for many ’simpler’ video games (e.g., Atari, Super Mario, Pong) using just curiosity as reward.', 'I believe that\\'s what \"Pred (One Step)\" expresses, but it would maybe be generally helpful to be more precise about the notation', 'The authors answered some of my questions but I still think it is a borderline submission.', 'Paper is quite easy to follow.', 'For this reason, I think it is okay but not good enough at this time.', 'It seems that there needs some way to enforce them to be zero to ensure that the propagation happens only among the entries belonging to the variables of interests (x, y and z).', \"Most importantly, I would like to see a measure of variance/uncertainty like confidence intervals included in the results; otherwise it's impossible to assess whether the results are likely to be significant or not. Other questions:\", 'What happens if there are multiple mentions in the text? Which one does it look at?', 'First, many details are missing, especially in the experiments, which makes the proposed method suspicious and non-convincing.', 'Rather they make generic smoothness assumptions (your test also makes such assumptions by the choice of neural network architecture).', 'I took into account the discussion and the newly added experiments and increased the score.', \"I also had a hard time going through the paper - there aren't many details.\", 'This is the case when you map an exchangeable matrix into a jointly exchangeable matrix by representing it as a bipartite graph [0, X; X^T, 0].', '1. Some experimental details are missing. I’m going to list them here:', 'This paper proposes a justification to one observation on VAE: \"restricting the family of variational approximations can, in fact, have a positive regularizing effect, leading to better generalization\".', 'For example, the paragraph around Eqn. (11) just says that the decoder takes in a concatenated latent vector.', '[1] Pham et al. Found in Translation: Learning Robust Joint Representations by Cyclic Translations Between Modalities, AAAI 2019', '- The authors show good results on several multimodal datasets, improving upon several recent works in learning from missing multimodal data.', '- Synthetic experiments of supplementary materials: the gains of the methods seem to be small. What are the numerical results? What would happen if you allow to select starting point at random (a more realistic case)?', '2) Applying the model of Hartford et al’18 to problems where interacting sets are identical is similar to applying convolution layer to a feature vector that is not equivariant to translation.', 'How does this term affect the learned distributions in case of CDNs?', 'The proposed method performs much better than baseline sentence embedding methods.', 'Therefore my score for now is a weak reject, but I am very happy to increase the score if the authors address my presentations concerns.', 'I am not an expert in this area, but this seems like a nice and non-trivial result.', 'However, the real-world experiments are not necessarily the easiest to read.', '10. Reading the baselines before the experiments is very confusing.', 'Indeed, the authors find that FW is 6x - 20x faster for constructing white-box adversarial examples than a range of relevant baseline, which is a significant speed-up.', 'The main idea seems similar to adopting active learning for the test set selection.', 'Firstly, I would like to thank the authors for providing detailed responses to my questions.', 'Overall, I think this paper has some interesting ideas, but those need to be fleshed out and clearly explained in a future revision.', 'Why does Experiment 1 present pairs of stimuli, rather than high-dimensional individual stimuli?', 'The authors present a video prediction model called SAVP that combines a Variational Auto-Encoder (VAE) model with a Generative Adversarial Network (GAN) to produce more realistic and diverse future samples.', '1. In section 3.1, the authors selected a snippet from each section, but this was not rigorously defined.', 'The authors should clearly explain how to update \\\\phi when optimizing Eq 12.', 'The motivation for the proposed method is to solve the mode collapse problem of MC-Dropout, but using ensemble loses the Bayesian support benefit of MC-Dropout.', 'positive examples relative to the negative set it sees during', 'This appears to be very restrictive, since typically the values of time series j at time t-1 are typically depending say of those that time t-2, t-3 etc.', 'The self-modulation mechanism itself is implemented via FiLM layers applied to all convolutional blocks in the generator and whose scaling and shifting parameters are predicted as a function of the noise vector z.', 'The results are then analyzed and insights are derived explaining where neural models seemingly cope well with math tasks, and where they fall down.', 'This paper is giving an information-theoretic perspective on existing variational inference methods.', '[B] Wan, Weitao, et al. \"Rethinking feature distribution for loss functions in image classification.\" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018.', \"- From figure 2 it's not clear why all non-zero connections in (b) are then equally weighted in (c). Would keeping the non-zero weightings be at all helpful?\", 'and', 'For the purpose of introduction, it might be better to give examples of expectation objectives such as:', '+ (implicitly) calls into question the value of these testbed environments', 'I disagree with the claim of practicality in the introduction (page 2, top).', 'Comments:', 'A good extension of this work would be to combine a text-derived embedding  or the synsets to interpolate the SPoSE dimensions for missing words in the original set.', 'I wonder why the authors didn’t compare or mention optimizers such as ADAM and ADAGRAD which adapt their parameters on-the-fly as well.', '>>', 'A proper baseline should have been compared.', 'Detailed comments:', '* Domhan, Tobias, Jost Tobias Springenberg, and Frank Hutter. \"Speeding up automatic hyperparameter optimization of deep neural networks by extrapolation of learning curves.\" Twenty-Fourth International Joint Conference on Artificial Intelligence. 2015.', 'The Conclusion is very sparse. \"In future work, we are planning to apply other algorithms requiring discrete inputs to audio data\":', 'This architecture reminds me of the good old MRFs for image denoising.', '2. The results on the synthetic datasets show that the resulting estimator does have low variance and the estimates are less than or equal to the true MI value, which is consistent with the fact that it is a lower bound estimation.', 'The derivation of the algorithm in Sec 3.2 is logically clear and easy to follow.', '- \"utlize\" -> utilize', 'This seems to be used only at test-time and is used to control variety-fidelity tradeoff.', 'Without doing so, it leaves the reader wondering why not simply a standard RBM trained using a standard method (e.g. contrastive divergence).', 'The character is then redrawn into the background with a neural net, and all of this is done in real time.', 'However, the un-standard design of the LSTM models makes it unclear whether the comparisons are solid enough.', 'For the case of multivariate statistical dependence testing, such corrections are described e.g. in:', 'reasonably on its own', '3. Authors should be much more clear about which is their original contribution to the problems stated in Section 4 and Section 5.', 'How do the FID/Inception improvements compare to (Mescheder et al 2018)?', 'They are also categorized by difficulty and area.', 'The key technical component in the proposed approach is the idea that keeping track of entity states requires (soft) coreference between newly read entities and locations and the ones existing in the knowledge graph constructed so far.', 'Comparisons which, while mentioned, should perhaps have been discussed and compared more in detail in this work.', 'In the real world, one would not have access to OOD data during training, how is one to pick \\\\lambda in such cases?', 'The authors propose a simple truncation trick to control the fidelity/variance which is interesting on its own but cannot always scale with the architecture.', 'Minor: The abstract could be improved by providing more clear pointers to the presented novelty.', 'This somewhat conflicts the commonsense of \"the deeper the better?\"', '- Limited analysis of model/architecture design choices', 'The paper presents an approach to extract a character from a video and then maneuver that character in the plane, optionally with other backgrounds.', 'Why, for example, did the authors only perform variational inference with the current and previous frames? Did conditioning on additional frames offer limited further improvement? Can the blurriness instead be attributable to the weak inference model?', 'Shows improvements on LSTM tasks, and is comparable with SGD, matching accuracy with time.', 'The improvement over previous SOTA is definitely significant.', 'It is difficult to know what the theory says about the empirical results, given the tweaks discussed in Sec 2.2, and so it is difficult to know what is the benefit of the method versus the tweaks.', 'capture both the dynamics and appearance of the person.', 'Typos & minor issues', 'I think there are ways this function composition approach could generalize, e.g. using skip connections and layer dropout (which encourages layers to be composable).', '- How does the estimation accuracy of GAN relate to the estimation accuracy of the proposed method? Showing a quantitative description would be nice.', 'The goal is to defend the model from mistakes in training labels and to be more robust to adversarial examples at test time.', 'This paper proposes an algorithm for auxiliary learning.', 'The difference is that Cakewalk uses the CDF directly, while CE uses a threshold function on the CDF.', '9. It is not clear that baseline 1 and 2 correspond to which baselines in later experiments.', \"But it's not guaranteed as the upper bound contains other variables, such as the number of training samples and model complexity.\", 'Additional feedback for authors (not part of the main decision reasoning):', 'is this approach e.g. to directly predicting a function R^3 -> R (a la DeepSDF)?', '3. How do PPO and SimPLe handle partial observability? Is it principled to apply them to partially-observable environments?', 'The extension is to take pixel count to compute the area instead of using rectangular region area, as well as taking the distance between pixels into account.', 'My main concerns are about the evaluation and comparison of standard neural models.', 'This is not to say that this way of doing things is wrong, but rather that it is misleading in the context of prior work.', '-- This paper seeks to combine several ideas together to propose an approach for image classification based continual learning tasks.', 'This paper addresses the computational aspects of Viterbi-based encoding for neural networks.', 'They improve the space bounds and interestingly in some cases the bounds are better than what is possible without the oracle assumption.', 'The idea of question generation: using logical form to generate meaningful questions for argumenting data of QA tasks is really interesting and useful.', 'The two architectures, though not novel enough, are towards representing “non-additive utility”.', 'One observation from the submission is that the token set may need to very large (from tens of thousands to millions) for the system to work well, making the BERT training computationally expensive (I noticed that the BERT model is trained on 128 GPUs)', 'The rationality that decoupling the point generator with the object generator is also discussed.', 'Secondly, if it is possible to obtain the ground-truth image of different latent canonicalizations, you can simply train a network to predict the canonicalizations by simple supervised learning.', 'Can this be useful for someone having less compute power and working on something similar to CelebA?', 'Results are then presented for this modified reward showing that it results in high-frequency control signals (and that the proposed constrained approach avoids this).', 'is proportional to the number of', 'The network is forced to memorize a set of', '3. It seems the domain shift in the paper is less dramatic.', 'The real data experiments (sections 4.2 and 4.3) are not very convincing, not only because of the very small size of N, but also because there is no comparison with the other approaches.', 'Figure 1:', 'Also, can cGAN be used estimate the density of X (posterior or not)?', 'Summary:', '= Summary', 'd. What is the metric in Table 1 and 2?', 'It would therefore be instructive to add experiments for ResNet to see how well the results generalize to other network architectures.', '2.\\tThe experimental data set is too small, with only 635 problems.', 'There are a number of interesting predictions made in this paper on the basis of this analysis.', 'this makes a lot of sense especially given that CPC / wav2vec recovers phonemes and quantizing the phonemes will recover a language-like version of the raw audio. And running BERT across those tokens will allow you to capture the dependencies at the phoneme level.', 'In fact, the key results obtained in this paper is that minimizing the KL-divergence between the parametric policy and the optimal policy (Nash equilibrium) (using SGD) will converge to the optimal policy.', 'Increase rating.', '--In Section 4.1, it appears that they are instead making a claim about Equation 2 being a bound for equation 1; but even this derivation appears to have a problem.', 'The overall idea is that, if the mutual information between learned parameters and the data is limited, then this prevents overfitting.', 'I was a bit surprised by just how much the decoder network could be shrunk by using fast weights.', 'The authors propose to combine a VAE model with a GAN objective to combine their strengths: good quality samples (GAN) that cover multiple possible futures (VAE).', 'But there are some errors of expression, so it should be checked.', 'The paper does not contain an attempted proof of equivariance, and if one tries to give one, the impossibility of doing so will become apparent.', 'So it seems not a reasonable solution for the mode collapse problem of MC-Dropout.', 'The text is over the 8 page limit, but I don’t think this is justified.', 'The paper provides some good experimental results.', 'Strengths:', '- Minor grammatical mistakes (missing \"a\" or \"the\" in front of some terms, suggest proofread.)', 'However, this paper rather than proposing solution address the meta-learning problem, albeit the title “meta domain adaptation”, only brings few-shot learning to domain adaptation.', '- One great benefit of having a module-based model is feed in the *ground truth* output for some of the modules.', '3.', 'This can help us not only better understand the models, but also the dataset (VQA) and the task in general.', 'Ability to implement in a straight-forward manner on GPU.', '3. Similarly, the paper will become stronger if it has some experimental results that compare quantization methods.', 'While the authors do report some interesting results, they do a poor job of motivating the proposed extensions.', '- Also, for MISC-r experiments, the weights between the intrinsic reward bonus and the extrinsic reward are not specified in the paper.', 'In Appendix B.1, they report mixed results in terms of wall-clock time, and I strongly feel that these results should be in the main body of the paper.', 'Comparison to \"SGD BN removed\" is not fair because the initialization is different (application of BN re-initializes weight scales and biases).', '* Page 9: term is included -> term included', '1. The work has limited novelty: the learning of the world model (recurrent state-space model) closely follows the prior work of PlaNet.', '* Section 4.2 line 3: “sample the class [for the] from a power law distribution\"', 'The column \"FLOPS\" in the result seems to measure the speedup, whereas the actual FLOPS should be less when the speed increases.', 'This is a thriving area that requires a careful literature review.', 'iii) For MUTAG dataset, the statistical information of this dataset is quite different from', 'Quality - The experiments are well chosen and seem technically sound.', '2. Concisely summarize the contributions of the paper.', 'For the second contribution, there isn’t any evaluation/justification about the quality of the generated questions and how useful this dataset would be in any KB-QA applications.', 'classifications.', 'Another somewhat jarring fact about the alt-az convolution is that it is not well defined on the south pole.', '-\\tThe paper has a specific form of formulation for abductive reasoning, where there are exactly two observations and one proceeds the other; the explanation happens in between. I can see this helps collect and annotate data, but also limit the form of abductive reasoning and how models should be developed.', \"It's an intuitive idea to weight different source domains in multi-source domain adaptation.\", 'Overall, I am ambivalent.', '* Strengths:', 'Also, in this quote “In Fig. 4(a) we plot the Mean Squared Error (MSE) over the number of iterations of the optimizer for fitting the noisy astronaut image x + η (i.e., FORMULA ...” the formula doesn’t correspond to the text.', '- The digression at the bottom of the first page about neural architecture search seem out of context and interrupts the flow of the introduction.', 'Better literature review to reflect the relevant previous video action recognitions, especially those on video compositional models.', 'Overall I find the idea of the paper interesting and worth publishing, but the exposition of the paper is less than ideal and needs further work.', 'The proposed strategy is demonstrated in stochastic control tasks and reinforcement learning applications.', 'Nevertheless, I believe that it still has to address some points in order to be better suited for publication:', 'is likely due to the input representation, which is lossy, leading to a maximal performance shared by all three methods.', 'The dataset is then used to evaluate a number of recurrent models (LSTM, LSTM+attention, transformer); these are very powerful models for general sequence-sequence tasks, but they are not explicitly tailored to math problems.', '[Weaknesses and Clarifications]:', 'The same applies for the notMNIST data which belong to a completely different data set compared to MNIST and thus out of sample prediction cannot benefit from the mixing; i.e., variations have to be explained by system’s noise.', 'This is potentially useful to practitioners which have to deal with incomplete point clouds acquired by range scanners.', 'I think that more casual ML/RL researchers will find these results controversial and surprising while more experienced researchers will see curiosity-driven learning to be explainable primarily by the intuition of the “The fact that the curiosity reward is often sufficient” paragraph of page 6, demanding more complex environments before accepting that this form of curiosity is particularly useful.', 'Then, numerical experiments using MNIST and Celeb-A datasets are presented.', 'In the abstract and introduction', 'The authors also cleverly adopt/adapt span-BERT which is more suited to this setting.', 'And it can significantly outperform vanilla PPO for environments with sparse rewards.', 'They can indeed be subsumed by generalization bounds based on VC theory.', 'The large number of experiment although welcoming needs to be properly discussed and related to the state of the art numbers, including any work that the authors are referring themselves in this submission.', '- Table 3 is a bit confusing as-is (lower is only better when controlling on the quality of the best sample.', 'With respect to this specific setting, the authors may want to consider [Mirowski, et al., Learning to Navigate in Complex Environments, ICLR17] with respect to auxiliary loss + RL extrinsic rewards to improve performance (in this case, also in maze environments).', 'It would be great if this paper could follow those recommendations to get better insights in the results.', 'Overall, I am in favour of accepting this paper given some clarifications and improving the evaluations.', 'GAN predictions on the other hand usually are more visually appealing but often lack diversity, producing just a few modes.', 'Instead of trying to learn idempotency by gradient descent, you could try to parametrize the canonicalizations with a matrix X, such that C =  X (X^T X)^{-1} X^T. C will be idempotent (although restricted to be symmetric).', 'This could have made the paper much stronger.', '- Detailed experimental setups are missing.', '[C] GEOMetrics: Exploiting Structure for Graph-Encoded Objects. Smith, Fujimoto, Romero and Meger. ICML 2019.', 'Then use the same network to extract the features and then using the nearest neighbor to retrieve the classes.', 'The authors do a nice lifting via complex operations, and both the hyperbolic and spherical spaces can devolve into the flat Euclidean space when their curvature goes to 0.', '[2] Srinivas, Aravind, et al. \"Universal planning networks.\" arXiv preprint arXiv:1804.00645 (2018).', 'A. You demonstrate the results on CIFAR-10 for 10% error rates which corresponds to networks which are far from what is currently used in deep learning.', 'Why does it go to nearly zero around x = 0, while being higher in surrounding regions with more data?', '==========', 'The experiments show', '# Strengths', 'Emergence of simple-cell receptive field properties by learning a sparse code for natural images.', 'I believe that a more challenging experiment should be conducted e.g. using celebA dataset.', 'Section 3.', 'Summary:', '-\\xa0The authors propose to rank the unlabeled samples of each domain according to the entropy of their classification of the current classifier.', '3. In the experiments, there are large discrepancies between different optimizers on Cakewalk (e.g. SGA vs AdaGrad, Table 4).', 'Some other points:', 'Also, the author should avoid using the word \"simply\" too often (see the last few paragraphs on page 5).', 'For the node-level pretraining, the goal is to map nodes with similar surrounding structures to nearby context (similarly to word2vec).', '- In section 4.3, what happens if we transfer the learned discriminator to Pick&Place from Push that has a gripper fixed to be closed, rather than the opposite direction (i.e. from Pick&Place to Push)? Does the MISC-t still well work? Can the learned MI discriminator be transferred to different tasks even when the state space is different?', 'A method to predict likely type of program variables in TypeScript is presented.', 'This leads to over-confident predictions which is problematic particularly in an active learning scenario.', '3. Could authors explain the time complexity of inner loop in Algorithm 1? Does it take more time than that of updating model parameters?', '* The authors duly acknowledge that ME bias is not necessarily desirable in all circumstances, namely in tasks that feature many-to-one mappings, citing polysemy and synonymy in machine translation as examples.', 'Weakness:', 'I have spent a lot of effort with MCTS. I can not find the corresponding allowance for stochastic jumps in the latent space long horizon learning.', 'More motivation for experimental section is needed.', 'Can we avoid having the PL condition?', '[1] Azizian, Waïss, et al. \"A Tight and Unified Analysis of Extragradient for a Whole Spectrum of Differentiable Games.\" arXiv preprint arXiv:1906.05945 (2019).', 'The paper goes on to test the method on a digit classification task, where the model is trained to learn a representation in a simulator for SVHN data where the transformations to be canonicalized can be controlled, and  used to train a classifier on unseen real data from the SVHN test set.', 'The method produces better results using ensemble learning.', 'Description', '#', 'g. The author mentioned, in Table , the last two rows serve the upper bound for other methods.', 'But I think there are lots of useful things you can do without that capability, e.g. do 3D point cloud completion, go image -> structure, etc.', 'I missed the point in the following sentence: “For each one-step prediction, the network has the freedom to choose to copy pixels from the previous frame, used transformed versions of the previous frame, or to synthesize pixels from scratch” .', 'Finally, I found the connection to CO valuable.', '+1 Simple and straightforward method with pretty good results on language translation.', 'And the experimental evaluations of this part are convincing and compare favourably with other state-of-the-art methods.', 'Based on these merits, I suggest this paper to be accepted.', 'Overall, while I think the computational cost of the proposed method is high, rendering it less practical at this point, I believe the approach has potential and the result obtained so far is already significant.', 'It is also unclear how the calculation of relative entropy \"D\" was performed in figure 3.', 'The paper is well written and the equations easy to follow.', 'However, the particular link here was interesting, and I appreciate the small number of parameters resulting in solid reconstruction performance.', 'My main concern with the paper is its limited applicability to robotic manipulation tasks with a clear divide between states of interest vs others.', 'For example, in Section 3.2, it can be made clear that the domain of the quantization function is the real and the codomain is a sequence k bits.', 'This paper proposes to extend VAE-GAN from the static image generation setting to the video generation setting.', 'The introduction contains too much related work, which should be divided in another section.', 'In the absence of citation or comparison with any of the prior work on multivariate statistical dependence testing, the current submission is not suitable for publication.', 'The evaluation of mathematical reasoning ability is an interesting perspective.', 'Some statements don\\'t make sense, however, eg. \"HSIC-based estimators tend to have loose confidence intervals due to the need to bound generalization error of kernels f and g on unseen data points.', \"I'd like to see more of a discussion of *prior data sets* rather than papers proposing models for problems.\", 'The objective of the paper is to extend the problem settings for which there is last iterate min-max convergence rates, which now exist for bilinear, strongly convex-strongly concave, and convex-strongly concave problems.', 'The evaluation compares different variants of this model to two recent VAE baselines.', 'Queries/ points that need some clarification', '#', '-\\tComputational budget required is massive.', 'First, the base class is classified into K super classes based on the spectral embedding of the graph (onto distributions over the corresponding graph spectrum) and the k-means algorithm with the Wasserstein metric.', '- offers minimal (but some) evidence that curiosity-based reward works in more realistic settings', 'The authors found that in many of the tasks, learning based on intrinsic rewards could generate good performance on extrinsic rewards, when the intrinsic rewards and extrinsic rewards are correlated.', 'In one of the experiments, the authors train neural networks over a concatenation of IP address embeddings.', 'I am not very familiar with this line of research so my comments will be more general in this case.', '- Perhaps because the authors study a very specific kind of question, they limit their analysis to only three modules and two structures (tree & chain).', 'The question asked is whether or not pretrained mean-field RBMs can help in preventing adversarial attacks.', '(2) The proposed approach produced effective empirical results.', '- Section 2.2: Similarly to the above question, would there be a way to incorporate the BERT principles directly into an end-to-end model, e.g. by randomly masking some of the continuous input speech?', 'Pros:  Shows how to make full matrix preconditioning efficient, via the use of clever linear algebra, and GPU computations.', '[-] The results indicate that SAVP offers a trade-off between the properties of GANs and VAEs, but does not go beyond its individual parts.', '- Does the MonoGAN exhibit stable training dynamics comparable to training WGAN on CIFAR-10, or do the training dynamics change on the single-image data set?', 'Some numerical studies were reported to confirm theoretical findings.', '-', '\"Regularizing by stopping early for regularization,\"', 'Another question which is not answered in the paper is the number $k$ of images to select for each pair of classifiers. Is this number task-dependent? Is it related to the number of classes?', 'Studying label propagation in the meta-learning setting is interesting and novel.', \"If the authors don't discuss a motivation then how will a reader know how to apply the tool?\", '- In Eq. (4), T(x_1:N, y_1:N) is assumed to be decomposable into the sum of T(x_t, y_t) / N. Can this make the lower bound (Eq. (3)) arbitrarily loose since the class of functions becomes very limited?', '(1) The experimental results cannot show the usefulness of the proposed GCN.', 'Right now, the numbers reported in Cohen et al. and Esteves et al. are copied over, but there are probably many differences between the precise setup and architectures used in these papers.', 'However, the original performance of the hand-engineered design is surprisingly bad (see first data point in any plot in Figure-4).', 'This claim can be made secondarily or as motivation for continued exploration along this direction, but I think listing them as two distinct contributions is necessary.', 'network.', '- I understand that no data augmentation was used during training?', 'They compare this to using log-mel filterbanks.', 'The explanation given in this work is based on Gaussian mean-field approximation.', 'Therefore, I don’t find interesting to report how DDGC improve upon “no baseline”, because known methods do even better.', 'This paper proposes an evaluation method for confidence thresholding defense models, as well as a new approach for generating of adversarial examples by choosing the wrong class with the most confidence when employing targeted attacks.', 'The authors also provide extensive supplementary material (around 20 pages) with detailed derivations and descriptions of experiments.', '- Below eq. 5, D_R^{-1} should equal D_R(-omega, -nu, -phi).', '[2] Zhang, Richard, Phillip Isola, and Alexei A. Efros. \"Colorful image colorization.\" European conference on computer vision. Springer, Cham, 2016.', '-1 The mathematics in section 3.1 is unclear and potentially flawed (more below).', 'In this paper, they aim to learn useful feature representations without meaningful low-level input representations, e.g. just an instance ID.', '- For the cart-pole task, the paper states that the reward is modified \"to exclude any cost objective\".', 'The main concern is the gap between the problem formulation and the actual optimization problem in Eq 12.', 'The authors point out that an extension of the gyrovector space formalization to spaces of constant positive curvature (spherical) is required, and with the corresponding formalization for hyperbolic spaces, one can arrive at a unified formalism that can interpolate smoothly between all geometries of constant curvature.', 'at 90\\\\%\". Is this training or validation accuracy?', 'I would like to suggest to use the state-of-the-arts classifier for the principal task and to evaluate how much gain your method can get with the help of auxiliary tasks.', '[2] Liang et al. Learning Representations from Imperfect Time Series Data via Tensor Rank Regularization, ACL 2019', '“vedio” op page 4, “circile” on page 5, “condct” on page 8, etc.', 'In PGM L_inf adversarial training/attack (column 3 of Figure 5), the prediction accuracy is roughly 50% under 0.1 infinity norm perturbation.', ', it mentions that the conventional k-bit quantization was tried and significant accuracy drops were observed.', 'Even more importantly, very recently there has been a number of papers investigating discrete representations of speech; see the review [2].', 'I suggest trying some other STOA attack methods (e.g., iterative methods).', '5. To train \\\\sigma and \\\\omega, the negative instances are selected randomly.', 'MUTAG is a standard dataset for testing graph-level classification for', '1. Could you comment on the differences in your setup in Section 4.1 compared to the VAEAC paper? I’ve noticed that the results you report for this method significantly differ from the original paper, e.g. for VAEAC on Phishing dataset you report PFC of 0.24, whereas the original paper reports 0.394; for Mushroom it’s 0.403 vs. 0.244. I’ve compared the experimental details yet couldn’t find any differences, for example the missing rate is 0.5 in both papers.', 'This paper uses graph network to train each morphology using RL.', 'The paper shows that with small amount of Gaussian multiplicative dropout, the algorithm can achieve the state-of-the-art results on benchmark environments.', 'https://openreview.net/pdf?id=HJDBUF5le', '- Sec 3.4 can you recap all the parameters after eq.11? going through Sec 3.2 and 2.2 to find them is quite annoying.', \"As an example, consider S^2 and the mean of two antipodal points on it---there's many choices for the midpoint.\", 'The contributions of this paper are to add datapoints based on the prediction of the confidence level by a separate selection network and a number of heuristics applied for better selection.', 'While the invertible model structure itself is essentially the same as Real-NVP, the use of observation variables in the framework with theoretically sound bidirectional training for safe use of the seemingly naïve inclusion of y (i.e., y and z can be independent).', 'The paper is clearly written.', '(5/10)', '- My main concern is reproducibility: the authors employ a number of large architectures, complex loss functions, and regularizers / \"additional improvements\".', '.', 'We should see the performance on other datasets  (e.g., some of the other datasets in Wu et al. (2018)).', '------------', 'Strengths:', '2: By examing different modules, the proposed method is more interpretable compare to canonical methods.', 'Comments:', 'However, because this matrix turns out to be a pxp matrix where p is the number of parameters in the model, maintaining and performing linear algebra with this pxp matrix is computationally intensive.', 'The authors need to describe in detail the algorithmic novelty of their work.', 'In addition, HSIC is a non-adaptive test, but your test is adaptive, so a fairer comparison would be to a modern adaptive test such as \"An Adaptive Test of Independence with Analytic Kernel Embeddings.\"', 'The ASR datasets used in this work are relatively clean (but there does exists domain difference between them).', 'Cons:', '- Have you checked / visualised what type of weight distributions do CDNs capture? It would be interesting to see if e.g. the marginal (across the dataset) weight distribution at each layer has any multimodality as that could hint that the network learns to properly specialise to individual data points.', '1. Versteegh, M., Anguera, X., Jansen, A. & Dupoux, E. (2016). The Zero Resource Speech Challenge 2015: Proposed Approaches and Results. In SLTU-2016 Procedia Computer Science, 81, (pp 67-72).', 'Doing this requires, in particular, developing a reasonable way to perform these operations in spherical space (since Euclidean is trivial and hyperbolic has been recently worked on).', '[4] J. Snoek and O. Rippel and K. Swersky and R. Kiros and N. Satish and N. Sundaram and M. Patwary and Prabhat and R. Adams', 'The paper talks about calculating various statistics over data streams.', 'This variant has the advantage that it provides a valid lower bound on the marginal likelihood, and exploits the well understood variational inference machinery.', 'The paper mentions model use from 128-256 TPUs, which severely limits reproducibility of results.', 'The images are well-presented and well-explained by the captions and the text.', '-\\tOne of the big pluses of this work is that authors try to \"quantify\" each proposed technique with training speed and/or performance improvement. This is really a good practice.', '- little methodological innovation or analytical explanations', 'It is interesting to empirically show that the mode collapse problem of MC-Dropout is important in active learning.', 'The paper selects MDAN, DANN, MDMN as the baselines.', 'Questions to the authors:', 'Cons.', 'i have change my rating from 5 to 6 after reading the numerous and thorough rebuttals from the authors.', '- the problem assumptions are too simplistic and unrealistic (feature distributions of target and auxiliary data are identical), so it is questionable if the proposed algorithm has practical importance', \"However, I feel like the main result is a bit too predictable, and for acceptance I'd like to see a much more detailed exploration of the questions around systematic generalization.\", '- Some of the datasets the authors currently test on are quite toy, especially for the image-based MNIST and SVHN datasets.', 'Then why does the figure says test and train loss? And why DIP loss goes up, it should be able to fit anything, right? If not and it’s a single model that gets fitted on the noisy image and tested on the noiseless image, then how can you estimate the level of noise fitting? ||G(C) - eta|| should be high if G(C) ~= x.', '- The 1+1+...+1 example is pretty intriguing, and could be a nice \"default\" question!', '----------------', 'Overall, the paper does not make a compelling case for the novelty of the problem or approach.', '1) In the abstract, I find the message for motivating the masking from the sentence  \"content based look-up results... which is not present in the key and need to be retrieved.\"  hard to understand by itself. When I first read the abstract, I couldn\\'t understand what the authors wanted to communicate with it. Later in 3.1 it became clear.', '1. The authors find a new angle for learning with noisy labels.', 'Similarity is measured by dot products in the space and', 'The paper proposes doubly sparse, which is a sparse mixture of sparse experts and learns a two-level class hierarchy, for efficient softmax inference.', 'However, as the authors acknowledge the overall simplicity of the tasks being evaluated with mostly marginal improvements makes the overall evaluation fall short.', 'Pros:', 'The key step in the proof is Lemma 2.', 'This is interpreted the learned the learned attention region.', 'Second, the caption mentions that early stopping is beneficial for the proposed method, but I can’t see it from the figure.', 'Is it known / not known in optimization?', '\"is also needed to tune to achieve\"', '\"which is to design to choose the right\"', 'My introduction suggestion: do not talk about Convolutional neural networks (CNNs). There is a *lot* of work on graph convolutional networks (GCNs).', 'Clarity and Quality: The paper is well written.', '- Page 14, Eq(14), \\\\lambda should be s', 'The false positive rate in the sanity check is far below the design level of 0.05.', '- Did the authors inspect generated samples of the baseline and the proposed method? Is there a notable qualitative difference?', 'Section 2.1 is where the method is proposed, yet most of the descriptions there are unclear. Without these details it is impossible to judge the novelty of the \"rule-based concept extractor\", which is the key technical innovation.', 'It would be interesting to see how these heuristics would do without \"selection network\", for example, either by doing simple self-training with thresholding on the score of the classifier or by applying only these heuristics in combination with TempEns+SNTG.', 'This paper is clearly written and in an interesting domain.', 'Weakness:', 'As other works, the solution is based on a proper initialization of the dictionary.', '5) In Theorem 5.2, the term 1/sqrt(2) is missing from the final bound.', '- As said in the my main comments, I am not convinced by the use of the term Axiom. They are not use as building blocks, and are rather used as desirable properties for which the authors prove that only \"path methods\" can satisfy.', 'These algorithms can be seen as preconditioned versions of gradient descent where the preconditioner applied is a matrix of second-order moments of the gradients.', '[+] It reduces computational cost compared to full softmax.', 'In their evaluation, they show that the backdoor attacks generated in this way are more effective, resilient to benign model parameter updates, and also survive better against existing defense algorithms against attacks in federated learning settings.', 'Finally, they consider the Unity maze testbed, combining intrinsic rewards with the end-state goal reward to generate a more dense reward space.', 'The paper is easy to understand but several sections (starting from section 2) could use an english language review (e.g. \"search right\" -> \"search for the right\", \"predict next word\" -> \"predict the next word\", ...) In section 3, can you be more specific about the gains in training versus inference time?', '.', '(2) The method is not well motivated.', 'The MNIST is a relatively simple experiment, and I would like to see how the method works in more challenging problems.', 'I would like to raise several important points:', 'To me this paper is just not good enough - the method essentially i) use \"a professor and two teaching assistants\" to build a \"rule-based concept extractor\" for problems, then ii) map problems into this \"concept space\" and simply treat them as words. There are several problems with this approach.', 'Besides CIFAR 10, authors test this hypothesis in two other datasets while tuning the learning rate.', 'The whole latent vector is passed through a decoder that predicts the mask of observed modalities, and another decoder that predicts the actual values of all modalities.', 'How can you interpret such a thought experiment?', 'They then use these estimates in hypothesis testing, on low dimensional toy datasets and on high dimensional real-world fMRI data.', 'In this paper, the authors propose a framework for continual learning based on explanations for performed classifications of previously learned tasks.', 'The key components of the method are: (1) meta-generator; (2) multi-task evaluator.', 'And honestly, I think the paper reads as if leaning toward the same conclusion.', 'Comments:', '3. The experimental study is weak.', 'At training time, the GAN and the VAE are trained simultaneously with a shared generator; at test time, prediction conditioned on initial frames is performed by sampling from a latent distribution and generating the next frames via an enhanced conv LST .', 'blurry results when making uncertain predictions.', \"In this reviewer's opinion, it would be a lot more reasonable to have instead a learning curve showing the results for, say, 100, 500, 1K, 5K, and 10K labeled examples for all three domains.\", 'Some intuitive explanation on why this should help and/or empirical comparison would be a great addition.', 'Furthermore, the authors propose a new metric for the goodness of a saliency map by taking into account the number of pixels in the map, the average distance between pixels in the map, as well as the prediction probability given only the salient pixels.', 'outperforms or is comparable with recent state-of-the-art approaches on cifar 10, especially in presence of fewer labelled data points.', '.', 'The authors propose PARCUS (\"Pattern Representations on Continuous Spaces\"), a model which computes a soft-matching probability for all words in an input sequence with so-called prototypes in order to predict a label for the input.', 'The problem that the paper tackles is very important and the approach to tackle it id appealing.', 'However, I do not think the metric they introduce is good enough to be recommended in future work, when comparing tunability of optimizers (or other algorithms with hyperparameters).', 'How did you chose the current values? How sensitive is it? Why various datasets need different settings? How the threshold value can be set in practice?', 'The paper proposed a hierarchical framework for problem embedding and intended to apply it to adaptive tutoring.', 'A more economical approach is to use BERT-trained model as initialization for acoustic model training, which is the classical way how RBMs pre-training were used in ASR.', '2. The human score of 91.4% is based on majority vote, which should be compared with an ensemble of deep learning prediction.', 'Comments/Questions:', 'Strengths', '- evaluated models (feed-forward NNs and LSTMs) are very basic and far from current SotA architectures', 'Along this direction, it is problematic that, in the synthetic examples, the relative  performance of methods changes significantly from experiment to experiment and there does not seem to be a simple way to control that.', '- There is no mention about variance of policy gradient estimates.', 'COLT 2015.', '- The paper should refer to 1) the reward shaping literature, 2) the growing line of works concerned with control variates for REINFORCE (such as VIMCO, MuProp, REBAR) and 3) the growing line of works concerned about combinatorial optimization with reinforcement learning (Neural Combinatorial Optimization with Reinforcement Learning, etc.)', 'As a minor weakness, some parts of the paper is not described in enough detail and the motivation is weak or not exactly clear (see detailed comments below).', 'conclusions are made: more parameters in the network implies more', '- for Figure 6, there is not a clear conclusion.', 'I read the response.', 'Comments / questions:', 'Pros.', 'The first is to quantize the weights after pruning, and the second is to further encode the quantized weights.', 'An application to omnidirectional vision might more clearly show the strength of the method, but this would be a lot of work so I do not expect the authors to do that for this paper.', '2. The  objective? The proposed objective,  Eq 5, trades off stochastically approximating the (conditional) marginal likelihood against not deviating too much from p(\\\\theta) =  MN(0, I, I) in the KL sense.', '3) Furthermore, for the experiments only one neural network architecture was considered and it remains an open question, how the presented results translate to other architectures.', 'This result, as the authors claim can be particularly useful in the future for analyzing more general settings (nonconvex-nonconcave min-max problems).', 'The clarity of the presentation (in particular the description of when the method is applicable) and the technical correctness of the paper are somewhat lacking.', 'Is this additional assumption realistic enough and has it been adopted in other previous works? Is there any way to discriminate robot states and object states automatically?', 'Is a simple algorithm enough? What algorithms should we ideally use in practice?', 'an in/out classification task?', 'The same holds for the p(Omega).', 'Skill discovery in this context implies being able to have a single agent execute a variety of learned skills, rather than having one agent per environment with each environment designed to elicit a specific skill.', 'For example, imagine the case of binary classification.', 'The meta-learning estimator seems to involve a significant implementation complexity, for instance heuristic switchs between estimation approaches.', 'However, I have a few concerns about the results.', 'Experiments show that the method substantially outperforms sound typing in the TypeScript compiler, as well as a recent method based on deep neural networks.', 'The method description was a bit confusing and unclear to me.', 'Essentially, the proposed method is a variant of WGAN, which estimates Wasserstein distance with lower bias but may suffer from worse stability.', 'ii) In table 2, I don’t really see any promising results compared to baselines. There are', 'The proposed approach combines a novel measure of the importance of fidelty in each variable to predictive accuracy of the future system state (\"learnable noise risk\") with a flexible functional approximation (neural network).', 'The proof given in the appendix is overly short and not detailed enough.', '#7) The optimization steps seem to be intuitive, however, there is not any actual proof of converge.', 'This restriction exists in Dreamer, and the method cannot be applied to discrete control tasks unless approximation techniques such as Gumbel-softmax are used.', 'The authors test their approach in the some algorithm task from the DNC paper (Copy, Associative Recall and Key-Value Retrieval), and also in the bAbi dataset.', 'One of the main contributions of this work is that the proposed analysis is focusing on last iterate convergence guarantees for the HGD.', 'This is', 'Is there an optional latent vector size across domains or is that optimal size task dependent?', 'The difference seems to be only in that the mapping tau may be different from Q^-1.', 'This paper investigates whether neural networks exhibit a ‘mutual exclusivity (ME) bias’, whereby novel inputs tend to be associated with previously unseen outputs, an inductive bias that is cited to be present in children when learning to associate new words and objects.', '===========================================================================', 'The paper proposes a new method for robustifying a pre-trained model improving its decision boundaries.', 'Post-rebuttal comments', '-\\tThe paper is well structured and easy to follow. It is well written.', 'are not state-of-the-art. There are some unsupervised sentence embedding methods other than the word-embedding based models.', 'There is actually not too much for me to critique and I would suggest this paper should be accepted.', 'The paper is well written in general, the experiments are extensive.', '- experiments are performed using a synthetic setup on a single data set, so it remains unclear if the algorithm would be successful in a real life scenario', '- My biggest concern is that the technical contributions of the paper are not clear at all.', 'This paper presents a deep decoder model which given a target natural image and a random noise tensor learns to decode the noise tensor into the target image by a series of 1x1 convolutions, RELUs, layer wise normalizations and upsampling.', 'While I appreciate the computational burden of testing more images, it does feel that Image A and B are quite cherry-picked in being very visually diverse.', 'Finally, could you show a plot of top-1 prediction of \"classification network\" vs score of \"selection network\" and elaborate on that?', '- First paragraph would be more clear with simple word explanation rather than maths.', 'Compressability is evaluated, but that was already present in the previous work.', 'You motivate some of the work by the fact that the experts have overlapping outputs.', 'The method is applied to vision-based continuous control in 20 tasks in the Deepmind control suite.', 'First, it doesn’t label the X axis.', '1. The proposed model? Is using a conditional weight prior p(\\\\theta | x) (Eq 3) instead of p(\\\\theta) (as in BNNs)  necessary for the inflated uncertainties on OOD data?', 'Any justification?', '- anyway, the theory part acts still more like a decoration. as the author mentioned, the assumption is not realistic.', '- Small typo in the last line of section 4.3 on page 7. It should say: This is in stark contrast with “NMN-Tree” …', 'However, this only affects the method of drawing the samples from a fixed known distribution and should have no more effect on the results than say a choice of a pseudo-random number generator.', '* The actual standard deviation of the noise is not mentioned in any of the experiments (as far as I could tell)', 'What is the first line of Table 4? Is it supposed to be combined with the second? If not, then it is missing results. And is the Pi model missing results or can it not be run on too few labels? If it can’t be run, it would be helpful to state this.', 'The paper observes that the best performing regularizer is the one which minimizes this mutual information.', '3) First paragraph in page 5 uses definition of acronyms DNC-MS and DNC-MDS before they are defined.', 'Furthermore, it may be informative to show the saliency maps in Figure 5 not only for cases in which the learner classified the image correctly in both time steps, but also cases in which the learner classified the image correctly the first time and incorrectly the second time.', '* Review', 'number and quality of the comparisons make it a worthwhile', 'Actually hyperparameters are highly correlated, such as momentum, batch size and learning rate are correlated in terms of effective learning rate [1,2], so as weight decay and learning rate are [3], which means using non-zero momentum is equivalent to using large learning rate as long as the effective learning rate is the same.', 'Page 3, Regularization methods, 3rd line, ````wwwdiscuss', '* Strengths', 'I believe this is because the proof of Theorem 2 is flawed in lines 5-6 on Page 16.', 'An interesting area for future work could be on early stopping techniques for embedding training - it seems that RFs perform well without any training while in some scenarios the IDFs work overall the best.', '…”', 'is very broad and should be watered down.', '[Capacity]', '- There are better words than \"decent\" to describe the performance of DARTS, as it\\'s very similar to the results in this work!', '- The ablation study is good, and the results are impressive.', 'The paper presents both an alternate model as well as an alternate objective function.', 'This paper proposes a self-supervised reinforcement learning approach, Mutual Information-based State-Control (MISC), which maximizes the mutual information between the context states (i.e. robot states) and the states of interest (i.e. states of an object to manipulate).', 'Weaknesses', \"Similarly, Gulcehre et al is a 2019 ICLR paper, and so on. It's always good to get these right.\", 'On the negative side, I think the relevance and novelty of the results should be explained better.', 'For example, EMLNP 2017 paper \"Deep Neural Solver for Math Word Problems\" mentions a size 60K problem dataset.', 'The experimental results are actually less impressive than what are claimed in contribution and conclusion.', 'As such the paper is not convincing.', 'Other concerns:', '* The approach taken to train on partially-observed data is described in three sentences after the Eqn. (10).', '- Additionally, they present interesting observations regarding how sensitive NMNs are to the layout of models.', 'For example, the two regimes mentioned in the paper has been identified by a few other works and the contribution of this paper is just to verify them again.', 'Some other comments: what are the parameters of the degradation in the applications? For example, in image inpainting, does the proposed method learn the mask as well? So it is blind inpainting?', 'despite a simple prior on the local latent variables z?', 'There is nothing strikingly novel in this work, using unlabeled test samples in a transductive way seem to help slightly.', '*', '1) Compare SAVP to the SVG-LP/FP model on a controlled synthetic dataset such as Stochastic Moving MNIST (Denton & Fergus, 2018)?', 'This paper improves it to at most linear dependence and can achieve at logarithmic dependence in some cases, which should be accredited.', '3. The results on fMRI dataset were interesting and showed that the method gives improvements over baselines were the estimates were made on a smaller sized dataset.', '- sigmoid belief networks: q is a Bayesian network where each conditional distribution is a logistic regression model.', 'The paper proposes a method to do math reasoning purely using formula embeddings.', 'This paper introduced a latent space model for reinforcement learning in vision-based control tasks.', 'To my knowledge, sinkhorn iteration is a very popular method to solve OT problem effectively.', 'Two points:', 'One of the main advantages of this approach is efficiency, which allows it to be used on large real-world datasets.', \"Update after authors' response.\", 'Minor comments:', 'This Boltzmann machine is pre-trained in two-stages using mean field inference of the binary latent variables and gradient estimation of the parameters.', 'The paper is easily readable.', '- In figure 5 (a) \"cencept\" should be \"concept\"', 'However, to state the result of Theorem 4.3, k should be bigger than M c_\\\\eta from the dentition of \\\\tilde{\\\\rho}_k^M, as shown under the equation (4).', '#After rebuttal#', 'The authors propose a new on-policy exploration strategy by using a policy with a hierarchy of stochasticity.', '- Where are they? No discussion? No conclusion?', '* The proposed method is clearly motivated', '\"However, in Nature,\" -- no caps', 'Comments:', 'That the model implicitly learns constraints from data is interesting!', '2. The experiments with \"Don\\'t Care\" should go to the experiment section, and the end-to-end results should be present but not the ratio of incorrect bits.', 'Why were zero-sequences necessary in Experiment 1?', '4. Are the authors willing to release the code and data to reproduce the results?', '- The bounds do not immediately apply in the batch normalization setting as used by neural network practitioners, however there are practical ways to link the two settings as pointed out in section 2.4', 'I would suggest rewriting this paragraph to make it clearer and less speculative, and acknowledge that although rotating filters might increase computational complexity, it has often been shown very effective.', 'Cons:', '+ Natural extension of VAE-GANs to video prediction setting', '3. Can the authors show if the decomposition is also useful for trigger patterns that are not necessarily regular shapes?', 'The method is also applied to unsupervised image-to-image \"translation\" tasks.', 'My concern is that is highly dependent  on the order of hyperparameter searched, which could impact the tunability significantly.', 'in the testing phase are only unperturbed versions of the positive', 'Comments:', \"* For the synthetic dataset one-to-one mapping of one-hot vectors it is clear that an ME bias would incur an advantage for classification on a zero-shot basis, i.e. an increased an accuracy for the first time a new input is observed. Is ME bias considered to be useful here because (i) we care directly about improving this type of zero-shot classification, or is it (ii) because it's implicitly assumed that a better initial prediction will lead to faster learning on these examples?\", 'Overall it appears to be a novel and interesting contribution.', '(2) Due to the use of the likelihood ratio trick, the authors use the mean policy as an approximation at Eq 12.', '5. lack of related work:  4D Spatio-Temporal ConvNets: Minkowski Convolutional Neural Networks, CVPR 2019.', 'The paper explores how well different visual reasoning models can learn systematic generalization on a simple binary task.', '4) Figure 3: Are the results averaged over multiple independent runs? If so, how many runs did you perform and could you also report confidence intervals? Since all methods are close to each other, it is hard to estimate how significant the difference is.', 'interesting. They show that regularization data-augmentation helps', 'memorize the image (some sort of reconstruction task) as opposed to', 'Is the number right?', 'This paper introduces a generative model for 3D point clouds.', 'However I find the white-box experiments lacking as almost every method has 100% success rate.', 'They suggest a surrogate reward function that mitigates the variance in the reward, and hence the update size.', 'In particular, it proposes to use the weight of evidence (WE) (from Zintgraf et al 2017) for each task.', 'Wittawat Jitkrittum, Zoltán Szabó, Arthur Gretton ; ICML 2017, PMLR 70:1742-1751', 'The reason is that simpler methods provide just as much information, and do not rely on the need of interpreting the choice of the weights and K. This point is proven in the paper itself, where for example Figure 2 provides a more concrete and easier to interpret information than the tunability metric, similar graphs could be easily provided per dataset.', '-', \"Moreover, as a non-trivial adaptation, the author defined the left matrix multiplication in GCN (that can be regarded as a weighted linear combination of several points on the same Poincare disk) with Ungar's (2010) weighted barycenter, which is from computational geometry but not the machine learning community.\", 'Concerning the experiments of section 4.2, how would the baseline methods of section 4.1 do in this case? Why did you select to study animal vs non-animals sets of classes?', 'Summary:', 'PROS:', 'Minor comments:', 'Chloe Kiddon Luke Zettlemoyer Yejin Choi: https://aclweb.org/anthology/D16-1032', '- In Section 5.2, it is said lambda is tuned by grid-search.', 'Since finding an adversarial with smaller perturbation is a harder problem, it is unclear how the algorithms compare for finding adversarial examples with similar distortion.', 'within neural networks.', 'In terms of actual technical contributions, I believe much less significant.', 'The absolute values are not important for a qualitative interpretation', '[6] S. Sukhbaatar, J. Bruna, M. Paluri, L. Bourdev, and R. Fergus. Training convolutional networks with noisy labels. In ICLR workshop, 2015.', 'Initially, I was a bit puzzled about why SO(3) augmentation seems to reduce accuracy in table 1.', 'Empirically the authors demonstrate that the method can be chained end-to-end to do multiple steps of reasoning purely in the latent space.', \"On the other hand, if these specialized approaches largely fail to outperform general-purpose models, we would have the opposite insights---that these models' benefits are dataset-specific and thus limited.\", 'It would help readers to understand it better if it has a corresponding figure which shows how a matrix is processed through the flowchart.', 'The paper is well-written and clear.', '---', 'could be incorporated into a more sophisticated analysis of the', 'Also, is there an option for \"unsolvable\"?', \"Specifically, prior work considered non-missing data during training, while we can't always guarantee that all the modalities are available.\", '[2] Shai Ben-David, John Blitzer, Koby Crammer, and Fernando Pereira. Analysis of representations for', 'A successful idea of self-supervised learning is to use the output feature map of the trained classification network to generate auxiliary training signals, since it provides extra information about the learned distance beyond the ground-truth labels.', 'The claims of the paper are verified by the experimental results.', 'This paper analyzes the AlphaGo Zero algorithm by showing that the optimal policy corresponds to a Nash equilibrium.', 'Yes', 'Most of my comments are improvements which can be easily included.', \"So clarifying the above question will help to judge the paper's novelty.\", '* Using \"scenarii\" for the plural of \"scenario\" I would say is pretty', 'What is the computation complexity of GO? How to explain the fast speed shown in the experiments?', 'Cons:', 'training. What is memorized I presume depends a lot on the negative', 'From this viewpoint they derive convergence rates for CO on the broader set of problems.', '[5] Deep Bayesian Bandits Showdown: An Empirical Comparison of Bayesian Deep Networks for Thompson Sampling', 'So maybe somewhere in between could be the sweet spot with training for a short while and then fixing the features.', 'However, the fact that models with less parameters perform better than BERT-based models in the low-resource case is not very surprising.', 'Writing', '[1]Narayanaswamy, Siddharth, T. Brooks Paige, Jan-Willem Van de Meent, Alban Desmaison, Noah Goodman, Pushmeet Kohli, Frank Wood, and Philip Torr. \"Learning disentangled representations with semi-supervised deep generative models.\" In Advances in Neural Information Processing Systems, pp. 5925-5935. 2017.', 'Experiment: The experiments are overall good.', 'The authors go on to argue that explicitly designing neural networks to reason by mutual exclusivity could lead to faster and more flexible learning.', 'I think the results as well as the new combination of existing approaches in the paper warrants publication. But it should be amended significantly to situate itself within the existing literature. I therefore award a \"weak accept\".', 'I also have concerns about the independence assumption in their sampling distribution (Section 3.2), and the fact that their experiments use the same set of (untuned) hyperparameters for each method.', 'The results on real datasets are similar to the regular GCN.', 'This illustrates that the conclusion of Theorem 2 may be wrong.', '- Minor typo: in the abstract: \"test spits\" should be \"test splits\"', 'In principle all the algorithms should have a similar bottleneck in each iteration (computing a gradient for the input image), but it would be good to verify this with an iteration count vs success rate (or distortion) plot.', 'Summary:', 'For the maze navigation task the modulated networks perform better than the non-modulated networks, though the effect is less pronounced.', 'The authors consider the few-shot / meta-learning scenario in which the test set of interest is drawn from a different distribution from the training set.', 'Deep 2D->3D is becoming a crowded space and there are many other models that encode image inputs, and many others that perform recursive or composition-based decoding.', 'The authors try to do this with composition, which is good, but I am not sure whether that captures the real important thing - the ability to generalize, say learning to factorise single-variable polynomials and test it on factorising polynomials with multiple variables? And what about the transfer between these tasks (e.g., if a network learns to solve equations with both x and y and also factorise a polynomial with x, can it generalize to the unseen case of factorising a polynomial with both x and y)?', 'The paper paper proposes a mutual information maximization objective for discovering unsupervised robotic manipulation skills.', 'Strong points:', '- The performance gain is not substantial in experiments.', 'You should use an existing ES implementation (e.g., from some well-known package) instead of a naive implementation, and as additional baseline also CMA-ES.', 'The experiments are well designed to verify their hypothesis, although there could be more to make sure those results are not particular to the few selected problems.', 'They approximate this penalty empirically by calculating stochastic finite differences of the generator’s latent variables.', 'There are some very interesting and strong contributions of this manuscript.', '2. The authors claim to be the first NAS algorithm to perform direct search on ImageNet.', 'This is basically analogous to the optimization-based inference proposed by Schott et al. (As a detail, this optimization can be expressed as computation over several layers of a neural net.)', 'In addition, the results seem very weak.', 'seems to cover similar features to those named by', '* The connections to deep learning seem arbitrary in some of the experiments.', \"While the section 4 has a discussion on related papers, there's no systematic experimental comparison across these methods.\", 'As with ensembles, clearly it only helps to have multiple agents (N>2) if the additional agents are distinct from f_1 (again without loss of generality this applies to g as well).', 'The idea is to use GAN to learn the manifold.', 'Because, the results are only shown on one dataset, it is harder to see how one might extend this work to other form of questions on slightly harder datasets.', '- It is unclear how MISC-p is performed. Please elaborate on how MISC-p works for prioritization.', 'This is particularly useful and important due to its applications to graphs and hyper-graphs, as demonstrated in experiments.', 'The paper proposes an approach to adapt hyperparameters online.', 'A sandwiching objective function is proposed to achieve a better estimation of Wasserstein distance.', 'I don’t get the details of the batch normalization used: with respect to which axis the mean and variance are computed?', 'Benchmark on several video classification dataset shows improvement.', \"The first point would be: what's the meaning of synthetically generating training curves other than proving that transformer achieves good performance in modeling discrete distribution? Most practical problems would not have the same distribution as the previously gathered public dataset, thus the data is not representative, and synthetic training curves just does not make sence.\", '.', '- What is dt in Algorithm 1 description?', '- Eq(2)', '1. For the experiment of 1D signal on sine wave, the AA-pooling and F-pooling give the same result?', '\" First, f and g are functions, not kernels.', 'Comments:', 'http://ai.bu.edu/DomainNet/', 'This paper presents non-trivial theoretical results that are worth to be published but as I argue below its has a weak relevance to practice and the applicability of the obtained results is unclear.', 'Strengths:', 'Such high-level introductions require more comprehensive literatures to support.', 'Also, your convergence results appear to rely on strong convexity of the loss.', 'It seems to me that for completeness, Table 4 should include the result of training a supervised network on top of random conv1/2 and Scattering network features, because this experiment is actually testing what we want - performance of the features when fine-tuned for a downstream task.', 'I think the authors should move a portion of the big bar plot (too low resolution, btw) into the main text and discuss it thoroughly.', 'It is true that current meta-learning approaches do not address the problem of domain shift, and as a result, the testing domain has to be the same with the training domain.', 'It would probably help to position the VAE component more precisely w.r.t. one of the two baselines, by indicating the differences.', '- Consequently why did not you compare simple projected gradient method ?', 'post rebuttal: thank your for your detailed reply, I acknowledge your new comparisons with the distributed robustness mechanisms of Krum and Bulyan, too bad time was short to compare with the other measures such as Draco and SignSGD.', 'It is not clear to me what the point of Sec. 5 is, given a trained model, one wants to figure out if an image was present in the training of the model.', 'Authors propose changes in the network architecture to solve all these three issues.', 'The danger is that could hard to make reliable in a wide set of applications.', 'Volume 35, Number 6 (2007), 2769-2794.', '- While the method is presented as jointly learning all the components, in the experimental section it is stated that the embedding network (the meta-learner) and the GAN-based domain adaptation are done separately. Can the authors comment on this further?', 'A brief list of typos:', 'Some additional comments about experiments follow.', '2. The Pose2Frame network is similar to previous works but learns to predict the soft mask to incorporate the complex background and to produce shallow.', 'The 4D convolution is integrated in resnet blocks and implemented via first applying 3D convolution to regular spatio-temporal video volumes and then the compositional space convolution, to leverage existing 3D operators.', 'Am I missing something?', 'The intuitive explanation of the proposed method is given in Section 3.', 'A performance gap exists for deeper layers, suggesting that larger datasets are required for self-supervised learning of useful filters in deeper network layers.', 'The paper starts from a self-attention GAN baseline (Zhang 2018), and proposes:', 'w.r.t. the original RMSprop.', 'It is necessary to test it on datasets with much more fine classes and much-complicated hierarchy, e.g., ImageNet, MS COCO or their subsets, which have ideal class hierarchy structures.', 'Comments:', '====================', 'I would suggest to use the \\\\simeq symbol instead.', 'Authors present an extensive experimental evaluation of the estimator on different challenging machine learning problems.', 'This work is an interesting application of deep learning, but it gives little insight as to why deep networks are able to solve the problem and how to solve ordinal embedding itself.', 'How does performance change for larger k, and what happens if k is larger at testing then on at training, etc.?', '1. The proposed method is novel and explores to use the existing modules as a black box for visual question answering.', '- I question the choice of sections chosen to be in the main paper/appendices.', 'It seems that the baseline SVG makes use of simpler ConLSTM for example.', 'Specifically, the paper answers three questions regarding AGZ: (i) What is the optimal policy that AGZ is trying to learn?', 'Specifically, the proposed method introduces scaling factors to connections between operations, and impose sparse regularizations to prune useless connections in the network.', 'On the other hand, this paper provides specific generalization bounds under three adversarial attack methods, which explains the power of SN under those settings.', '[3] van Laarhoven et al, L2 Regularization versus Batch and Weight Normalization, https://arxiv.org/abs/1706.05350', 'The present work separates the data fitting from the mutual information evaluation to decrease sample complexity, the argument being that the function class is no longer a limiting factor to sample complexity of the mutual information estimation.', 'The authors might also want to compare', 'Also, does this section use the classifier for predicting the dataset, or is the approach reported in the section, the MAT approach?', 'Hence, I would wholeheartedly recommend acceptance of this paper if the authors correct the factual errors (e.g. the claim of SO(3)-equivariance) and provide a clear discussion of the issues.', '-\\tProposed techniques are intuitive and very well motivated', '→ what do the authors mean “', 'Perhaps I missed it, but I believe Dan Ciresan\\'s paper \"Multi-Column Deep Neural Networks for Image Classification\" should be cited.', 'The test set should not used before the best compression scheme is selected.', '- Really would be good to do real-world tests in a more extensive way.', 'One question which is not addressed is the reason for only one RBM layer.', 'However, the experiments feel like they are missing motivation as to why this method is being used.', 'In particular, [B] performs experiments on adversarial examples.', 'It is based on a bound (2), which states that when the KL-divergence between a policy and the optimal policy goes to zero then the return for the policy will approach that of the optimal policy.', 'Here are a few examples: The ICLR citation style needs to use sometimes \\\\citep.', 'Comments:', 'Hence it is unclear how large the running time improvement is compared to a well-tuned baseline.', '- You should add DARTS 1st order to table 1.', 'Embeddings of mathematical theorems and rewrite rules are presented.', \"Why mention it here, if it's not being defined.\", 'The model can then be used by researchers or a reinforcement learning agent to make better hyperparameter choices.', '-\\tCan you elaborate more on why BatchNorm statistics are computed across all devices as opposed to per-device? Was this crucial for best performance?', 'I found that the core technical description was quite brief and would have benefited from simply more detail and space.', '* the results linking the COCO and MINE estimators are interesting.', '4. Why not use continuous actions with a parameterized policy (e.g. Gaussian)?', '- I am not sure what you mean in 5.4 “we omit all grid search/ binary search steps', 'This paper proposes a distributed backdoor attack strategy, framed differently from the previous two main approches (1) the centralised backdoor approach and (2) the (less discussed in the paper) distributed fault tolerance approach (often named \"Byzantine\").', 'The core novel element of the paper is the truncation trick: At train time, the input z is sampled from a normal distribution but at test time, a truncated normal distribution is used: when the magnitude of elements of z are above a certain threshold, they are re-sampled.', 'The author is correct that in many tasks, well-behaved extrinsic rewards are hard to find.', 'A typo in page 6, last line: wth -> with', 'b) Down weighting the KL term by lambda for the VI techniques unfairly biases the comparison.', '3- Is the key that you used only 679 patches containing 98% of occurrences in the dataset as the first stage? What if we vary this percentage? How sensitive are the results? Such experiments could be useful to understand better why your method appears to work well.', 'Typhimurium. Could you explain why your MUTAG is now a single graph and is cast as node', 'Snapshot Ensembles: Train 1, get {M} for free}', 'Section 6:', 'Additionally, authors combined W_U with W_L with a mixture 20:1, i.e., the s in Eqs(6, 13, 14) is smaller than 0.05.', 'Clarity', '1. Can you clarify why the proposed approach is better than the Meta-RevGrad baseline?', 'In particular, it is unclear what the assumption on the size of the unlabelled test set is.', '- Sec 2.2: \"(GNNs) are very effective\" effective at what? what is the metric that you consider?', 'As the second network maps 3D points to 3D points it is composable, which can be used to interpolate between different shapes.', 'This is true, but there are a', '- In the training procedure of \"selection network\" of Sections 3.1, do you use the same datapoints to train a \"classification network\" and \"selection network\"? If it is the case, how do you insure that the \"classification network\" does not learn to fit the data perfectly and thus all labels s_i are 1?', 'They should consider larger-scale datasets including image and text-based like VQA/VCR, or video-based like the datasets in (Tsai et al., ICLR 2019).', '[7] T. Miyato, S. Maeda, M. Koyama, and S. Ishii. Virtual adversarial training: A regularization method for supervised and semi-supervised learning. ICLR, 2016.', 'I enjoyed reading the submission, which is very clearly written, but due to the relatively limited value of the contributions, and excessive focus on the tunability metric which I do not feel is giustified, I slightly lean against acceptance here at ICLR. I do think, however, that it would make a great submission to a smaller venue or workshop.', '* Very well-written, and clear paper', '##', 'This seems incomplete to me: there are tens of thousands (probably more) of exercises and problems available in workbooks for elementary, middle, and high school students.', 'While I like the premise of the paper, I feel that it needs more work.', '1. In figure-4 you mention that entropy regularizer helps to keep the initial ME score. Can you elaborate more about the way in which entropy regularizer is used with regular MLE training?', 'Its abilities to model the posterior distributions of the inputs are supported by both quantitative and qualitative experiments.', '<<Stochastic back-propagation (Rezende et al., 2014; Fan et al., 2015), focusing mainly on re-parameterizable Gaussian random variables and deep latent Gaussian models, exploits the product rule for an integral to derive gradient backpropagation through several continuous random variables.', 'A KL divergence term is attached to the optimization objective to avoid generating trivial and collapsing auxiliary classes.', 'Summarizing, the paper addresses a relevant problem but they do not state which their main contributions are, and reintroduce some ideas previously published in the literature.', 'The key idea is to learn a mapping from the source domain to the target domain.', 'This work seems to be the first attempt to adopt the few-shot learning in graph classification tasks.', 'Typos possible errors spotted along the way:', 'capacity of a network.', '1. As mentioned in the paragraph before Section 4.1, it would be much simpler to consider a single latent embedding space L. In that case, \\\\sigma and \\\\alpha become unnecessary and we only need to train \\\\omega.', '- The authors claim that adding p(m|z) to the objective function (i.e., generating m from the decoder) allows the latent variable to have mask information.', '-\\tThe paper is generally well-written.', '- the authors address most of my concerns.', 'This paper studies \"Noisy Information Bottlenecks\".', 'Another weakness of the paper is that the empirical evaluation is not sufficiently rigorous:', 'That said, in the sort of settings under consideration (low dimensional state variables and environmental variations are simple to create) MISC does appear to be superior to prior work.', 'On the other hand, the MIME-f-ES tends to have a reasonably good failure mode.', 'The first level of sparsity comes from the first expert.', 'Therefore, it may be a good idea for the authors to analyze the correlation between FSM changes and accuracy changes.', 'What I really want this paper to explore is when and why this happens.', 'The paper is well written and easy to follow.', 'It would also be interesting to see if additional assumptions, such as the existence of clusters or separation between clusters, make ordinal embedding simpler and thus tractable.', 'Their experiments confirm all the hypotheses (DSO-NAS can find architectures, having small FLOP counts, having good performances on CIFAR-10 and ImageNet).', 'You say that you \"informally state the main theorem.\"  The level of formality/informality makes it hard to know what is really being said.  You should remove it if it is not worth stating precisely, or state it precisely.  (It\\'s fair to modularize the proof, but as it is it\\'s hard to know what it\\'s saying, except that your method comes with some guarantee that isn\\'t stated.)', 'Detailed experiment setups are provided in the appendix.', 'They end up with a space (SPoSE) with relatively low dimensionality with respect to usual word embeddings (49 dimension) but perhaps not surprising when considering the small size of the words to embed.', 'The proposed method is based on the theoretical extension of the single-source domain discrepancy measure proposed by Mansour et al. 2009 to the multi-source setting.', 'Note: after reading the comments updated by authors, I remain my opinions: even though exact meta-testing data is unseen during training, the domain is seen during training, and therefore it cannot be qualified for being \"meta domain adaptation\".', 'Also, since it should be fairly easy to distinguish between cars and animals or cars and food, it may be more interesting to focus on the heat-maps from along the block diagonal of Figure 5 (a) and talk about what relationships may have been uncovered within the animal or food subsets.', 'Strengths, Weaknesses, Recommendation:', '- Minor, but some of these citations can be updated.', 'Pros:', '- It would be nice to add a figure of random generations.', '[A] Pixel2Mesh: Generating 3D Mesh Models from Single RGB Images. Wang, Zhang, Li, Fu, Liu and Jiang. ECCV 2018.', 'It is interesting to take the perspective from signal processing to give pooling operation in CNN a formal treatment.', 'Finally, Figure 5 is interesting in showing that ‘curiosity + extrinsic’ improves over extrinsic rewards — although this isn’t particularly surprising for maze navigation that has such sparse rewards and can be viewed as something like ‘active exploration’.', 'I think this is a very interesting area to explore, and the paper is clearly written and presented.', 'This paper presents a world model-based approach in which behaviours are optimised by rollouts (i.e. imagination) in latent space.', 'The idea is to interpret some regularization technics as a from of noisy bottleneck, where the mutual information b tween learned parameters and the data is limited through the injection of noise.', 'Besides some issues in the technical details, the major problem of this paper is that it uses the data processing inequality (DPI) in a **wrong** way.', 'This estimator is unbiased, has low variance and, in contrast to other previous approaches, applies to either continuous and discrete random variables.', 'What is the benefit of DEBAL over an ensemble method if both of them do not have Bayesian theoretic support?', 'Authors verify large batch size reduces test accuracy while improving train.', 'Further, it establishes a useful experimental benchmark for this task, and provides what appear to be reasonable results (though this is somewhat difficult to judge due to the lack of baseline approaches).', 'But there are better baselines possible.', '- Can you explain the sentence \"To prevent data being added suddenly, no data was added until 5 iterations\"?', 'This paper researches the pooling operation, which is an important component in convolutional neural networks (CNN) for image classification.', 'The main contribution is the use of a IHT-based strategy to update the coefficients, with a gradient-based update for the dictionary (NOODL algorithm).', 'One restriction of re-parameterized gradients is that the technique is not applicable to discrete random variables.', '#4) I think that the function F has to be differentiable, and this should be mentioned in the text.', 'This paper proposes to combine unsupervised adversarial domain adaptation with prototypical networks and finds that the proposed model performs well on few-shot learning task with domain shift, much better than other few-shot learning baselines that do not consider.', \"* The authors say that “ME can be generalized from applying to 'novel versus familiar’ stimuli to instead handling ‘rare versus frequent’ stimuli” and they cite the fact that neural networks take longer to learn from rare stimuli, suggesting that ME could help for reason (ii) above.\", '(ii) a “decoder” that takes the estimated global latent variable, and a local latent variable, and maps it to an “output” point in the cloud to be produced by the model.', 'For example,  on miniImageNet, TADAM(Oreshkin et al, 2018) reported 58.5 (1-shot) and 76.7(5-shot), which are way better than the results reported in this work.', 'The main idea is to train a LDA on top of the last-layer, or many layers in its ensemble version, making use of a small set of clean labels after training the main model.', 'I believe this paper may stimulate some interesting ideas for other researchers.', 'I will suggest a few improvements:', '(4/10)', 'Furthermore, it has been shown that adding the proposed manifold regularization technique to the training of GAN greatly improves the image quality of generated images (in terms of FID scores and inception scores).', 'Relevance:', '* The issue of possible false positives due to sample dependence in fMRI data has again been ignored in the rebuttal.', 'The paper proposed to use a prior distribution to constraint the network embedding.', 'w.r.t. the baselines, the model behaves well for the “realistic” and “diversity” measures.', 'The Kronecker product between two diagonal matrices results in another diagonal matrix, i.e., diagonal covariance, which implies that the weights within a layer are given by an independent multivariate Gaussian.', 'I would have been interested in \"false detection\" experiments: comparing estimator in a variety of problems where the mutual information is zero, but for different marginal distribution.', 'However, this can be tricky in practice since it requires having an estimate for Q_r(s,a) as a function of the state (in order to ensure that the state-dependent lower bound can indeed be satisfied).', \"This seems a much more natural approach to me, and I'm surprised that you did not start with that.\", 'I take issue with the usage of the phrase \"skill discovery\".', 'Some less important points:', 'This is not correct: in meta-learning, each task is a subset of classes drawn from a ground set of classes, and different tasks are independently sampled.', 'For example in abstract there is a simple claim that is presented too strong: We also demonstrate that training such an augmented CNN with representative out-distribution natural datasets and some interpolated samples allows it to better handle a wide range of unseen out-distribution samples and black-box adversarial examples without training it on any adversaries.', '- it is not defined in which space theta lives, it is not clear what the authors intend with the notation G_theta(u) \\\\sim p(theta).', 'Can the author be more specific about the x-axis in the illustration 1.a and 1.b? If I understand correctly, they are not the number of trails.', 'However, my criticisms remain that the paper is a simple combination of cycle GAN and prototypical networks, and lacks new insights/novelty.', 'These feature representations, however, are learned transformations of low-level input representations, e.g. RGB values of an image.', '#2) Clearly, the efficiency of the method is highly based on the ability of the GAN to approximate well the prior distribution of the noise-free images.', 'I would like to see evidence that the main benefit from batch normalization indeed comes from picking a good adaptive learning rate.', 'see e.g. Draco (Chen et al 2018 SysML), Bulyan (El Mhamdi et al 2018 ICML) and SignSGD (Bernstein et al 2019 ICLR) which have proposed more sophisticated approches to distributed robustness.', \"Paper's positive points\", '- Some assumptions are not explicitly stated.', 'An Adaptive Test of Independence with Analytic Kernel Embeddings', 'A few papers have recently tried to go past embeddings into building non-Euclidean models, requiring the lifting of standard operations in Euclidean space to non-Euclidean settings.', \"Overall I recommend accepting it; I think it's a solid contribution.\", 'They also give a fairly nice introduction to all of these ideas in an extended appendix.', 'I find this paper is interesting and I like the strong results.', 'So the experiments in this paper is also not convincing.', '- \"For spherical functions there is no consistent and well defined convolution operators.\" As discussed above, the issue is quite a bit more subtle. There are exactly two well-defined convolution operators, but they have some characteristics deemed undesirable by the authors.', 'For example, it is unclear to me why some larger models are not amenable to truncation.', '-\\xa0The idea of using discriminators for separating the labeled samples from unlabeled ones that most likely belong\\xa0to extra classes is interesting.', 'Hence, to characterize different tasks, as far as I am concerned, the classification should take both the graph G and the label Y into consideration, instead of solely the graph.', 'More importantly, it is already well established that it is possible to learn, from only a few images, filter sets that resemble the early layers of filters learned by CNNs.', 'These regimes are fairly well covered by previous works (e.g. Belkin et al as well as others).', 'The paper used very restricted Gaussian distributions for the formulation.', '*** Increased to weak accept after discussion of merits of ME bias was improved in the paper ***', 'There is in fact no experimental evidence that the practical advantages of BN are relevant to the results proven.', '- [1] found that the network architecture for self-supervised learning can matter a lot, and that by using a ResNet architecture, performance of SSL methods can be significantly improved.', 'http://arxiv.org/abs/1802.07044', 'Two technical questions:', \"Although the authors mention in the conclusion that it's future work to merge their proposed changes into the sparse DNC, it is hard to know how relevant the improvements are, knowing that there are much better baselines for this task.\", '* Technical Correctness', '2, I would say, the SOTA neural networks fundamentally are just representation learned, i.e., feature representation learning.', 'It would be informative to show the generated images w/ and w/o manifold regularization.', 'Obtaining the normalized marginal density in a BM is very challenging due to the partition function.', '2) Comment on the plausibility of the samples generated by SAVP? Do some samples show imagined objects – implausible interactions for the robotic arm dataset? If so, what would be the advantage over blurry but plausible generations of a VAE?', '- A reference to Appendix C could be given at the beginning of the Experiments section to help the reader find these details more easily.', '5. What does it mean that in training time the model “updates” the location node representation with the encoding of correct span. Do you mean you use the encoding instead?', 'The main difference between Dreamer and SVG is that Dreamer incorporates a latent representation model.', 'The method proposed and the general problem it aims at tackling seem interesting enough, the toy experiments demonstrates well the advantage of the method.', 'Beyond this simplification, I am not clear if that is actually intended by the authors.', 'This statement is ambiguous and potentially unsupported by evidence. how do you define complex? that can or that did discover?', 'Empirical evaluation on three benchmarks against other baselines suggested the advantage of the proposed method.', 'I will be curious to know how it performs.', \"- In Figure 2 it's pretty hard to see the differences between the methods. What exactly is being visualized here? DeepSDF shold be visualizing surface normals vs. HOF which is point clouds, right?\", 'On the digit dataset, the model can outperform the baselines.', 'Originality: It appears to be a novel application of meta-learning.', 'Second, for each super class, the classification is done through a feature extractor and a classifier.', 'This setting is further generalized to multiple sets.', 'The methods are compared with some other word-embedding based methods and showed 100% accuracy in a similarity detection test on a very small dataset.', 'Quality', '2. The author should try to avoid using yellow color in plots.', 'In Lemma 2, the spectral norms of weight matrices and the number of weight parameters are decoupled.', '- very incremental improvements on PTB over a very simple baseline (far from SotA)', 'Also, by combining the proposed regularizer with a classical supervised classifier (via pre-training a GAN and using it for regularization) decreases classification error by 2 to 3%.', 'However, the studies in this paper are still valuable and I strongly recommend continuing on the same direction.', 'Much of the relevant related work is discussed, and this is done in a balanced way.', '##', 'There are a few more references that had similar ideas and might be worth adding: Brabandere et al. \"Dynamic Filter Networks\", Klein et al. \"A dynamic convolutional layer for short range weather prediction\", Riegler et al. \"Conditioned regression models for non-blind single image super-resolution\", and maybe newer works along the line of Su et al. \"Pixel-Adaptive Convolutional Neural Networks\".', 'The topic is similar to safe reinforcement learning.', '+ The simulation results for the Minitaur quadruped robot are performed using a realistic model of the robot.', '- Paper is often hard to follow, and contains a significant number of typos.', 'The exposition is clear and the presented inverse problems as well as demonstrated performance are sufficient.', 'A nice, easy to read paper, with an original idea.', 'However, while the authors made intuitive explanation about the tunability in section 2.2, I did not see the actual plot of the true hyperpaparameter loss surface of each optimizer to verify these intuitions.', 'It would be very interesting to contrast the proposed method with other previously proposed MF based method, in particular using Free energy to approximate the expectation of the model without constraints.', 'My major concern is that the authors assume the hyperparameters to be independent (section 3.2), which is not necessarily true.', 'The paper presents two loss functions at the beginning of section 3.1 - the non-saturating loss and the hinge loss.', 'I am leaning towards weak accepting the paper.', 'In the current experiments there is a comparison only with CO algorithm and SGDA.', 'As noted in the related work section this is not a completely new idea (cf. Schmidhuber, Ha et al.).', 'Section 2.', '(*)', 'Thus, the paper is concerned with unsupervised version of the meta-learning problem under domain shift (i.e., a large amount of data unlabelled are available from the target domain).', 'Nonparametric independence testing via mutual information', \"- In order to pre-train the discriminator network, additional (s,a,s') experiences are required, thus it seems difficult to say that it is better for exploration than VIME.\", '1. In the PTB experiment, it looks like the human only adapts the learning rate and leaves the rest of the hyperparameters alone.', '3) Readability: Consider explaining what is meant by \"warm-up\", \"epoch budget\", \"step budget\" clearly and upfront.', 'The model goes through the following three steps: 1) it reads a sentence at each time step t and identifies the location of each entity via machine reading comprehension model such as DrQA (entities are predefined).', 'Heller, Ruth and Heller, Yair', 'The authors propose a procedure for improving neural mutual information estimates, via a combination of data augmentation and cross validation.', '-The average distortion metric (that’s unfavourable to your method anyway) doesn’t really mean anything as the constraint optimization has no incentive to find a value smaller than the constraint.', 'This paper proposes a method for unsupervised learning  of data representations that can be manipulated to remove factors of variation via linear transformations.', 'Introduction:', 'The authors propose to use the combination of model ensemble and MC dropout in Bayesian deep active learning.', '[8] A. Tarvainen and H. Valpola. Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results.', 'The generalisation from the finite MDN to the continuous CDN seems straightforward, the model is relatively easy to implement and it is evaluated extensively against several modern baselines.', 'The drawbacks  of the work include the following: (1) There is not much technical contribution.', 'There are challenging desiderata involved in building the training+tests sets, and the authors have an interesting and involved methodology to accomplish these.', '* I really like the idea of explicitly modelling the mask/missingness vector. I agree with the authors that this should help a lot with non completely random missingness.', 'Cons:', 'This is an interesting point to discuss (some of the NeurIPS papers I mentioned train the curvature for each layer).', 'There are many typos and mistakes (e.g., the last paragraph of the paper starts with a sentence that does not make any sense), missing references (e.g., there is an empty parenthesis at the end of the second paragraph on the second page) and in at least two cases, there are references to a formula that is not in the manuscript (e.g., reference to formula 15 on line 3 of page 5).', 'However, their algorithm--while much less computationally expensive than true full-matrix adaptive preconditioning---is still far more expensive than the usual diagonal version.', 'This work is interesting since the authors use dropout for policy learning and exploration.', 'is well-motivated by the \"researcher example\" given throughout the paper.', 'I don’t have any comments except the following ones.', 'I don’t think it is of any practical use to show that a new algorithm (such at DDGD) provide some defence compared to no defence.', \"Why couldn't we take another layer's neuron as the neuron of interest, bounding the conductance measure on one layer as the input and the output of the model? If we make the input to be a neuron y rather than the true input x, we could take another neuron z in a subsequent layer to be the neuron of interest, resulting in conductance measure Cond^z_i(y).\", 'The authors consider a model wherein the weights of the network (\\\\theta) are drawn from a matrix normal distribution whose parameters are in-turn a (non-linear; parameterized by a another network) function of the covariates (x).', '# Strengths:', 'For instance, the SHREC17 results are on par with Cohen et al. and Esteves et al., presumably at a significantly reduced computational cost.', 'Experimental results show significant improvement over vanilla adversarial training.', '7. In Table 4, the accuracy of number on Zhang et.al is 49.39, which is higher than other methods, while on test-dev, the accuracy is 51.62, which is lower than others.', 'This is great, but could you also report the number when the full dataset is used?', 'The generator is encouraged to be smooth using an orthogonal regularization term.', 'What is the benefit of using DL algorithms within the oracle-augmented datastream model?', 'It would be good to see some more insight in order to relate to interpretability of the importance of neurons, although there has been no claims made on it as its hard to measure importance without interpretability.', 'Since a direct application of video prediction is model-based planning, it seems that plausibility might be as important as sample quality.', '- The baseline and added modifications are well presented and clearly explained.', '* Thorough theoretical explanation', '- Pg. 5, Section 3.4: \"...this is would achieve...\"', 'The basic idea is to train the classification network to predict fine-level auxiliary labels in addition to the ground-truth coarse label, where the auxiliary labels used in training is generated by a generator network.', 'Somehow, those connections are not clear to me.', '1- Why not use a single Boltzmann machine with 128 fully connected latent variables? Could you add this experiment please.', 'The originality stems from a new training criterion which combines a VAE and a GAN criteria.', 'Furthermore PTB is not a \"challenging\" LM benchmark.', '- Measuring in GPU days is only meaningful if you use the same GPU make for every experiment. Which did you use?', 'Similar experiments are conducted on the Loan text dataset, using appropriate analogs of local triggers, with similar results.', 'In most of experiments, the baseline methods, i.e. RMSProp are used with fixed rates.', 'I believe these experiments are novel and the results are interesting.', 'However, the paper contains only little novelty and does not provide sufficiently new scientific insights.', 'Cons: While doing this leads to better convergence, each update is still very expensive compared to standard SGD, and for instance on vision tasks the algorithm needs to run for almost double the time to get similar accuracies as an SGD, adam solver.', 'This network takes some identifying information (ID, shape etc.) about body part as input.', 'Authors provide a variant of WGAN, called PC-GAN, to generate 3D point clouds.', '- Although disc can be easily estimated in the regression task (differently from d_A distance which is a special case of disc), there are no experimental results of the regression task even in the synthetic data.', 'In the discriminator, the sudden increase of the ratio of first/second singular value of weight matrices indicate collapse in GANs.', '- Last paragraph page 4: \"when the accuracy gets over 60\\\\% and again', '+ Large scale experiments are reported on modern architectures.', 'Therefore, an interesting baseline for the evaluation of the ICL approach would be a predefined, fixed attention map consisting of concentric circles with the image center as their center, to show that the proposed approach does more than just deemphasizing the corners of the image)', 'Overall, this paper seems like a solid contribution to the literature.', 'In this sense, claims about the low-variance of GO-gradient wrt to other reparametrization baed estimators should be removed, as they are the same.', 'It would be nice to have a version of Figure 6 with k = 6, so that one can see all feature maps (in contrast to a subset of them).', 'Consequently, I think this is a nice paper and should be accepted.', 'Suggestions/questions:', 'This paper presents a new method to measure the importance of hidden neurons in deep neural networks.', 'Moreover, the sparse DNC (Rae el at., 2016) is already a much better performant in this task.', 'Hence, the claims in multiple places of this paper and the names for the two networks are misleading.', 'The set of points on the sphere can only be viewed as the quotient SO(3)/S(2).', 'Comments:', 'The work is now improving, and is on the right track for publication at a future conference.', 'The authors argue that not knowing the distribution of rewards observed in the policy gradient algorithm hinders learning (and the tuning process).', 'what I used to use.', 'The overall approach has a flavor of genetical algorithms, as it also performs evolutionary operations on the graph, but it also allows for a better mechanism for policy sharing across the different topologies, which is nice.', 'They find that not all pre-training strategies work well and can in fact lead to negative transfer.', '- the sentence \"Bacause the identity of the datapoint can never be learned by ...\" What is the identity of a dat point?', '1.  I work at the intersection of machine learning and biological vision', \"Overall I like the paper, it's systematic and follows a series of practical considerations and step-by-step experimentations.\", 'DiVA modifies the objective function of VAE by introducing an additional term that maximizes the mutual information between the latent variables and the class labels.', 'Here the end goal is less clear; this is understandable in the sense that the work is positioned as a piece in a grand objective, but it would seem valuable to nevertheless describe some concrete example(s) to elucidate this aspect of the algorithm (use case / error effects downstream).', 'Does this imply any topological constraints?', 'The authors invite five volunteer graduate students to annotate the selected example.', 'Did the authors also examine the relationship between mutual information and generalization error for CIFAR data sets? Does it not make sense to examine this for all (most of) the setups considered in Table 4, 8, and 9.', 'My conjecture is that the observed improvements are mainly due to the softness of the auxiliary labels, which has been proved by model compression/knowledge distillation and recent \"born-again neural networks\".', \"Edit (leaving everything else unchanged for now): After reading R3's assessment, I agree with them that it's worrying that the Deterministic Meta-Dropout performs better than baseline MAML - maybe it's an effect of a larger number of parameters in the model?\", 'a perturbed positive image considered a positive training image? And', 'The presented approach is demonstrated on a cart-pole swing-up task as well as a quadruped locomotion task.', '3: The experiment results are good, especially for the counting problem.', '- VAE, GAN: q is the generative model defined as a mapping of a standard multivariate normal distribution by a NN.', 'Offline gradient descent and stochastic gradient descent cases are considered.', 'The authors should rethink the structure of the experimental section from the standpoint of convincing someone to use this method.', 'Summary:', 'The authors provided code, which could help other researchers reproduce their result.', 'I would suggest, at least, to include some empirical evidences in the experiments that show convergence.', 'Update after rebuttal:', 'Because of the above many discussions about discrete vs. continuous variables are missleading.', '=> Robot design area has been explored extensively in classical work of Sims (1994) etc. using ES.', '+ The problem of designing generative models for 3D data is important.', 'Also, I find the experiments done in section 3 and 4 are similar to previous works and even the conclusions are similar.', 'capacity of a network in more', \"I'd be interested to hear if the authors see a connection between their formalism and the one of Reference prior in Bayesian inference (Bernardo et al https://arxiv.org/pdf/0904.0156)\", 'Compared to using just a very simple dense reward (e.g. negative L2 distance between the robot and the object), what would be the advantage of using MI discriminator? It would be great to show the comparison between using simple dense reward and MI discriminator for each Push, Pick&Place, and Slide task.', 'Please proof read your paper and fix these.', '6. While this paper is not especially surprising or ground breaking, the', '6. Similar as above question, on the object counting task, is there a way to compare with previous counting methods?', 'This claim is bigger than just CNNs and needs to be studied in a theoretical framework not an empirical one.', 'The main focus in experiments and the exposition is on 3D point clouds representing object shapes (seems the surface, but could also be the interior of objects, please clarify).', 'The reparameterization trick does not apply to all continuous random variables, only to such that the reparameterization satisfies certain smoothness conditions.', '- Abstract Scenes VQA dataset introduced in“Yin and Yang: Balancing and Answering Binary Visual Questions” by Zhang and Goyal et al. They provide a balanced dataset in which there are a pairs of scenes for every question, such that the answer to the question is “yes” for one scene, and “no” for the other for the exact same question.', 'The paper proposes a neural-network-based estimation of mutual information, following the earlier line of work in [A].', 'Similarly, figure 3 as well as figures 5-7 and 8 in the appendix provide very good information about the tunability of the various optimizers without using the introduced metrics.', 'I believe that this paper is not the first one to study question generation from logical form (cf. Guo et al, 2018 as cited), so it is unclear what is the contribution of this paper in that respect.', 'Again, discussions are lacking, and it doesn’t seem the authors tried to understand why such behaviors were shown.', 'it better to re-organize the paper and devote more of it to the', 'Regarding the practical limitation of this work:', ',  the paper motivates the multi-source domain adaptation (MSDA) problem by arguing that the MSDA has a lot of real applications.', '- page 5, paragraph 3: \"we from some\" -> \"we start from some\"', 'Summary:', 'I agree that local SO(2) invariance is too limiting. But it is not true that rotating filters is not effective in planar/volumetric CNNs, as shown by many recent papers on equivariant networks.', '1. A new few shot learning with domain shift problem is studied in the paper.', 'The paper also introduces an efficient way of processing geodesic / icosahedral spherical grids, avoiding complicated spectral algorithms.', 'Strong points:', 'Are there any other advantages for F-pooling s.t. people might want to use it as opposed to AA-pooling?', 'I don’t understand the claim that “GANs prioritize matching joint distributions of pixels over per-pixel reconstruction” and its implication that VAEs do not prioritize joint distribution matching.', 'What if the proposed method performs that much better than baselines but they hyperparameters are not set correctly?', '2) In equation (8), lambda is a trade-off between cost and return.', 'The total error in estimating the mutual information must take this error in account, and not only the validation error.', 'Finally, the paper conduct experiments on sentimental analysis benchmark, Amazon Review and digit datasets.', 'However, given that the datasets used in the experiments were not used in the associated benchmark papers, it is necessary for authors to explain how they trained competing models.', 'The authors propose three improvements to the DNC model: masked attention, erasion of de-allocated elements, and sharpened temporal links --- and show that this allows the model to solve synthetic memory tasks faster and with better precision.', '- the use of shared embeddings for the class conditioned batch norm layers, orthonormal regularization and hierarchical latent space bring an additional boost of IS 99.', 'However, all experiments only show results in the MCAR setting, so the claim is not experimentally validated.', 'Strengths:', 'On the contrary, in this work, they decompose the global pattern into several small local patterns, and each adversarial party only uses a local pattern to generate poisoning samples.', 'The proposed method achieves better uncertainty estimates than a single Bayesian neural networks model and improves upon the baseline in an active learning setting for image classification.', 'what I found most interesting in the paper is Section 3.4, presenting an appreciable attempt to \"interpret\" poisoning. Together with Section 4.', '3. Evaluate the quality and composition of the work.', 'As is, the counter example proof itself is quite confusing: it would really help if the proof was more formal.', '- What are the practical implication of your work ? for instance does it say anything on how to tune $\\\\gamma$ for CO ?', 'Does it mean that the W_s is not as stable as W_L or W_U during training?', 'All architectures in the search space thus share their weights, like ENAS (Pham et al 2018) and DARTS (Liu et al 2018a).', '- some parts of the paper are quite unclear', 'There are a large variety of modules, and trying to not generate either trivial or impossible problems is a plus in my opinion.', 'In the introduction, the paper motivates the multi-source domain adaptation (MSDA) problem by arguing that the MSDA has a lot of real applications.', 'This paper presents a method to update hyper-parameters (e.g. learning rate) before updating of model parameters.', 'MUTAG is a dataset of 188 mutagenic aromatic and', 'Further, I could not find what k was set to in the evaluation of Tab. 1.', 'and', 'Summary of the paper:', 'It seems to me; the author assumed data from one modality is generated by all the latent factors (see Eq. (11)), then what is the point for assuming the prior of the latent factor is factorized (see Eq. (4) and (5))?', '* The paper states that “it can model the joint distribution of the data and the mask together and avoid limiting assumptions such as MCAR”.', '2. Annotation is not clear in this paper.', 'This work presents Backpropamine, a neuromodulated plastic LSTM training regime.', 'From the perspective of a purely technical contribution, there are fewer exciting results.', 'With the additional experiments (testing a new image, testing fine-tuning of hand-crafted features), additions to related work, and clarifications, I am happy to raise my score to accept.', \"6. Did you consider using an inverse function (say G), that maps an embedding in L / L' back to S (i.e. the inverse function of gamma / gamma').\", '-\\xa0The paper was clear, well written, well-motivated and nicely structured.', 'A special emphasis is put on the quantitative evaluation: several criteria are introduced for characterizing different properties of the models with a focus on diversity.', 'Note that there is no direct correspondence between the loss of BN-normalized network (2) and the loss of the original network because of dependence of the normalization on the batches.', 'The paper presents a Neural Network based method for learning ordinal embeddings only from triplet comparisons.', 'Weaknesses:', 'The demonstration on practical examples is a plus.', 'controlling the effective capacity of a network.', '- the sufficient bilinearity condition are hard to meet in practice. (even for convex-concave problems)', 'These need to be further clarified.', '[3] Bartlett, Peter L., Dylan J. Foster, and Matus J. Telgarsky. \"Spectrally-normalized margin bounds for neural networks.\" Advances in Neural Information Processing Systems. 2017.', 'The mixing distribution is also parameterised by a neural network.', 'Yeh, Raymond A., et al. \"Image Restoration with Deep Generative Models.\" 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2018.', 'This paper proposes a self-auxiliary-training method that aims to improve the generalization performance of simple supervised learning.', 'In addition, the authors introduce retroactive modulation that basically allows the system to delay incorporation of plasticity updates via so eligibility traces.', 'At the very least we need another \"partial\" sign in front of the \"\\\\delta\" function in the numerator.', 'I have no idea that what is the propose of defining a new evaluation method and how this new evaluation method helps in the further design of the MaxConfidence method.', '5. Is it reasonable to assume that the learning dynamics of all deep learning architectures are similar enough that a model trained on one set of deep learning architectures and problems will generalize to new architectures and problems?', 'Two new large-scale pre-training datasets are created and extensive experiments are conducted to demonstrate the benefits of PT upon different GNN architectures.', '* Summary', 'progress of variance matrices between layers.', 'The tasks are explicitly designed to exploit these additional parameters - so framing the synthetic experiments as, \"here are some simple functions for which we would need the additional parameters that we define\" makes sense; but arguing that Hartford et al. \"fail approximating rather simple functions\" (page 7) is misleading because the functions are precisely the functions on which you would expect Hartford et al. to fail (because it\\'s designed for a different setting).', 'Specifically, the method is derived via batch normalization (BN), i.e. the learnable scale and shift parameters in BN are assumed to depend on z, through a small one-hidden layer MLP.', 'What if you used simpler online learning algorithms with formal accuracy guarantees?', 'datasets such as ImageNet22k and modern deep ConvNets architectures', '- The equation (1) should hold for any \\\\theta’, not \\\\theta.', 'Evaluation on real world datasets are proposed to show the efficiency of the proposed method.', '-\\tIncreasing batch size (8x) and model size (2x)', '6. In Table 3, the figure in bold is not the lowest (best) in the table.', '- clarification of the truncation trick: if the elements of z are re-sampled and are still above the threshold, are they re-sampled again and again until they are all below the given threshold?', 'The idea shows clear liaison to the \"differential testing\" concept in software engineering besides the cited work of perceptual quality assessment.', 'I also think the explanation in Sec. 3.2, while kind of obvious, is a nice way think about decoder vs. fast weights.', 'There are now dozens of defence methods that work (partially) for improving robustness.', 'This result is beyond machine learning and unfortunately I cannot evaluate its merit.', '###', 'Although there is a clear improvement in FID scores for Cifar10.', '[3] Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wortman', \"***After the authors' response****\", \"The author's present a dual learning framework that, instead of using a single mapping for each mapping task between two respective domains, the authors learn multiple diverse mappings.\", 'All of the testbeds have been used previously.', 'They first train a small mean field boltzmann machine on 4x4 patches of MNIST, then combine 4 of these into a larger 8x8 feature extractor.', 'Major points:', 'In other terms, the control of th 1 does not control the estimation error of T. This is the reason why it is independent from the function class.', '- for the experiment with Imagenet images, it is not very clear how many pictures are used. Is this number 2500?', 'Otherwise, some poorly performed models could lead to near-random or adversarial inter-model discrepancies, failing the proposed approach.', 'This dataset seems valuable. And the paper is simple and well-written.', '- Abstract: \"to extract non-trivial features\".', 'There are 3 phases of optimization:', 'Questions: I would like to see more discussion about difference between this work and [Z Hu, arXiv:1905.13728].', 'This expedites the score function evaluation improving the time complexity of the evolutionary process.', 'When a new image is presented, how is the probability mass distributed across previously seen classes versus across unseen ones?', '3. One concern I have with discrete representation is how robust they are wrt different dataset.', \"The authors discuss their results on k-medoids in the appendices, but it seems like these results aren't quite complete yet.\", 'The proposed improvements seem to result in very clear improvements.', 'I think that this paper makes a good contribution by establishing a benchmark and providing some preliminary results.', 'A 40-question exam for 16 year olds is probably far too challenging for the current state of general recurrent models. Can you add some additional grades here, and more questions?', 'Results are much lower in less favorable cases, sometimes close to random (see last line of Table 3).', 'This paper explores maximally expressive linear layers for jointly exchangeable data and in doing so presents a surprisingly expressive model.', 'On the negative side:', 'The paper should quantify that this leads to diverse \"agents\".', 'are relatively little structure can be captured using this distribution.', 'I thank the authors for their detailed response.', 'The paper is well written and the main contributions are clear.', '==', 'Depending on \\\\lambda, the objective either closely approximates the marginal likelihood or not.', '- p6par7: \"measure how rate\" -> \"measure the rate\"', 'As far as I understand, the main contribution of this paper is the use of separate \"selection network\" to estimate the confidence of predictions by \"classification network\".', 'I think some more motivations or explorations (what kind of information did BERT learn) are needed to understand why that is the case.', 'In short, the meta-learning part stays in the regular few-shot learning module (which is implemented as a prototypical network), and has nothing related to domain adaptation.', 'http://papers.nips.cc/paper/3201-a-kernel-statistical-test-of-independence.pdf', 'I have some concerns about the motivation of this method:', 'The second point is that the proposed approach seems to modify a few things from the ES baseline.', '- For the number of thinkings steps, how does it scale up as you increase it from 0 to 16? Is there a clear relationships here?', '- Evaluation Datasets: Did you take duplication in the crawled datasets into account? (Lopes et al. 2017 (DéjàVu: a map of code duplicates on GitHub) suggests that this is particularly problematic for JavaScript/TypeScript)', 'What is a general guideline for one to choose this number $k$ given a new application scenario?', 'That said, the claims should be weakened to reflect this gap, and domain knowledge should be mentioned more prominently (e.g. states of interest vs context are given, not learned).', '- Writing sometimes a bit overcomplicated (e.g., Sect. 4.5 could just be a figure of a commuting diagram and two sentences...)', 'This is especially the case in a few places involving coreference:', '- \"work around\" -> \"workaround\"', 'As the submission is a theoretical result with no immediate applicability, it would be very helpful if the authors could detail the technical improvements over this related work.', 'There might be other constructions that are more efficient and less restrictive.', '* Figure 5 should appear after Figure 4.', 'The learning is based on a large set of base class labeled graphs and a small set of novel class labeled graphs.', 'tandem with trying to develop procedures related to identifying', 'The paper is clearly written, and the proposed architecture has too cool properties: it’s compact enough to be used for image compression; and it doesn’t overfit thus making early stopping notnesesary (which was crucial for the original DIP model).', '1. All edges are activated and the shared weights W are trained using normal SGD.', 'A variety of experiments', '- Sec 3.3 the argmin is a set, then it is LMO $\\\\in$ argmin.', 'This paper present extensions of the Self-Attention Generative Adversarial Network approach SAGAN, leading to impressive images generations conditioned on imagenet classes.', 'However, I think that there are some weaknesses (see comments):', 'Lee et al. seem to make similar mistakes, and it is likely that their experimental design is also flawed.', 'While the model shares a similar spirit to EntNet, I think the model has enough distinctions / contributions, especially given that it outperforms EntNet by a large margin.', '- a showcase of the power of neural networks as a tool to approximately solve NP-hard optimization problems with discrete inputs', '[A] Wen, Yandong, et al. \"A discriminative feature learning approach for deep face recognition.\" European Conference on Computer Vision. Springer, Cham, 2016.', 'The notations are overall confusing and not explained well.', 'Detailed comments about experiments:', 'The linear probing accuracy from the first few conv layers of the network trained on the single/few image data set is found to be comparable to or better than that of the same model trained on the full reference data set.', \"First, it is somewhat misleading about its contributions: it's not obvious from abstract/introduction that the whole model is the same as DIP except for the proposed architecture.\", 'Strength:', 'Even if we constrain ||w ||_2 = 1', '2017', '“We adopt the sticking approach hereafter”. Does it mean it is applied with all experiments with GO?', '3.\\t“we find it advantageous to pad both the in- and output of the network with equal number of zeros”: Is this to effectively increase the intermediate network dimensions?', ', the ME bias will be solved.', 'It is unclear to me how difficult/easy these results would be to reproduce. Do the authors intend to release code for their implementations and experiments?', '3) The experiments lack comparisons to several important baselines from self-supervised learning community, and methods using soft labels for training (as mentioned in 2) above).', 'How was it chosen?', 'Intuitively, transductive label propagation should improve supervised learning when the number of labeled instances is low.', 'Simulating long-horizon trajectories with a probabilistic model is known to be unsuitable for model-based RL due to accumulated errors.', '- \"gumbel\" -> \"Gumbel\" (throughout; or just be consistent in capitalization)', 'Will it still be more favorable than other concerning baselines?', '-\\xa0all the experiments except the last row of Table 2 concern adaptation between two domains.', '\" If things change very little from one second to the next, the signals could be very similar and may not really be, intuitively, independent samples and may bias result of the study.', 'One thing that I missed while reading the paper is more comment on negative results.', 'The paper considers the problem of dictionary learning.', 'The main idea is to treat all architectures as a Directed Acyclic Graph (DAG), where each architecture is realized by a subgraph.', '- It is unclear why the results on WikiSQL is presented in Appendix. Combining the results on both datasets in the experiments section would be more convincing.', \"I gather that Experiment 3 presents small LSTMs without recurrent dropout instead because combining plasticity and dropout proved challenging (or at least the authors haven't tried it yet).\", 'Weak points:', 'For example, total variation regularization can do all these tasks.', 'Figure 4 of [1] shows results for learning 16x16 filters using \"ten 512x512 images of natural scenes\".', 'Specifically, it proposes a specific Boltzmann machine to be used as a first layer of neural networks for MNIST classification.', 'At a high level the work seems good and interesting for a large audience interested in streaming data analysis.', '* One aspects that is mostly left out of the discussion (except from one side comment) is the wallclock time, as some optimizers might be on average quicker to train (for example due to quicker convergence), this can easily lead it to be quicker to tune even though it requires a higher budget of trials.', 'Quality and Clarity:', 'Much easier if you just try to solve the problem using a set of n Sigmoids (n total number of classes) and consider each output a probability distribution.', 'In robot design for a given task using rewards, training each robot design using RL with rewards is an expensive process and not scalable.', 'This would correspond to maximizing a valid lower bound to the marginal likelihood of a BNN model p(y | x, \\\\theta) p(\\\\theta), while interpreting p(\\\\theta | x) as an amortized variational approximation.', 'The results indicate that, while discritization in itself does not give improvements, coupling this with the BERT-objective results in speech features which are better in downstream speech recognition than standard features. I think the main technical novelty is in combining discritization with future time step prediction (but see the weakness below).', '1, the ME problem is quite similar to the concept ontology, e.g. , a “Dalmatian,” a “dog”, or a “mammal”.', 'During training, the classification network and the generator network are alternatively updated, and the update of the latter aims to maximize the improvement of the former after using the generated auxiliary label for training.', 'Strengths:', '* Comparison with other methods did not take into account a variety of hyperparameters.', '- abbreviation IPM is referred several times in the paper, but remains undefined in the paper until end of page 4, please define earlier.', '- The experimental results show that the proposed strategy performs well in problems that are low-rank, while the performance may degrade in problems where the low-rank assumption is not met. Would it be possible to detect the rank of the problem in a dynamical manner (i.e., during the learning), so that the number of incomplete observations of Q can be increased to improve the performance, or the solution strategy (e.g. whether to use the low-rank assumption or not) can be adapted to the nature of the problem?', 'I wonder the motivation of analyzing generalization of RNNs by the techniques established by Bartlett.', 'compared to most standard semi-supervised learning settings', 'And finally, the discussion of this figure makes claims about the behaviour of the model that seems to be too strong to be based on a single image experiment.', 'Is it indeed a discrete output (problem with lack of differentiability)? Softwax probability? Other modelling choice?', 'The weights of this weighted average and K are \"hyper-parameters\" of the metric itself.', 'Secondly, based on the theoretical result, the paper proposes an algorithm to minimize the upper bound of the theoretical result.', 'However, in its current state the work lacks sufficiently strong baselines to support the paper’s claims; thus, the merits of this approach cannot yet be properly assessed.', 'A variation on this dynamic learning rate related to eligibility traces is also proposed.', 'The paper extends previous work on differentiable placticity to include neuro modulation by parameterizing the learning rate of Hebbs update rule.', '#3) There are several Equations that can be combined, such that to save enough white space in order to discuss further some actual technical details.', 'I think this paper should be accepted as it proposes a novel idea, which does not seem too difficult to reproduce, describes a simulator for synthetic data for digit recognition, and proposes it as a benchmark for learning representations, and provides experimental results that help in better understanding the representation learned by the model.', 'However, it seems a little unexplainable to apply a technique developed from classification to analyze RNNs, since the main task of RNNs never should be classification.', '[1]. Kai Zhang, Wangmeng Zuo, Shuhang Gu, Lei Zhang: Learning Deep CNN Denoiser Prior for Image Restoration. CVPR 2017: 2808-2817', 'I found the paper interesting to read and well written.', 'I don’t recommend to accept this paper, at least in the current format.', 'Although the concept of \"task\" is not explicitly defined in this paper, the authors seem to associate each task with a specific class.', 'In Section 3.3.', 'It also proposes a loss function and sampling scheme for training the model.', 'Also, without BERT pre-training, using directly the discrete tokens seems to consistently give worse performance for ASR.', 'In other words: at test time, do you present the original images only or transformed images too?', '.', 'The two regime claim of the paper is not really novel.', 'The authors suggest using Aurora 2015 as a possible initialization.', 'It seems like a large assumption that the same learning rate would work for different methods, especially when some of them are normalizing the objective function.', 'This mapping is learned jointly with the meta-learner, who performs the meta-learning in the target domain, on examples from the labelled domain.', '* First paragraph page 5: \"more shallow\" --> \"shallower\"', '- Lack of clarity in the following passage: “In our setting, each point xi in the point cloud can be considered to correspond to single images when we train GANs over images”', 'Motivated by the fact that LDA-like generative classifer assuming the class-wise unimodal distribution might be robust, they introduce a generative classifer on top of hidden feature spaces of the discriminative deep model.', 'Quantitative evaluation of generative modeling performance is unfortunately missing from this paper, as it is in much of the GAN literature.', '- \"P1 is agent\" -> \"P1 is the agent\"', 'prevent a network from explicit memorization and could be used as a', 'A minor one: “Table 7 reports results on the αNLI task.” Should it be “Table 2”?', 'Additionally, the previously mentioned evaluation steps, i.e., using a fixed attention map as baseline for the evaluation and evaluating the correlation between FSM and accuracy may be informative to illustrate the advantages of the proposed approach.', 'See below for detailed comments.', '[1] Kolesnikov, A., Zhai, X. and Beyer, L., 2019. Revisiting self-supervised visual representation learning.', 'The technical results obtained are non-trivial and detailed proofs are presented.', 'This is the format that will be used to encode the row indices in a matrix, with n columns.', 'Algorithms for Streaming data using a machine learning oracle is analyzed theoretically and empirically.', '3, In Sec. 3, the first and section paragraph, I can not quite understand how the ME bias theory guide the following synthetic experiments.', 'This could be a possibility to remove the noise you have when doing multi-step operations (and potentially go way beyond 4 steps).', 'Section 2:', '2.', 'In a multiclass setting, the idea is to take the output of a NN and turn it into a gating function to choose one expert.', 'The proposed model extends previous work by adding an adversarial loss to a VAE video prediction model.', 'Besides, as the base classifier is different for various baselines, it is hard to compare the methods.', 'In Table 3, the F-pooling consistently shows inferior classification performance, although obtaining slightly higher consistency.', 'comments after reading response ===========', 'The combination is new and the result is encouraging.', '- It is not clear to me how the analysis done in this paper will generalize to other more complex datasets where the network layout NMN might be more complex, the number of modules and type of modules might also be more.', '= Recommendation', 'and have worked on modeling word representations.', 'Roughly, the theorem states that if for example we fix any matrix B of size e.g. 256 x k and matrix U of size 512 x 256 and then compute U relu(B C) where C is the vector of parameters of size k x 1, AND if k < 2.5 (i.e. if we use at most 2 parameters), then it would be very hard to fit 512 iid gaussian values (i.e. min_C ||U relu(B C) - eta|| where eta ~ N(0, 1)).', 'ResNets also have this ability to “propagate” the noise signal more easily, but it appears that having a self-modulation mechanism on top of that is still beneficial.', 'The ostensible goal of learning more about observation representations is mostly preliminary — and this direction holds promise of for a stronger set of findings.', 'For instance, are BSB1 fixed points good for training neural networks?', '- impact: the results achieved in the experiments are very small improvements compared to the baseline of RGCN (~ +0.01 in two experiments and ~ -0.04 in another) and often these small variations in results can be compensated with better baselines training (e.g. better hyper-params, ...)', '- The equation (1) should contain \\\\rho, not p.', 'However, as presented these ideas are poorly justified and careful comparisons against sensible baselines are missing.', '- cue reward experiment exemplifies limitations of current plasticity approaches and clearly shows the potential benefits of neuro modulation', 'Second, the proposed method does not sound scalable - the use of a professor and two teaching assistants to construct the concept extractor, and the use of an expert TA to select a small set of informative words.', '- Globally Coherent Text Generation with Neural Checklist Models', 'The definition of the tuning budgets is not clear, is it the number of trials or the time/computation budgets?', '- no qualitative analysis on how modulation is actually use by the systems.', 'Experiments are conducted on a simple toy data set, as  a proof of concept, and on data from ModelNet10 and ModelNet40.', '4, in Sec. 3.1, considering the small number of training instances, whether it is large enough to train the NN/ not overfitting issues?', 'At the end of the subsection, the authors argue that 4D convolution is just k times larger than 3D kernels, which sounds like contradicting.', 'The proposed model combines few shot meta-learning with the adversarial domain adaptation to demonstrate performance improvements in several experiments.', '- Can MISC deal with the problems where the number of objects of interest is more than two? In this case, how can we define mutual information?', 'I find the paper very well written and easy to follow.', 'An in-depth analysis of the resulting embeddings is presented, showing that a network can learn to \"apply\" embedded rewrite rules to embedded theorems, yielding results that are similar to the embedding of the rewritten theorem.', 'Now consider a codebook with n convolutional codes, of rate 1/k.', '- \"Pinker\" -> \"Pinsker\"', 'e.g., architectures and hyper-parameters used, and training procedures (I encourage the authors to utilize the appendices for this).', 'In 4.1, you are using different epsilon policies for synthetic vs organic datasets; why?', 'In terms of graph classification, the task distribution is supported on the joint distributions (G, Y).', \"Another factor might be the model structure. If Alexnet has much lower accuracy, it's probably worthwhile to conduct experiments on the same structure with previous works (Madry et al and Athalye et al) to make the conclusion more clear.\", 'They have addressed 1) the title issue and 2) adding domain adaptation baselines.', 'The mean field theory in statistical mechanics was employed to analyze the', '2) In assumption 3.1 is not clear what $L_H$ is.', 'In this paper, the authors focus on alleviating the catastrophic forgetting problem in continual learning.', 'In addition to the extensive experimentation on different settings showing performance improvements, the authors also present an ablation study, that shows the impact of the method when applied to different layers.', 'This paper is generally easy to follow and written clearly.', 'I am also not entirely satisfied by using accuracy as the only metric; how about using something like beam search to build a \"soft\", secondary metric?', 'This aspect of the task seems somewhat contrived, and it makes me wonder whether the striking failure of the non-modulated RNNs depends on this detail.', 'First, the labeled data portion is fixed and is relatively high', 'The proposed approach is very similar to the CE method by Rubinstein (as stated by the authors in the related work section), limiting the contributions of this paper.', 'Note that the gradient is biased if the mean policy approximation is used.', 'However, as also observed by the authors, the sampling distribution can also be replaced by more sophisticated distributions.', 'The difficulty is that (the gradient of) Laplacian norm is impractical to compute for DNNs.', 'In Theorem 4.3', 'This paper proposes a new kind of spherical convolution for use in spherical CNNs, and evaluates it on rigid and non-rigid 3D shape recognition and retrieval problems.', 'Also, MCTS has been used extensively to find Nash equilibria in both perfect and imperfect games, e.g., \"Online monte carlo counterfactual regret minimization for search in imperfect information games\".', 'And the results are obtained by G(z).', 'While the improvement of DEBAL compared to an ensemble is marginal but is reasonable.', 'humans.', '3) The novelty of this paper is incremental as the theoretical results are extended from Cortes et al (2019) and Zhao et al (2018).', 'This paper study the convergence of Hamiltonian gradient descent (HGD) on minmax games.', '3. Assume y_v is M-way categorical distribution, Eq(8) evaluates f by 2*V*M times which can be computationally expensive.', 'The cost of evaluating the function is typically more pressing, and as a result it is important to have algorithms that can converge within a small number of iterations/generations.', 'Especially in the field of multimodal learning, we often face the issue of imperfect sensors.', 'Some of these papers specifically use VQ-VAEs [3].', 'In addition, the classification models for different tasks are independent, though their training might be related by a meta-learner.', 'Another contribution of the paper is theoretical treatment of (a simplified version of) the proposed architecture showing that it can’t fit random noise (and thus maybe better suited for denoising).', 'Alg 1 seems a fairly generic neural-network fitting algorithm.', '.', 'Additionally, I find the motivation for caring about local optimality unconvincing.', 'I am very happy to give a higher rating if the authors address the following points.', '+ Establishes a good baseline for future video prediction work', 'If the Hebbian synapses are not considered, then the authors need a control with matched memory-capacities to account for the extra capacity afforded by the Hebbian synapses.', 'The paper is well written and easy to follow. The topic is apt.', 'It should be pointed out that the D in the hinge loss represents a neural network output without range restriction, while the D in the non-saturating loss represents sigmoid output, limiting to take in [0,1].', 'I wonder how the straightforward regression term plus the smooth term will perform for the mask.', 'To continue with the experimental evaluation, I found the plots with the predictive uncertainty in Figure 3 a bit confusing.', '[Area to improve]', 'There is an increasing need to interpretability of deep neural networks as they get more and more applied to real-world problems.', 'Also, since the synthetic image pairs are not multimodal in nature, it is unclear to me for what the messages are conveyed in Figure 3 and 4.', 'Nevertheless, the results are consistent across those experiments.', 'Firstly, most prior works such as beta-vae, info-gan do not assume that the factors / canonicalizers are known beforehand.', \"For example, it's not close to getting realistic spatial movement relative to the plane nor is the control that impressive wrt limbs.\", 'The main contributions of this work are essentially on the theoretical aspects.', '-\\tIn section 4, it is unclear why only the maximal activation of the softmax layer is used to characterize a sample? Why not considering the full distribution that should contain richer information? Why just focusing on the output layer and why not using the info available at intermediate layers?', 'The existence of couplings J_h among the hidden units means that we have to carry out mean field inference over several iterations to compute the output activations of the model.', 'With toy experiments, authors demonstrate the usefulness of the proposed modifications to DNC.', 'In section 3.2, to characterize its difficulties of finding best hyperparameters or tunability, the authors seem to try to connect the concept of “sharpness” of a minima in loss surface to the tunability of an optimizer, which is similar to comparing the loss landscape of minimums.', '- the data sets used in the experiments are very small', 'The main criticism I have is that I found the paper harder to read.', 'This paper introduces a simple measure of tunability that allows to compare optimizers under varying resource constraints.', \"For example, the absolute value function has the same limit from the left and the right at 0, but it's not differentiable there. Is it actually true that if we take the derivatives of the piecewise hyperbolic/spherical distance function that it's differentiable at c=0?\", 'For example, “Fuzzy Choquet Integration of Deep Convolutional Neural Networks for Remote Sensing” by Derek T. Anderson et al.', 'Presumably the cost here would be to physically scan some of these workbooks, but this seems like a very limited investment. Why not build datasets based on workbooks, problem solving books, etc?', 'There are a few grammatical/spelling errors that need ironing out.', 'This paper proposes new pre-training strategies for GNN with both a node-level and a graph-level pretraining.', 'Cons:', '[1] Adversarial Dropout for Supervised and Semi-Supervised Learning, AAAI 2018', '- Limited novelty', 'So it is not convincing to me that the auxiliary labels generated by the generator can be really helpful.', 'This paper seeks to understand the role of the *number of training examples* in self-supervised learning with images.', 'This paper proposes a new task/dataset for language-based abductive reasoning in narrative texts.', 'Hence it is a natural question to understand whether FW performs significantly better than current algorithms in this context.', 'Additional comments:', 'one quick question:', 'Here are the questions I would like the authors to discuss further:', 'I am biased because I once did exactly the same thing as this paper, although at a much smaller scale; I am thus happy to see such a public dataset.', '[4] actually compares VQ-VAE and the Gumbel-Softmax approach.', 'It is better to explain the major difference and the motivation of updating the hidden states.', 'First, the authors claim that they are the first to combine Choquet integral with deep learning.', 'Overview:', \"I'm wondering whether other smoothness regularizations can achieve the same effect when applied to semi-supervised learning, e.g. spectral normalization[3].\", 'e.g: REINFORCE has lower best-sample to total-sample ratio but its solutions are worse)', 'The method is only evaluated on a single task and many confounding variables (the design of the reward function, factorizing the parametric distribution into marginals, reporting results for a single (non-tuned?) learning rate, etc.) make evaluation difficult.', 'The paper is generally well written with a clear presentation of the proposed model.', 'There are also several interesting design choices that I think are worth of further exposition.', 'The input and output types of each block in Figure 1. should be clearly stated.', 'The construction of the dataset was performed carefully (e.g., avoiding annotation artifacts).', 'As for the qualitative evaluation, the model corrects the blurring effect of the reference SV2P baseline, and produces quite realistic predictions on these datasets.', 'But I assume it is very cheap compared to training e.g. the ResNet, correct?', '2. https://arxiv.org/abs/1904.11469', '[1] Koiran, Pascal, and Eduardo D. Sontag. \"Vapnik-Chervonenkis Dimension of Recurrent Neural Networks.\" Discrete Applied Mathematics 86.1 (1998): 63-79.', 'Although some promising', 'The experiment is also well structured and shows higher performance than the existing models.', 'al. 2017)', 'based on a graph representation of the robot structure, and a graph-neural-network as controllers.', 'Detailed questions and suggestions:', 'that the proposed scheme is able to produce walking and swimming robots in simulation.', \"You probably have to limit the operation to a half-sphere (there's some ideas for this in Gu et al).\", 'The figures are almost useless, because the captions contain very little information.', 'The authors evaluate their models over a number of datasets.', 'I believe that the presentation of the proposed method can be significantly improved.', '- motivation: the natural extension of previous work on differentiable plasticity based on existing knowledge from neuro science is an important next step', 'Rebuttal EDIT:', 'In addition, the applications are very similar: image inpainting, denoising, super-resolution etc. Yeh et al.’s method should be the right baseline instead of the nearest neighbor algorithm.', 'to user-defined control signals, typically marking the displacement of the moving', 'This paper proposes an approach for automatic robot design based on Neural graph evolution.', '+ positive results on CIFAR data set', 'It is easy to ask for additional experiments (i.e., other mechanisms of uncertainty such as the count-based discussed in related work, other settings in 2.2) — but the quality seems high enough that I basically trust the settings and findings.', 'This is an open issue in model-based RL.', 'For example, what do you mean by classifying the image as “new”? Is “new” a class name? Also, how is P(N|t) computed? Please explain.', \"4- Could you please add the found J_h's to the appendix.\", '- For the medical application, INN outperforms other methods (except sometimes for ABC, which is far more expensive, or direct predictor, which doesn’t provide uncertainty estimates) over some metrics such as the error on parameters recovery (Table 1), calibration error, and does indeed have a approximate posterior which seems to correspond to the ABC solution better.', 'Needing new environment variations to obtain new skills is a large step backwards from things like DIAYN (the MISC/DIAYN combination needs more evidence to be considered a possible solution), and the s_i/s_c distinction is non-trivial to specify or learn for harder problems (e.g. pixel observations).', 'Overall, the paper proposes a method combining a number of existing useful works (prototypical networks for meta-learning and image-to-image translation for domain adaptation) to tackle an important problem setting that is not currently addressed in existing meta-learning research.', 'Also, the work by Platanios, et al. on contextual parameter generation is very relevant to this work as it tackles multi-task learning using HyperNetworks.', 'Also, second sentence is not a complete sentence', 'It is possible that due to this design choice the method would not produce sharper reconstructions than the original samples from the prior.', 'It did also not match any numbers in Tab. 4 of the appendix.', 'Experimental results itself are fine but not complete.', 'However, on less restricted data, more general architectures seem to show better generalization (even if it is not systematic).', '- The end of the 2nd line of lemma 1: P, G should be \\\\mathbb{P}, \\\\mathbb{G}', '- novelty is low: the proposed algorithm is a heuristic similar to previously proposed algorithms in the transfer learning and auxiliary learning space', '- Regarding lambda>1, you write that “we argue this modification makes our algorithm more general, and gives rise to better attack results”.', 'They empirically show that there exists the mode collapse problem due to the MC dropout which can be regarded as a variational approximation.', 'This paper introduces a technique using ensembles of models with MC-dropout to perform uncertainty sampling for active learning.', 'The second contribution uses meta learning to decrease the sample complexity required to fit the neural network, creating a family of tasks derived from the data with data transformation that do not modify the mutual information.', 'Therefore, I cannot find any advantage in the proposed method, compared with these existing MAP based image restoration approaches.', 'The method is shown to serve as a good model for natural image for a variety of image processing tasks such as denoising and compression.', 'For example, what benefit we can get if we have perfect object detection? Where can we get if we have perfect relationships?', '+ well written, with a large set of experiments and some interesting observations/discussions', '2- Why is a two-stage pre-training (Figure 2) process needed? Why not just a single stage?', 'Adaptivity ratio is mentioned in the intro but not defined there.', '[Schulman, J., Heess, N., Weber, T., Abbeel, P.: Gradient estimation using stochastic computation graphs.]', 'notation: \\\\forall s is missing', 'Why was this policy used as the baseline? It seems extremely basic and unlikely to truly lead to optimal performance.', 'I am still a bit confused about the difference between \"zero-confidence attacks\" and those that don\\'t fall into that category such as PGD.', 'Compared to several baselines with a fixed encoder, the proposed model allows the decoder to attentively write “decoding information” to the “encoder” output.', 'It would be more convincing to see more experiments.', 'Nevertheless, this paper clearly explores and offers a novel approach for more efficient on-policy exploration which allows for more stable learning compared to traditional approaches.', \"Depending on the answer to these questions I'm planning to move up or down my grade.\", '(Figurnov et al. 2018, Jankowiack & Obermeyer,2018).', 'This very much limits the utility of the method.', \"2. I think that the claim that the use of neural networks with discrete inputs can approximately solve NP-hard optimization problems is an exciting one, which likely necessitates more experiments (or theoretical results), but as it stands I don't think it is a fundamentally different conclusion from the fact that this method provides a great scalable solution for the ordinal embedding problem.\", '- Section 4: Would it be possible to train the vq-wav2vec model jointly with BERT, i.e. as one model? I suspect it would be difficult since, for the masking objective, the discrete units are already required, but maybe there is a scheme where this could work.', 'The authors created two new large scale pre-training graph datasets.', 'Likewise, it is not well explained what is the value of the learnt relationships, and how uncertainty and errors in the causal learning are relevant to the downstream use of the learnt model.', '\"it might also like appear\"', \"Though I am not familiar with BLEU scores and though I didn't grasp some of the details in 3.1, the algorithm yielded consistent improvement over the given baselines.\", 'Measuring and testing dependence by correlation of distances', 'Finally, the binary vectors produced by the quantization algorithm are further compressed with the Viterbi-based algorithm.', 'The model also obtains non-trivial improvement over previous SOTA models.', 'The paper experiments the popular deep learning models on the dataset and observe shortcoming of deep learning on this task.', 'I think the topic studied in this paper is very important and meaningful, and I am convinced that by decomposing a global pattern into several smaller local pieces, the model parameter updates computed by each party should be more similar to benign updates and thus can better bypass the defense algorithms.', 'This paper studies the characteristics of representations and their roles in neural network expressiveness.', 'Massive effort, nice results. Now for learning on our part (the humans).', 'Specifically, I have the following questions for clarification:', 'The literature on distillation indicates that overparameterization can be beneficial for convergence and final performance.', 'Besides, the power of AlphaGo Zero resides in the combination of the MCTS together with the compact representation learning of the value functions.', 'Both Cakewalk and CE approximate the reward CDF from K samples and use this to construct a surrogate reward.', 'This restriction should be noted in the paper.', 'Those are important as well for the field, and I suspect that this direction could be pushed a lot more.', 'The paper does not talk about settings where states of interest are not known, so all of the experiments are based on this strong assumption.', '- in the whole paper there is $y$ which is not defined.', '2. A more detailed illustration of the system / network is needed. Would have made it much easier to understand the paper.', 'The authors also well- placed their work in the literature, as described in Section 3.', 'How do we choose a proper beta, and will the algorithm be sensitive to beta?', 'The case-study is also interesting, although does not improve current state-of-the-art.', 'These numbers correspond to several images, or to a unique image?', 'Overall, the paper proposes an extension of VAE based video prediction models and produces an extensive evaluation.', 'Page 2: to literature -> to the literature', 'networks.', 'The authors address this by saying 3M of the 15 M negatives have been seen.', 'This paper provides a new dynamic perspective on deep neural network.', 'Using the proposed  dropout transformation that is differentiable, the authors show that the KL regularizers on policy-space play an important role in stabilizing its learning.', 'The author states their method to determine the priors by training each task specified in the DEEPOBS with a large number of hyperparameter samplings and retain the hyperparameters which resulted in performance within 20% of the best performance obtained.', 'But other than this, the empirical performance seem not showing particular advantage over AA-pooling.', 'This seems to be too low to be of practical use.', 'Previous work has either used general anisotropic convolution or azimuthally isotropic convolution.', 'The paper is well written and easy to read.', 'I note that recent papers in this field tend to perform significantly more extensive experimental evaluation, typically selecting a wider range of competitors and using a number of more standardized metrics including IOU, F1 score and CD and typically repeating these at a variety of resolutions or on additional datasets or category splits etc.', 'In the case when Rep trick is applicable, is it identical to GO?', '-- In Equation (13), is there an activation function between W1 and W2?', 'which is much smaller than the number of time series usually involved say in gene regulatory network data', '- On the human evaluation: showing the gold standard reference to the judges introduces bias to the evaluation which is inappropriate as in language generation tasks there are multiple correct answers.', 'That said, I would like to see more analysis on the behavior of the proposed method under various interesting cases not tested yet.', '2. The paper proposes a metric for the saliency map naming FSM which is an extension of existing metric SSR.', '？', 'For the language translation results, were there any other state-of-the-art methods that the authors could compare against? It seems they are only comparing against their own implementations.', 'The paper focuses on the generalization performance of RNNs and its variant in a theoretical perspective.', 'Since q_rel require one hot vector as input, how to sample the q_rel given the importance score and how backprob the gradient in this case?', '- The main contribution of the proposed theory is the alpha term.', 'The present paper proposes a fast approximation to the softmax computation when the number of classes is very large.', 'In order for a meta-learning model to be called “meta domain adaptation,” the type of adaptation cannot be seen during training, and the goal is to test on adaptation that the model has not seen before.', '2. A new model combining prototypical network with GAN and cycle-consistency loss for addressing meta-learning domain shift scenario.', 'The contributions of the method could also be underlined more clearly in the abstract and introduction.', 'The creation of the upper limit based on noise between', 'Similarly, you did not indicate what the deterministic version of your model is.', 'Pros:', '[2]', '==================', 'Fixing this would significantly improve the paper.', 'explores if a network can be forced to explicitly memorize a set of', 'For example, how do I even know that the oracle in question exists? What are the particular assumptions under which it exists? What are the requirements on the training data, optimization ability, generalization error, etc. How do we know that we can create in practice ML learning models that are sufficiently accurate to serve as an oracle?', '1. The paper says at the top of page 6 that the result of Eq 1 is a disambiguated intermediate node representation.', 'Uses unintroduced (at that point) notation and is very confusing.', 'In \"Stacks of convolutional Restricted Boltzmann Machines for shift-invariant feature learning\" by Norouzi et al, several RBM layers are trained greedily (same as here, only difference is contrastive loss vs mean field) and they achieve 0.67% error on MNIST.', 'Due to the different objects used in the different datasets, some of the experiments have a smaller set of words.', 'For the same set of regularizers and the MNIST dataset, the paper then explores the mutual information between the inputs and the hidden layer activations.', '- List of importance measure at beginning of Section 4 should probably have citations.', '-\\tSection 5 is somewhat less clear than the previous sections.', 'Unfortunately, the new results are not presented in the revision.', \"After rebuttal: I have read the authors' response and  I stand by my decision.\", '1. [The claim] One of my concerns for this paper is the assumption of the factorized latent variables from multimodal data.', '* Criticism', 'Pros:', 'The paper proposes an improved method for computing derivatives of the expectation.', '2. Why were more baselines from the related work not included? I understand the experiments are a proof of concept, but it would be nice to get a feeling for what some of the other methods do.', 'classification problem?', '- How do you specifically encode the number of surviving connections? Is it entirely dependent on budget?', 'To improve the rating, the author should explain the following questions:', 'However, they find that pre-training in general helps over non pre-training.', 'While this is primarily an empirical study, one aspect considered was the observation representation (raw pixels, random features, VAE, and inverse dynamics features [Pathak, et al., ICML17]).', 'However, it affects the gradient norm.', 'The proposed conditioned residual block, occlusion augmentation and stopping criteria seem to help the Pose2Pose network work well.', 'Third, it evaluates whether we can detect that a single images was used to train a convnet.', '1. The paper should also consider more recently proposed evaluation metrics such as discussed in https://arxiv.org/pdf/1805.09733.pdf', 'It should be stated that bold values in tables do not represent best results (as it is usually the case) but rather results for the proposed approach.', 'The paper derives the weight by the Lagrangian form to minimize the upper bound.', 'Moreover, due to the trace based loss function, the computational cost will also be very high.', 'Pros:', 'Other comments/notes:', 'The experiments currently demonstrate that optimizing both controller and hardware is better than optimizing just the controller, which is not surprising and is a phenomenon which has been previously studied in the literature.', 'That is, the dependence between z and y does not vary with regularizers but the one between z and x does.', '1. I had hard time to understand latent canonicalization.', '[1]  Bagdasaryan et al., How to backdoor federated learning.', 'Pros:', 'For example, representing a movie or news article by tags or topics.', 'Can you explain this inconsistency i.e. why is there such a huge jump from 4 to 32 (in reality we expect the effective LR to stay constant in the curvature regime).', 'images and how the size of this set is affected by the number of', 'The comments that more data helps (internal dataset experiments) is also informative.', 'Overall, the proposed method is elegant; however, the presentation, the claim, and the experiments suffer from significant flaws.', '4. A most simple way to reduce the variance of REINFORCE gradient is to take multiple Monte-Carlo samples at the cost of more computation with multiple function f evaluations.', '3. The mask term seems to work well for the shallow part.', 'That makes sense -- but at what cost?', '(7) The authors give the derivations about \\\\theta such as the gradient and the regularization term about \\\\theta (see, Eq 18-19).', 'E.g., when is modulation strong and when is it not used', '-What’s the 3d plot supposed to represent?', 'If only global optimizers were available, DIP wouldn\\'t work, showing its value is in the interplay of the \"cost\" function and a specific optimization algorithm.', 'The authors realise CDNs by treating the weights of each neural network layer probabilistically, and letting them be matrix variate Gaussians (MVGs) with their parameters given as a function of the layer input via a hypernetwork.', 'Fig 4 is very confusing.', 'From a practical point of view, local optimality is a mean (that can be achieved via heuristic algorithms) to an end (the objective itself).', '- In Section 3.1 : “Across runs in Table 1, we observe that without Orthogonal Regularization, only 16% of models are amenable to truncation compared to 60% with Orthogonal Regularization.” For me, this is not particularly clear.', 'For instance the simulation experiments use  N=30,', 'Given a target prediction task to be learned on training data, the auxiliary learning utilizes external training data to improve learning.', 'The assumption of independent hyperparameters might be fine for black box optimization or with the assumption that practitioners have no knowledge of the importance of each hyperparameter, then the tunability of the optimizer could be different based on the prior knowledge of hyperparameter and their correlations. But it is not rigorous enough to make the conclusion that Adam is easier to tune than SGD.', '- Figure 4 could be more interesting if compared with other classes, like other animal faces.', '4. When you say \"slow\" form something and propose a method to address it, I\\'d like to see some benchmark numbers. There is an experiment with simulation, but that does not seem to simulate the slow \"sequential sparse matrix decoding process\".', '- novelty: the additions proposed are small modifications to existing algorithms and there are other methods of attention on graphs which have been discussed in the paper but not directly compared to (e.g. this method builds heavily on Velickovic et al 2017)', 'I think the paper is relevant and proposes an interesting contribution.', 'If the goal is to learn representation for low-shot setting, the method needs to be compared with other representation learning methods such as jigsaw[1], colorization[2] and rotation[3].', 'In SGD,  function H does not necessarily decrease over the course of the algorithm.', 'Then, a Pose2Frame network is applied to generate the final result.', 'The algorithm here seems to build off of the former algorithm; essentially replacing a single hard thresholding step with an IHT-like step.', 'The paper proposes using the Frank-Wolfe algorithm for fast adversarial attacks.', '1. Since the method is only evaluated on several video sequences, I am not sure how the method will perform on other different scenes.', 'So is the REINFORCE estimator used or something? Not that the importance sampling matter is orthogonal.', 'This idea is something new, although quite straight-forward.', '= Detailed Comments', 'Empirically, they find that a single image along with heavy data augmentation suffices for learning the first 2-3 layers of convolutional weights, while later layers improve with more self-supervised training images.', 'I do not see in the updated paper that this typo (in differentiating D in hinge loss and non-saturating loss) is corrected.', 'I would also be curious to see the proposed techniques applied on simpler datasets.', '- The authors provide a nice summary of all the models analyzed in Section 3.1 and Section 3.2.', 'Minor:', 'If that is true, I would suggest the authors to make this hidden assumption clearer in the paper', 'Please clarify.', '-2: Diversity of Agents.', 'They first reduce the sample complexity of estimating mutual information by decoupling the network learning problem and the estimation problem by creating a training and validation set and then using the validation set for estimating mutual information.', 'This paper proposes to address few-shot learning in a transductive way by learning a label propagation model in an end-to-end manner.', 'Pros:', 'Second update:', 'In addition, the authors propose to minimize a weighted average of a lower bound and upper bound on the Wasserstein distance between the distributions of points corresponding to given shapes.', 'What happen for Meta-RevGrad + idt or Meta-RevGrad + revMap?', 'What were the guidelines used?', 'Did you try to have a single network?', 'It would also be great if the paper could related to other existing work that uses Bayesian neural networks in an active learning setting such as Bayesian optimization [3, 4] or Bandits[5].', \"This is a practical contribution but not a theoretical contribution, as there is no main theorem (or equivalent statements), and the main novelty is on using Ungar's (2010) weighted barycenter to perform neighborhood features aggregations.\", 'in section 3.3, the authors compare against RFA and take what is claimed in Pillulata et al as granted (that RFA detects more nuanced outliers than the wort-case of the Byzantine setting', 'images can be memorized and data-augmentation makes explicit', 'Second, the authors claim they are using/motivated by Choquet integral, but do not have any (appendix) sections to explain how this mathematical tool is really integrated into their models.', 'Another potential issue is that the proposed approach cannot handle training set bias. If all models are biased in similar ways (e.g., toward a particular class or domain), they will not reveal informative discrepancies for the images over which they all make similar mistakes.', \"I don't think this is really a fair comparison; I would have liked to have seen results for the unmodified reward function.\", '#4) In Section 2 paragraph 2, the sentence \"However, they only ... and directly\" is not clear what means.', '(However, CO is)', 'Cons:', 'The authors test their model on three tasks: cue-award association, maze learning, and Penn Treebank (PTB).', 'The paper clearly states the objective and provides a nice general description of the method.', 'Also, I felt that the proposed approach in Section 5 is very similar to MAML intuitively. This makes a comparison with MAML even more desirable.', '1. Regularization by sparsity is a neat idea.', '======', 'Authors analyze theoretically the asymptotic convergence rate for objectives involving normalization, not necessarily BN, and show that for scale-invariant groups of parameters (appearing as a result of normalization) the initial learning rate may be set arbitrary while still asymptotic convergence is guaranteed with the same rate as the best known in the general case.', '4. Most importantly, for table4, authors are comparing to the ResNet18 ARNet instead of ResNet50? which is better than the proposed method.', 'memorization more difficult. Then it is off to considering', '-Some technical details  are missing.', 'Without careful treatment of this effect, these results are vacuous.', 'The paper addresses an interesting problem and makes some good contributions.', 'The authors implement these GCNs, train the curvatures, and demonstrate performance improvements over Euclidean only versions on node classification on benchmark datasets.', 'With normalization we still have to tune the learning rate (as scale-variant parameters remain or are reintroduced with each invariance to preserve the degrees of freedom), then we have to wait for the phase two of Lemma 3.2 so that the learning rate of scale-invariant parameters adapts, and from then on the optimal convergence rate can be guaranteed.', 'I support accepting this paper.', 'For example, an additional plot after Figure 5 (b) which shows a few scatter plots of points (color coded by class) for training with different numbers of collected triplets.', 'The results shown in Figure 4 are much more interesting than the usual training curves which are shown in the other figures.', '-  Why have you chosen the 4 operations at the bottom of page 4? It appears to be a subset of those used in DARTS.', '* an interesting model which is quite intriguing in its simplicity.', 'One minor issue is that the first sentence of the third paragraph in Section 4 is not a full sentence and therefore difficult to understand.', 'Still, there are some issues the authors should address:', '[10] C. Nicholas and W. David. Towards evaluating the robustness of neural networks. In IEEE Symposium on SP, 2017.', 'With Lemma 2, it is natural to construct a epsilon-net for RNNs and then upper bound the empirical Rademacher complexity by Dudley’s entropy integral, since such methodology is not so novel.', 'It extends previous research on differentiable Hebbian plasticity by introducing a neuromodulatory term to help gate information into the Hebbian synapse.', '+ Simply a cute result, showing that proof search can remain in embedding space for a limited time horizon without having to switch back into the theorem prover environment.', 'In NIPS, 2017.', 'The formula embeddings are then combined with theorem embeddings (also formulas, computed in the same way as formula embeddings) to predict whether one can do one step of math reasoning using the corresponding theorem, and also to predict the embeddings of the resulting formula.', 'The papers are \"Hyperbolic Graph Convolutional Neural Networks\" by Chami et al and Hyperbolic Graph Neural Networks by Liu et al.', 'All of these pieces are very well-known methods (e.g. VAEs, conditional VAEs, CL, catastrophic forgetting, domain transformation) in the literature and this paper puts them together in a straightforward way.', '- The claim made in the first line of the abstract (applying RL algorithms to continuous control problems often leads to bang-bang control)', '* Page 5, bottom: the word “simply” is used twice', 'It seems modulation on layer 4 comes in as a close second.', 'They create a simple synthetic dataset, involving asking if particular types of objects are in a spatial relation to others.', 'There is no value in being very confident if you are wrong and vice-versa, so unless there is an accompanying plot/table reporting the accuracy I see not much value from this plot alone.', 'On the methodology side, using predicted learning curves instead of real ones can speed up training significantly.', \"However, the experiments are not enough to show that the proposed method is really needed to achieve the results. If these are answered well, I'd be happy to change my evaluation.\", 'Finally, it seems that the goal here is to test for independence.', 'Third, I don’t get what is plotted on different subplots.', 'More explanation of canonicalization is needed.', 'How do these compare? Does the proposed approach offer  insights on these datasets which are not captured by the comparison methods?', 'It seems that the goal of this model is to keep updating the encoder hidden vectors (h_0, .., h_T) instead of fixing them at the decoder stage.', 'Intuitively, when the upper bound of the loss is minimized, it will be beneficial to minimize the loss itself.', '- The paper reads well and is easy to follow.', 'The result does not at all apply to all of them.', ', the result holds for any k and M. The authors claim that if we take a limit of M -> ∞ with fixed k, the practical dynamics converges to the discrete-time mean-field limit, in Section 4.', 'This paper studies multi-source domain adaptation problem.', '.', 'This also shows the power of such an oracle.', 'This appears strange, since it would mean that $x_i$ cannot be in the center of $\\\\hat{x}_i$.', 'In the main paper, only proof of concept experiments are provided (one experiment), that nevertheless show competitive performance under varying settings.', 'http://proceedings.mlr.press/v70/jitkrittum17a/jitkrittum17a.pdf', 'This paper presents a method for adapting a model that has been trained to perform one task, so that it can perform a new task (potentially without using any new training data at all—i.e., zero-shot learning).', 'Conductance seems to satisfy the completeness of hidden features.', 'Important points for the rebuttal are marked with (*).', 'In fact, the optimal auxiliary labels minimizing the objective is the ground truth label for principle classes.', 'Anyhow, I understand that those were chosen based on the subset of classes used for the experiments.', '4. I do not understand this: \"to fit well the method overfitting rate\" in Section 3.3.', '“reparametrizable distributions”', 'e. What are the two modalities in Table 2? The author should explain.', 'Summary: This paper is about models for solving basic math problems.', 'In this work the authors propose an extension of mixture density networks to the continuous domain, named compound density networks.', 'This was not done.', 'adagrad, adadelta are both popular adaptive variations of sgd.', '[Summary]:', 'It also contains lots of additional technical details and experiments in the appendix, which I unfortunately did not review.', 'The paper can be made more mathematically precise.', 'Experimental results show that the proposed method works much better than baselines in similar problem detection, on an undergraduate probability data set.', '* Side Notes (not affecting the review recommendation)', 'For instance, in case of grid search HPO and 0.1 is the best learning rate, different search order such as [10, 1, 0.01, 0.1] and [0.1, 0.01, 1, 10] could results in dramatic different CPE and CPL.', 'The key ingredient of the pipeline is a modern variational auto-encoder (VAE) that is trained with class labels with respect to a mutual information maximization criterion.', 'b. In Eq. (6), it also surprises me to see no description of \\\\phi and \\\\psi.', 'To test generalization, they lower the ratio of observed  combinations of objects in the training data.', 'The theoretical justification is spread out in the main body and appendices.', '2. In a similar vein, there a number of highly technical results in the paper and it would be great if the authors provide an intuitive explanation of their theorems.', 'For backdoor attacks, a line of work studies physical triggers, e.g., glasses in [2].', 'This investigation of GAN scalability is successful results-wise even though the inability to stabilize training without sacrificing great performance on ImageNet is disappointing.', 'I vote for rejection for four major weaknesses explained as follows.', 'judgements of images.', 'Overall, if we focus on the balance between the classification accuracy and computational efficiency, the proposed method is promising.', 'Assuming that there is only one variable w, then \\\\nabla L(w) is not perpendicular to w in general.', 'This paper studies the properties of SGD as a function of batch size and learning rate.', 'The method is well developed and explained.', '1.', 'The contribution of this paper seems more on the training method for imbalanced data sets.', '- The manuscript contains many typos.', 'One way to improve to work would be to provide practical takeaways from the theory or to provide experiments in the main paper.', '---', 'Even more, it seems that forward does always better than DDGC with noise 60% on every dataset.', 'The authors argue that the main advantage of their framework is that it incorporates the structure of independent cascades into the model which predicts the diffusion process.', 'I note that the alt-az convolution *is* equivariant to rotations in the subgroup SO(2) of rotations around the Z-axis.', 'Of course, the example in the Figure 3 is very nice and intuitive, but it is also rather simple.', 'In this simple model, can we not calculate the mutual information directly (i.e., without the bottleneck)?', 'This scenario', '- Inapplicability to discrete controls:', 'This is too general and not enough as a motivation.', 'Then, the likelihood induces a constraint which is based on the degradation function.', 'Overall the work is nicely motivated and clearly presented.', 'Is it better to decay learning rates for toy data sets?', '2. once you have a discrete representation, you could train BERT (as if it were a seq of language tokens).', '*', 'As the results, the batch normalization itself is found to be the cause of gradient explosion.', '1). Why minimizing the upper bound in this scenario would help to minimize the loss on the target domain, when the upper bound can be substantially huge? How about just evaluate \\\\alpha by the closeness of the source domain with the target domain?', 'Can the authors elaborate on this?', '.', '-\\tHow do you see the impact of the suggested techniques on tackling harder data-modalities for GANs, e.g. text or sequential data in general?', '- Authors claim to identify a fundamental problem with the existing generative models for point clouds, yet Section 2 tries to show that a _specific version_ that uses DeepSet does not satisfy theoretical guarantees. What if we use e.g. a recurrent network instead?', 'Although this extension seems to be easily derived using the contributions made at point 2.', '------------', 'Comparatively small changes in FSM may not affect the classification performance at all, while larger changes may not necessarily lead to worse classifications either.', \"Authors should clarify the justification behind experimenting only on 'first 500 test images'.\", 'The paper is well-written, has good experiments, and has a comprehensive related work section.', 'It is also nontrivial to control that the images contain only one salient object per image.', 'Pro:', 'There are a few typos throughout the paper such as:', '- Similar, it is interesting to evaluate how auto encoding performs when non-uniform decimation of the input cloud is performed, eg what happens if we “chop off” part of the input point cloud (eg the legs of the chair), does the model recover and add the removed parts?', 'The Rayleigh quotient example L(w)  = w’*A*w/ (w’*w) for a symmetric matrix A, then \\\\nabla L(w) = (2/w’*w)(Aw - L(w)*w), which is not perpendicular to w.', 'Weaknesses:', 'Summary:', 'in appendix: \"(see Figure A.4)\" -- the figure is labeled \"Figure 3\"', 'From a strategic standpoint, in a scenario where it is difficult to determine whether a new image belongs to a new class or not, could it plausibly be more sensible for the network to make a strong prediction on its best guess out of previously seen classes (that it knows more about) rather than a scattered prediction on the unseen classes?', 'Given that rxr is a small constant sized matrix and that matrix-vector multiplication can be efficiently computed on GPUs, this matrix adapted SGD can be made scalable.', 'In particular, since this paper does not show large-scale experiments, the connection serves to provide some more theoretical evidence for they CO performs well in practice.', 'The insights from the analysis of the failure cases are intriguing, but it also points out that the neural networks models are not really performing mathematical reasoning since the generalization is very limited.', 'Specifically, the first contribution listed in the introduction makes it look like this paper introduces the idea of not learning the decoder on the dataset (the one that starts with “The network is not learned and itself incorporates all assumptions on the data.”).', '- In the last paragraph of Section 3.3, the paper proposes making the lower-bound on the reward state-dependent.', 'It would be good to have some experiments in the sequence to sequence setting to understand if the obtained complexities are in fact what one would expect in practice.', 'This is misleading.', 'This provides some further theoretical justification of the success of CO on large scale GAN problems.', '- the authors fix the number of layers of the used network based on \"our experience\". For the sake of completeness, more experiments in this area would be nice.', 'It was nice that the paper iterated and reviewed the possible inference and learning ways.', '-', 'In general, I think of self-modulation as a way for the network to interact with itself, which can be a local interaction, like for squeeze-and-excitation blocks.', 'Some baseline DA methods [A, B] and datasets [C, D] are not considered.', '- In the second paragraph of A2 \"hypothesize the that relation\" should be \"hypothesize that the relation\".', '- Pros', 'Three popular self-supervised learning algorithms are then trained on this data sets, namely (Bi)GAN, RotNet, and DeepCluster, and the linear probing accuracy on different blocks is compared to that obtained by training the same methods on the reference data sets.', '1) The critical issue of this paper is that the algorithm is designed to minimize the upper bound.', '1. The dataset is adversarially filtered using BERT and GPT, which gives deep learning model a huge disadvantage. After all, the paper says BERT scores 88% before the dataset is attacked.', 'It’s not obvious to me that ME bias will provide a significant advantage in a one-shot or few-shot learning setting - i.e. how useful is having made a better initial guess (which is random amongst the unseen classes) after you gain some specific information about the target class? Perhaps the benefit is small relative to that of the one-shot or few-shot information gained - I think this needs to be quantified empirically.', 'Still, such approximations would make learning more challenging, especially with long-horizon backpropagation.', '3. The case of Batch Normalization.', 'However, I worry that one of the central theoretical arguments does not seem correct to me.', 'I wonder why the numbers are so different.', 'This is exactly what authors do in these sections.', '===== After rebuttal ======', 'The algorithm proposed in this paper is interesting.', 'In particular, the authors prove that the Hamiltonian Gradient Descent (HGD) algorithm converges with linear convergence rate to the min-max solution.', 'Furthermore, If I deploy the learning algorithm on a physical robot, will the temporally consistent exploration cause less wear and tear to the actuators when the robot explores.', 'The authors introduce a  novel  on-policy  temporally  consistent  exploration  strategy, named Neural  AdaptiveDropout Policy Exploration (NADPEx), for deep reinforcement learning agents.', 'I’m also wondering, is it harder to optimize the proposed architecture compared to DIP?', '- In 5.1, imagenet results are normally top-5 error rate not top-1 acc, would be better to report that more familiar number.', 'While sensible, this seems to me to be too minor a contribution to stand alone as a paper.', 'In considering combining with extrinsic rewards, I would also consider [Mirowski, et al., ICLR17], which is actually more involved in this regard.', 'But does this paper really achieves this goal?', 'The main problem with this paper is that it is difficult to identify its main and novel contributions.', '.', 'With some improvements in the evaluation and comparison, I believe this paper will be more complete and much stronger.', '4. Mean field analysis, although it lends an insight into the statistics of the activations, needs to connected with empirical observations.', 'It’s a well-written, simple paper that capitalizes on the trade-off between model realism and diversity, and the fact that VAEs and GANs (at least empirically) tend to lie on different sides of this spectrum.', 'The paper proposes pre-training strategies (PT) for graph neural networks (GNN) from both node and graph levels.', 'This should at the very least be explained more clearly.', 'What are the differences of the 6 pictures in Figure A7? Iterations?', 'Tsung-Hsien Wen, Milica Gasic, Nikola Mrksic, Pei-Hao Su, David Vandyke, Steve Young, EMNLP 2015: https://arxiv.org/abs/1508.01745', 'Ablation study shows that the use of the \"selection network\" strategy does not improve the results without these heuristics.', 'Both the proposed method and Yeh et al.’s method optimize the latent variable z of the generator using MAP, although the loss functions are slightly different.', 'The authors introduce strategies for pre-training graph neural networks.', '1. Related works: In deep learning with noisy labels, there are three main directions, including small-loss trick [1-3], estimating noise transition matrix [4-6], and explicit and implicit regularization [7-9].', 'The introduction can start at a lower level (such as flat/hyperbolic neural networks).', 'The authors propose a graph structure for entity states, which is updated at each step using the outputs of a machine comprehension system.', 'There is no direct comparison of performance.', 'Carlos Riquelme, George Tucker, Jasper Snoek', 'It merely just puts the CNN representation learning and the label propagation together to perform end-to-end learning.', 'The authors give provably better algorithms for the distinct elements problem, F_p moment problem (p > 2), and some more problems.', '[4]\\xa0Hoffman, Judy, Mehryar Mohri, and Ningshan Zhang. \"Algorithms and Theory for Multiple-Source Adaptation.\"\\xa0\\xa0Advances in Neural Information Processing Systems. 2018.', 'This complex flowchart does not even describe the complete task.', 'Indeed, each task in meta domain adaptation should be seen as a pair of source task and target task.', 'The Appendices also have great value in that regard.', 'This paper is reasonably interesting---it joins an effort to produce non-Euclidean models in a tractable way, which is fairly challenging, but could have a good impact.', 'Firstly, this WE can be used to visualize the learned model (as used in Zintgraf et. al. 2017).', 'As compared to prior work by Achiloptas et al (2017), the proposed approach has the advantage to allow for sampling an arbitrary number of points from the target shape, rather than a fixed pre-defined number.', 'You are heavy on the machinery and math. I find the learning in the latent space the important part and there are things like how much simulation is done in the latent learning not clearly spelled out. How does the effort compare to the 1E9 steps of the base line your refer to?', 'Evaluations are conducted on the standard ShapeNet dataset and the yields results close to the state-of-the-art, but using significantly less parameters.', 'The paper propose to incorporate natural out-distribution images and interpolated images to the additional class, but the problem of selecting the out-distribution images is itself an important problem.', 'The reason for this is only given in a single sentence at the end of Section 6, so it is a little confusing.', 'Also how does this compare to adaptive hyperparameter training techniques such as population based training?', \"I like that the authors are straightforward about the deficiency of the method (i.e. that you can't interpolate in latent space).\", 'The results are more mitigated for measures of accuracy.', '= Minor Comments', 'The main focus has been to develop an estimator that can reliably work with small dataset sizes.', 'For example, will the dropout cause the policy to be more robust?', 'Thus I believe authors must compare their method with these state-of-the-art approaches.', \"The paper is very clearly written. I'm not aware of a comparable work, so the novelty here seems good.\", 'For example,', '- There is a slight oxymoron in the premise of the first set of', 'The main idea is to represent a point cloud using a global latent variable that captures the overall shape, and a collection of local latent variables that code for the position of a point on the shape.', 'The notation of the true distribution as “q” the model as p and the approximate posterior of the model as “q” again is inconsistent.', 'Overall, the model is a bit complicated (e.g. question 1.), but the results are promising, the paper is well written, and the ability to manipulate formula embeddings is probably going to be useful in the context of theorem proving.', 'This forces the VI solution to tend to the MLE, sacrificing uncertainty in the variational distribution.', '[1] SGDR: Stochastic Gradient Descent with Warm Restarts. https://arxiv.org/pdf/1608.03983.pdf', 'Efficiently learning a policy from visual inputs is an important research direction in RL.', 'Significance:', 'Originality', 'In the case of this paper, the self-interaction allows the noise vector z to interact with various intermediate features across the generation process, which for me appears to be different than allowing intermediate features to interact with themselves.', 'DomainNet: Moment Matching for Multi-Source Domain Adaptation, ICCV 2019.', 'There exists more principled approaches for selecting out-distribution images that has not considered here like those based on uncertainty estimation or recently proposed direct out-distribution detectors.', 'The experiments demonstrate a superior performance of the proposed model compared to alternative benchmarks.', 'In general, recent work has found that the raw number of parameters has little to do with the size of the model class or the capacity of a model for deep models, and thus work like [A] has been trying to come up with better complexity measures for models to explain generalization.', 'Is this a coincidence or a general phenomenon? Is there a theoretical explanation?', '- The importance of section 4.2 should be clarified.', 'Comparisons for Interactive Fine-Grained Categorization', 'Local performance around the fixed point is explored.', 'So far, hand-crafted auxiliary tasks are generated, tailored for a problem of interest.', \"Ideally, an experimental setup with previously published results (e.g. control suite for DIAYN, Seaquest for DISCERN) would be considered, but I can understand why this wasn't done as incorporating domain knowledge is the main contribution of the paper.\", 'Obviously there must be some false ranking (specially at the initial stages of updating the classifier) for the unlabeled samples (e.g. the classifier may output high entropy for the unlabeled samples of the classes with labeled samples) and they may harm the performance of adaptation.', 'How about combining only Pose2Pose/ Pose2Frame  with pix2pixHD? Whether the performance can get improved?', '2. It is not very clear how is the base rate computed in Figure 5. I have a guess. But it is better to explain it in detail.', \"While the results seem impressive, the method to obtain them is not very novel; nonetheless, I would not have a problem with it being accepted, but I don't think it would be a loss if it were not accepted.\", 'I think that the current draft lacks strong experimental results to properly demonstrate the usefulness of the method.', '[Summary]', 'Regarding the main evaluation: The paper follows the \"standard\" protocol on ShapeNet.', 'Since BN is used, Glorot initialization has no effect for a forward pass.', \"On the downside, everything here is an extension of existing work, and the body of the paper is hard to read (though this may be inevitable, there's a lot of background to go over here).\", 'Also, it seems quite strange to me that making the FW overshot and then projecting back would be beneficial.', '- It would be nice to display more Nearest neighbors for the dog image.', 'If the selection network predicts 0 in come cases, it can be used to improve the result of \"classification network\" by flipping the corresponding label.', 'Did you try applying the classification loss to both the encoded representation and the canonicalized representation at the same time?', 'It spots the problem of each existing approach and solve the problems by combining each method.', '- The authors mention that in order to avoid overfitting they add an extra (weighted) KL-divergence term to the log-likelihood of the dataset, that encourages the weight distributions for specific points to be close to simple priors.', 'Even with no data augmentation, and even with the original networks, membership can only be assessed with a 90% accuracy.', 'If the authors have any ablation studies to back up their design choices, that would also be much appreciated, and will make this a more valuable paper for readers.', 'Indeed, the manuscript introduces sample complexity results to justify the benefits of the out-of-sample procedure (th 1), but it seems to me that these give an incomplete picture.', '* It was a bit hard to understand how a matrix is processed through the flowchart in Fig. 1 at first glance.', 'to similar work that looked at similarities among triplets (Similarity', 'In the abstract the paper promises more than it delivers.', 'I would appreciate it if the author could provide more explanation on the introduction of the super-graph in training.', 'page 3: “freefrom inputs and outputs” -> “freeform inputs and outputs”', 'The discussion of convergence to first order critical points is straightforward.', 'The non-observed dimensions are imputed by reconstructions from the prior from a partially trained model.', 'The neuromodulatory term is placed under network control, allowing it to be time varying (and hence to be sensitive to the input, for example).', 'This is achieved by treating the compositional space of local 3D video snippets as an individual dimension where an individual convolution is applied.', 'Combining SST with other existing techniques can help.', 'However, you are in a different computational model in which you now have access to an oracle.', 'The former produces feature maps on SO(3), which is deemed undesirable because processing 3-dimensional feature maps is costly.', 'The experimental results show that the proposed methods works well on this particular setup on CIFAR data set.', '*', 'The problem with the current model is that during training, it is trained to target at one specific type of test domain--the generator network G aims to generated images that align with the unsupervised  test domain X_test.', 'The horizontal axes are unlabelled, and \"margin normalization\" is confusing when shown together with SN without an explanation.', 'Large-scale kernel methods for independence testing', 'Also, G could be used to check whether you obtain the expected formula after 4 steps, which would be a more informative information than the L2 distance between the resulting embedding and the embedding of the final formula.', 'This paper proposed a framework to incorporate GAN into MAP inference process for general image restoration.', '- Work on image indicators of importance could be compared better with current work.', 'mistaken, from this result the authors extrapolate that the capacity', 'Another concurrent submission [4] verified this equivalence and showed one can also just tune learning rate for SGDM.', '-\\tThe paper established many reasonable baselines.', '3.\\tThe proposed method, which decomposes a problem into multiple concepts, looks general for many problem settings.', 'Then they continue to Equation (5) which they present as the combination of the true likelihood with a KL regularisation term.', 'Note, ICLR policy says that arxiv preprints earlier than one month before submission are considered a prior art. Could the authors elaborate more on possible practical/theoretical applications?', 'a) Why not try to do this with Variational inference? It should conceptually still work and be fast and potentially more robust.', '## Summary ##', 'The DAG’s edges can be pruned via a sparsity regularization term.', 'Even for federated learning where the adversary can control multiple parties, such as [1], all parties still use the same global backdoor pattern to generate poisoning samples locally.', '> The deep decoder is a deep image model G: R N → R n, where N is the number of parameters of the model, and n is the output dimension, which is typically much larger than the number of parameters (N << n).', 'Error bar may also be needed for comparison.', '2. Since the actor-critic model is the novel component in this model (propagating gradients through the learned dynamics), I would like to see additional analysis and baseline comparisons of this method to previous actor-critic policy learning methods, such as DDPG and SAC training on the (fixed) latent state representations, and recent work of MVE or STEVE that use the learned dynamics to accelerate policy learning with multi-step updates.', 'Formulating the abduction task as a (binary) classification problem is less interesting.', \"I think the authors should just acknowledge that you can't soundly *sample* from their generative model the way e.g. VAE or GAN allows (their function composition is not a sampling method).\", 'Furthermore, as authors discussed the iterative weight sharing which increases the depth can vanish the gradient toward input.', '- CLEVR environment or a modification of that dataset to reflect the form of systematic the authors are studying in the paper.', 'The intuition behind the idea is that since RBMs are generative, the RBM layer will act as a denoiser.', 'It makes a nice story that the theoretical properties justify the observations, but they may be as well completely unrelated.', 'It is still not clear why self-modulation stabilizes the generator towards small conditioning values.', 'Section 3:', 'The paper addresses a challenging problem of predicting the states of entities over the description of a process.', \"The authors adapted the Hyperbolic Neural Networks by Ganea et al. (2018) into the Kipf & Welling's (2016) version of GCN.\", 'It’s contributions include: i) using the H-divergence to bound both the risk across all domains and the worst-domain risk (imbalance on a specific domain); ii) a new loss to accommodate semi-supervised multi-domain learning and domain adaptation; iii) the experimental validation of the approach, improving on the state-of-the-art on two standard image benchmarks, and a novel bioimage dataset, CELL.', '* It is unclear to me whether the \"efficient method for SN in convolutional nets\" is more efficient than the power iteration algorithm employed in previous work, such as Miyato et al. 2018, which also used SN in conv nets with different strides.', 'The paper should at minimum engage with this extensive history, and, in light of it, explain whether its claims are actually novel.', '3. The world model is fixed while learning the action and value models, meaning that reinforcement learning of the actor-critic model cannot be used to improve the latent state model.', 'The drawback of a vanilla GAN with a DeepSet classifier is analyzed.', 'A lot of work has been done in this general area and on the problems that are discussed in the paper.', 'What would make it stronger imo is to address the issue of how much is gained from a discrete vs. continuous representation.', 'The connection between mutual information and generalization has been studied in several contexts [see, e.g., the references in this paper and https://arxiv.org/abs/1511.05219 https://arxiv.org/abs/1705.07809 https://arxiv.org/abs/1712.07196 https://arxiv.org/pdf/1605.02277.pdf https://arxiv.org/abs/1710.05233 https://arxiv.org/pdf/1706.00820.pdf ] and further exploration is desirable.', 'Clarity:', 'While the experiments across several domains convincingly show the presence of an anti-ME bias in neural network training, I recommend that the paper be rejected because it falls short in demonstrating to what extent performance could be improved by correcting or reversing this bias.', 'Maybe in section 3.7 you can address how often that occurs as well?', '- There is no motivations for the use of $\\\\lambda >1$ neither practical or theoretical since the results are only proven for $\\\\lambda =1$ whereas the experiments are done with \\\\lambda = 5,20 or 30.', 'This is because the digitals images in MNIST do not have rich texture and detail structures, thus are not very challenging for standard image restoration methods.', 'Specifically, how can mutual information in this context be formally linked to generalization/overfitting?', '1) Although the ensemble idea is new, the idea of selective self-training is not novel in self-training or co-training of SSL as in the following survey.', 'They use K=100 and suggest 3 possible choices of weights.', 'The authors might want to compare to \"Mathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze.', 'I suppose the proof is in the pudding; as we have argued, multiple agents can only improve performance if they are distinct, and Figure 1 shows some improvement as the number of agents are increase (no error bars though).', '###Clarity##', 'Same for action units.', '----', 'The novelty of this paper is:', 'The paragraph motivating the alt-az convolution on page 4 is not very clear, and some claims are questionable.', 'They show empirically that the learned models then are more likely to be successfully forced to misclassified images in which all the local triggers are present at test time, than are models learned using centralized backdoor attacks, where all attackers use the same trigger pattern (one of the same size as the concatenation of the local triggers, to be fair in the comparison).', 'Can the authors elaborate on why this choice should intuitively be better than the proposed method alone?', '* Minor notes:', 'Overall, the paper addresses an important problem in computer vision (video action recognition) with an interesting.', '(i) an “encoder” that takes a point cloud as input and maps it to a (point estimate of) the global latent variable of the shape represented by the input cloud, a point-net architecture is used here', 'Results on more scenes will make the performance more convincing.', \"I think that would shed light on what structures are learned and how they propagated in the image, possibly more than Figure 6 (which should really have something to compare to because it's not very informative out of context).\", 'Although I do understand the problem of evaluation in unsupervised DA, this should have at least been done in the semi-supervised case, and some analysis/discussion should be included for the unsupervised one.', 'It seems that with a single sample the method becomes an instance of VIB [1], only considering the weights of the network as latent variables rather than the hidden units.', 'The improvements are also seen in more realistic bAbI tasks.', \"This is discussed on p4, but it's unclear to me how keeping $\\\\sigma$ separate is benefitial for the analysis, and this is not picked up again explicitly again?\", 'This paper tackles the multi-source domain adaptation by aggregate multiple source domains dynamically during the training phase.', 'The paper devises a pipeline that aims to address catastrophic forgetting in continual learning (CL) by the well-known generative replay (GR) technique.', 'This should be stated clearly in the main text, and not disclosed in the final sentence of the final page of the appendix.', 'Authors argue that ME bias could help the model to handle new classes and rare events better.', 'The paper proposes using conductance to not only evaluate importance of hidden unit to the prediction for a specific input but also over a set of inputs.', 'I think that contributions of the paper are good enough to accept the paper.', '…', 'On the amazon review dataset, the performance of the proposed DARN model is comparable with the MDMN baseline.', 'Bartlett, et al. (2017) developed this technique to analyze the generalization bound for neural networks in a margin-based multiclass classification.', 'Overall, the paper is well written with clarity.', 'This baseline can be thought of a shared parameter graph with no message passing.', 'Minor points:', \"The author's included many different experiments to show this.\", '* The author say explicitly that this is not a convolutional model because of the use of 1x1 convolutions.', 'generalization/memorization properties of large and deep ConvNets in', 'A couple things I thought were missing in the paper:', 'novelty is quite limited.', 'Furthermore, in the beginning of Section 3.1 the authors present their idea on probabilistic hypernetoworks which “maps x to a distribution over parameters instead of specific value \\\\theta.” How is this different from the case that we were considering so far? If we had a point estimate for \\\\theta we would not require to take an expectation in Equation (3) in the first place.', 'The reviewer is also interested to see how the the generalization bound introduced in this paper is related to the recent theoretical works [3],[4] on MDL.', 'The results are mixed, we see slightly advantage over human baseline on one task but worse in the other.', 'This claim is unfortunately unfounded for a very important reason: the LSTM performance is not at all close to that which can be achieved by LSTMs in general.', 'The study is motivated by the observation that the Q-value matrix in reinforcement learning problems often has a low-rank structure.', 'References:', 'The paper investigates the possibility of learning a model to predict the training behaviour of deep learning architectures from hyperparameter information and a history of training observations.', '- I might have missed that, but are the authors offering an interpretation of their observation that the performance of the self-modulation model performs worse in the combination of spectral normalization and the SNDC architecture?', 'equations (3) and (4)', 'Minor comments:', '+ The paper is generally clear and readable.', '2. The paper says v_i (initial representation of each entity) is obtained by looking at the contextualized representations (LSTM outputs) of entity mention in the context.', 'This paper proposes a meta-learning framework that leverages unlabeled data by learning the graph-based label propogation in an end-to-end manner.', 'See minor comments for more specific suggestions.', 'To synthesis the next frame, a Pose2Pose network is proposed to first transfer the input information into the next frame body structure and object.', '2. Page 5: last line: estimate for -> estimated for', \"The writing and experiments could use some improvement, but I believe that the majority of the ICLR audience would enjoy seeing this result (even though it would have no impact on most people's research)\", 'I think this will be a good comparison.', '-\\tThe experimental results of section 5.2 are somewhat disappointing.', 'The idea is to first learn discrete representation (vector quantization is done by Gumbel softmax or k-means) from audio samples with contrastive prediction coding type objective, and then perform BERT-style pre-training (borrowed from NLP).', '5. Whether the ME bias mostly attributed to the one-hot representation? If one uses word2vec as the representation in NN', '- It’s interesting to me that the ResNet architecture performs better with self-modulation in all settings, considering that one possible explanation for why self-modulation is helpful is that it allows the “information” contained in the noise vector to better propagate to and influence different parts of the generator.', 'Maybe the authors can elaborate more on the significance/relevance of this contribution.', 'Then to state to which of these cases the results of the paper are applicable, allow for an improvement of the variance and at what additional computational cost (considering the cost of evaluating the discrete derivatives).', '*', '8. The itemized part in 5.3, \"...carefully selected baselines: 1.xxx, 2.xxx, 3. xxx, 4. xxx\". However, both 3 and 4 are not baselines!', 'Especially, I believe that the VAE will provide a way to approximate the prior easier than the GAN.', 'Authors proved that the sandwiching object W_s is closer to the real Wasserstein distance, but it increases the variance of the loss function.', '1) The problem it aims to solve is neither multi-task learning nor meta-learning: it tries to solve a supervised classification problem defined on principle classes, with the help of simultaneously predicting/generating auxiliary class labels.', 'Firstly, the paper derives a multiple-source domain adaptation upper-bound from single-to-single domain adaptation generalization bound, based on the theoretical work from Cortes et al (2019).', 'However, this task is an instance of natural language generation: given a meaning representation (quite often a database record), generate the natural language text correspoding to it. And previous work on this topic has proposed very similar ideas to the scratchpad proposed here in order to keep track of what the neural decoder has already generated, here are two of them:', 'The proposal of applying spectral normalisation (SN) is well motivated, and is supported by margin-based bounds.', 'First, doing so does not touch the core of the proposed application.', 'It would be nice to see a better case made for spherical convolutions within the experimental section.', \"Maybe it's because of different scales? Previous works usually scale each pixel to [0,1] or [-1,1], maybe the authors use the [0, 255] scale? But 0.1/255 will be much smaller than 0.031.\", 'The motivation, based on the writing, is that constant curvature spaces are more general where the computation is easy to handle.', \"I doubt this would have worked so well hadn't it been modeled this way (not to mention this allows a small number of parameters).\", 'However, since the authors adopted an affinity-aware distance, two incorrect predictions can still be compared based on their semantic tree distances to the true class.', 'The explanation for underperforming on SVHM (page 7) may be valid, but you could easily prove it right or wrong by adding an option to SST for \"stratified SSL.\" Without this extra work, your claim is just a conjecture.', 'This paper tackles the problem of automatic robot design.', 'The proposed method employs a graph neural network to embed math formulas to a latent space.', '--“Redundant weights” seems like not a very strong constraint especially for a small cardinality label space (like 10, in the case of this paper).', 'In general, I feel this section could use some tighter formalism and justifications.', 'In other words, the auxiliary labels for a specific principle class are very possible to be multiple noisy copies of the principal label with random perturbations.', '- Capitalize: “section” -> “Section”, “appendix” -> “Appendix”, “fig.” -> “Figure”.', '- In figure 8 \"Each column corresponds to ...\" should be \"Each row corresponds to ...\".', 'Pros:', '# Summary', 'However, DSO-NAS further proposes to incorporate the computation expense of architectures into step (2) above, leading to their found architectures having fewer parameters and a smaller FLOP counts.', 'Is it the threshold that maximizes F1?', 'The authors then note the fact that their method lacks a continuous latent space that allows for interpolation, as provided by existing (VAE-like) methods.', 'Also, I believe that some actual (analytic) examples of F should be provided, at least  in the experiments.', '.', 'Pros:', 'Experiments are shown on various architectures (CNN, RNN) and comparisons are made against SGD, ADAM.', 'I tend to lean toward the result carrying some significance since it does extend the class of problems for which the convergence rates exists.', '[2] Dasgupta, Bhaskar, and Eduardo D. Sontag. \"Sample complexity for learning recurrent perceptron mappings.\" Advances in Neural Information Processing Systems. 1996.', 'The proposed approach should compared to other stronger methods such as graph convolution neural network/message passing neural networks/structure2vec.', '- The stated contributions number 3 and 5 are not truly contributions.', 'For example, it is not clear the strong emphasis on the robust MDP formalization and the fact that MCTS finds a Nash equilibrium.', 'The paper proposes to learn task-level modules progressively to perform the task of VQA.', 'For example', 'Using the conditional batch normalization mechanism, the input noise vector is allowed to modulate layers of the generator.', \"However, this paper tries to argue that such efforts are not very useful as these statistical characteristics don't provide any systematic explanation for the performance of DNNs across different settings.\", 'The text mentions that (a) is fitting the noisy image, (b) is fitting the noiseless image, and (c) is fitting noise. Is it all done independently with three different models?', 'It is known that SGD with fixed step-size can not find the optimal for convex (perhaps, also simple) problems.', '-\\tShould the title of the paper specify the paper is about “language-based” abductive reasoning.', 'The main contributions of the submission are:', '2. No need to write so much decorated bounds in section 3.', 'A comparison with Graph Convolutional Network based techniques seems appropriate (e.g. Kipf and Welling 2017).', 'The new bound provides new insight and helps the design of algorithm.', 'I only have a couple of concerns:', 'The authors compare to the work of Schott for one type of attack.', 'I greatly appreciated the negative results reported in the main text as well as in the appendices and this has significant value.', 'The optimization objective of DSO-NAS is thus:', '* There is still no comparison with competing nonparametric tests on the fMRI data.', 'The paper studies few-host learning in a transductive setting: using meta learning to learn to propagate labels from training samples to test samples.', 'I assume the authors first train the Pose2Pose network, then use the output to train the Pose2Frame network.', 'Finally, the proposed approach seems to sort of straddle the line between traditional convex optimization algorithms, and the fast stochastic algorithms favored in machine learning.', 'A few (roughly chronological comments).', 'The proposed method is very simple and frames the problem basically as a supervised learning problem.', 'Eventually the matrices achieve a stationary point, i.e., fixed point of the dynamic system.', 'I believe that the paper should currently be rejected, but I encourage the authors to revise the paper.', '\" There are multiple new variables in this paragraph, without specifying the dimension and meaning for each attribute, it\\'s really hard to understand.', 'Experiments are performed on real as well as synthetic datasets using Hsu et al.’s method as an oracle.', '[B] MeshCNN: A Network with an Edge. Hanocka, Hertz, Fish, Giryes, Fleishman and Cohen-Or. SIGGRAPH 2019.', 'This poses a challenge in evaluating this paper.', 'Thank you for the feedback.', 'This paper studies backdoor attacks under federated learning setting.', 'Major concern:', 'I propose a marginal acceptance for this paper as it produces impressive results in what appears to be a short amount of search time.', 'The second one considers that a sample is part of the training set if its loss is below a threshold.', '[Strengths]:', 'Could you comment on the importance of this component? Did you test the model on other types of videos where this hypothesis is less relevant?', 'Comments/Questions:', 'b) The prior seems to be picked according to properties of the observed data and expressed in a product of constraints.', '- The method nicely blends a few components such as self-supervised learning, meta-learning, auxiliary tasks into a single model to tackle the meta auxiliary learning.', 'New state-of-the-art results are achieved on two datasets.', '=> Environment: The experimental section of the paper can be further improved.', '-- solving Eq 3 by matrix inversion does not scale. Would be best to also show results using iterative optimization', 'While the model seems to perform well, the originality and the improvement w.r.t. baselines are somewhat limited.', 'The difference is that Cakewalk uses the CDF directly, while CE uses a threshold function on the CDF.', '1. For the evaluation of DBA, I assume that there are 4 adversarial parties, controlling each of the 4 local triggers. When using centralized attacks, are there still 4 adversarial parties, although they share the same global trigger, or if there is only 1 adversarial party?', 'https://papers.nips.cc/paper/5452-a-wild-bootstrap-for-degenerate-kernel-tests.pdf', 'This paper proposes an approach for mitigating issues associated with high-frequency/amplitude control signals that may be obtained when one applies reinforcement learning algorithms to continuous control tasks.', '1. It is not clear that training a network to classify a set from another set is necessarily equivalent to ``memorization’’.', 'Additional feedback:', 'Did the authors tried any version of their model with convolutions or pooling and found it not to perform as well? Measuring the number of parameters when including pooling or convolutions can become tricky, was that part of the reason?', '- Jointly learning an inference network (Q) has certainly been done before, and I am not sure authors provide an elaborate enough explanation of what is the difference with adversarially learned inference /  adversarial feature learning.', '- The idea is simple and easy to implement based on a standard GAN.', 'Performance is measured in terms of Fréchet Inception Distance (FID) for models trained with and without self-modulation on a fairly comprehensive range of model architectures (DCGAN-based, ResNet-based), discriminator regularization techniques (gradient penalty, spectral normalization), and datasets (CIFAR10, CelebA-HQ, LSUN-Bedroom, ImageNet).', 'The paper addresses the problem of producing sensible (high) uncertainties on out of distribution (OOD) data along with accurate predictions on in-distribution data.', 'Then the representations are fed to the four architectures you listed in figure one.', 'They proposed a manifold regularization by approximating the Laplacian norm using the stochastic finite difference.', '3. Section 4.2 need more clarity.', 'Minors:', 'In the training of the feature extractor and classifier, the author introduces a super-graph with each node representing a super class.', '- Figure 4 does not illustrate M=4 and N=4, e.g. which operation belongs to which layer?', 'The authors use a two-level hierarchical distribution as a policy, where the global variable is used for dropout.', 'The authors support their theoretical analysis with experiments in which the oracle is represented by a deep neural network and demonstrate improvement over classical algorithms that do not use machine learning.', 'I also wonder if the video data will be released, which could be important for the following comparisons.', 'Also, I think that additional methods to compute the image prior should be included in the experiments.', 'For noisy K-FAC and MNF the lambda (which should be fixed to 1 for a correct Bayesian model) was tuned and in general lower than 1 lambdas lead to models that are overconfident and hence underperform on uncertainty tasks.', '(6) If the authors jointly and simultaneously optimize \\\\theta and \\\\phi, why a regularization term about q_{\\\\phi}(z)  is missing in Eq 12 while a regularization term about \\\\pi_{\\\\theta|z} does appear in Eq 12?', '=> The results shown in Figure-4 (Section-4.2) seems unclear to me.', 'I have one technical question as follows. If the authors reply appropriately, I will raise the score to accept.', 'However, there are several concerns about this paper.', 'I disagree and I actually think this is important for two reasons.', '* The introduction distinguishes between \"gradient-based methods\" and \"optimization-based methods\".', 'However, as a next-contribution, this work deserves to be seen more widely.', 'Bottomline: The paper presents interesting ideas and good results, but would be better if the modeling choices were better explored/motivated.', 'Smaller Notes.', '- \"rationally symmetric\" -> rotationally', 'Could you elaborate more on why the selection network is needed? How would it compare to a simple strategy of only including the datapoints whose top-1 prediction of \"classification network\" is greater than some threshold?', 'Is this how you implemented minibatching in practice? How easily is this applied to convolutional architectures?', \"Because these differences aren't explained, the synthetic tasks in the experimental section make this approach look artificially good in comparison to Hartford et al.\", 'Weaknesses:', 'Although the recommended length is 8 pages, the strict limit is 10, so I would recommended to use a bit of the remaining extra space to conclude the paper properly with a discussion on the results and their consequences, as well as a conclusion to wrap up the paper.', '===========', 'Another variant proposes updating the Hebbian synapse with modulated exponential average of the Hebbian product.', 'Plugging p(I|D) in Eq. 12 and using decomposition of p(D ,I) used in Eq. 10, we arrive at a formulation which drops all p(I|D) terms.', 'Is it the data fit of the neural networks? With what specific measure?', 'It may be worth adding a short discussion/comparison to that work as it also considers zero-shot learning.', 'MINE involves fitting a very flexible parametric form of the distribution, such as a neural network, to the data to derive a mutual information lower bound.', 'In this paper, the authors propose a method for dimensionality reduction of image data.', 'Equations (4, 5): you can save some space by only specifying the factorization (left column) and merging the two equations on one row', '3. Incorporating architecture costs into the search objective is nice.', 'The authors did not comment on the computational overhead of training their LDA.', 'See e.g. [1].', 'It would be nice if authors can provide some reasons and comparisons for their choice on the optimizer of W_U.', 'How would the given graph network compare to this?', 'They observe that NMNs generalize better than other neural models when an appropriate choice of layout and parametrization is made.', 'It would have been useful to compare the general models here with some specific math problem-focused ones as well.', 'Additionally, the authors’ use of gradients of the generator as an approximation for the manifold penalty is a clever.', \"1. I don't see why MINE cannot be applied to the fMRI dataset and results be reported.\", '- evaluation: only on toy tasks (which includes PTB), no real world tasks', 'The resulting method is tested on a toy problem and semi-supervised node classification of commonly used datasets, showing the possibility of improvement.', 'It would be interesting to see if the proposed method is competitive for training contemporary networks and w.r.t. simple schedule schemes.', 'However, it seems the only Figure showing D’s loss when unconstrained is Figure 26, in which it is hard to notice any significant jump in the loss.', '2) It discusses the class collapsing problem in generating pseudo (auxiliary) labels and provides a reasonable solution, i.e., using KL divergence as regularization.', '2. The introduction of a novel metric that tries to capture the \"tunability\" of an optimizer.', 'Cons:', 'However, the additional cost is expensive.', 'Rather than motivating the work in this way, it might be helpful to focus the contribution as a combination of future time step prediction and discretization (both of which have been considered in previous work, but not in combination).', 'It seems like a valid and flexible extension that can be used in other continual learning frameworks.', \"- How multimodal are the datasets provided by UCI? It seems like they consist of different tabular datasets with numerical or categorical variables, but it was not clear what the modalities are (each variable is a modality?) and how correlated the modalities are. If they are not correlated at all and share no joint information I'm not sure how these experiments can represent multimodal data.\", 'In Figure 6 there seem to be unnecessary discrepancies between the y-axis and colorbar of subplots (a) and (b), and keeping those more consistent would improve readability.', 'Experimental results on a few data sets prove the effective of the proposed approaches over existing state-of-the-art approaches for unsupervised node embeddings.', 'The basic method is taken from [Parthak, et al., ICML17] (modulo some empirical choices such as using PPO).', 'The proposed work is significant, since it explores a new direction of using learner generated, interpretable explanations of the currently learned task as help for learning new tasks.', 'Comments:', 'I also understand that one is constant epoch and other is constant step.', 'The paper claims that only the nearest neighbor algorithm can handle different degradations.', 'set used for training (its diversity, closeness to the positive set', 'The paper claims that for each dataset the best performing network cannot be attributed to any single regularizer.', 'I however maintain my grade.', \"- I'm a little unclear when data-augmentation is included in the\", 'Second, due to the norm based constraints, authors actually need to optimize a highly nonconvex optimization problem.', 'Section 2 states that section 4.2 proves that a \"path method\" must be used in order to satisfy the axioms, but why such axioms are important is not stressed enough.', '[Pros]', 'The idea that multiple mappings will produce better results than a single mapping is reasonable given previous results on ensemble methods.', 'However, I am wondering whether this paper is perfectly suited to ICLR conference due to the lack of experiment, practical implication given by the theory, or theory in the non-convex setting (I know that the latter is a huge open question and I am not criticizing the absence of theory in the non-convex-concave setting).', 'This paper presents  a controllable model from a video of a person performing a certain', 'While these may indeed be working as advertised, it would be good to see some evaluation that verifies that after learning, what is actually happening is coreference.', '3. The ground truth is selected by human.', 'The problem is important to understand in the theoretical machine learning community.', 'These are solved, and only require very limited validation.', 'These pre-trained mappings (\"agents\") generate targets from the primal to the dual domain, which need to be mapped back to the original input.', 'Decision: Weak Reject', 'The idea of carving up the icosahedral grid in just the right way, so that the spherical convolution can be computed as a planar convolution with funny boundary conditions, is very clever, elegant, and practical.', '- The counter example at the bottom of page 2 is limited, in the sense that the oracle assumption seems highly non-realistic, casting doubt on the relevance of the argument.', '- Alg. 1 for T>1 is very similar to I-FGM, but also ‘pulls’ x_t towards x_orig.', 'This paper gives an improved definition on shift-equivalent functions and shows that the proposed F-pooling is optimal in the sense of reconstructing the orignal signal.', 'The experimental sections look rather mechanical. I would have put some results on the learned embedding. Or some demonstration of the embedded history or probability to intuitively convey the idea and how it works.', '- The paper is well written and easy to read.', 'The level of disagreement was measured by a semantic-aware distance derived from WordNet ontology.', 'I am curious whether the paper has other good side effects.', 'The gradient is designed in the way to approximate the  property of reparameterization gradient.', 'This makes the contribution of this paper in terms of the method', 'This bound is not so surprising because as long as certain regularity condition holds, the policies being close should lead to the returns being close.', 'In numerical experiments, there is no comparison with major competitors besides random sampling in the active learning setup.', 'Also, my understanding is that the optimization goal is to find first a feasible solution, and then find the point that maximizes f. I think that this can be clarified in the text.', 'It is amazing that one can do multiple steps of math reasoning after only training the model using data from one single step.', 'I believe it should be discussed in related work.', '+ The paper has a very comprehensive set of references in the areas it', 'There are other minor details:', '***', 'can  you elaborate?', 'More precisely, they use the ‘bilinearity’ of the objective (due to the interaction between the players) to prove that the squared norm of the vector field of the game follows some Polyak Lojasiewicz condition.', 'For instance we can multiply Rz(phi) Ry(nu) by the element Rz(omega)Ry(0) = Rz(omega), which gives the element Rz(phi) Ry(nu) Rz(omega).', 'Thank you for your response.', 'The authors further argue that in case that overfitting is present at CDNs, then an extra KL-divergence term can be employed such that the input dependent MVG distribution is close to a simple prior that is input agnostic.', '2. In section 3.2, the authors argued that 3D kernel suffers from trade-off between receptive field and cost of computation.', 'Missing references:', 'First by introducing a masked content-based addressing which dynamically induces a key-value separation', '********', 'However, the running time of attacks is typically measured in seconds and should not be the limiting element in real-world attacks on deep learning systems.', 'Of course, there is still the problem of learning the network with smaller sized data.', 'This would mean that the layer is actually SO(3)-equivariant, but it has been proven [1], that any rotation equivariant layer between scalar spherical feature maps can be expressed as an azimuthally isotropic convolution.', 'The paper conducts experiments with different regularizers associated with some of the standard statistical characteristics using the MNIST, CIFAR-10, and CIFAR-100 datasets.', 'is the difference here?', 'However, in real world scenarios, it is actually challenging to obtain exact degradation information.', '[Summary]', 'In particular, the node-level pretraining described in section 3.1.1. seems rather complicated to implement as a context graph needs to be computed for each node in the graph.', 'This is interesting but no formal results are presented.', 'In its current state, I am not sure that it adds a lot to the manuscript.', 'The paper proposes a new architecture for the DIP method which has much less parameters, but works on par with DIP.', '(ii) Why is cross-entropy the right objective?', '[4] Rethinking the Hyperparameters for Fine-tuning', 'These goals are to illuminate the', 'Also, it is clear in the experiments the superiority with respect to Arora in terms of iterations (and error), but what about computational time?', 'The testbeds all existed previously and this is mostly the effort of pulling then together.', 'Compared with AAE and the simplified variants of the proposed PC-GAN, the proposed PC-GAN achieves incremental results on point cloud generation.', 'The authors either need to remove these results or restate them in a different way in order to satisfy the assumed conditions.', 'This paper proposes an extension to the continual learning framework using existing variational continual learning (VCL) as the base method.', 'The reviewer encourages the authors to make further new developments and have a more comprehensive literature review. But in the current form, the paper has less value to be published in ICLR.', 'Intuitively the objective wants f_0 to generate samples which, when mapped back to the X domain, have high log-probability under G, but its samples cannot be differentiated in the case of discrete data.', 'Reading it feels like reading about lattices when the work is about general graphs, and lattices provide no intuition about the proposed solution.', 'My biggest concern in the methodology, however, has to do with the selection of the matrix variate normal prior for the weights and the imposition of diagonal covariances (diag(a) and diag(b)).', 'The proposed model enables the decoder at each time step to modify all the encoder outputs, thus using the encoder as a “scratchpad” memory to keep track of what has been generated so far and to guide future generation.', 'The performance of the proposed method is worse than the previous work but they claimed \"state-of-the-art\" results.', 'attack seems odd.', '[D]H.Venkateswara, J.Eusebio, S.Chakraborty, and S.Panchanathan. Deep hashing network for unsupervised domain adaptation. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017.', '1) I think definition 2.5 of Higher order Lipschitz is very strong assumption to have. What exactly means? Essentially the authors upper bounded any difficult term appear in the theorems. Is it possible to avoid having something so strong? Please elaborate.', 'the representation (i.e. fewer total parameters needed to have the', '- Even though oracle results are interesting, to make it practical it may make sense to talk about a more realistic, weaker oracle where some of the queries may be incorrect.', 'The result holds for three state-of-the-art self-supervised methods, tested with two single-image training examples.', 'I understand that the main contribution of the work is the theoretical analysis of the proposed method but would like to see some numerical evaluation in the main paper.', '2008', 'On page 11, “in Figure A2” the first word needs to be capitalized.', 'Modifying the objective to incorporate domain knowledge (as done in your DIAYN baseline) yields I(a; s_i | s_{t-1}) and is amenable to maximization by either of the lower bounds considered here.', 'Also, this work would benefit significantly from a better experimental evaluation.', 'According to the illustration, it seems that you first obtain “features/representations”.', 'I guess it is the $y_{tar}$ fixed in the problem formulation Sec 3.2.', '[1] Arora, S. Ge, R., Ma, T. and Moitra, A. Simple, Efficient, and Neural Algorithms for Sparse Coding.', 'Some of the problems are listed as follows:', 'This will allow the model to better describe a larger set of words.', 'The authors do not thoroughly explain the motivation of this paper.', '+ This paper proposes elegant method to tackle new terms in the loss function and gives its complexity analysis.', 'Finally, the show on a classical scheduling task.', '3. The best architecture is selected and retrained from scratch.', 'This is different from most existing work.', 'It is well written, clearly structured, and the mathematics is clear and precise while avoiding unnecessary complexity.', 'When optimizing the compression rate, it is important not to look at the test set error.', 'The paper proposes to use different random seeds and iterate over the dataset in a different order for distinct pretrained f_i.', '- also, if the authors can explain more on sec 2.5 it will be helpful. If we simply ignore the imaginary part, although the theory is not applicable, but what would the empirical performance be?', \"Overall I recommend publishing the paper as it is a well-written and insightful discussion of batch normalization. Be aware that I read the paper and wrote this review on short notice, so I didn't have time to go through all the arguments in detail.\", 'If the training samples and model complexity (think about the parameters in the deep models) are significantly large, the upper bound of the loss might be also very large.', '-\\tThis is a topic that should be of broad interest to the ICLR community.', \"Moreover, I don't think some of the presented experiments are necessary.\", 'On the technical side, the author presented a discretization step to use transformer for learning curve predictions.', 'Meanwhile, the computational cost on large dataset such as ImageNet could be huge, the authors should further develop the method to make sure it works in all situations.', 'Third, the comparison to baseline and “DeepSet” is not fair.', '[1]\\xa0Tzeng, Eric, et al. \"Adversarial discriminative domain adaptation.\"\\xa0Computer Vision and Pattern Recognition (CVPR). Vol. 1. No. 2. 2017.', '1. When the layers tends to infinity, the covariance matrix reaches stationary (fixed) point.', 'Also, the claim in Fig. 1 that the transition from ‘’high capacity’’ to low capacity happens at the number of parameters in the network seems a bit loose and hard to substantiate from what I understand, and should be toned down. (*)', 'Thus, without sufficient justification the assertion in the paper that the capacity of the network is well approximated by the number of parameters does not seem correct.', '### Post rebuttal', '1. Authors calculate W_U in a primal form via solving an assignment programming problem.', 'The paper achieves impressive results across a large selection of tasks, both in terms of sample efficiency and final performance.', 'An encoder outputs the parameters of a hierarchy of reconstruction networks that can be applied in succession to map random samples on a unit sphere to the surface of the reconstructed shape.', 'The authors proposed meta domain adaptation to address domain shift scenario in meta learning setup.', 'Page 9: conditions on -> conditioning on', 'It would be interesting to see how it can improve existing learning-based theorem provers.', 'In the original coverage paper (Tu et.al, 2016), they also proposed a “neural network based coverage model” where they used a general neural network output to encode attention history, although this paper works differently where it directly updates the encoder hidden states with an update vector from the decoder.', 'The authors proposed another improved gradient approximation by first computing the normalized manifold gradient \\\\bar r(z) and then adding a tunable magnitude of \\\\bar r(z) to g(z), i.e., ||f(z) - f(g(z) +\\\\epsilon \\\\bar r(z) )||_F. Since several previous works Kumar et al. (2017) and Qi et al. (2018) also applied the idea of manifold regularization into GAN, the authors pointed out several advantages of their new regularization.', 'This paper proposes modifications to the original Differentiable Neural Computer architecture in three ways.', '[1] Noroozi, Mehdi, and Paolo Favaro. \"Unsupervised learning of visual representations by solving jigsaw puzzles.\" European Conference on Computer Vision. Springer, Cham, 2016.', '- The authors conduct various experiments to show the interaction of the regularization and the generator.', \"The method integrates notions of activation value, input influence to a neuron and neuron influence to the network's output.\", 'On the other hand it is clear that using the confidence of the model to predict the dataset is a useful property, but the right side of the Fig. is very confusing.', '* The text is quite hard to read.', 'Experimental evidence (albeit on specific type of question) of this behaviour will be helpful for the community and hopefully motivate them to incorporate regularizers or priors that steer the learning towards better layouts.', 'The use of Glorot uniform initializer is somewhat subtle.', 'But context prediction has also been considered before, also for speech [5, 6, 7].', 'First, the method prunes a dense matrix based on the Viterbi-based pruning.', 'The paper also demonstrates superior performance with the proposed method on continual learning on all classic tasks comparing with VCL and EWC.', 'The improvements over basic ensembling are rather minimal, at the cost of extra computation.', 'There are some minor comments below.', 'However, the reviewer feels that the brief treatment of mutual information regularizer leaves something to be desired.', 'It is also not clear to me why CIFAR datasets involve two domains and how these domains are relevant in each of the tasks.', 'As in (Cover and Thomas, 2012), which is also cited in this paper, DPI is defined on a Markov chain X -> Y -> Z and we have I(X,Y) >= I(X,Z).', 'The paper is very well organized and written.', '* The epsilons in Fig. 5 have very different scales (0 - 0.5 vs. 0 - 5). Are these relevant to the specific algorithms and why?', '- there is no attempt to provide a theoretical insight into the performance of the algorithm', 'Here are a few questions and concerns:', 'Moreover, in spite of the authors writing that their goal is “completely different” from [Lee at al 18a, Ma et al 18a], I found the two cited papers having a similar intent and approach to the problem, but a comparison is completely missing.', 'The interesting part in my opinion is the experiments on constant steps.', 'It is therefore the case that the fMRI results may be false positives.', \"Also it's not clear from the details in the paper what are the architectures for the VAE and RFs (there's a reference to the code but would've been better to have sufficient details in the paper).\", 'This paper presents a method for unsupervised representation learning of speech.', 'In contrast, the studied learning rates are asymptotic and there is a big discrepancy.', '[2] Chen et al., Targeted Backdoor Attacks on Deep Learning Systems Using Data Poisoning.', \"Unfortunately the paper doesn't provide any qualitative analysis on how modulation is employed by the models after training.\", 'Equation (7) (integration by parts) holds only with some additional requires on f.', 'The paper studies one instantiation of this systematic generalization for the setting of binary “yes” or “no” visual question answering task.', 'Also, I am left wondering what are considered the parameters of the models -- are only the neuromodulatory terms considered as the additional trainable parameters compared to baseline LSTMs? How are the Hebbian synapses themselves considered in this calculation?', 'At least, it should be compared with \"Multi-bit quantization only (Xu et al., 2018)\" and \"Multi-bit-quantization + Viterbi-based binary code encoding\".', 'It is fine, but it seems weird that the author still mentioned the setup of MNIST+SVHN in the main text.', 'Intuitively, these criteria are well motivated, but unfortunately, the combination of all the intuitions (including \"selection network\" with threshold) is not very principled.', 'Are all these features taken directly from the self-supervised network or is it fine-tuned in some way?', '- The authors only provide running times, not the number of iterations.', 'Page 2 paragraph 1: “an neural attention mechanism”', 'Here’s why:', 'The paper is not well-organized and not written in a consistent way. For the method and the experiment sections, I need to jump back and forth several times in order to understand what the authors are trying to say.', 'This kind of fine-grained analysis of poisoning is highly needed.', 'The proposed regularization strategy enforces that a discriminator or a given classifier should be invariant to small perturbations on the data manifold z. It is empirically shown that naively enforcing such a constraint by randomly adding noise to z could lead to under-smoothing or over-smoothing in some cases which can harm the final classification performance.', 'The same way for unit consistency I would use $L_H^2$ instead of $L_H$', 'Minor comments:', 'What are reasons for these?', 'The paper show that under some assumption on the cost function of the min max that are (in some sense) weaker than strong convex-concavity.', 'The paper presents a way to learn a vectorial representation for items which are only described by triplet similiarity expressions.', 'For its original importance measure and the proper experiment benchmarks, I believe this paper should be accepted.', 'RFs are never trained and IDFs are continuously trained.', 'The authors propose a scheme', '- In the last paragraph of A1 \"growth of the layer width respect\" should be \"growth of the layer width with respect\"', 'The approach is evaluated only in three cases: fish, walker, cheetah. Can it be applied to more complex morphologies? Humanoid etc. maybe?', 'The authors need to elaborate more on this.', 'To learn long-range evolution of spatio-temporal representation of videos, the authors proposed V4D convolution layer.', 'In the algorithm, the authors need to define the HT function in (3) and (4).', 'However, the modification is slightly marginal but seems quite effective.', '- In the paper, the states are represented by only object positions (x, y, z). Is this sufficient? (e.g. velocity is unnecessary?)', 'That is to say, unlike Pearson correlation, they do not make specific parametric assumptions on the nature of the dependence.', 'This metric attempts to trade off the performance of an optimizer when tuned only with a small number of hyper-parameter trials, and its performance when carefully tuned.', 'I very much enjoyed reading this paper.', 'The authors introduce an ensemble of MC-Dropout models with different initialization to remedy this mode collapse problem.', '- The authors claim that this is the first zeroth-order non-convex FW convergence rate, I am not familiar enough with the field to evaluate this claim and its significance.', 'Experimental setup is easy to understand.', 'However, the proposed upper bound in the paper involves other parameters, such as the model complexity and the number of training samples.', 'However, it looks to me that the authors need to better explain the motivation of DiVA, the differences of DiVA from existing supervised VAE, and the experimental settings, before the acceptance of this paper.', 'Then, there will be another question: how the two networks are trained? Are they trained separately or jointly?', 'It is an interesting combination of methods.', 'hard to judge. Please include further description of the ES cost function and algorithm in the main body of the paper.', '- In terms of prediction performance, the authors should also compare to [1] and [2] which either predict the other modalities completely during training or use tensor-based methods to learn from noisy or missing time-series data.', 'This was helpful to get a sense of the main points before going through the appendix.', \"The authors' response addressed my remaining questions.\", 'They also explore whether appropriate structures can be learned.', \"In sec. 4, there's a reference to an initial set of experiments with count-based methods without much details.\", 'The novelty of this method is minimal.', 'The authors present a variational selective autoencoder model that learns only from partially-observed data.', 'The contribution improves Arora 2015 in that it converges linearly and recovers both the dictionary and the coefficients with no bias.', 'This paper shows that Hamiltonian gradient descent (HGD), which is gradient descent on the norm of the squared norm of the vector field, achieves linear convergence for a broader range of problems than bilinear and convex-strongly concave formulations.', 'The idea is intuitive and justified by their theoretical analysis.', 'Fourth, please make it clear that the proposed method aims to estimate \"causality-in-mean\" because of the formulation in terms of regression.', 'This means that it is not apriori clear if using this solver instead of standard SGD, ADAM is any good.', 'This is also lacking from the description of the experimental protocol, which does not address the data-splits (how many classes were used for each) and size of the unlabelled test set.', '* Page 9: learn to useful -> learn useful', 'In particular, the idea of the selective proposal distribution is interesting and provides an effective solution to deal with the problem of missing modality in conventional multimodal learning.', 'Both the results on the development set and on the test set should be reported for the validity of the experiments.', 'For graph-level pretraining, some general graph properties need to be predicted by the graph.', 'Comments.', 'Perhaps there is room for memory/memories in the latent space?', 'In terms of modeling, since the input into the prior network has finite possible discrete values, we do not need a fully connected network to generate $\\\\hat{\\\\mu}_c$ and $\\\\hat{\\\\sigma}_c$. Instead, we can directly optimize $\\\\hat{\\\\mu}_c$ and $\\\\hat{\\\\sigma}_c$ for each $c$ as parameters.', '1.\\tThe main idea of using contents to represent a problem is quite simple and straightforward.', 'First, In Theorem 2, which seems to be a main result of the paper, the authors were concerned with the condition when W_{ji} >0, but there is not conclusion if W_{ji} =0.', 'The (mutual-information) intrinsic reward is computed by the trained neural discriminator, which is used for policy pre-training.', '- Section 6: It seems that in all cases to obtain improvements from discritization, BERT is required on top of the vq-wav2vec discrete symbols.', \"I've read the authors's response.\", '- In Table 2 and 3, how are the degree and block information leveraged into the model?', 'Again it seems that the real contribution of this paper here is to extend this stochastic back-propagation (Rezende et al., 2014; Fan et al., 2015) ideas to discrete variables.', 'contribution and the resulting embeddings are worth further exploration', 'I think these issues are against the goal of evaluating standard neural models on the benchmark and will raise doubts about the comparison between different models.', 'Indeed, why the commonly used attack success ratio or other similar measures cannot be used in the case?', 'This is a major concern.', '- on page 2, the two terms classification & selection network appear \"out of the blue;\" it would be quite helpful to make it clear from the abstract that the proposed implementation is for neural networks.', 'When learning rates are in focus, a convincing message would be to show that adaptation of learning rates is more efficient and simpler than their scheduling when tested on state-of-the-art architectures.', 'It would be nice to have some more intuitive explanations at least of Theorem 1.', 'The proposed improvements also improve the convergence of the model.', 'My main concern about this paper is why this algorithm has a better performance than CW attack?', 'For example, the experiment of Figure 5 does not show SAVP being significantly more diverse than GANs for KTH (as compared to VAEs).', 'would argue that computational cost is rarely a concern among evolutionary algorithms.', 'However, the weakness is that the condition is opaque and it is not entirely clear how broad of class of problems this condition would apply to.', 'In general, the whole paper tries to tell a very interesting, and good story.', '= Strong/Weak Points', '(7/10)', 'To improve the robustness, their DDGC model leverages the minimum covariance determinant (MCD) estimator.', 'The takeaway is that self-modulation is an architectural feature that helps improve modeling performance by a significant margin in most settings.', 'Furthermore, the robust MDP view of the AGZ in sequential decision-making problems is not so impressive either.', 'However, based on the definition of \\\\theta and \\\\tilde{\\\\theta} given in the first sentence of section 2.3, the relation between \\\\theta, \\\\tilde{\\\\theta} and D', 'The fact that masked attention works so much better than the standard cosine-weighted content-based attention is pretty interesting in itself.', '+ a new auxiliary learning algorithm', 'That suggests that from the point of view of training, there are many equally good solutions, which suggests a number of interesting questions. If you did large numbers of training runs, would the models occasionally find the right solution? Could you somehow test for if a given trained model will show systematic generalization?', '-\\tIt is not clear if provided analysis for large-scale GANs apply for small-medium sized GANs.', 'January 2018, Volume 28, Issue 1, pp 113–130| Cite as', 'However, it is unclear whether or not VIME and PER were modified to incorporate domain knowledge (i.e. s_i/s_c distinction).', 'In terms of the empirical results, the better performance of DEBAL compared to a single MC-Dropout model is not supervising as Beluch et al. (2018) already demonstrated that an ensemble is better than a single MC-Dropout.', 'For instance, if x^j_{t-1} influences only the variance of x^i_{t}, but not its mean, then the proposed method may not detect such a causal influence, although the constraint-based methods can.', '-- The paper is well-written and easy to read.', 'Strengths:', '2. Figure 3: it is confusing to call the cumulative distribution of the maximum classification score as the CDF of the model (y-axis fig. 3 left) as CDF means something else generally in such contexts, as the CDF of a predictor.', 'Rather than adapting diagonal elements of the adaptivity matrix, the paper proposes to consider a low-rank approximation to the Gram/correlation matrix.', 'this is with or without normalization.', '- the sentence under eq. (2)', 'Furthermore, it is not obvious to me why these prior samples would be sensible at all, given that all modalities have independent latents by construction.', 'Do the authors think that ultimately learning priors with models like GraphRNN might be more promising for certain applications?', 'Overall, I think the paper seems to be  an interesting direction which has both formal guarantees and experiments validating them in real-world datasets.', 'This paper is built on the top of DNC model.', 'Their main idea is to induce a generative classifer on top of hidden feature spaces of the discriminative deep model.', 'Nits:', '[-] The combination of VAEs and GANs, while new for videos, had already been proposed for image generation as indicated in the Related Work section and its formulation for video prediction is relatively straightforward given existing VAE (Denton & Fergus 2018) and GAN models (Tulyakov et al. 2018).', 'Minor comments:', '4. Fig. 3 (right): It is not clear', 'Without a proper comparison (formal and experimental) with these lines of work, the paper is incomplete.', 'In the experimental studies, the authors also compared several feature space embedding methods, including identical mapping (pixels), random embedding, variational autoencoders and inverse dynamics features.', 'typo:', 'The use of “blank inputs (referred to as “thinking steps”)” in “Simple LSTM” and “Attentional LSTM\" doesn’t seem to be a standard approach.', 'Comparison of batch normalization and weight normalization algorithms for the large-scale image classification.', 'While the general description of the model is clear, details are lacking.', 'training phase whether the goal is to be able to also recognise', 'The paper would be improved if the following points are taken into account:', 'Weaknesses: Paper could have been written better. I had hard time understanding it.', 'However, there is a lot of important material in the Appendix, which I think may be relevant to the readers.', 'significance of the proposed model.', \"Does the controller also start from scratch? If so, why? Also, it is not clear what is the meaning of generations if the graph is fixed, can't it be learned altogether at once?\", 'Section 2 contains mostly high level explanations of the work, which should be integrated in the introduction, and thus before the related work section, to improve readability.', 'This is rather subject, and I would tend to disagree.', 'Again, this confuses me on what is the main topic of this paper.', 'However, there are a few, though not many, works in the literature trying to combine Choquet integral with deep learning.', '1. Page 3: second para, line 4: “our aim is to study”', 'For example, in Table 2, the F-pooling only wins at either accuracy (marginally) or consistency, but not both.', 'The authors propose two architectures.', 'On the other hand, I would suggest directions for investigation and improvements as follows.', '- can you motivate why you are not using perplexity in section 3.2?', 'The two primary contributions of the paper are given as:', 'the different sections are related but it is does not feel like they', 'Besides, on the computation side, it would be complexity, an explicit comparison of complexity makes it easier to evaluate the performance when compared to other state-of-the-art methods.', 'The selected baselines are not sufficient.', '1. Sec 3.2. According to my understanding, Meta dropout introduces a learnable prior for latent $z$, but the training objective does not require posterior inference and thus no variational inference is needed.', '-Something is a bit weird with the FGM results. While it is a weaker attack, a 0%/100% disparity between it and every other', 'This also becomes apparent in the experiments section, where rotational data augmentation is found to be necessary.', 'In this way, the proposed method can be tested in a broader domain and on larger datasets.', 'Things to improve the paper that did not impact the score:', '-', \"B. You don't schedule learning rates for your baseline methods except for a single experiment for some initial learning rate.\", 'The paper studies self-supervised learning from very few unlabeled images, down to the extreme case where only a single image is used for training.', 'When learning the next task, this average saliency map is used in an attention mechanism to help learning the new task and to prevent catastrophic forgetting of previously learned tasks.', '6. https://arxiv.org/abs/1807.03748 (this paper is cited)', '- How is the linearization of the inout done? It  typically matters', \"- It isn't clear from the tables that OCE_0.1 outperforms REINFORCE_Z (as is mentioned in the discussion).\", 'For the experiments, there seems to have a great win of the proposed algorithm against the baselines.', '[1] Yishay Mansour, Mehryar Mohri, and Afshin Rostamizadeh. Domain adaptation: Learning bounds and', 'NIPS 2016', 'MAC in particular seems to have an inductive bias that might make some forms of systematic generalization possible.', 'Nonetheless, I am keen to acceptance. I would increase the rating from 6 to 7, but I will keep the rating of 6 since the rating of 7 is not possible.', '- Last paragraph of section 2 can be true for any well-performing importance measure.', 'I think that this is the crux of the paper that should be significantly expanded and experimentally validated.', 'The idea of studying based on the combination of the results from two previous papers is quite natural, since one uses spectral normalization in practice for GAN training, and the other provides generalization bound that depends on spectral norm.', 'Then, they gathered real-world data using MTurk applied to a subset of ImageNet and applied OENN to learning embeddings of different image instances using only the MTurk triplet information rather than the input RGB input features.', 'But for problems with well-defined extrinsic rewards, such a comparison could help readers understand the relative performance of curiosity-based learning and/or how much headroom there exists to improve the current methods.', 'Comment:', 'These tests are therefore relevant prior work.', 'Summary:', 'This paper proposed \"F pooling\" for Frequency Pooling, which is a pooling operation satisfying shift equivalence and anti-aliasing properties.', 'The authors first derived the theory of F-pooling to be optimal anti-aliasing down sampling and is shift-equivalent in sec 2, and then demonstrated the experimental results of 1D signals and image classification tasks.', '###Summary###', 'The purpose of generative models is not to interpolate per se; the interpolation is really a sanity check that the model is capturing the underlying distribution rather than just memorizing training examples.', 'In general I find Section 3 pretty difficult to follow.', 'In the longer run it would be extremely beneficial to the community if this approach is applied to the standard benchmarks as set out in [2].', '.', 'It has two components, i.e., Pose2Pose and Pose2Frame.', 'This gives nice insight into why this should intuitively work better.', '*', 'One suggestion is that it might be useful to also release the structured (parsed) form besides the freeform inputs and outputs, for analysis and for evaluating structured neural network models like the graph networks.', 'The paper reports positive evaluations on three different tasks.', 'It would be interesting to discuss why the FID score for SVHN gets worse in presence of 1000 labels.', '-\\tThe experiments are reported on large-scale datasets on high-capacity networks which is more realistic than small-scale settings.', 'Since in the supplementary experiments also, it seems that curvature does have small variance in the results, how would the authors assess the robustness of the curvature sampling method with respect to the results?', 'The model consists of thee components:', 'Authors conduct through numerical experiments highlighting how learning rate changes as a function of batch size (initially linear growth and then saturates).', 'I especially appreciate that analysis is done taking into consideration of extra computation cost of the large model; the extra data used for visual relationship detection.', 'In practice, such hierarchy might be much harder to achieve than the primary (coarse) labels, and might be as costly to obtain as the true fine-class labels.', 'Thus the main novelty claim of the paper needs to be hedged appropriately.', '\"with γ is a lasso threshold\"', 'and it also includes some weak claims that should be removed.', '[2] M. Ren, W. Zeng, B. Yang, and R. Urtasun. Learning to reweight examples for robust deep learning. In ICML, 2018.', 'I am giving a score of 3.', 'It seems that the discriminator takes a whole sequence as input, but some precision on how this done could be added.', 'Authors propose a novel combination of RBM feature extractor and CNN classifiers to gain robustness toward adversarial attacks.', '- Sec 3.1 theta_i -> x_i', 'While most related work was covered well, I believe the authors could have a more up-to-date list of recent work that reconstructs triangle-mesh representations from images [A-C] (especially since several of these methods has an architecture that involves encoding and subsequent compositional refinement).', 'It is limited, however, in the sense that it (only) measures auto-encoding capabilities: to what extent can the shape be reproduced given a sample point cloud from the given shape.', 'categories well.', 'The most important contribution in my opinion is the efficient data structure presented in section 4, which allows the spherical convolution to be computed efficiently on GPUs for a grid that is much more homogeneous than the lat/lon grids used in previous works (which have very high resolution near the poles, and low resolution at the equator).', 'The input 3D points are sampled from a unit sphere.', 'The same holds for the type of data, since the paper only shows results for image classification benchmarks.', 'For the modeling contribution, although it shows some improvements on the benchmarks and some nice analysis, the paper really doesn’t explain well the intuition of this “write” operation/Scratchpad (also the improvement of Scratchpad vs coverage is relatively limited). Is this something tailored to question generation? Why does it expect to improve on the question generation or it can improve any tasks which build on top of seq2seq+att framework (e.g., machine translation, summarization -- if some results can be shown on the most competitive benchmarks, that would be much more convincing)?', 'sparsly', 'However, this makes the analysis substantially more difficult.', '- There seems to be no mention of the dimension of the “local” latent variables z_i.', 'I do not see much insight into the problem.', \"Paper's negative points\", 'The structure of G is as follows: starting with a small fixed, multichannel white noise image, linearly mix the channels, truncate the negative values to zero and upsample.', '[4] Seiichi Kuroki, Nontawat Charoenphakdee, Han Bao, Junya Honda, Issei Sato, and Masashi Sugiyama.', 'Authors argue that SGD has two regimes:  a noise dominated regime (small batch size) and curvature dominated regime (large batch size).', 'This paper studied learning unsupervised node embeddings by considering the structural properties of networks.', 'little improvements over the baselines or even significantly worse. More importantly,', \"In section 4.1 the authors have a good discussion on what is wrong with other methods in order to motivate their approach but then they don't deliver significant evidence in the later part of the section.\", '2) The experimental results provided in this paper are weak.', 'The objective is represented using a regularization style parameter lambda and a distance metric D that, depending on its definition, reduces the optimization procedure to Gauss-Newton, General Gauss Newton or Natural Gradient Descent.', 'Thus, it reads like one interesting finding around curiosity-driven RL working in games plus a bunch of preliminary findings trying to grasp at some explanations and potential future directions.', 'Also the result on Stochastic HGD is very interesting.', 'However, given that the achieved performance gains over the state-of-the-art are fairly small, it would be good to assess if the obtained improvements are statistically significant.', 'This paper takes a step in this direction by improving existing model-based methods (the world models and PlaNet) using the actor-critic approach.', \"Specifically, the authors variated the right matrix multiplication in GCN with Ganea et al.'s Mobius matrix-vector multiplication (that can be regarded as a transformation between two Poincare disks).\", 'They prove upper bounds on the Frank-Wolfe gap and show experimentally that they can attack successfully much faster than other algorithms.', 'In this case, the paper proves that no careful selection of the learning rate is necessary.', 'Pros:', 'This also immediately suggests, that the variational approximation , p (\\\\theta | x)  should probably depend on both x and y rather than only on x and the flexibility of the hyper networks g would govern how well the true posterior over weights \\\\theta can be approximated.', 'The experiments show that the approach works fine for link porediction and can be used for visualization.', 'Here, there is only a single (unconditioned) policy, and the different \"skills\" come from modifications of the environment -- the number of skills is tied to the number of environments.', 'Unsupervised domain adaptation based on source-guided discrepancy. In AAAI, 2019.', 'A weakness of CSR and CSC (carried over to the previous work) is that since each row may have a different number of nonzeros, then finding the value of any particular nonzero requires going through the list to find the right corresponding nonzero, a sequential task.', '- p6par6: \"on formulas that with\" -> no that', 'As pointed out by R2, with depth there are a lot more number of possible ways in which one could carve out decision boundaries to separate data points, thus, it is not clear that the loose linear upper bound holds Specifically, as one might expect with depth it could be possible that linear capacity increase is a lower bound (I am not suggesting that it is, but that possibility should be considered and explained in the paper).', 'Finally, it is unclear how the authors have picked the best \\\\lambda parameter for their approach? On page 5 they state that they “pick the value that results in a good trade-off between high uncertainty estimates and high prediction accuracy.” Does this mean that you get to observe the performance in the test in order to select the appropriate value for \\\\lambda? If this is the case this is completely undesirable and is considered a bad practice.', 'Another method used is attribute masking where some masked node and edge attributes need to be predicted by the GNN.', 'a paper in itself, but this paper dips into each one and then', '-In the introduction, \"it is in general impossible to find an embedding in R^d such that ...\", why do we have to make v and v\\'(and u, and u\\') far from each other?', 'Overall, I like this paper. It is well written.', 'The reviewer feels that for CIFAR-10 and 100, some regularizers do consistently give best or close to best networks. Could the authors comment on this?', '##', '- The assumption that the Q matrix should be low-rank is demonstrated with several experiments.', '* Original idea of using separate \"discriminator\" paths for unknown classes', 'That is, the issue still exists, and Dreamer is less effective with very long horizon.', 'It also makes sense that BERT helps for the k-means (vq) setting since the number of codes is large.', 'In these tables, how do the authors decide which hidden layer representations should be explored for their statistical characteristics?', 'The authors are motivated by constant curvature geometries, that can  provide a useful trade-off between Euclidean representations and Riemannian manifolds, i.e. arriving at more suitable representations than possible in the Euclidean space, while not sacrifising closed-form formulae for estimating distances, gradients and so on.', 'Edit:', 'function (once the network has a large enough width) represented by', 'Pros:', '- the paper is poorly written and sentences are generally very hard to parse. For example, section 3.1 is opened by statements such as \"(we use) a multi-task evaluator which trains on the principal and auxiliary tasks, and evaluates the performance of the auxiliary tasks on a meta set\"??', \"It'd be interesting to see how optimization of the actions would lead to better state representations by propagating gradients from the actor-critic model to the world model.\", 'On the positive side:', 'In terms of learning rate scaling, this paper gets similar conclusions as Shallue et al. (2018).', 'One of the main claims of this paper is that neuromodulated plastic LSTMs...outperform standard LSTMs on a benchmark language modeling task”, and that therefore “differentiable neuromodulation of plasticity offers a powerful new framework for training neural networks”.', 'This paper describes a contextual encoding scheme for reconstruction of 3D pointclouds from 2D images.', 'The loss is aggregation with the parameter \\\\alpha.', 'The paper is not very self-contained, and I have to constantly go back to Lee et al. and Xu et al. in order to read through the paper.', 'Besides, the settings are described in details that could help for the reproducibility of the results.', 'The authors claim that the proposed model can result in better uncertainty estimates and the experiments attempt to demonstrate the benefits of the approach, especially in cases of having to deal with adversarial attacks.', '\"Table 4 shows that our first results are promising, even though they are not as good as the state of the art.\" The state of the art on LibriSpeech is not Mohamed at al. 2019. See e.g. Irie et al. Interspeech 2019 for better result', 'It highlights one of the missing inductive biases in ML and proposes it as a challenge.', 'Equations (5) and (6) require a theorem of differentiating under integral (expectation), such as Leibnitz rule, which in case of (6) requires q_gamma(y)f(y) to be continuous in y and q_gamma(y) continuously differentiable in gamma.', 'Constructing the full weight matrix requires the whole dataset as input and selecting k-nearest neighbor is a non-differentiable operation. Can you give more explanations?', 'This paper makes concerted efforts to examine the existing beliefs about the significance of various statistical characteristics of hidden layer activations (or representations) in a DNN.', 'However, as the \"selection network\" uses exactly the same input as \"classification network\", it is hard to imagine how it can learn additional information.', 'Many problems can be cast as optimizing an expectation-based objective.', 'In conclusion, the paper presents a interesting combination of ProtoNet + Adversarial DA + Cycle consistency.', 'Finally, the experimental part is also too weak to evaluate the proposed method.', 'The authors show the result that tree structured neural module networks generalize very well, but other strong visual reasoning approaches do not.', 'This works is cited as a source of inspiration in several places in the paper.', 'Authors should analyze the stability of their method in details.', 'Compare to the conv1 filters in Figure 2 of the paper under review.', 'This motivates the use of spectral regularization (similar to Miyato et al 2018) in adversarial training.', 'The authors say that they match the theoretical lower bounds for several problems.', 'inclusion of the pruning step, and experiments with the AF-function but without the pruning step.', 'The authors claim two contributions towards understanding how the hyper-parameters of SGD affect final training and test performance: (1) SGD exhibits two regimes with different behaviours and (2) large-batch training leads to degradation of test performance even with same step budgets.', 'In particular, the exact difference between the proposed method and the ES baseline is not as clear as it could be.', 'How does that differs from Titsias & Lazaro-Gredilla(2015) both mathematically and experimentally?', '[+] GANs are notoriously unstable to train, especially for video.', 'also says in his book (chapter 44 quoting findings from Radford', 'The authors propose a method for image restoration, where the restored image is the MAP estimate.', 'However,', 'It first learns a latent dynamics model, in which the transition model and the reward model can be learned on the latent state representations.', 'Novelty/Significance:', 'The main idea is reasonable, but it requires that the models to compare all perform reasonably well.', 'Minor, 1/2 is missing in the last line of Eq 19.', 'Noise 60% on CIFAR10, DDGC improves 60.05-> 71.38, while (hard) bootstrap and forward do better.', 'The efficacy of the separate modifications should be tested.', 'Admittedly this is a difficult question to quantify, but my point is to question whether the anti-ME bias shown in Figure 6 is necessarily suboptimal, given the difficulty of classifying a new image as a new class or not.', 'Con: not clear to me how strong and wide the implications are, beyond the analogies and the reinterpretation', 'Preliminary Evaluation', 'What would have been helpful is to show the accuracy of their margin for simple binary toy 2D problems, where the true margin and their approximation can be visualized.', 'However, the experiments conducted generally show that SAVP offers only a trade-off between the visual quality of GANs and the coverage of VAEs, and does not show a clear advantage over current VAE models (Denton & Fergus, 2018) that with simpler architectures obtain similar results.', 'The latter produces feature maps on the sphere, but requires that filters be circularly symmetric / azimuthally isotropic, which limits modeling capacity.', 'The experiments use fairly small datasets, where the performance can be largely influenced by how good the feature extractor backbone is (e.g. training on more data and using deeper architecture would warrant better performance, and thus may change the conclusion).', '[3] Spectral Normalization for Generative Adversarial Networks, ICLR 2018', 'images given to the ConvNet as input?', 'The methods marginally outperform plastic non-modulated recurrent networks on a 9x9 water maze task.', 'This paper also showed the effectiveness of the proposed method on some real-world datasets.', 'This paper presents a gradient estimator for expectation-based objectives, which is called Go-gradient.', '1. In the case of continuous random variables, Go-gradient is equal to Implicit Rep gradients (Figurnov et al. 2018) and pathwise gradients (Jankowiack & Obermeyer,2018).', 'J. Springenberg and A. Klein and S.Falkner and F. Hutter', 'However, the paper does cover a setup that I am not aware that was studied before.', 'Another important parameters is the number of iterations of the algorithm.', '-The empirical results show improvement over the baselines, which are expected.', 'For example, by increasing the amount or size of image regions to be considered, the classifier may accidentally become more robust on an old task.', 'I tend to reject this paper because (1) the first contribution of the paper is not new as it has already been recognized by a few paper that SGD exhibits two different regimes; (2) this paper makes the debate of large-batch training even muddier.', '3. Scenario discussed in Sec. 4 seems somewhat impractical.', 'The dataset is designed carefully so that it is very unlikely there will be any duplicate between train/test split and the difficulty can be controlled.', 'The paper is also quite well written.', 'At the point (number of extra ensembles) where the computational time is equivalent, is the learning curve still better?', '- Reading Figure 2b, one could be tempted to draw a correlation between the complexity of the dataset and the gains achieved by self-modulation over the baseline (e.g., Bedroom shows less difference between the two approaches than ImageNet). Do the authors agree with that?', '> Specifically, we took a deep decoder G with d = 6 layers and output dimension 512×512×3, and choose k = 64 and k = 128 for the respective compression ratios.', 'Could the authors be more specific on the hyperparameters searched? Is this process counted in the tunability measurement?', '* minor: in figure 8 in the appendix, the results after 100 iterations is, as far as I understand, over a single replication, so is not particularly reliable (and will always be 100% of a single optimizer)', '2. during sampling, either training or testing, how do authors handle temporal overlap or make it overlap?', 'However, in the most general setting NMN will form a DAG and it would have been interesting to see what form of DAGs generalize better than other.', \"Other Comments - The citation to Zaremba et al. in Table 1 made it seem like the perplexity result on that line of the table was directly from Zaremba et al's paper. I'd recommend removing the citation from that line to avoid confusion.\", 'The authors report that their approach achieves the best average classification accuracy for 3 out of 4 benchmark datasets compared to other state-of-the-art approaches.', 'However, the issue still exists in Dreamer, since there seems to be an upper limit of effective horizon length (perhaps around 40, according to Figure 4).', '2. In the caption of figure 2, there should be a space after `\":\".', 'Overall, I think this paper is not good enough for an ICLR paper and the presentation is confusing in both its contributions and its technical novelty.', 'This paper explores self-supervised learning in the low-data regime, comparing results to self-supervised learning on larger datasets.', '- In Section 3.3, it is written that “The sparse regularization of \\\\lambda induces great difficulties in optimization”.', 'Concerns: I find the claim on deep networks kind of irresponsible.', 'Minor comments:', '-------------------', 'Evaluation:', 'The first is that the coefficient of', 'The authors then show that the equilibrium corresponds to a KL-minimization.', '1. There are many characteristics of representations such as scaling, permutation, covariance, correlation, sparsity, dead units, rank.', 'The proofs are quite dense and I was unable to verify them carefully.', 'Overall, I think that this is an elegant idea and I’m convinced that it’s a good algorithm, at least on a per-iteration basis.', 'Models are evaluated on the ImageNet and on this internal, bigger dataset.', 'In the first half of the paper, the general framework of the Bayes estimation is introduced.', 'This is not necessarily a drawback, I am curious if this model could succeed on higher dimensional data (e.g., image), especially with the observation y.', 'At a high level, the idea of imposing a mixture of gaussian structure in the feature space of a deep neural network classifier is not new.', \"The empowerment baseline is much appreciated, and while modifications of PER and VIME that incorporate prior knowledge would've also been nice, the experimental results pass the bar for acceptance in my view.\", '(2)', 'Details on how to generate the dataset, however, can be moved into the appendix.', '+ The graph representation of the problem is novel, and draws both on core ideas from Hindley-Milner typing (in the subtyping/assignment graph bits) as well as neural ideas (in name similiarity)', 'exponential of the similarity.', 'Critically different from conventional GANs, the discriminator is optimized *per shape*, ie each point cloud is considered as a *distribution* over R^3 specific to that shape.', 'Some of the reconstructions shown in this paper are quite impressive, and the quantitative results show outperforming 2 recent methods.', 'So, bounds here can not be used to explain empirical observations in Section 5.', 'Advances in Neural Information Processing Systems 20', 'Overall, this is a nice, somewhat surprising result.', 'Cons:', 'Please provide a response to these questions.', '1) The paper proposes a new theoretical upper-bound based on the prior works, the upper-bound and its derivation are interesting and heuristic to the domain adaptation research community.', 'However, unlike as advertised, the paper does not address the domain shift issue in meta-learning, and the experiments lack thorough evaluation as the paper considers itself as a meta-learning paper and only compares to other meta-learning approaches without much comparison to domain adaptation papers.', 'the results of the experiments performed. Do the conclusions accurately', 'From the few/single image(s) available for training, a data set of the same size as some unmodified reference data set (ImageNet, Cifar-10/100) is generated through heavy data augmentation (cropping, scaling, rotation, contrast changes, adding noise).', 'The authors propose using non-Euclidean spaces for GCNs.', '- The idea is simple.', 'Interestingly, their experiments show that NOODL converges linearly in number of iterations, while Arora gets stuck after some iterations.', 'Bousmalis, Konstantinos, et al. \"Domain separation networks.\"\\xa0Advances in Neural Information Processing Systems. 2016.', 'Another approach is to assume the solution to have low surrogate loss (4), and any convex solver with sufficiently large number of points is able to find such a solution.', 'The idea is definitely novel and interesting.', 'Since this paper select the top-k images in D, if k is large the annotating for S will be very tedious, however if k is relatively small the method seems very sensitive to selected examples, which will make the comparison not totally convincing.', 'reflected in the \"Related work\" section where 4 different', '\"will only consists partial\"', 'On the other hand, the obtained results are very weak: only one layered version of the paper is analysed and the theorem applies only to networks with less than some threshold of parameters.', 'Semi-supervised few-shot learning is important considering the limitation of the very few labeled instances.', 'Instead, on solves a linear system, via other means.', '2) To provide a better understanding of the paper, it would also be interesting to see how sensitive it is with respect to the ensemble size M.', 'The explanation provided by the authors is thus not sufficiently precise and I recommend the retraction of this claim.', 'Zhu, Xiaojin. \"Semi-supervised learning literature survey.\" Computer Science, University of Wisconsin-Madison 2.3 (2006): 4.', 'In general, the idea is very intuitive and make sense.', '.', 'The approach is not linked to so called Tandem approach that was/is popular in speech recognition where a generative model (GMM) is trained on top of features extracted by a neural network model.', 'The main technique is Lagrangian relaxation and experiments are focus on cart-pole and locomotion task.', 'Novelty and significance', 'that with increasing degrees of regularization (figure 1, figure 2)', 'Furthermore, Figure 6 and Figure 7 in general show SAVP performing worse than SVG (Denton & Fergus 2018), a VAE model with a significantly less complex generator, including for the metric (VGG cosine similarity) that the authors introduce arguing that PSNR and SSIM do not necessarily indicate prediction quality.', 'The definition of “recovering true factor exactly” need to be given.', 'For example, see the following paper (over 5600 citations according to Google scholar):', 'Strengths: I am happy to see the proposal of a very large dataset with a lot of different axes for measuring and examining the performance of models.', '- How do are the difficulty levels synthetically determined?', '[Weakness]', 'The flowchart in Fig 1 is rather a system design consisting of many components, the functionality of which is not clearly described and existence of which is not justified.', 'They both approximate the reward CDF from K samples and use this to construct a surrogate reward.', 'Ultimately, the analysis boils down to looking at a signal-to-noise ratio of the algorithm, which looks very much like regularization.', 'The paper proposes two additional steps to improve the compression of weights in deep neural networks.', 'As indicated in the recent literature, enforcing shift-invariance does help to improve the performance of a CNN on classification accuracy and the robustness with respect to image shift.', 'In machine learning:', 'The language around skills and the extent of prior knowledge still downplays things a bit too much for my liking.', 'Moreover, from my understanding the analysis in David McKay’s book (Chapter 41) concerns a single neuron (and the number of parameters for a single neuron)', 'It is definitely worth some discussion on this path.', '-\\tWhile this is a new and interesting task, the contribution (as discussed above in “pros” above) is somewhat limited.', 'One would *expect* the proposed approach to work better than diagonal preconditioning on a per-iteration basis (at least in terms of training loss).', 'Specifically, how performance differs compared to when p (m | z) is not used and the decoder p (x | z, m) is conditioned by the mask included in the training set instead of the generated mask?', 'This paper introduces Amortized Proximal Optimization (APO) that optimizes a proximal objective at each optimization step.', 'This paper presents 4D convolutional neural networks for video-level representations.', \"Given the authors' rebuttal to all reviews, I am upgrading my score to a 6.\", 'The paper is presented in a clear manner, with the objectives and analysis techniques delineated in the main paper.', 'The comparisons are also absent in experiments.', 'The paper proposes a way to pre-train quantized representations for speech.', 'Comments:', 'For this reason, I would recommend accepting this paper.', '2. How do you explain that all methods have NRMSE > 1 on the Glass dataset (Table 1), meaning that they all most likely perform worse than a constant baseline?', 'This is an interesting work.', 'The paper presents an interesting idea, but there are some issues that need to be addressed before published on ICLR.', 'In contrast to PlaNet, the difference is that this work learns an actor-critic model in place of online planning with the cross entropy method.', 'workarounds\"', 'There is long history of sparse coding and dictionary learning techniques, including multilayer representations, that follows from the early work of [1].', '\"an arbitrarily distance function\"', '..', 'Overall, the paper seems solid.', 'I know that the variance in estimation is large, but it would still be useful to look at the performance of MINE in comparison to DEMINE.', 'Last but not least, you have repeatedly made the claim combining SST and other SSL may further improve the performance;', '# Weaknesses', 'This work is relevant to researchers in the field of continual/life-long learning, since it proposes a framework, which should be possible to integrate into different approaches in this field.', 'The main theorem of this paper is an extension of existing methods, so the novelty of theoretical analysis is somewhat limited.', 'An investigation of training collapse at large scale is also performed; the authors investigate some regularization schemes based on gathered empirical evidence.', 'Conclusion', '- I would like to see some more interpretation on why this method works.', 'As noted in the paper, this is a general element of SO(3) (and hence not in the set of alt-az rotations)', 'experiments.', 'I can see the paper to be of interest to a quite broad audience and can motivate many subsequent works.', \"Although most recent memory architectures do not seem to have incorporated the DNC's slightly complex memory de-allocation scheme, any resurgent work in this area would benefit from this study.\", 'If the test set is not shuffled (by emphasis on first I assume not) these images are from training NIST (cleaner) set and may not include samples of all digits.', '- Regarding the diversity/fidelity tradeoff using different truncation thresholds, I think constraining the norm of the sampled noise vectors to the exact threshold value (by projecting the samples on the 0-centered hyper-sphere of radius = threshold) could yield even more interesting or more informative Figures, as obtained scores or samples on the edge of that hyper-sphere might provide information on the ‘guaranteed’ (not proven) quality/fidelity of samples mapped from inside that hyper-sphere.', 'The motivation of defining tunability of optimizer is a very interesting question, however, the study seems to preliminary and the conclusion is not quite convencing due to several reasons:', 'For the bidirectional training, did the ratios of the losses (L_z, L_y, L_x) have to be changed, or the iterations of forward/backward trainings have to be changed (e.g., 1 forward, 1 backward vs. 2 forward, 1 backward)?', 'The paper addresses an important and timely problem.', '* Baker, Bowen, et al. \"Accelerating neural architecture search using performance prediction.\" arXiv preprint arXiv:1705.10823 (2017).', \"* I would have loved to see actual analysis of the method's performance as a function of the noise standard deviation.\", 'Reference is missing in section 2.1', '- tau Yih et al, 2016 --> Yih et al, 2016', 'Thus, comparison with the prior work in Statistics and Machine Learning is relevant, since these tests have the same aims and scope as your tests.', 'This issue is not really commented upon in the paper. Is', 'Their proposed solution of functional composition is exceedingly clever but in my opinion too impractical to really be useful.', 'The work is motivated by the empirical performance of Batch Normalization and in particular the observed better robustness of the choice of the learning rate.', '- The paper makes use of a result from the David MacKay textbook', \"2. If you consider \\\\sigma, why do you also predict the rewrite success with \\\\omega? Couldn't it be simply a function from S x S -> L ?\", '4. How were the hyperparameters (learning rate, AdaGrad $\\\\delta$, Adam $\\\\beta_1, \\\\beta_2$) chosen?', 'It is the issue of MLE.', 'The paper introduces new technical insights to understand their bound, e.g. effective sample size.', 'The main weakness of the paper is that it does not situate itself within existing literature in this area.', 'First it evaluates whether convnets can learn to distinguish images from two different sets by training a binary classifier.', '=== Cons ===', \"Verdict: I thought this was generally an interesting paper that has some very nice benefits, but also has some weaknesses that could be resolved. I view it as borderline, but I'm willing to change my mind based on the discussion.\", 'Other than completely relying on curiously-based reward exclusively, there is little here.', '- a scalable approach for the ordinal embedding problem', '4. Regarding Recipe experiments, the paper says it reaches a better performance than the baseline using just 10k examples out of 60k.', '- increasing the batch size by a factor 8', 'I am overall positive about the paper: the proposed idea is simple, but is well-explained and backed by rigorous evaluation.', 'The proposed algorithm involves several tuning parameters, when alternating between two updating rules, an IHT-based update for coefficients and a gradient descent-based update for the dictionary.', '4. Can the authors show concrete examples on how the attacks are generated?', 'statistics of the responses at different layers one can decide if a', 'This paper proposes a new application of embedding techniques for mathematical problem retrieval in adaptive tutoring.', 'As far as I understand, the model starts with hand-engineered design and then finetuned using evolutionary process.', 'Second, SST itself is only comparable with or even worse than the state-of-art methods.', 'It would be nice to see how a “gold standard” HMC based inference does on at least the small toy problem of Sec 5.1?', 'I disagree on the importance of the numbers reported on the abstract: DenseNet on Cifar10 with 60% goes from 53.34 to 74.72.', '[+] Ablation study is done for group lasso, expert lasso and load balancing, which help understand the effect of different components of the proposed', 'The second weakness is experimental design.', 'How are the lambda and threshold parameters tuned? The authors mention a validation set, are they just exhaustively explored on a 3D grid on the validation set?', '*Questions*', 'What is the purpose then for introducing the matrix variate Gaussian?', 'My comments:', '- The definition of g should depend on only \\\\theta_k^I and \\\\hat{\\\\delta}_k^M, not \\\\theta_k^k.', 'Positives and Negatives', 'The biggest jump seems to come from N=1 -> N=2 (although N=4 -> N=5 does see a jump as well).', 'By this I mean, that you may be able to use relationships learned using conventional triplet methods which use input RGB features as ground truth, and test your learned relationships against those.', 'This paper proposes to relax the assumption of disentangled representation and encourage the model to learn linearly manipulable representations.', 'On the writing part, it would be nice to provide more context for both transformer, Proximal Policy Optimization and Simulated Policy Learning to make the paper more self-complete.', '- Cons', '-------------------', 'Minor Comments', '- The toy data set experiments could be dropped  to make room for experiments suggested below.', 'This is an interesting paper.', 'Also stating the 100% success rate in the abstract is a bit misleading for the this reason.']\n"
     ]
    }
   ],
   "source": [
    "print(len(train_texts))\n",
    "print(len(val_texts)/len(data_texts))\n",
    "print(len(test_texts)/len(data_texts))\n",
    "\n",
    "print(train_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Daten tokenisieren\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
    "val_encodings = tokenizer(val_texts, truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "#Erstellen der TensorFlow-Datasets\n",
    "\n",
    "# Konvertieren Sie die Labels in Tensoren\n",
    "train_labels = tf.convert_to_tensor(train_labels)\n",
    "val_labels = tf.convert_to_tensor(val_labels)\n",
    "\n",
    "# Erstellen Sie TensorFlow-Datasets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(train_encodings),\n",
    "    train_labels\n",
    "))\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(val_encodings),\n",
    "    val_labels\n",
    "))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=9)\n",
    " \n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5, epsilon=1e-08)\n",
    "model.compile(optimizer=optimizer, loss=model.hf_compute_loss, metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Kompilieren des Modells\n",
    "\n",
    "#from tensorflow.keras.metrics import Precision, Recall\n",
    "\n",
    "\n",
    "\n",
    "# optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5, epsilon=1e-08)\n",
    "# model.compile(optimizer=optimizer, loss=model.hf_compute_loss, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "259/259 [==============================] - 1549s 6s/step - loss: 1.3186 - accuracy: 0.5661 - val_loss: 1.1874 - val_accuracy: 0.5948\n",
      "Epoch 2/2\n",
      "259/259 [==============================] - 2799s 11s/step - loss: 0.9754 - accuracy: 0.6748 - val_loss: 1.1730 - val_accuracy: 0.6188\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf_keras.src.callbacks.History at 0x28e64515b90>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from tensorflow.keras.callbacks import EarlyStopping\n",
    " \n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    " \n",
    "model.fit(\n",
    "    train_dataset.shuffle(1000).batch(16),\n",
    "    epochs=2,\n",
    "    batch_size=16,\n",
    "    validation_data=val_dataset.shuffle(1000).batch(16),\n",
    "    callbacks=[early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_category(text):\n",
    "    predict_input = tokenizer.encode(text,\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    return_tensors=\"tf\")\n",
    "    output = model(predict_input)[0]\n",
    "    prediction_value = tf.argmax(output, axis=1).numpy()[0]\n",
    "    return prediction_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Contrary to the discussion, there are examples of non-negative distributions to which the reparameterization trick can be applied, including log-Normal and Gamma distributions.',\n",
       " 'Secondly, it would be good to include a GAN-based baseline such as GAIN, as well as some more classic feature imputation method, e.g. MICE or MissForest.',\n",
       " 'A stronger contribution is 1., which however is somewhat incremental compared to similar comparisons made in the past.',\n",
       " '- \"reprort\" -> report',\n",
       " '- “Hold-out” vs “held-out”',\n",
       " '- Backward reference to section 3 seems to be a mistake, should it be subsection 4.2?',\n",
       " '+ The paper thoroughly covers related work and provides context.',\n",
       " 'We think that this is not enough, and more extensive experimental results would provide a better paper.',\n",
       " '5. https://arxiv.org/abs/1904.03240',\n",
       " 'Another possible extension is to test this larger set of words on a non-behavioral NLP task to show possible improvements that the behavioral data and the interpretable space give.',\n",
       " 'I believe that the idea of the paper is interesting and the convergence analysis seems correct, however i have some concerns regarding  the presentation and the combination of different assumptions used in the theory.',\n",
       " 'The idea is to build on some recent work (Hsu 19) which used RNNs to predict heavy hitters in streaming data.',\n",
       " '3. The paper has some grammatical errors. I obviously missed many, but here are the one I found:',\n",
       " '2. When showing the optimality of F-pooling in Section 2.3, the criterion is to reconstruct the original signal x.',\n",
       " \"Neither of these assumptions apply in the exchangeable setting (where the matrix may not be square so the diagonal and transpose can't be used).\",\n",
       " '2. Probably the most interesting experimental finding of this paper is that the mutual information between z and output is constant, while the one between z and input strongly depends on the regularizers.',\n",
       " 'A few typos found:',\n",
       " \"There are several issues convolved here: one is ``full-matrix,'' another is that this is really a low-rank approximation to a matrix and so not full matrix, another is that this may or may not be implementable on GPUs.\",\n",
       " '(but this paper did not combine it with adv training).',\n",
       " 'Update 11/21',\n",
       " 'The novelty of the algorithm itself is limited, since GAN and adversarial training are both minmax problems, and the original algorithm can be carried over easily.',\n",
       " 'way to help make training data more anonymous.',\n",
       " 'training).',\n",
       " 'The motivation of this paper is mostly about learning with sparse reward.',\n",
       " 'I really like the idea of doing math reasoning in latent space.',\n",
       " '- One thing I am confused about is the residual model, which seems quite important for the pipeline but I cannot find details describing it and much analysis on this component.',\n",
       " 'More in the comments below.',\n",
       " 'These assumptions hold mostly true for the three datasets used in the experiment, and also are suggested by results in table 2(e), where 3 parts are necessary to achieve optimal results.',\n",
       " 'I do not see these experimental settings mentioned anywhere in the paper, and this is very concerning.',\n",
       " 'Thinking along these directions would bring more insight and impact to both the ordinal embedding problem and optimization in deep networks.',\n",
       " 'It’s not obvious to me how useful the ME bias would be when there is a large number of classes since the probability assigned to the true novel class with a perfect ME bias would be 1 / (# unseen classes).',\n",
       " 'Moreover, very recent works have also successfully incorporate both generative and discriminative network architectures (e.g., [1,2]) into the optimization process.',\n",
       " 'Also, one simple way to stop these adversarial cases would be to explore using Sigmoid as opposed to softmax.',\n",
       " 'The resulting embedding\\tdoes a good job\\tof predicting human similarity',\n",
       " 'Then, in sections 3.2 and 3.3 the authors introduce a few additional tricks for self-training: exclude datapoints whose predictions are changing and balance the classes.',\n",
       " 'However, when there is no defense deployed in the training process, it is not intuitive to see why the proposed attack is more effective and persistent than the centralized attack, given that a smaller trigger usually results in a worse attack performance.',\n",
       " '2. There are several errors in the writing - hyperparamters in Abstract, repetition of the word \"Section\" in fMRI experiment section etc. - which needs to be fixed.',\n",
       " 'The experimentation is extensive and results are convincing in that they show a clear improvement in performance using the method in a large majority of settings.',\n",
       " 'The authors achieve this by analyzing HGD and giving convergence rates for when a “sufficiently bilinear condition is satisfied”.',\n",
       " '- complex video event modeling (e.g., recognizing activities in extended video of TRECVID).',\n",
       " 'I had trouble to understand some parts of this paper, since some of the sentences do not make sense to me. For example',\n",
       " '#5 is weak, and tell us more about the limitations of random search and naive ES than necessarily a merit of your approach.',\n",
       " 'But the problem settings are not clear to me.',\n",
       " 'Since quantitative real-world results are challenging to obtain, improved presentation of the qualitative results would be helpful as well.',\n",
       " '-- What can be said about how computationally demanding the procedure is? running label propagation within meta learning might be too costly.',\n",
       " 'Also, this would allow to compare the references against each other (filling in the missing number in Table 4) and this would allow an evaluation of the evaluation itself: while perfect scores are unlikely, the human references should be much better than the systems.',\n",
       " 'The work addresses better theoretical understanding of successful heuristics in deep learning, namely batch normalization and other normalizations.',\n",
       " 'You may be able to show more plots which help display the quality of the embedding space varying with the number of triplets used.',\n",
       " 'Deep Clustering for Unsupervised Learning of Visual Features. ECCV 2018.\" and \"Carl Doersch and Andrew Zisserman. Multi-task self-supervised visual learning. ICCV 2017.\" Moreover, since the method is not a meta-learning approach for few-shot learning, it is not fair and also not appropriate to compare with Prototypical Network.',\n",
       " 'In summary, the paper is interesting, however, more experiments could be added to concretely demonstrate the advantage',\n",
       " '\"if we only search right answer\"',\n",
       " '*Remarks*',\n",
       " 'The authors take it as a given that discrete is good because it allows us to leverage work in NLP.',\n",
       " 'images. It would be great if the paper also made some attempt to',\n",
       " \"For the bAbi task, although there is a significant improvement (43%) in the mean error rate compared to the original DNC, it's important to note that performance in this task has improved a lot since the DNC paper was release. Since this is the only non-toy task in the paper, in my opinion, the authors have to discuss current SOTA on it, and have to cite, for example the universal transformer[1], entnet[2], relational nets [3], among others architectures that shown recent advances on this benchmark.\",\n",
       " 'As a matter of good implementation, one never takes the inverse of anything.',\n",
       " '(Blanchard et al 2017) )',\n",
       " 'The south pole can be represented by any pair of coordinates of the form phi in [0, 2pi], nu = +/- pi.',\n",
       " '- p8par1: \"approximate embedding $\\\\alpha(e(\\\\gamma\\'(...)))$ - $e$ is undefined and should probably be $e\\'$ (this is also the case in the caption of Fig. 5), and $c\\'$ should probably be included as well.',\n",
       " 'The paper contributes a novel model for conducting backdoor attacks in the FL setup, and shows that this model is more successful at attacking when training using robust FL algorithms than the standard centralized backdoor attack model.',\n",
       " 'Each MC-Dropout model is an approximation of the posterior, but the ensemble of them may not.',\n",
       " 'Then the question becomes how deep networks solve the particular convex optimization problem.',\n",
       " 'However, the authors mention that this is in their planned future work.',\n",
       " 'In their response they mentioned that the analysis only requires that  H is smooth and that $\\\\|\\\\xi(x^{(0)})\\\\|$ is sufficient bound.',\n",
       " 'Instead, BN should be able to quickly adapt to the covariate shift when it occurs.',\n",
       " '- \"which can be mitigated my workarounds\" -> \"which can be mitigated *by*',\n",
       " 'distribution of observed and unobserved modalities as well as the imputation mask, resulting in a model that is suitable for various down-stream tasks including data generation and imputation',\n",
       " 'The paper attempts to solve this issue by backpropagating policy gradients through the transition model, which is known to be more robust against model errors (see e.g., PILCO (Deisenroth et al., 2011)).',\n",
       " '- Discussions sometimes lack depth or are absent.',\n",
       " 'The idea has a cross-disciplinary nature and is fairly interesting to me.',\n",
       " '== Method',\n",
       " 'This paper proposed an aggregation algorithm (DARN) for the multi-source domain adaptation problem which is highly useful in real-world applications.',\n",
       " 'There are numerous issues with the writing and clarity of the paper, while it seems like some of the observations around the confidence of classifiers are interesting, in general the connection between those set of results and the ``memorization’’ capabilities of the classifier trained to remember train vs val images is not clear in general.',\n",
       " 'In hypothesis testing, it is important to verify that the test has the correct level (false positive rate).',\n",
       " 'However, it’s noteworthy that embedding to a vector could be useful too if the embedding espace is representative of the entire history and the timing of the events.',\n",
       " 'To inject a certain backdoor pattern, existing work generate poisoning samples by blending the same pattern with different input samples.',\n",
       " 'We thank the authors for updating their paper.',\n",
       " 'The overall pipeline makes sense; and the paper is well written.',\n",
       " 'However, since quantitative exploration of large real-world datasets may be challenging and expensive to collect, the synthetic experiments could have been more detailed.',\n",
       " 'c. Figure 1 is over-complicated.',\n",
       " '3. [Presentation.] The presentation is undesirable. It may make the readers hard to follow the paper.',\n",
       " 'While training on one image does reduce the burden of number of images, the computational burden remains the same. And as mentioned above, it doesn’t seem likely that *any* image would work for this method.',\n",
       " 'I tend to accept this paper, (but also OK if it gets rejected), for the following reasons: (1) the idea is novel and interesting; (2) the writing of the paper is below conference standard and very hard to read, especially the method and the experiment sections.',\n",
       " '- In figure 3 (c) \"number |T of input\" should be  \"number |T| of input\"',\n",
       " 'This method looks reasonable to achieve good results.',\n",
       " 'However, previous papers (e.g., \"Obfuscated Gradients Give a False Sense of Security\") reported 55% accuracy under 0.031 infinity norm perturbation.',\n",
       " '- In Equation (2), How is P_ij defined exactly, are they parameters? I am quite confused about this part',\n",
       " 'The method is tested on Resnet/Desnet on CIFAR-100 and subsets of ImageNet, showing better performance than the original models.',\n",
       " 'The main idea is to sample from a distribution of plausible subnetworks modeling the temporally consistent exploration.',\n",
       " 'The paper in general is well written and easy to follow.',\n",
       " 'Why does the graph update require coreference pooling again?',\n",
       " 'The model repeats the three steps for each sentence.',\n",
       " 'A missing empirical analysis is on class-conditional noise (see for example Patrini et al. 17 for a definition).',\n",
       " 'Applications are a bit unclear.',\n",
       " 'Limitations / where the proposed method brings an improvement should be highlighted.',\n",
       " 'Conditional Similarity Networks https://arxiv.org/abs/1603.07810 ).',\n",
       " 'From my experience, aligning embedding spaces is something that usually does not work very well, especially in high dimension.',\n",
       " 'Additional citations suggested:',\n",
       " 'There is a great deal of discussion about full-matrix preconditioning, but there is no full matrix here.',\n",
       " 'Overall, I like the approach of the proposed method, especially tuning coefficient during training procedure although novelty in the theoretical analysis is somewhat limited.',\n",
       " 'The paper is clearly written and easy to follow.',\n",
       " 'Also can it work with theorems that decompose the current goal into several sub-goals?',\n",
       " 'This paper presents a new synthetic dataset to evaluate the mathematical reasoning ability of sequence-to-sequence models.',\n",
       " 'This also holds for k-means, which is usually run multiple times with different starting conditions.',\n",
       " 'Only some heuristic results are obtained for them without rigorous theory.',\n",
       " 'is not one main coherent argument or goal for the paper.',\n",
       " 'The optimization hyperparameters are optimized to best minimize the proximal objective.',\n",
       " '2) page 3, beta in that equation is not defined',\n",
       " 'The difference with the other reference model (SVG) is less clear.',\n",
       " 'Here the model that we are given samples y, where we know that y = Ax where A is a dictionary matrix, and x is a random sparse vector.',\n",
       " \"I don't think Cakewalk is different enough from the cross-entropy method to warrant acceptance in ICLR.\",\n",
       " 'The algorithm optimizes the base optimizer on a number of domains and shows state-of-the-art results over a grid search of the hyperparameters on the same optimizer.',\n",
       " 'For this, they propose the strategy of creating multiple tasks from the same dataset, where the dataset is run through transformations that do not affect mutual information.',\n",
       " '- Given that the accuracy drop is very significant moving from NMN-Tree to NMN-Chain, is there an explanation for this drop?',\n",
       " 'In first paragraph of page 5 where the authors divide the existing literature into the three particular cases, I am suggesting to add the refereed papers inside each one of this cases (which papers assumed function g bilinear , which papers strongly convex-concave etc.)',\n",
       " 'Consider $x$ a binary vector and reward equal to the parity $S(x) = \\\\sum{x_j} % 2$.',\n",
       " 'In this framework, an average saliency map is computed for all images in the test set of a previous task to identify image regions, which are important for that task.',\n",
       " 'The paper motivates dynamic plasticity by analogy to the hypothesized role of dopamine in reward-driven learning in humans and animals.',\n",
       " 'This makes the advantage of F-pooling over the existing AA-pooling unclear.',\n",
       " 'The experimental results presented were all done on small synthetic datasets and it’s hard to evaluate whether the method is practically useful.',\n",
       " 'The probabilistic model contains a latent vector per modality.',\n",
       " 'However, there are several points about the experiments that are unclear to me:',\n",
       " '+ Some of the experimental results presented are quite',\n",
       " 'Decision:',\n",
       " 'I list some instances here.',\n",
       " '- 4.1.2 top of page 7: claims such as \"SST could have obtained better performance\" have no place in such a paper; you could instead make a note about the method being \"prohibitively CPU intensive for the time being\"',\n",
       " 'Otherwise, the artifacts from Pose2Pose will affect the testing performance of the Pose2Frame network.',\n",
       " 'The experiments on SHREC17 show all three spherical methods under-performing other approaches.',\n",
       " 'Minor comments:',\n",
       " 'The method can be implemented using FFT and auto differentiation frameworks.',\n",
       " 'State-of-the-art performance is achieved on several datasets.',\n",
       " '- Did the experiments on CIFAR-10 and ImageNet use the cosine learning rate schedule [1]? If or if not, either way, you should specify it in a revised version of this paper, e.g. did you use the cosine schedule in the first 120 steps to train the shared parameters W, did you use it in the retraining from scratch?',\n",
       " 'Second, testing with HSIC or COCO does not require generalisation to unseen data points: this is why testing is an easier problem than regression.',\n",
       " 'Regardless, given the prevalence of these types of testbed environments, either is a useful discussion to have. Maybe the end result could minimally be a new baseline that can help quantify the ‘difficulty’ of a particular environment.',\n",
       " 'Detail review of different aspects and questions are as follows.',\n",
       " 'In this effort, the paper combines previously published approaches from generative modeling with VAEs, mutual information regularization and domain adaptation.',\n",
       " '- To my best knowledge, the idea of applying the meta-learning to the automatic generation of auxiliary tasks is novel.',\n",
       " 'C. Your method involves a hyperparameter to be tuned which affects the shape of the schedule.',\n",
       " 'I am not aware of a setting where the running time of an attack is the main computational bottleneck (outside adversarial training).',\n",
       " 'Therefore, an appropriate choice of their values need to be given.',\n",
       " 'h. Generally speaking, the paper does require a significant effort to polish Section 3 and 4.',\n",
       " 'experimentally shows that is the case multiple times as it is shown',\n",
       " 'Experimental results show that MISC helps to improve the performance of DDPG/SAC and the learned discriminator can be transferred to different environments.',\n",
       " 'Pre-training is done at the node level as well as at the graph level.',\n",
       " 'Some remarks:',\n",
       " 'Specifically, which features are perturbed, what are the values assigned as the trigger, and what is the corresponding target label?',\n",
       " 'The paper also lacks experimental results, and the main conclusion from these results seems to be \"MNIST is not suitable for benchmarking of adversarial attacks\".',\n",
       " 'For KFLA a hyper parameter “tau” was tuned; this hyperparameter instead corresponds to the precision of the Gaussian prior on the parameters.',\n",
       " '## Specific Comments and Questions ##',\n",
       " 'In terms of the difference between vanilla SGD and SGD with momentum, Zhang et al. (2019) already argued that the difference depends on specific batch sizes and SGD with momentum only outperforms SGD in the curvature dominated regime.',\n",
       " 'The paper proposed the way to estimate coefficient, optimal \\\\alpha, with theoretical justification, and I think this is the biggest contribution of this paper and is interesting.',\n",
       " \"[i.e., app'(emb(thm), emb(rule)) is near to emb(app(thm, rule))] This is an interesting property for the application of deep learning to automated theorem proving, though not directly a breakthrough result.\",\n",
       " 'It would be very useful to write the update more explicitly and compare and contrast this 2 very similar updates.',\n",
       " 'https://openreview.net/forum?id=B1g8VkHFPH',\n",
       " 'I find the observations interesting, but the contribution is empirical and not entirely new. It would be nice if there were some theoretical results to back up the observations.',\n",
       " '2. Experiment:',\n",
       " 'There are a few comments on the new results, and suggestions for further improvement:',\n",
       " '- important problem and interesting idea',\n",
       " 'Minor comments:',\n",
       " 'Thus we may only apply the proposed model on a few tasks with exactly known F.',\n",
       " 'This work extends Schlichtkrull et al. (2018) by adding attention in two distinct ways: attention between pairs of nodes per relation, and attention between pairs of nodes averaged over all relations.',\n",
       " 'But the authors do not seem to describe how they chose the hyperparameters for the baselines algorithms.',\n",
       " \"2. It's very confusing when the authors introduce \\\\sigma and \\\\omega in the beginning of section 4: why would you need two networks predict the same thing?\",\n",
       " 'In any case, I very strongly suggest using a permutation approach to obtain the test threshold for HSIC, which is by far the most robust and reliable method.',\n",
       " 'A important number of related works are cited and compared with the current work.',\n",
       " '- it would be nice to also propose unconditioned experiments.',\n",
       " 'Other:',\n",
       " 'The authors derive empirical conclusions and perform experiments in different settings.',\n",
       " '- \"computer the convolution\" -> compute',\n",
       " 'This is a very good point, however the paper do not compare or contrast with existing methods.',\n",
       " 'Given a set of m images, it is not clear that a classifier that is trained to detect between train and validation is sufficient, as one might also need to figure out if it is neither train nor val, which is a very practical scenario.',\n",
       " 'The idea, which combines those of previous work (wav2vec and BERT) synergetically, is intuitive and clearly presented, significant improvements over log-mel and wav2vec were achieved on ASR benchmarks WSJ and TIMIT.',\n",
       " '3) Given that x^1_{t-2} is useful for predicting x^3_{t}, when (2) is minimized, \\\\eta_1 will not go to infinity, resulting in a non-zero W_{13), which *mistakenly* tells us that X^{1}_{t-1} directly structurally causes x^{(3)}_t.',\n",
       " 'Does the hyper parameter setting favor the GO gradient in the reported experiments?',\n",
       " 'There are many typos and grammar errors',\n",
       " '2. matrix-vector multiplication between a pxr matrix and rx1 vector.',\n",
       " 'It produces good network blocks relatively quickly that perform well on CIFAR/ImageNet.',\n",
       " 'The main contribution is a synthetically generated dataset that includes a variety of types and difficulties of math problems; it is both larger and more varied than previous datasets of this type.',\n",
       " 'The proposed approaches are evaluated on two few-shot datasets and achieves the state-of-the-art results.',\n",
       " '1. What is the \\\\kappa in “variable-nabla” stands for? What is the gradient w.r.t. \\\\kappa?',\n",
       " 'The theoretical results in this paper are extended from Cortes et al (2019) and Zhao et al (2018).',\n",
       " 'the relational graph networks (Schlichtkrull et al. 2018) to derive a hybrid',\n",
       " 'Although the experimental results seem promising when comparing the modified architecture to the original DNC, in my opinion there are a few fundamental problems in the empirical session (see weakness discussion bellow).',\n",
       " 'In particular it is not applicable to learning e.g. sigmoid belief networks [Neal, 92] (with conditional Bernoulli units) and many other problems.',\n",
       " 'Besides, in evaluation, the paper only compares Doubly Sparse with full softmax.',\n",
       " '4) Although the paper claims that the ground truth fine labels are not required, it requires a class hierarchy, which in the experiments are provided by the dataset and defined between true coarse and fine classes.',\n",
       " 'In Section 3.2.2, the authors only explain how they learn example-based \\\\sigma, but details on how to make graph construction end-to-end trainable are missing.',\n",
       " 'Suggestions/Comments:',\n",
       " '- \"exact hierarchical spherical patterns\" -> extract',\n",
       " 'After derive a new generalization bound, this paper also proposes a new method based on the theory.',\n",
       " 'Dreamer does not use any variance reduction technique, so the gradient estimates could have very large variance.',\n",
       " 'They also show the model performs better on average on bAbI than the original DNC.',\n",
       " 'Many more can be said in all the figures.',\n",
       " 'Two real-world problems are selected, i.e., distinct packets in a network flow, Number of occurrences of each type of search query, and it is shown that using a oracle improves performance as compared to methods that do not use the oracle.',\n",
       " 'Therefore I would like to see experiments with the ES cost function, but with',\n",
       " 'While the experiments are only performed on sentimental analysis and digit recognition. How about evaluating the proposed methods on real image recognition such as DomainNet or Office-Home?',\n",
       " 'However, I think the paper could and should be improved with some more detailed analysis and discussions of exhibited behaviors in order to further guide and encourage future work.',\n",
       " \"5. Provide critique of each theorem or experiment that is relevant to your judgment of the paper's novelty and quality.\",\n",
       " 'Detailed comments and questions:',\n",
       " 'While I think the paper could be much more impactful if the experimental section was greatly reworked; I believe the first 5 pages of the paper are a very good contribution and it should be accepted.',\n",
       " 'For this purpose, it is proposed to compute the distribution of maximal activation scores of the output softmax layer and to make use of the Kolmogorov-Smirov distance between the cumulative distributions.',\n",
       " '[1] Bruno A. Olshausen and David J. Field.',\n",
       " 'Hence, I rate it as a weak accept.',\n",
       " 'While, it supports that \" that logarithmic growth of the layer width respect to n is enough to obtain desirable performance.\"  I don\\'t see a clear conclusion of how to pick the width of hidden layers, maybe a better representation could be used.',\n",
       " '3D convolution is already expensive and not scalable, but 4D operation sounds even more expensive and more prohibitive.',\n",
       " 'However, what I see in the experiments is on ~300 frames for Mini-Kinetics and 36-72 frames for Something-Something dataset.',\n",
       " 'Furthermore, for training, PARCUS makes use of rationales.',\n",
       " \"2 - Those more familiar of the graph convolution literature will be more familiar with GCN [kipf et al. 2016] / GraphSAGE [Hamilton et al. 2017] / Monti et al [2017] / etc.. Most of these approaches are more restricted version of this work / Hartford et al. so we wouldn't expect them to perform any differently from the Hartford et al.  baseline on the synthetic dataset, but including them will strengthen the author's argument in favour of the work.\",\n",
       " 'This update rule is able to remove the error floor and achieve exact recovery.',\n",
       " 'However I am concerned by the assumption that the lagged variables X_{t-1}^{(j)} follow a diagonal gaussian distribution.',\n",
       " 'They also conduct more in-depth experiments on specific testbeds to study the dynamics (e.g., Super Mario, Juggling, Ant Robot, Multi-agent Pong) — perhaps most interestingly showing representation-based transfer of different embeddings across levels in Super Mario.',\n",
       " 'I think the authors should instead focus on the discussion of generalization performance and the observation that training loss and test accuracy are independent of batch size in noise dominated regime.',\n",
       " 'Given the paper title, the reviewer would have expected more experiments in a multiple domain context.',\n",
       " 'This paper studies the generalization properties of Recurrent Neural Networks (RNN) and its variants for the sequence to sequence multiclass prediction problem.',\n",
       " 'compared two schemes of this work, the ones with attentions are “almost” identical with ones',\n",
       " 'Even though the proposed approach seems to have significant potential, the experimental',\n",
       " '* Page 9: variable follows Bernoulli -> variable following Bernoulli',\n",
       " 'ICML 2015',\n",
       " 'Similarly, it would be good to formally connect the capacity to the rate of memorization before making a statement about them being related (as suggested in the initial review).',\n",
       " '- Theorem 2 should be Theorem 1',\n",
       " 'Strengths:',\n",
       " '(4) The writing quality is not satisfactory.',\n",
       " 'For instance, Eq. 2,3 can be easily combined using the proportional symbol, Eq. 8,9,10,11 show actually the same thing.',\n",
       " 'This paper proposes variational selective autoencoders (VSAE) to learn the joint distribution model of full data (both observed and unobserved modalities) and the mask information from arbitrary partial-observation data.',\n",
       " '3. Section 4.2: 3rd line: “the class for the from”',\n",
       " '1. the NPN model seems a good alternative, will be good to have a discussion about why your model is better than NPN.',\n",
       " 'This is as I expected, given the use of the Hoeffding bound.',\n",
       " 'Overall, I do not feel the comparisons dramatically change the qualitative understanding the field has of the different optimizers and their tunability.',\n",
       " 'Instead, m new Viterbi decompressers are included, where each row becomes (s_1*codeword_1 + s_2*codeword2 + ...) cdot mask, and the new scalar are the results of the linear combinations of the codewords.',\n",
       " 'ICLR 2017',\n",
       " 'However, for many categories, it’s nor easy for normal people to distinguish.',\n",
       " 'non-standard and most people would use \"scenarios\"',\n",
       " 'Reference:',\n",
       " \"However, the implementation details are hazy, and some design choices (which operations, hyperparameters etc.) aren't well justified.\",\n",
       " 'I think it would be worth discussing this more.',\n",
       " 'The paper proposes a model architecture based on a denoising autoencoder, where the canonicalizations are applied to the encoded representation.',\n",
       " 'Reading the Lemmas of Section 4 (Lemma 4.4) you can see that it is the smoothness parameter of function $H$. Thus, is not necessary to have it there (not important for the definition).',\n",
       " 'Some explanations should be devoted to it.',\n",
       " 'The MNIST+SVHN dataset setup is described in detail, yet there is no summary of the experimental results, which are presented in the appendix.',\n",
       " '- I think the work addressed here is important, and though the details are hard to parse and the new contributions seemingly small, it is important enough for practical performance.',\n",
       " '6. The paper claims GO has less parameters than REBAR/RELAX. But in Figure 9, GO has more severe overfitting. How to explain this contradicts between the model complexity and overfitting?',\n",
       " 'So at the beginning of training, while there are lots of new classes to benefit from the ME bias, the maximum advantage per example is small, and vice versa at the end of training - how do these conflicting effects balance each other out over the course of training and how do the benefits of ME bias scale with the number of classes?',\n",
       " 'Considering these improvements, I would like to raise the score to 5, since the setting of combining few-shot learning and domain adaptation is interesting and the proposed model outperforms the baselines.',\n",
       " 'the authors propose to use gumbel softmax / VQ codebook for the vector quantization.',\n",
       " 'The results in this paper are impressive, and the paper seems free of technical errors.',\n",
       " 'I believe that the claim that “BN reduces covariate shift” (actively discussed in the intro) was an imprecise statement in the original work.',\n",
       " '1. The proposed Pose2Pose successfully transfer the pose conditioned on the past pose and the input control signal.',\n",
       " 'Then, the important hyperparameter of the method---threshold---seems to be hard to select (both in sections 4.1 and 4.2).',\n",
       " 'This issues makes reviewing this paper very difficult.',\n",
       " 'At first, it compares to random graph search and ES.',\n",
       " 'Do you mean that each latent variable is fixed to a value, such that this factor of variation is disabled? Are the canonicalizers pre-specified using meta-labels? Are they updated/learned during model training?',\n",
       " '.',\n",
       " 'Looking at the experiments, the results on HATESPEECH show less differences between models than for SPOUSE or MOVIEREVIEW.',\n",
       " 'Discrete variables are supported by the method only in the case that the distribution factors over all discrete variables conditionally on any additional “continuous variables” (to which the reparameterization trick is applicable).',\n",
       " '- How many samples did you use from p(theta|x) during training?',\n",
       " '-\\xa0The authors perform numerous empirical experiments on several types of problems on various datasets (Digit, OFFICE,CELL) successfully showing how the MULANN can reduce the nasty effects of the adversarial domain\\xa0discriminator and repulse (a fraction of) unlabeled examples from labeled ones in each domain.',\n",
       " 'The results presented are very interesting, specially showing that stochastic approximation of a KL-divergence minimization.',\n",
       " 'With regard to my first negative point above about the lack of discussions, it seems the analysis of Section 4 is disproportionate compared to other places.',\n",
       " 'This means that we are not interested in the minimum of the cost function associated to the model, which contradicts the very concept of \"cost function\".',\n",
       " 'All in all, the results show that the proposed method provides a significant speedup with respect to Shim et al., but it lacks comparison with other methods in the literature.',\n",
       " '3) The paper is overall well-organized and well-written.',\n",
       " 'Also Table 3 seems to have standard deviations missing in Supervised DCGANs and Improved GAN for 4000 labels. And is there an explanation on why there isn’t an improvement in the FID score of SVHN for 1000 labels?',\n",
       " 'However, I have the following concerns.',\n",
       " 'In fact, in machine learning and NLP there is the OOV class which sometimes people in computer vision also use.',\n",
       " \"-3 For image-to-image translation experiments, no quantitative analysis whatsoever is offered so the reader can't really conclude anything about the effect of the proposed method in this domain.\",\n",
       " 'Typos and minor comments:',\n",
       " 'See a list of typos below.',\n",
       " 'The authors apply policy gradients to combinatorial optimization problems.',\n",
       " '* mostly clear writing and presentation (few typos etc. nothing too serious).',\n",
       " 'It is therefore an \"error spotting\" mechanism, rather than a drop-in replacement of standard test accuracy.',\n",
       " 'For now I will give an intermediate rating to the paper.',\n",
       " 'This claim is supported by ample experimental evidence and comparisons against many baselines, as well as additional ablation studies w.r.t design choices of the algorithm itself.',\n",
       " 'Please notice that standard MAP based methods only need to solve a simple convex optimization model (e.g., TV) and these methods can also be applied for different restoration tasks.',\n",
       " 'It achieves this by using the parameterization in which the mean and variance statistics of neurons (the quantities whose change is called the covariate shift) depend on variables that are local to the layer (gamma, beta in (1)) rather than on the cumulative effect of all of the preceding layers.',\n",
       " 'Then, given the expert, output a particular category.',\n",
       " 'If the compression rate is optimized on the test set, then the compressed model is nothing but a model overfit to the test set.',\n",
       " 'However in Figure 1c SGD with momentum at batch size 8k uses an effective LR of 4.',\n",
       " 'I think this is because SO(3) augmentation actually makes the classification problem harder if the input is initially aligned.',\n",
       " 'This paper is different in that it incorporates future time step prediction.',\n",
       " '- The analysis in [1] handles the case of noisy updates, whereas the analysis given here only works for exact updates.',\n",
       " '- dropout: q is the distribution of NN outputs given the input image and integrating out latent dropout noises, gamma are parameters of this NN.',\n",
       " 'In order to better assess this model and compare it to its individual parts and other VAE models, could the authors:',\n",
       " '- The conclusions focus on the importance of section 3 and',\n",
       " 'Be consistent and use “held-out” throughout.',\n",
       " 'Statistics and Computing',\n",
       " 'Specifically, for a fixed k, how would performance increase or decrease, and vice versa - for a given noise level, how would k affect performance.',\n",
       " 'For clarity it might be interesting to use the notation $\\\\alpha^2$ in Lemma 4.7.',\n",
       " 'While I agree with the authors that it is generally desirable for a model to be more confident when predicting in MNIST (since it has already seen samples of it) compared to when predicting in notMNIST (completely different data), these plots tells us nothing regarding the predictive power of the model.',\n",
       " \"4) Table 1 difference between DNC and DNC (DM) is not clear. I am assuming it's the numbers reported in the paper, vs the author's implementation?\",\n",
       " 'Second, the conclusion of Theorem 2 seems to be flawed.',\n",
       " 'The descriptions of the datasets used are not clear, e.g., the number of classes for each data.',\n",
       " 'As I have mentioned above, actually a lot of methods have been developed to address general image restoration tasks.',\n",
       " 'Section 3.4, as the main technical innovation, can be extended and includes some demonstrations.',\n",
       " 'While the presentation is clear and the evaluation of the model is thorough, I am unsure of the significance of the proposed method.',\n",
       " '(The value of each nonzero is stored separately.)',\n",
       " 'If I had a complaint, it would be that I did not learn anything scientifically from the paper.',\n",
       " 'Such a simple',\n",
       " 'The drawback in this model here is that ultimately networks are embedded, but not really generated during test time.',\n",
       " 'Also, I suggest moving Section 4 to be right after Section 2, since Section 4 presents existing techniques similarly to Section 2, while Section 3 presents the main contribution.',\n",
       " 'the',\n",
       " '- Is there any actual empirical importance from recovering the Euclidean case exactly for 0 curvature?',\n",
       " '-- Difference from Wu et al. 2018',\n",
       " 'Attacking CRBMs is highly relevant and should be included as a baseline.',\n",
       " 'For example, on page 3, Query transmitter and receiver, \"the output o_k = M_k(q_k) received from M_k is modified using receiver function as v_k = R_{k->n}(s^t, o_k).',\n",
       " \"If I'm not\",\n",
       " 'I think this paper presents a useful contribution as far as improving speech / phoneme recognition using self-supervised learning goes, and also has useful engineering aspects in terms of combining CPC and BERT. I would like to see this paper accepted.',\n",
       " 'The following footnote',\n",
       " 'where W is the shared weights and \\\\lambda specifies which edges in the DAG are used.',\n",
       " 'However, the novelty is rather limited as similar ideas have been undertaken (e.g., Mescheder et al 2018), but in different contexts.',\n",
       " 'Interestingly, the paper suggests that various regularization methods which can improve stability in GAN training, do not necessarily correspond to improvement in performance.',\n",
       " 'While I could imagine human judges preferring them as they are fluent, I think they are wrong as they express a different meaning than the SPARQL query they are supposed to express.',\n",
       " 'The paper is nicely written and the experimental results are quite strong and comprehensive. I really like the paper but I have two questions about the results:',\n",
       " \"4. How large is the training set of (T, P) pairs? I don't think this is mentioned in the paper.\",\n",
       " 'During training the inference network is used, but during testing optimization based inference is carried out to compute loglikelihoods.',\n",
       " 'In addition, the proposed stability metric seems not quite related with the above intuitions, as the illustrations (1.a and 1b) define the tunability to be the flatness of hyperparameter space around the best configurations, but the proposed definition is a weighted sum of the incumbents in terms of the HPO budgets.',\n",
       " '(3) Instead of using the mean policy approximation in Eq 12, the authors should consider existing Monte Carlo techniques to reduce the variance of the gradient estimation.',\n",
       " 'This paper proposes a method for mathematical problem embedding, which firstly decomposes problems into concepts by an abstraction step and then trains a skip-gram model to learn concept embedding.',\n",
       " 'The results  are overall not very impressive.',\n",
       " 'Meanwhile, the evaluation is pretty comprehensive and it is good to see that the conducted backdoor attacks are effective.',\n",
       " 'of the proposed MF BMs in increasing robustness against adversarial attacks.',\n",
       " 'Authors demonstrate this in both synthetic tasks and real-world tasks like object recognition and machine translation.',\n",
       " 'Regarding the experiments on adversarial examples, I am not convinced of their relevance at all.',\n",
       " 'Please give more explanations.',\n",
       " 'In general, the paper is well written and easy to follow. And the experimental evaluation is extensive and compares with relevant state-of-the-art methods.',\n",
       " 'However, it trades-off computational cost for progress-per-iteration, so I think that an explicit analysis of this trade-off (beyond what’s in Appendix B.1) must be in the main body of the paper.',\n",
       " '.',\n",
       " 'Suppose x^1_{t-2} directly causes x^2_{t-1} and that x^2_{t-1} directly causes x^3_{t}, without a direct influence from x^1_{t-2}  to x^3_{t}. Then when minimizing (2), we have the following results step by step:',\n",
       " 'Some works actually also incorporate generative and/or discriminative networks into MAP inference process for these tasks.',\n",
       " '- There is also a closely related variant of Eq 3 which we can arrive at by switching the log and the expectation in the first term of Eq 5 and applying Jensen’s inequality —> E_p(\\\\theta| x)[ln p(y | x, \\\\theta)] - KL (p(\\\\theta | x) || p(\\\\theta)).',\n",
       " 'However, I have some questions and comments, so I’d like you to answer them.',\n",
       " 'Further demonstrations are necessary for the proposed SST method.',\n",
       " 'features and relational information (edge features) to perform node-level and graph-level',\n",
       " 'Theorem 1 does not take account for the above conditions.',\n",
       " 'It is unclear how important this particular objective is to the results.',\n",
       " 'Re level:\"This proof could be experimentally verified ...\"  This should be verified. In particular, Hoeffding can be very loose in practice, which is likely to be observed in experiments.',\n",
       " '## Assessment ##',\n",
       " 'Furthermore, no comparisons were provided to any baselines/alternative methods.',\n",
       " 'Other Questions / Remarks:',\n",
       " 'This restriction of the number of parameters to be small is only mentioned in the theorem itself, not in the discussion of its implications.',\n",
       " 'The paper is clearly written and addresses an important problem.',\n",
       " 'Post Author Response: Thanks for the response. I agree with your perspective and think this paper should be accepted.',\n",
       " '(*)',\n",
       " 'In Figure 5, a legend indicating the relationship between color intensity and distance would be helpful.',\n",
       " '* Summary',\n",
       " 'Evaluations are performed on two movement video datasets classically used for benchmarking  this task - several quantitative evaluation criteria are considered.',\n",
       " 'Those are indicators of input importance, and help to boost the loss for relevant tokens.',\n",
       " 'Significance:',\n",
       " 'The main contribution (backpropagating analytic gradients through imagined trajectories?) could potentially be highlighted more but otherwise the paper was clear.',\n",
       " 'While I find this idea interesting and of potential practical use, I have concerns about novelty and the experimental results and overall I recommend rejection.',\n",
       " 'They also extend this estimator to problems where the gradient should be \"backpropagated\" through a nested combination of random variables and a (non-linear) functions.',\n",
       " 'Summary:',\n",
       " 'The selected images A and B are of very high entropy and show a lot of different objects (image A) and animals (image B). How do the results change if e.g. a landscape image or an abstract architecture photo is used?',\n",
       " '--Another issue is that the regularization lambda should apply to both the terms in the bound but in Equation (7) only appears selectively for one of the two terms.',\n",
       " '7. https://arxiv.org/abs/1803.08976',\n",
       " 'Cons:',\n",
       " 'I wonder if the learned features require fewer fully supervised images to obtain the same performance on the downstream task?',\n",
       " \"I read the other reviewers' comments as well as the rebuttal.\",\n",
       " '-------------------------------------------------------------',\n",
       " 'The selection of the datasets is appropriate.',\n",
       " 'Also, NPN can probably be modified to output spans of a sentence.',\n",
       " '- Effectiveness on very long horizon trajectories:',\n",
       " 'Weak reject because the idea is quite interesting, but I believe a more thorough explanation and expanded experimental comparison would be of great help to ensure the community can appreciate this work.',\n",
       " '3. The idea of using SN to improve robustness has been introduced in the following paper:',\n",
       " 'The vector representation for objects is',\n",
       " \"For example, how to build ''adversarial learning baseline'' in meta learning settings and why the result implies the perturbation directions for generalization and robustness relates to each other; (2) how other regularization methods (e.g., Mixup, VIB and Information dropout) perform on adversarial robustness? Does Meta dropout performs better than them?\",\n",
       " 'The computational inefficiency of these methods is that you learn a (big) fixed decoder for all objects (all z latents), and then need to apply it individually on either each point cloud point you want to produce, or each voxel in the output (this problem exists for both the class of methods that deform a uniform distribution R^3 -> R^3 a la FoldingNet, or directly predict the 3D function R^3 -> R e.g. DeepSDF).',\n",
       " 'This process is repeated multiple times and finally the output is squashed through a sigmoid function for the output to remain in the 0..1 range.',\n",
       " 'Additionally, I believe the experimental tasks are new, and as a result all implementations of competing techniques are by the paper authors. This makes it difficult to have confidence in the higher reported performance of the proposed techniques.',\n",
       " 'Based on the summary, cons, and pros, the current rating I am giving now is weak reject. I would like to discuss the final rating with other reviewers, ACs.',\n",
       " 'The second part of the paper associate good performance in preventing adversarial attacks with the possibility of denoising by the pretrained BM.',\n",
       " \"The author's model was quite novel in my opinion.\",\n",
       " 'Another key concern concerns scalability.',\n",
       " 'Please carefully check it, especially the argument given in lines 10-13.',\n",
       " 'However, the results are unconvincing: only the results for epsilon = 0.1 are shown, and even so the advantage is marginal.',\n",
       " '#3 is so generic that a large part of the previous literature on the topic fall under this category -- not new.',\n",
       " 'Since the alt-az convolution is not isotropic and maps between scalars on S2, it cannot be equivariant.',\n",
       " '2) The writing is poor and hard to follow.',\n",
       " 'I find these assumptions too strong for the task of learning disentangled representation.',\n",
       " '1. The method should be compared with other combinations of components.',\n",
       " 'The paper presents a comprehensive generalization of a recently proposed model for interaction across sets, to the setting where some of these sets are identical.',\n",
       " 'without attentions, which implies that the proposed attentions mechanism is not really useful',\n",
       " 'The alternative tests listed are nonparametric.',\n",
       " '1. The numbers reported in Figure 5 do not match with the performance of adversarial training in previous paper.',\n",
       " 'The authors mentioned that this is softly calculated by softmax on the importance score.',\n",
       " 'Both splitting data into training and validation and then using task augmentation to make learning robust are pretty nice ideas.',\n",
       " '- page 4, Sect. 4.4: Architecture of $\\\\alpha$ would be nice (more than a linear layer?)',\n",
       " 'The MDP formalization is rather straightforward.',\n",
       " 'Instead of reporting a single time-vs-distortion data point, the authors could show the full trade-off curve.',\n",
       " '*',\n",
       " 'It would be better if these heuristic arguments can be formed as theorems as well.',\n",
       " 'The most interesting one is the prediction of its dimensions by the CSLB features, which reveals a nice clustering in the different SPoSE dimensions.',\n",
       " 'I raised my rating. After the rebuttal.',\n",
       " 'The number of parameters C is smaller than the number of free parameters in the image X, so this results in a predictive model that can be used for compression, denoising, inpainting, superresolution and other inverse problems.',\n",
       " \"The negation example is nice but this doesn't seem to display the potential power of the method to understand a neural network.\",\n",
       " 'How does the proposed method perform in more complicated tasks such as',\n",
       " 'I find the background on ELBO and GANs unnecessary occluding the clarity at this point.',\n",
       " 'Does the fMRI data exhibit time dependence?',\n",
       " 'Questions:',\n",
       " 'Or perhaps the object similarity ratings could be used in a semi-supervised setting to inform the learning of a co-occurence word embedding.',\n",
       " 'However, the following implicit assumptions/limitations of the approach should be made more clear:',\n",
       " 'is somewhat disorganized,',\n",
       " '- The experiments were entirely focused on uncertainty quality but we are always interested in both performance on the task at hand as as well as good uncertainty estimates. What was the performance based on e.g. classification accuracy on each of these tasks compared to the baselines? I believe that including these results will strengthen the paper and provide a more complete picture.',\n",
       " 'I think this is a very interesting direction, but the present paper is somewhat unclear.',\n",
       " '- What are the motivations to use Frank-Wolfe ? Usually this algorithm is used when the constraints are to complicated to have a tractable projection (which is not the case for the L_2 and L_\\\\infty balls) or when one wants to have sparse iterates which do not seem to be the case here.',\n",
       " 'Without any comparisons it’s hard to tell how difficult the tasks under consideration are and what would amount to good performance on the held-out tasks.',\n",
       " '4. The cider score of image captioning is 109 compared to the baseline 108.',\n",
       " 'reflect the opinions of the author? If yes, would',\n",
       " 'My main concern about the paper is that, currently, the experiments do not include any strong baseline (the ES currently is not a strong baseline, see comments below).',\n",
       " 'representations from training on data collected from odd-one-out human',\n",
       " '- For the MISC+DIAYN, what if we train the agent using MISC and DIAYN at the same time, instead of pre-training MISC first and fine-tuning DIAYN later?',\n",
       " 'The authors also attribute the existence of adversarial examples to the small validation/test set, which I agree to some degree.',\n",
       " 'issues/tasks are referred to.',\n",
       " 'How to understand this phenomenon? Does this mean that the distribution of the layer outputs will not change too much if the layer is deep enough?',\n",
       " '2) it is surprising that random features perform so well in the experiments.',\n",
       " 'The paper addresses active deep learning which is certainly an interesting research direction since in practice, labeled data is notoriously scarce.',\n",
       " 'These diverse mappings are learned before the two main mappings are trained and are kept constant during the training of the two main mappings.',\n",
       " 'For example, in Sections 4 and 5 I was hoping to see comparisons to methods like MAML.',\n",
       " 'Presumably, whether the benefit scales well will depend on the loss function - in any case, this warrants some analysis/discussion especially given that there are no experiments to test it.',\n",
       " 'This is unlike existing methods which use model-free or planning methods on simulated trajectories to learn the optimal policy.',\n",
       " '2. This paper develops a new representation system for object',\n",
       " 'The latter may be important in practice, but it is orthogonal to the full matrix theory.',\n",
       " '+ Nicely designed experiments testing this (somewhat surprising) property empirically',\n",
       " 'Despite the claim that the proposed method can capture long-term video patterns, the static compositional nature seems to work best for activities with well-defined local events and clear temporal boundaries.',\n",
       " '.',\n",
       " '[Decision]',\n",
       " '###Pros#',\n",
       " 'As a result, constraining the model will alter the mutual information and I think the effect of this should be remarked on.',\n",
       " '[9] S. Laine and T. Aila. Temporal ensembling for semi-supervised learning. In ICLR, 2017.',\n",
       " 'BiGAN, RotNet, and DeepCluster serve as the reference self-supervised methods.',\n",
       " 'Comments:',\n",
       " 'There is however many minor issues that should be fixed for the camera-ready version.',\n",
       " '==============Final Evaluation================',\n",
       " 'For example in section 3 the paper',\n",
       " '- lower on the same page you say: \"SST may get better performance\" - see above',\n",
       " 'However 4 to 32 seems a bit inconsistent.',\n",
       " 'This paper presented a relational graph attention networks that could consider both node',\n",
       " 'This work proposes to combine VAEs and GANs in a single model to get the benefits of both models.',\n",
       " 'Dual-1 and Dual-5 are introduced without explanation.',\n",
       " 'Comments: overall, the problem on abductive inference and abductive generation in language in very interesting and important.',\n",
       " 'Very well written.',\n",
       " 'It is also not clear why Table. 3 does not report the Bayes baseline results.',\n",
       " 'Besides, the object is also considered in this network, which makes the method generalized well to the videos where human holds some rigid object.',\n",
       " '- Sec 3 \"(PS), where weights are reused\" can you already go into more details or refer to later sections?',\n",
       " 'A key point of interest is the following: very exciting recent work (GraphRNN: Generating Realistic Graphs with Deep Auto-regressive Models by You et al ICML2018) has proposed neural generative models of networks with a high degree of fidelity and much less hand-picked features.',\n",
       " 'The cited Berrett and Samworth MI test uses a permutation approach to obtaining the test threshold, not an asymptotic approach  (see the results of Section 4 of that paper).',\n",
       " '- I also like the ablation study showing the impact of the method applied at different layers.',\n",
       " 'While the authors say \"attributing a deep network’s prediction to its input is well-studied\" they don\\'t compare directly against these methods.',\n",
       " '2. It seems that the main novel contribution of the paper is to extend the ideas of (Figurnov et al. 2018, Jankowiack & Obermeyer,2018) to discrete variables. And this is a relevant contribution.',\n",
       " 'On SVHN however, the proposed approach fails in comparison with (Kumar et al 2017) but performs better than other approaches.',\n",
       " 'In fact, there is more to the Byzantine setting than that,',\n",
       " 'Pro: nicely written, clear interpretation of regularization as a noise injection technics, explicit link with information theoery and Shanon capacity.',\n",
       " 'Summary:',\n",
       " '[5] J. Goldberger and E. Ben-Reuven. Training deep neural-networks using a noise adaptation layer. In ICLR, 2017.',\n",
       " 'The paper is easy to follow.',\n",
       " '- no need to write so much in section 2.1, the surrogate is simple and common in optimization for parameters.',\n",
       " '5. It would be nice to see experimental results on more than one problem.',\n",
       " 'BSD 500 only contains 500 images, and it would be good if more diverse set of images are considered.',\n",
       " 'The method from Gatys et al. requires features from different layers of the feature hierarchy, including deeper layers.',\n",
       " 'The paper also misses several powerful baselines of semi-supervised learning, e.g. [1,2].',\n",
       " 'Experiments are conducted on datasets in the chemistry domain and the biology domain showing the benefit of the pre-training.',\n",
       " 'They test their methods on a toy task that consists in finding inclusion maximal cliques (which tests for local optimality) against REINFORCE (including their variants: centering the rewards with a mean baseline or normalizing them), the cross-entropy method and Exp3.',\n",
       " 'Abstract node notations? It is not clear in the model section.',\n",
       " 'Finally, more images are needed to learn the deeper layers for the downstream task anyway.',\n",
       " 'Typos:',\n",
       " 'The paper shows that the number of free parameters in invariant and equivariant layers corresponds to the different partitioning of the index-set of input and output tensors.',\n",
       " '1. Briefly establish your personal expertise in the field of the paper.',\n",
       " '- I think parameters of training the networks from the beginning of section 4 could be moved to the supplementary materials.',\n",
       " 'This is probably obvious given the results from Zhang et.al. but should be included as a sanity check.',\n",
       " 'However, I really think that the community is currently lacking of understanding on minmax optimization and that we need better training method in many practical emergent frameworks that are minmax (such as GANs or multi agent learning). That is why, I would vote for a weak accept.',\n",
       " '- The paper gives theoretical insight into why Batch Normalization is useful in making neural network training more robust and is therefore an important contribution to the literature.',\n",
       " 'In general it is very unlikely that you will be able to choose every variation of out-distribution cases.',\n",
       " 'The \"Double Viterbi\" (new contribution) refers to the storage of the nonzero values themselves.',\n",
       " '(iii) a “discriminator” network that aims to distinguish points from a *given* shape, and the points produced by pipe-lining the encoder and decoder.',\n",
       " 'More discussion on both of these aspects can help in improving this paper.',\n",
       " \"Since all of these are approximations, and we cannot know how far we are from the true margin, I don't see why these categories help.\",\n",
       " 'It adds extra complexity, requires you to do function composition which may be less expressive and takes more coomputation, etc. And to what end?',\n",
       " 'I would like to thank the authors for the response and updating the draft.',\n",
       " 'Then, the pruned matrix is quantized with alternating multi-bit quantization.',\n",
       " 'This average is linked to the notion of an eligibility trace, and ties into some recent biological work that shows the role of dopamine in retroactively modulating synaptic plasticity.',\n",
       " 'The learning process constitutes of the following steps.',\n",
       " 'There is however a misunderstanding about the properties of the alt-az convolution that must be cleared up before this paper can be published.',\n",
       " 'This paper presents a new way to represent a dense matrix in a compact format.',\n",
       " 'However, on a positive note, the paper has been written very well and I really liked the frank discussion on page 8 about results on MUTAG dataset.',\n",
       " \"Increasing the network's depth increases the efficiency of\",\n",
       " 'However, it seems to me that such contributions in the rising field of geometric deep learning, where several challenges are yet to be overcome, can be beneficial for future research.',\n",
       " 'Summary: the paper purposes a dataset of abductive language inference and generation.',\n",
       " '- While the authors mention multiple times that #rhs/#lhs = 1 and 2 are more challenging than #rhs/#lhs=18, they do not sufficiently explain why this is the case anywhere in the paper.',\n",
       " '1 - I would have liked to see more discussion about why the two results differ to give readers intuition about where the extra flexibility comes from.',\n",
       " 'probabilities of which pair of items will be paired are modeled as the',\n",
       " 'As seen from the formal construction, the theoretical results apply equally well to all normalization methods.',\n",
       " '* Weaknesses:',\n",
       " 'Via three different sets of experiments, the authors conclude that, under standard training procedures, neural networks in fact display an anti-ME bias: when faced with a novel input, they tend to assign less probability mass to unobserved outputs than is justified by the incoming data.',\n",
       " 'Some typos:',\n",
       " 'Comments:',\n",
       " 'The test error given by the systems is comparable, but there are clear speed benefits to the proposed method OENN as the other techniques could not be run for a dataset size of 20k, 50k, or 100k.',\n",
       " 'This quantity never mentioned before.',\n",
       " 'One thing I would have loved to see from this paper is a comparison of modulated-plasticity LSTMs with the sota from Melis et al., 2017.',\n",
       " '.',\n",
       " '- the locations for important features should be comparatively stable (for example, one would expect the average saliency map to become fairly meaningless if important features, such as the face of a dog, can appear anywhere in the image.',\n",
       " 'Thus, I would like to see more possible explanation on it.',\n",
       " 'should be: D <- \\\\theta -> \\\\tilde{\\\\theta} (if it is a generative model) or D -> \\\\theta -> \\\\tilde{\\\\theta} (if a discriminative model).',\n",
       " 'Although it is not directly measured, it does seem highly likely that the alt-az convolution is more computationally efficient than SO(3) convolution, and more expressive than isotropic S2 convolution.',\n",
       " 'A problem can be represented as the average concept (corresponding to those in the problem) embeddings.',\n",
       " 'There are several weaknesses in this paper.',\n",
       " 'The two things that make me more skeptical, is the convergence of the proposed algorithm and the experiments.',\n",
       " 'I have given it a strong accept because the paper takes a very well-studied area (convolutions on graphs) and manages to find a far more expressive model (in terms of numbers of parameters) than what was previously known by carefully exploring the implications of the equivariance assumptions implied by graph data.',\n",
       " 'Since the latent models are learned based on existing techniques, the paper presents an incremental contribution.',\n",
       " \"The authors also qualitatively show the model's reasoning process and human study on judging answering quality.\",\n",
       " \"4. Place the work in context of prior work, and evaluate this work's novelty.\",\n",
       " 'The authors of this paper are proposing a neural network approach for learning diffusion dynamics in networks.',\n",
       " 'The first one is clarity.',\n",
       " 'There should be a better discussion of related work on the topic.',\n",
       " '[Recommendation]:',\n",
       " 'However, I think the paper is still worth reading if the authors can reorganize the paper and I might increase my score if my concerns get resolved.',\n",
       " 'Overall, the ideas presented in the paper are intriguing, and further research down this line is encouraged.',\n",
       " 'The KL divergence regularization introduces extra randomness to the auxiliary labels and thus mitigates the problem, but it hardly provides any useful information except randomness.',\n",
       " 'It is difficult to judge the performance of the proposed model based on so small data set.',\n",
       " 'Paper 93 proposes an empirical evaluation of the memorization properties of convnets.',\n",
       " 'Have you considered training different agents on different subsets of the data, or trying different learning algorithms/architectures to learn them?',\n",
       " 'Also, in figure 4, the authors mention that their pre-trained models tend to converge faster.',\n",
       " 'Summary:',\n",
       " 'The paper proposes 4d convolution and an enhanced inference strategy to improve the feature interaction for video classification.',\n",
       " 'The paper appears to treat 2. as the main contribution.',\n",
       " '- Bottom of page 7, capitalize \"for\".',\n",
       " '\"Sparse Mixture of Sparse of Sparse Experts\"',\n",
       " 'The critical contribution of this work appears to be the observation that large batch size can be worse than small under same number of steps demonstrating implicit regularization of small batch size.',\n",
       " 'In particular, one can propose several combinations of assumptions in order for the function H to satisfy the PL condition.',\n",
       " 'The most popular approach to doing this has been evolutionary methods which work by evolving morphology of agents in a feed-forward manner using a propagation and mutation rules.',\n",
       " 'Based on Gaussian weights and biases, the paper investigates the evolution of the covariance matrix along with the layers.',\n",
       " 'My assumption is the visual feature already contains the label information for image captioning.',\n",
       " 'In the semi-supervised self-training setting, this paper proposes to select a certain subset of unlabelled data for training rather than all unlabelled data, where the ensemble of confidence scores of the trained model in iterations is used to guide the selection.',\n",
       " '(1) The authors give the derivation for Eq 10.',\n",
       " 'Overall the paper is well written.',\n",
       " 'They also suggest that when the tuning budget is low, using Adam but tuning only the learning rate is beneficial, which could be a valuable and practical suggestion.',\n",
       " 'I would replace these values with N/A or something similar.',\n",
       " 'So the closure axiom of a group is violated.',\n",
       " 'However, the motivation of the proposed method is to minimize the upper bound, not the loss itself, i.e. L_T(h, f_T).',\n",
       " '[4] G. Patrini, A. Rozza, A. Menon, R. Nock, and L. Qu. Making deep neural networks robust to label noise: A loss correction approach. In CVPR, 2017.',\n",
       " 'Although experiments show that learning such representations are beneficial for low-shot setting of SVHN, it is not clear whether such improvement generalizes to more realistic datasets such as ImageNet.',\n",
       " '1. The classification of base class into super classes seems questionable to me.',\n",
       " 'The paper is written well overall, clearly explaining the results obtained.',\n",
       " 'SImilarly, being married to someone is not the same as having a baby with someone.',\n",
       " 'Many prior works have found that the features output by the final layer of neural networks can often be used as informative representations for many tasks despite being trained for one in particular.',\n",
       " '- the complexity of the proposed algorithm seems to be very high',\n",
       " 'Taking a random example (there are others by simple searching), in the ECCV paper \"DFT-based Transformation Invariant Pooling Layer for Visual Classification, Ryu et al., 2018\" The DFT magnitude pooling is almost the same as the authors\\' propositions, where the \"Fourier coefficients are cropped by cutting off high-frequency components\".',\n",
       " 'In this paper, the authors proposed a multi-domain adversarial learning approach, MULANN, to improve the classification accuracy on three datasets-DIGITS, OFFICE and CELL-in the semi-supervised DA setting.',\n",
       " '- Analysis of shapes with different genus and dimensions would be interesting.',\n",
       " '(iv) a “shape prior” that, once the encoder-decoder model from above is trained, is used to model the distribution over the global latent variables.',\n",
       " 'This results in an objective function which only involves infected nodes (and no term associated with the parent node), weighted by likelihood of each node j infecting the node at step i. This should make the training more simplified than what is discussed in Algorithm 2.',\n",
       " 'What is the point that you are trying to make? Also, note that some of the algorithms that you are citing there have indeed applied beyond architecture search, eg. Bayesian optimization is used for gait optimization in robotics, and Genetic algorithms have been used for automatic robot design.',\n",
       " \"There isn't a tested hypothesis, but rather it's a feat of engineering to get this to work.\",\n",
       " 'Besides the above comments, I have these additional comments.',\n",
       " 'The dropout happens at the beginning of each episode, and thus leads to a temporally consistent exploration.',\n",
       " 'Overall:',\n",
       " 'same representational power as a shallow network). And as MacKay',\n",
       " 'For example, by taking s on a grid from 0 to 1, one could plot the coverage and distance-to-face measures.',\n",
       " 'It is not clear from the presentation that this has been confirmed.',\n",
       " 'The paper does not follow a smooth story line, where an open research question is presented and a solution to this problem is developed in steps.',\n",
       " 'and interesting contribution.',\n",
       " 'The paper presents a maximally expressive parameter-sharing scheme for hypergraphs, and in general when modeling the high order interactions between elements of a set.',\n",
       " 'Furthermore, on page 6, it is stated that the surrounding square $\\\\hat{x}_i$ is 15 x 15 pixels, while the size of the square $x_i$ is 10 x 10.',\n",
       " '-2 Diversity of additional \"agents\" not analyzed (more below).',\n",
       " 'It might be the case that their version of NMN can only really do well on this specific task, which would be less interesting.',\n",
       " 'The benefit of using modules for reasoning allows one to visualize the reasoning process more easily to understand the model better.',\n",
       " '- Why did you not do image inpainting in higher-dimensional experiments like Ivanov et al. (2019), i.e., considering each pixel as a different modality? Of course, I know that Ivanov et al. require the full data as input during training, but I’m interested in whether VSAE can perform inpainting properly even if trained given imperfect images.',\n",
       " '- Please clearly define the notation used in Section 4.2.',\n",
       " 'However, in Figure 2, it is used for evaluating defense schemes.',\n",
       " 'The authors pose an interesting hypothesis, but it would gain a lot of credibility if they could provide an empirical analysis of an algorithm that uses ME reasoning to improve learning in a realistic setting or if they could at least perform some quantitative analysis of the effects of ME bias on task performance for a toy example.',\n",
       " '-\\xa0A new generalization bound for MDL is introduced.',\n",
       " 'I feel that this is a kind of things which support the proposed method if it is properly assessed.',\n",
       " 'The paper is written in a way that makes following it a bit difficult, for example, the experimental setups.',\n",
       " 'Summary:',\n",
       " 'You can refer to the state-of-the-arts performance on CIFAR.',\n",
       " '* The biggest problem for me was the unconvincing results.',\n",
       " 'Yet the experiments considered in the paper are limited to very few time series.',\n",
       " \"For example, the weighted midpoint of Def. 3.2 doesn't immediately extend to spherical space (or at least won't be unique).\",\n",
       " 'This is an novel, interesting paper on an important topic: semi-supervised learning.',\n",
       " 'For example, I don\\'t understand what does it mean in \"However, if training data is complete, ..... handle during missing data during test.\" Another example would be the last few paragraphs on page 4; they are very unclear.',\n",
       " 'Some more explanation / discussion would be good.',\n",
       " 'The tunability of the optimizer is a weighted sum of best performance at a given budget.',\n",
       " 'The papers discusses some (not surprising) theoretical properties relating to scaling, permutation, covariance, correlation, while making less efforts on the more interesting characteristics  sparsity, dead units, rank, mutual information.',\n",
       " '[B] Saito, Kuniaki, et al. \"Maximum classifier discrepancy for unsupervised domain adaptation.\" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018.',\n",
       " 'The paper addresses a relevant problem which appears in many machine learning settings, as it is the problem of estimating the gradient of an expectation-based objective.',\n",
       " 'There are two key convergence results which are dependent on the meta-objective being optimized directly which, while not practical, gives some insight into the inner workings of the algorithm.',\n",
       " 'The Gamma approximation has no statistical guarantees, as stated explicitly in the HSIC testing paper of NeurIPS 2007.',\n",
       " 'The details are especially unclear on LOAN.',\n",
       " 'The author never explains. E.g., link to NRMSE and PFC to the Table.',\n",
       " 'Overall, I’m leaning towards accepting the paper, but it would be important to see how well the experiments generalize to i) ResNet and ii) other (lower entropy) input images.',\n",
       " 'Could it be that what we are seeing is the attack being denoised?',\n",
       " \"I think everyone agrees that we can design models that show particular kinds of generalization by carefully building inductive bias into the architecture, and that it's easy to make these work on the right toy data.\",\n",
       " 'It is well aligned with tools that are needed to understand neural networks.',\n",
       " \"However, the experimental results are weak in justifying the paper's claims.\",\n",
       " \"Edit: Based on the feedback from the authors, I changed my rating from a 'weak accept' to an 'accept'.\",\n",
       " 'However, it is not clear to me that these are some novel results that can better help adversarial training.',\n",
       " 'both graph kernels and graph networks.',\n",
       " '2. In Eq(8), does the outer expectation w.r.t . y_{-v} be approximated by one sample? If so, it is using the local expectation method.',\n",
       " \"Originality - I don't know of any other work that models the role of dopamine in quite this way, or that applies dynamic plasticity modulation in settings like these.\",\n",
       " 'Why should we use embedding to compare the similarity?',\n",
       " '- In the last sentences of the first paragraph on p.2 you make a contrast between using softmax and sigmoid functions, however, normally the difference between them is their use in binary or multiclass classification. Is there anything special that you want to show in you case?',\n",
       " 'Can the authors clarify how the neural style transfer experiment is performed?',\n",
       " 'and how frequently each negative example is seen during',\n",
       " '(iii) How does AGZ extend to generic sequential decision-making problems?',\n",
       " 'This paper is an empirical contribution regarding SGD arguing that it presents two different behaviors which the authors name a noise dominated regimen, and a curvature dominated regime.',\n",
       " 'It might be possible that if one performs few steps of GGT optimizer in the initial stages and then switches to SGD/ADAM in the later stages, then some of the computational concerns that arise are eliminated.',\n",
       " \"The only new observation I'm aware of in these two sections is that the training loss and test accuracy are independent of batch size in the noise dominated regime.\",\n",
       " '- the more interesting problem, RL + auxiliary loss isn’t evaluated in detail',\n",
       " 'The paper proposes a new gradient estimator of low variance applicable in certain scenarios, in particular it allows training of generative models in which observations and/or latent variables are discrete.',\n",
       " '- The proposed method shows competitive or better performance than existing neural architecture search methods.',\n",
       " 'The MTurk experiment gives a qualitative picture, but it could be improved with comparisons to pairwise distances learned through alternative means using the RGB image itself (given that images would permit such a comparison).',\n",
       " '- When generating the questions, the authors \"first sample the answer\". What\\'s the distribution you use on the answer? This seems like it dramatically affects the resulting questions, so I\\'m curious how it\\'s selected.',\n",
       " 'But first, it’s not obvious why this would be a good thing (or a bad thing for that matter)',\n",
       " 'The real data experiments use N= 6 or N=2.',\n",
       " 'Accuracy + L2-regularization(W) + L1-sparsity(\\\\lambda),',\n",
       " '- White-box attack experiments don’t really prove the strength of the method, even with imagenet experiments, as almost all attacks get 100% success rate making it hard to compare.',\n",
       " 'This is a very nice work with impressive results, a great progress achievement in the field of image generation.',\n",
       " 'The first result indicates strong convergence when using the Euclidean distance as the distance measure D.',\n",
       " 'I see this is a challenge for MLE than DNNs.',\n",
       " 'For HSIC testing, I am surprised to read in the footnote that the Gamma approximation report significantly more than 5% errors, especially given the Table 4 results that show the correct level.',\n",
       " 'There are some typos that can be easily found, such as “of the out algorithm”.',\n",
       " '1. The main idea of the paper is to propose a generative model that can handle partially-observed multimodal data during training.',\n",
       " 'The work is rather incremental from current state-of-the-art methods.',\n",
       " '* The baselines in the experiments could be improved.',\n",
       " 'I wonder except for the difference of memory consumption, how different it is compared to parameter space exploration.',\n",
       " 'In the current form of evaluation, it is hard to say if there is any benefit of using the \"selection network\" that is the main novelty of the paper.',\n",
       " 'Additionally, the authors state “we remark that accessing the parameters of the generative classifiers […] is not a mild assumption since the information about training data is required to compute them”.',\n",
       " 'I am wondering this method can be applied to other complex datasets whose latent factors are unknown.',\n",
       " 'Further, there a number of experimental details that need to be further elaborated upon.',\n",
       " 'While this is a simple and seemingly obvious approach, it had to be done by someone.',\n",
       " 'The authors provide both a theoretical analysis (convergence to a stationary point) and experiments for an InceptionV3 network on ImageNet.',\n",
       " 'In such a situation, there are better strategies for higher-order tests of independence than estimating mutual information, in particular estimator that give a control p-value.',\n",
       " 'I would like to see additional experiments to answer this questions.',\n",
       " 'Multivariate tests of association based on univariate tests',\n",
       " 'I would have liked to see a bit more analysis as to why some pre-training strategies work over others.',\n",
       " 'Strengths:',\n",
       " 'This paper proposes a generative point cloud model based on adversarial learning and definitti’s representation theorem of exchangeable variables.',\n",
       " 'I don’t follow this argument: this is just part of the classifier. White box attacks are by definition performed with the knowledge of the model, what',\n",
       " 'In particular, the authors show the result for convex-concave problems satisfying a “sufficiently bilinear” condition that is related to the PL conditions.',\n",
       " 'In the future, I am quite curious about how these mono-image learned features would fare on more complex downstream tasks (e.g., segmentation, keypoint detection) which necessarily rely less on texture.',\n",
       " 'Minor comment:',\n",
       " \"But again, it's not super clear how the paper estimates this derivative.\",\n",
       " 'But it is easy to see that eq. 10 will give different results for each of these coordinates, because they correspond to different rotations of the filter about the Z-axis.',\n",
       " 'This paper studies non-additive utility aggregation for sets.',\n",
       " 'Specifically, Dreamer is an actor-critic method that learns an optimal policy by backpropagating re-parameterized gradients through a value function, a latent transition model, and a latent representation model.',\n",
       " '- in page 3, 2line above eq(2): what is delta in the variance of the multivariate normal distribution?',\n",
       " 'Second, it’s not exactly correct (as noted in the paper itself): the architecture uses 1x1 convolutions and upsampling, which combined give a weak and underparametrized analog of convolutions.',\n",
       " 'Further, it also satisfies the layer-wise conservation principle with the outputs completely redistributed  to the inputs.',\n",
       " '- Each of the justifications to get around the issue of distinguishing strange model behavior from bad feature importance technique should be explained briefly in paragraph before section 4.1.',\n",
       " 'Then, the neural discriminator is trained to estimate the (lower-bound of) mutual information between the two states.',\n",
       " 'This pre-trained model is then incorporated into the neural net for MNIST classification.',\n",
       " 'The work here tries to not learn a lot of these structures but impose them.',\n",
       " 'In general i find the paper interesting, with nice ideas and I believe that will be appreciated from researchers that are interested on smooth games and their connections to machine learning applications.',\n",
       " 'Of course, approximate linear system solvers then permit a wide tradeoff space to speed things up.',\n",
       " 'This needs to be made more explicitly, and language could be a bit toned down (e.g. in this model, we can obtain runtime that match or improve over lower bounds...)',\n",
       " 'Thus their novelty rests on the definition of zero confidence attack, and of the importance of such a attack.',\n",
       " 'Including at least one set of black box attacks is necessary to verify to what degree the vanishing gradient is the case here.',\n",
       " '- It may even make sense to minimise the number of oracle calls which can be thought of as a resource and discuss the relationship between number of oracle calls and other resources such as space.',\n",
       " 'The surrogate is constructed with the goal of reducing the need of hyper-parameter tuning and evaluated on a clique finding task.',\n",
       " 'compared to previous methods.',\n",
       " 'Experiments on few shot learning show that meta dropout achieves better performance.',\n",
       " 'Since this is qualitative, I suggest to change the saturation of the images to make them easier to interpret.',\n",
       " 'In the experiment there is no details on how you set the hyperparameters of CW and EAD.',\n",
       " '[Strength]',\n",
       " 'They propose to solve this by learning an MLP that produces the output by recurrent application, and then composing subapplications of different networks as a type of interpolation.',\n",
       " '- It seems that the model is not very scalable; while the authors do provide a way of reducing the necessary parameters that the hypernetwork has to predict, minibatching can still be an issue as it is implied that you draw a separate random weight matrix for each datapoint due to the input specific distribution (as shown at Algorithm 1).',\n",
       " 'In summary, since DiVA gives a good experimental performance, the proposed method might be promising.',\n",
       " 'The usefulness of the approach is also lessened by the greater importance of the choice of the optimizer.',\n",
       " 'Since this paper is about distributed robustness and distributed attacks, it would be very informative to the community to illustrate DBA attack on these methods to have a more compelling message.',\n",
       " 'Such methods are directly applicable to time-delayed causal relations by further considering the constraint that effects temporally follow the causes.',\n",
       " 'The role of auxiliary tasks is to improve the generalization performance of the principal task of interest.',\n",
       " '3 - Appendix A - the 6 parameters for the symmetric case with zero diagonal reduces to the same 4 parameters from Hartford et al. if we constrained the diagonal to be zero in the output as well as the input.',\n",
       " 'Although the alt-az convolution lacks the mathematical elegance of the general anisotropic and azimuthally isotropic spherical convolutions, it still seems like a practically useful operation for some kinds of data, particularly when implemented using the homogeneous icosahedral/hexagonal grid and fast algorithm presented in this paper.',\n",
       " 'The paper is presented in a clear manner with only minor issues.',\n",
       " '- The authors propose a novel task and experimental framework for considering their method, and show (somewhat unsurprisingly) that their method outperforms standard meta-learning methods that do not properly account for domain shift.',\n",
       " '- I am not sure what the authors mean by “the Frank-Wolfe gap is affine invariant”. If we scale the input space by a, the gap should be scaled by a^2 - how/why is it invariant?',\n",
       " 'Further, it seems that the results in Table 4 might be a bit obscured by the size of the downstream task dataset.',\n",
       " 'It is not natural to decompose such kind of patterns into several smaller pieces, unless the performance is significantly boosted.',\n",
       " \"Using spectral regularization to improve robustness is not new, but it's interesting to combine spectral regularization and adversarial training.\",\n",
       " 'The authors instead used \\\\cite everywhere, making the paper hard to read.',\n",
       " 'These strategies are not surprising for me and the novelty is incremental.',\n",
       " 'Providing such analysis would be also helpful for the community.',\n",
       " 'et al. 2017)',\n",
       " 'network.',\n",
       " '=====================================================',\n",
       " 'The message of synthetic experiments would be stronger if more of them were available and if the comparison between LOE, TSTE, and OENN was made on more of them.',\n",
       " 'Hence, I kindly do not think the outcome is truly a research result.',\n",
       " 'Finally, it has also been shown that after training a GAN using the manifold regularization, the algorithm is able to produce similar images giving a low enough perturbation of the data manifold z.',\n",
       " 'These synthetic setting can be used for sanity check, but cannot be the main part of the experiments.',\n",
       " 'Is there a task that this representation significantly outperforms other spherical methods and non-spherical methods?',\n",
       " '2. [Phrasing.] There are too many unconcise or informal phrases in the paper.',\n",
       " '- Page 14, Eqs(13, 14), w(\\\\mathbb{P}, \\\\mathbb{G}) should appear on the right.',\n",
       " 'Areas for improvement:',\n",
       " '2. Vacuous bounds in the regime \\\\beta >1: Most recurrent architectures with no restrictions on the transition matrices work in the regime where they are more expressive and \\\\beta >1.',\n",
       " 'My main concern is that sampling the target values for the unobserved modalities from the prior would almost necessarily lead to blurry synthetic “ground truth” for these modalities, which in turn means that the model would produce underconfident predictions for them.',\n",
       " '- Sec 3.1: the statements about MB and MF algorithms are inaccurate.',\n",
       " 'This metric can be used to evaluate the level of catastrophic forgetting.',\n",
       " 'The paper does not do a great job in clarify the debate.',\n",
       " 'One possible explanation is because we want to handle the partially-observable issues from multimodal data, and it would be easier to make the latent factors factorized (see Eq. (6)).',\n",
       " \"Don't the updates in Eq 1 and 2 take care of this? The ablation does not test this, right?\",\n",
       " 'In prior work (e.g. VIC, DIAYN), this meant learning a skill-conditional policy.',\n",
       " '4. Missing experiments to validate nature of bounds: Bartlett et al. [3] performed extensive experiments to exhibit the correct scaling of the generalization bounds with the \"model complexity\" introduced upto numerical constants.',\n",
       " 'In practice, the training is stopped much before convergence, in the hope of finding solutions close to minimum with high probability.',\n",
       " '49D)',\n",
       " 'This line of work should be encouraged.',\n",
       " 'While the baseline approaches seem to make use of the model confidence, I cannot see how the proposed approach (which uses a classifier) makes use of the original model.',\n",
       " '- \"principle curvatures\" -> principal.',\n",
       " 'There are a number of prior approaches to testing for multivariate statistical dependence in the machine learning and statistics literature (including a 2017 paper which uses mutual information).',\n",
       " 'The results (WER / LER) is lower for the proposed pipeline compared to using dense wav2vec representation for n-gram and character LM.',\n",
       " 'Therefore, I vote for weak rejection at this moment.',\n",
       " 'In usual Viterbi codes, input messages are encoded via a convolution with a codeword, and then decoded using a trellis.',\n",
       " 'As the authors admit, the main result is not especially surprising.',\n",
       " 'Specifically, the policy update in Dreamer resembles that of SVG (Heess et al., 2015), which also backpropagates re-parameterized gradients through a value function and a transition model.',\n",
       " '3. What are the limitations of the F-pooling? It is good to me that the authors discuss one limitation on the imaginary part of output and I would like to hear more on other potential limitations for this method.',\n",
       " 'Considering theoretically, what advantages truly follow from the paper for optimizing a given function? Let’s consider the following cases.',\n",
       " 'f. The author completely moved the results of MNIST-SVHN to Supplementary.',\n",
       " 'Furthermore, the way that CDNs are constructed seems to be more appropriate at capturing input specific uncertainty (i.e. aleatoric) rather than global uncertainty about the data (i.e. epistemic).',\n",
       " 'The authors are suggested to use unified notations to denote vectors/matrices (e.g. all bold).',\n",
       " 'This claim is simply not true, and more care is needed in reporting the results here in the wider context of the literature.',\n",
       " 'The paper is clearly written and the experimental results show that the proposed strategy leads to performance gains especially in problems where the Q matrix indeed conforms to a low-rank model.',\n",
       " 'I feel that it is a straightforward extension/generalization of the parameter space exploration. But the stochastic alignment and policy space constraint seem novel and important.',\n",
       " 'Reward prediction along --> Reward prediction alone',\n",
       " '#1) In the paragraph after Eq. 4 the equality p_r(x)=p_G(x) is very strong assumption.',\n",
       " 'The paper currently only mentions the most related work for the proposed method,  using the whole section 2 to describe VCL and use section 3 to describe FSM and half of section 5 to describe SSR.',\n",
       " '2) Co-reference module adjusts relationship scores (soft adjacency matrix) among nodes, including possibly new nodes introduced by the MRC model.',\n",
       " 'Cons:',\n",
       " 'But the paper only provides empirical results on sentimental analysis and digit recognition.',\n",
       " '- This is an interesting paper that is well written and motivated.',\n",
       " 'adaptive versions of sgd are commonly used in machine learning.',\n",
       " 'As a result, the main question in evaluating this paper is on the significance of the result and the generality of the “sufficiently bilinear” condition.',\n",
       " 'Extensions are provided to include the batch normalization.',\n",
       " 'Additionally, CNNs are not a particularly good architecture for fMRI, as fMRI is not locally translation invariant (see Aydöre ICML 2019 for instance).',\n",
       " 'I also appreciate that the authors have submitted the code to reproduce the results.',\n",
       " 'Finally, the experimental results do not show any significant advantage over PGD, either in running time (they are slower) or norm perturbation.',\n",
       " '[1] Back to the Past: Source Identification in Diffusion Networks from Partially Observed Cascades, AISTATS 2015',\n",
       " 'The authors present novel theoretical results supporting this.',\n",
       " 'After reading, the reviewer cannot understand why the user should bother to use hyperbolic representations that are more complex to compute in GCNs, given that the experimental results are roughly the same.',\n",
       " 'There is one more way.',\n",
       " 'The motivation is that making the classifier invariant to perturbations along the manifold is more reasonable than random perturbations.',\n",
       " 'This hyperparameter itself benefits from (requires?) some scheduling.',\n",
       " 'This paper points out that the traditional way of model selection is flawed due to that the validation/test set is often small.',\n",
       " 'The paper has two intertwined goals.',\n",
       " '2. As to the results of the Pose2Pose network, I wonder if there are some artifacts that will affect the performance of the Pose2Frame network.',\n",
       " 'designed to be sparse and low dimensional (and ends up being about',\n",
       " 'In Figure A1, why is there a dark point at one point in the inner circle? What makes the gradient super high there?',\n",
       " 'the paper clearly defines what is being done.',\n",
       " '2. The self attention in Eq 2 performs coreference disamguation which prevents different instances of the same location from being predicted for multiple entities.',\n",
       " 'Evaluating the paper along the requested dimensions:',\n",
       " 'The experiments were only done on simple image datasets.',\n",
       " 'Subsection 4.1:',\n",
       " 'However, I do not understand how are the *discrete* output y is handled.',\n",
       " '- The Q-value matrices and functions considered in the problem have a special structure as they result from Markov Decision Processes. Would it be possible to go beyond the low-rank assumption and propose and use a more elaborate type of prior that employs the special structure of MDPs?',\n",
       " 'Though rather dense in its exposition, this paper is an interesting contribution to the area of self-supervised learning  based on discrete representations.',\n",
       " 'Given that, the novelty of the paper is fairly incremental as it uses NerveNet to evaluate fitness and ES for the main design search.',\n",
       " 'It would be nice to explain the spherical parameterization in more detail. Is this operation itself rotation equivariant?',\n",
       " 'Indeed, an appendix would be greatly appreciated, as many experimental details were omitted.',\n",
       " '- The notion of divergence D(P|G) is not made concrete in section 3 and 3.1, which makes the notation of rather little use.',\n",
       " 'This paper proposed to train a network with training curves and corresponding parameters, and use policy search to find optimal parameter to replace hundreds or thousands of training in real case scenario, and it is clearly much faster using the trained network to infer parameters, instead of tuning the network manually.',\n",
       " 'A pretrained GAN is utilized to approximate the prior distribution of the noise-free images.',\n",
       " 'The idea is simple but intuitive. I am conservative about my rating now, I will consider raising it after the rebuttal.',\n",
       " 'One good motivation for example is adversarial training, e.g. Kurakin et al 2017 ‘ADVERSARIAL MACHINE LEARNING AT SCALE’ that would benefit greatly from faster attacks',\n",
       " 'In addition, this paper has a clear logic to explain and prove the problem to be solved, and has ample experimental evidence.',\n",
       " 'I believe the results all relate to inference but it would be good to get an overview of the impact of training time as well.',\n",
       " 'It does intuitively makes sense, but more mathematical definition (e.g., dimensionality) may be needed.',\n",
       " 'It would help to clarify the connection between the Gram/correlation matrix of gradients and the Hessian and what is being done to ill-conditioning, since second order methods are basically designed for ill-conditioned problems..',\n",
       " 'Overview:',\n",
       " 'For the rest of my review, I will treat the paper as “few-shot learning with domain adaptation” for more appropriate analysis.',\n",
       " '“TOWARDS A NEURAL STATISTICIAN”, ICLR’17,',\n",
       " '- It is not clear why authors did not follow the evaluation protocol of [Achlioptas’17] or [Wu’16] more closely.',\n",
       " 'This is because there is an important gap: the MCTS policy is not the same as the optimal policy.',\n",
       " '1) The noise standard deviation in x^2_{t-1}, denoted by \\\\eta_2, may be non-zero.',\n",
       " '- Section 3 should be introduced by explaining the goal of the section otherwise it breaks the flow of reading.',\n",
       " 'The authors do not explicitly define continual learning, incremental learning, and catastrophic forgetting problem.',\n",
       " 'This paper proposes Direct Sparse Optimization (DSO)-NAS, which is a method to obtain neural architectures on specific problems, at a reasonable computational cost.',\n",
       " 'The dataset is generated by human, while the testing set is adversarially selected using BERT.',\n",
       " 'Actually, we only need to specify particular fidelity terms for different tasks.',\n",
       " 'It occludes the clarity that BN is emphasized amongst them.',\n",
       " \"Another selling point of PARCUS is that it's interpretable. While neural networks can also be analyzed in different ways, I agree with the authors that this is nice to have.\",\n",
       " 'The experimental results are not very convincing because many importance baselines are neglected.',\n",
       " 'The authors claim that some amount of noise can be tolerated, but do not quantify how much.',\n",
       " '- For the synthetic tree, why is the number of edges 2(|V|-1) rather than |V|-1?',\n",
       " '* The introduction claims that black-box attacks need to estimate gradients coordinate-wise.',\n",
       " '[A] S. Sankaranarayanan, Y. Balaji, C. D. Castillo, and R. Chellappa. Generate to adapt: Aligning domains using generative adversarial networks. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.',\n",
       " 'This paper builds a new graph convolutional network (GCN) based on hyperbolic representations of the graph nodes: all latent representations of the nodes are points on Poincare disks.',\n",
       " 'The purpose of this paper is to analyze whether such an oracle can help streaming algorithms to obtain improved bounds.',\n",
       " 'They propose to replace the reward term in the policy gradient algorithm with its centered empirical cumulative distribution, which has a fixed and known U[-1, 1] distribution.',\n",
       " 'This is something which would need to be explained/ substantiated separately.',\n",
       " '.',\n",
       " 'A modality can be a whole block of features (e.g., a MNIST image) or just a single scalar feature.',\n",
       " 'I appreciate the efforts of the authors into investigating the issues raised, the described experiments sound promising.',\n",
       " 'The proposed MAD competition distinguishes classifiers by finding their respective counterexamples.',\n",
       " 'The paper has several strong points.',\n",
       " 'Finally, on PTB the authors report improvements over baseline LSTMs.',\n",
       " 'Next, the method is compared against a human and a model-free baseline training Transformer models on the Penn Treebank dataset.',\n",
       " 'The simulation and real data experiments are interesting and seem well applied.',\n",
       " 'The paper propose to incorporate an additional class for adversarial and out-distribution samples in CNNs.',\n",
       " '-------------',\n",
       " '- This paper is a slightly difficult read - not because of the',\n",
       " 'Overall assessment:',\n",
       " 'The paper presents a very simple approaches for selecting the out-distribution images that relies on many hidden assumptions on the images source or the base classier, and the interpolation mechanism is also too simple and there is the implicit assumption of low complexity images.',\n",
       " 'The authors start from a very high-level description of machine learning in constant curvature spaces.',\n",
       " \"The results only compare with Shim et al. Why only this method? Why would it be expected to be faster than all the other alternatives? Wouldn't similar alternatives like the sparsely gated MoE, D-softmax and adaptive-softmax have chances of being faster?\",\n",
       " 'The paper proposes to use the triplet loss as a convex relaxation of the ordinal embedding problem.',\n",
       " 'Could the authors please either correct this logic or provide the experiments?',\n",
       " 'This paper presents a multi-task learning approach for VQA that represent a solver for each task as a neural module that calls existing modules in a program manner.',\n",
       " 'The analysis makes efficient use of mean value theorem in the context of  parametrization of the loss function.',\n",
       " 'In addition, it could be worthwhile to compare and benchmark on existing evaluations: https://arxiv.org/pdf/1802.06806.pdf',\n",
       " '- In Appendix F, Figure 20 d), the title seems wrong. It seems to report sigma^2 values, but the title says “losses”.',\n",
       " 'Overall, this paper is good, but is not novel or important enough for acceptance.',\n",
       " 'In summary, I feel the paper tackles an interesting problem with an interesting approach, but the content could be organized much better.',\n",
       " 'Firstly, the paper shows that deep learning models are able to learn such classifiers and get low training loss.',\n",
       " 'the per-epoch time for each method is different.',\n",
       " 'The combination of ideas is ok, however, it is unclear how novel or how good is the proposed MF training of the RBMs.',\n",
       " 'UPDATE: Score changed based on author resposne',\n",
       " 'E.g., in Table 1 and 2,  the best result of VAT (Miyato et al., 2017) is VAT+Ent, 13.15 for CIFAR-10 (4000 labels) and 4.28 for SVHN (1000 labels).',\n",
       " '#3) Additional density estimation models can be used e.g. VAEs, GMM.',\n",
       " 'I appreciate the idea of testing full white-box adversarial attacks here. But I don’t understand how it is possible that DDGC is more robust, with higher adversarial test accuracy, than in Table 3.',\n",
       " 'The paper proposes a technique to perform reasoning on mathematical formulas in a latent space.',\n",
       " 'The boundary method on MNIST could be  weaker than a black box attack.',\n",
       " 'Specific comments:',\n",
       " 'I have not gone over the proofs in detail (much of which is in the appendix).',\n",
       " 'Summary: The paper focuses on comparing the impact of explicit modularity and structure on systematic generalization by studying neural modular networks and “generic” models.',\n",
       " 'Many of the parameters here are also unclear and not properly defined/introduced.',\n",
       " 'I take exception that people care more about local optimality than the actual objective.',\n",
       " 'Once you have verified the correct false positive rate for the permutation threshold, then you can compute the p-value on the alternative.',\n",
       " '-Does episode training help label propagation? How about the results of label propagation without the episode training?',\n",
       " 'Besides, the author proposes Theorem 1 to justify their MCD-based generative classifer.',\n",
       " 'Questions:',\n",
       " 'The authors spend two paragraphs in the introduction trying to draw a distinction but I am still not convinced.',\n",
       " \"However, these were recently NeurIPS papers, and the text is not yet out, so I don't think this should affect the authors' independent work (and also the product part is new).\",\n",
       " '\\\\alpha^i_t and u^i are also pretty complex and it would be good to conduct some ablation analysis.',\n",
       " 'An additional column on the table showing that the algorithm can also work in this case would improve the confidence that the proposed method is useful in practice.',\n",
       " 'This paper targets at studying the mutual exclusive bias which existed in children learning, to help understand whether there exists similar bias in deep networks.',\n",
       " 'The idea that introduces labels in VAE is not novel.',\n",
       " 'VAEs prioritize matching joint distributions of pixels and latent space: min KL(q(z, x) || p(z, x)) and is a variational approximation of the problem min KL(q(x) || p(x)), where q(x) is the data distribution.',\n",
       " '------------',\n",
       " 'The problem with this strategy is that the rigid composition only works for actions that can be split into consecutive temporal parts with prefixed duration and anchor points in time, which is clearly challenged by many works later when more complicated video events are studied.',\n",
       " 'It seems to me that those are rather \"desirable properties\" than axioms.',\n",
       " 'The key idea is to use two types of encoder networks: a unimodal encoder for every modality which is used when the modality is observed, and a shared multimodal encoder that is provided all the observed modalities and produces the latent vectors for the unobserved modalities.',\n",
       " 'One other thing I want to see is a test set with multiple different difficulty levels.',\n",
       " 'On a more detailed level it is not clear why the log n representation for items is choosen -- why not just map to embeddings directly?',\n",
       " '- Choices for experiments should be explained. Why choosing layers mixed** rather than others? Why choosing filters?',\n",
       " 'Experiments could be conducted on more benchmark datasets with more CNN architectures to convincingly show the effectiveness of the proposed F-pooling.',\n",
       " 'The main claim is that the proposed algorithm can construct adversarial examples faster than various baselines (PGD, I-FGSM, CW, etc.), and from fewer queries in a black-box setting.',\n",
       " '2) Does momentum help in constant step budget (with sufficiently large steps so that training loss is small)?',\n",
       " 'Significance: It is hard to assess given the current submission.',\n",
       " '- The term flow is never defined precisely, we need to infer it',\n",
       " 'This easy to see, because a composition of rotations of the form Rz(phi) Ry(nu) is not generally of that form.',\n",
       " '1) The main idea is simple and easy to understand.',\n",
       " 'A reader’s most natural question is whether there is a large enough improvement to offset the extra computational cost, so the fact that wall-clock times are relegated to the appendix is a significant weakness.',\n",
       " 'Apart from this missing baseline, the experimental results seem convincing.',\n",
       " 'Variations of this threshold lead to variations in FD and IS, as shown in insightful experiments.',\n",
       " \"This is the case only when one considers a poorly-designed cost function that doesn't take into account realistic factors such as actuator limits.\",\n",
       " 'Beyond the core findings, the other settings are less convincingly supported by seem more like work in progress and this paper is really just a scaling-up of [Pathak, et al., ICML17] without generating any strong results regarding questions around representation, what to do about stochasticity (although the discussion regarding something like ‘curiosity honeypots’ is interesting).',\n",
       " 'Is this something the reader should understand from Table 1?',\n",
       " 'touches upon.',\n",
       " 'In this paper \"large compression ratios\" means little compression, which I found confusing.',\n",
       " \"- it wasn't clear how the sparsity percentage on page 3 was defined?\",\n",
       " \"since Eq. (6)  stands a crucial role in the author's method.\",\n",
       " \"First, the paper argues that given a DNN, it's possible to construct either an identical output network or a comparable network that can have very different behavior for some of the statistical characteristics.\",\n",
       " 'As we saw before, this is the whole rotation group.',\n",
       " 'Pros:',\n",
       " 'So it would be interesting to explore how much training is needed for the embedding model.',\n",
       " 'Weaknesses:',\n",
       " 'Instead of inferring a posterior over theta that then induces the predictive uncertainties, uncertainties here arise from a regularizer that penalizes the distribution over theta from deviating too far from a standard Normal.',\n",
       " 'An ablation study is also performed on the location where self-modulation is applied, showing that it is beneficial across all locations but has more impact towards the later layers of the generator.',\n",
       " 'The authors propose in this paper an approach for learning models with tractable approximate posterior inference.',\n",
       " '4. Even when the authors formally introduce \\\\sigma and \\\\omega in 4.2, it is still not clear that why both of them are used for modelling the success probability.',\n",
       " '- Summary',\n",
       " '4. The novelty of the model is relatively limited as it is a combination of previous techniques on a new problem.',\n",
       " 'It might be interesting to try to generate synthetic images (or modify real ones) that are good for this purpose and observe their properties.',\n",
       " '2. Sec. 3.3, Fig. 3: The capacity (in terms of parameters)of both Resnet-18 and VGG-16 is higher than the capcity for YFCC100M dataset for n=10K images (comes to 161K bits), while the capacity of Resnet-18, with 14.7 million parameters (assuming float32 encoding) has 14.7 * 32 bits = 470.4 million bits, thus capacity alone cannot explain why VGG converges faster than Resnet-18, since both networks exceed the capacity, and capacity does not seem to have an established formal connection to rate of memorization.',\n",
       " 'The theoretical proof depends on the convexity assumption, I would also suggest comparing the proposed attack with CW and other benchmarks on some simple models that satisfy the assumptions.',\n",
       " 'The authors explain how they trained their own model but there is no mention on how they trained benchmark models.',\n",
       " 'The generator model with its warping component makes a strong hypothesis on the nature of the videos: it seems especially well suited for translations or for other simple geometric transformations characteristics of the benchmarking videos .',\n",
       " 'Another area of improvement is the experiments around VAE.',\n",
       " '- Section 4.4.1: “DSO-NAS can also search architecture [...]”  -> “DSO-NAS can also search for architectures [...]”',\n",
       " 'This approach makes sense and the model is indeed more principled than the one taken by Ulyanov et al. In fact, the DIP of Ulyanov et al. can hardly be considered \"a model\" (or a prior, for that matter), and instead should be considered \"an algorithm\", since it relies on the early stopping of a specific optimization algorithm.',\n",
       " 'It would be nice to see a more direct comparison between the three definitions of spherical convolution (general SO3, isotropic S2, and anisotropic S2).',\n",
       " '- For the automatic evaluation measures there should be multiple references per SPARQL query since this is how BLEU et al are supposed to be used.',\n",
       " 'In practice however, it appears from the experimental section that the domain mapping is learned offline, and then frozen for the meta-learning phase.',\n",
       " 'Weaknesses:',\n",
       " '- Pioneering work is not necessarily equivalent to \"using all the GPUs\"',\n",
       " 'Hence, while I agree with the authors that existing approaches to comparing deep neural network classifiers could be improved, I think the proposed solution is not a good alternative yet.',\n",
       " '1. The method used a latent dynamics model, which avoids reconstruction of the future images during inference.',\n",
       " 'It is unclear whether the data augmentation techniques is applied only at training time or also at test time.',\n",
       " 'It is also not clear to me how domain translation is relevant to continual learning.',\n",
       " 'The motivation is to generate more training data for knowledge base question answering systems to be trained on.',\n",
       " 'It will be also interesting to see how does the proposed method perform on large-scale datasets such as DomainNet and Office-Home dataset:',\n",
       " 'This parameter Omega is estimated individually for each degraded image?',\n",
       " 'This is a very popular topic and is very relevant in big data analysis.',\n",
       " 'The improvement from the baselines is also limited.',\n",
       " '- Problem explained in fifth Paragraph of section is not clear unless what the influence of the unit is clearly stated. Is it simply dg/df?',\n",
       " 'If one were to interpret the second one as the unnormalized distribution on z defined via the likelihood for c given z; even this has an issue because then the expression for KL where we plug the unnormalized density in place of the normalized need not be positive which is something they need to derive their bound.',\n",
       " 'Weaknesses: Not all model modifications are studied in all the algorithmic tasks.',\n",
       " '- The authors state that x_j is sampled from the \"prior network\" to calculate E_x_j in Equation 10, but I didn’t understand how this network is set up. Could you explain it in detail?',\n",
       " '*',\n",
       " 'To compare the author should use the average score of human.',\n",
       " 'To start with, the set of rotations R(phi, nu, 0) called the alt-az group in this paper is not a group in the mathematical sense.',\n",
       " 'It proposes to create a \"bottleneck\" to limit the mutual information.',\n",
       " 'Thus, at test time, given examples from the target domain, the meta-learner can perform few-shot learning.',\n",
       " 'This paper seeks to understand the AlphaGo Zero (AGZ) algorithm and extend the algorithm to regular sequential decision-making problems.',\n",
       " '2.2 Datasets: For datasets, I think the author should first compare their methods on symmetric and aysmmetric noisy data.',\n",
       " 'After experimentation on synthetic data, they compare the effectiveness of their proposed method Ordinal Embedding Neural Network (OENN) against the baseline techniques of Local Ordinal Embedding (LOE) and t-distributed Stochastic Triplet Embedding (TSTE).',\n",
       " '4.\\tIt seems that most of the experiments are done in relatively small dimensional data.',\n",
       " '#6) I believe that it would be nice to include a magnified image of Fig. 3, where the gradient steps are shown.',\n",
       " 'From the plots in Figure 2 and 3, it is hard to find the convergence of the method within 100 iterations.',\n",
       " 'The purpose of this paper is presumably to approximate the margin of a sample as accurately as possible.',\n",
       " '.',\n",
       " 'This is inspired by the recent work into non-Euclidean, and especially hyperbolic, embeddings.',\n",
       " 'If there were theory to be had here, then I would guess that the low-rank approximation may work even when full matrix did not, e.g., since the full matrix case would involve too may parameters.',\n",
       " 'For Figure 2, Did you try applying canonicalizations in different orders? Do they give the same results?',\n",
       " 'At the conceptual level, the idea of jointly modeling local video events is not novel, and can date back to at least ten years ago in the paper “Learning realistic human actions from movies”, where the temporal pyramid matching was combined with the bag-of-visual-words framework to capture long-term temporal structure.',\n",
       " 'They first build and describe a strong baseline based on recently proposed techniques for GANs and push the performance on large datasets with several modifications presented sequentially, to obtain strong state-of-the-art IS/FID scores, as well as impressive visual results.',\n",
       " 'The authors propose a generative model of networks by learning embeddings and pairing the embeddings with a prior distribution over networks.',\n",
       " '* The abstract claims that the poor time complexity of adversarial attacks limits their practical usefulness.',\n",
       " 'is with [1].',\n",
       " 'The first contribution is based on adapting the MINE energy-based MI estimator family to out-of-sample testing.',\n",
       " 'However, in its current state it does seem to be presented and motivated in a way that is appropriate for the audience of ML researchers at ICLR.',\n",
       " 'The authors should clarify this point.',\n",
       " 'The bounds obtained are polynomial in the sequence length T (often sublinear or linear) and should be compared with the existing bounds for a thorough comparison.',\n",
       " 'I also did not find an explanation of which version backward/forward losses [Patrini et al. 17] is used in the experiments: are the noise transition matrices estimated on the data or assumed to be known (for fair comparison, I would do the former).',\n",
       " 'On the other hand, contribution is somewhat incremental given observations made by related literature (Keskar et al and others).',\n",
       " 'Major comments:',\n",
       " 'Simply because for continuous variables similar experiments have been reported before',\n",
       " 'From this viewpoint, the actor-critic component in Dreamer is an incremental contribution.',\n",
       " 'The authors have proposed a model that due to the mixing is better suited for predictions with heteroscedastic noise and can better quantify the aleatoric uncertainty.',\n",
       " 'Tuning a good lambda v.s. tuning a good step-size, which one costs more?',\n",
       " 'Need to compare in more challenging settings where the success rate is meaningful, e.g. smaller epsilon or a more robust NN using some defence.',\n",
       " 'VSAE is capable of learning the joint',\n",
       " 'In this paper the authors introduce a new technique for softmax inference.',\n",
       " '- It seems quite likely that the unpacking of the icosahedral/hexagonal grid as done in this paper has been studied before in other fields.',\n",
       " '- One area that could stand to be improved is prior work.',\n",
       " 'Ann. Statist.',\n",
       " 'It then proposes to use this model to ``attack’’ task-specific models to perform membership inference, i.e. figuring out if an image provided in a set was used in training or not.',\n",
       " 'In the manuscript entitled \"Neural Causal Discovery with Learnable Input Noise\" the authors describe a method for automated causal inference under the scenario of a stream of temporally structured random variables (with no missingness and a look-back window of given size).',\n",
       " 'The proposed method seems plausible, but some details are impressionistic and it is not clear why and whether the modeling choices do what the paper says.',\n",
       " 'Again, why not using the distribution of the outputs of all layers? Why focusing only on the output of the last layer?',\n",
       " 'As the authors also agree, ME bias is missing not just in DNNs.',\n",
       " \"But I'm concerned with the novelty and contributions of this paper.\",\n",
       " 'See the NeurIPS paper for details.',\n",
       " 'Sampling from truncated normal distribution, where samples with norms that exceed a specific threshold are resampled.',\n",
       " 'Furthermore, for the Gaussian case, Implicit Rep gradients (and Go-gradient too) are equal to the standard reparametrization trick estimator (Kingma & Welling, 2014).',\n",
       " 'Let me elaborate.',\n",
       " \"MNIST-to-MNIST-M has better baselines  (PixelDA performed better on this task for example), Office is not suitable for domain adaptation experiments anymore unless one wants to be in a few-datasample regime or work with data with noisy labels(the dataset is plagued with label pollution, and there are too few examples per class per domain for NN-based domain adaptation); the results on CELL were not convincing, I don't know the dataset but it seems that baseline NN does better than DA most of the times.\",\n",
       " '- This sentence is not clear: \"[...]; the nature of correlations in the two models may differ\".',\n",
       " '1) In Table 1, batch size 16k has effective LR of 32.',\n",
       " \"Using the learned latent state representations, it used an actor-critic model to learn a reactive policy to optimize the agent's behaviors in long-horizon continuous control tasks.\",\n",
       " 'Minor comment:',\n",
       " 'First of all, I don’t understand how the main equation of the compound density network in Equation (3) is different from the general case of a Bayesian neural network? Can the authors please comment on that?',\n",
       " 'Some questions and comments:',\n",
       " 'To handle the imbalanced dataset, a negative pre-training method is proposed to decrease false and false positives.',\n",
       " 'This is a really nice trick.',\n",
       " 'Note that the importance sampling does not affect this issue.',\n",
       " 'swiftly moves onto the next one.',\n",
       " \"2. Typos: the weight matrix in the end of page 2 should be N_l times N_{l-1}. Also, the x_i's in the first line of page 3 should be bold.\",\n",
       " 'They combined Langevin dynamics with Stein variational gradient descent and theoretically justified that the proposed method successfully converges to the stationary distribution with only a single chain, unlike SVGD.',\n",
       " '- could the authors compare with changing step-size?',\n",
       " '- It seems that MISC is beneficial when the robot should get closer to the object for the success of the task.',\n",
       " '(1) than -> that',\n",
       " 'See this paper for discussion in the context of machine translation: http://www.aclweb.org/anthology/P16-2013',\n",
       " 'This is way to small.',\n",
       " 'Overall, I enjoyed reading the paper. My only concern is the experiments:',\n",
       " 'I like the directions using surrogate to speed up HPO in general but I feel the learning curve prediction part can be improved. There are already some works, not using deep learning method, for example the following:',\n",
       " 'This is not true.',\n",
       " 'This is because we minimize a tradeoff of the prediction error (the first term in (2)) and a function of the reciprocal of the noise standard deviation \\\\eta_2 (the second term in (2)), not only the prediction error.',\n",
       " 'I do recommend that the authors compare against those results in a future update of this work.',\n",
       " '- The effect of each proposed technique is appropriately evaluated.',\n",
       " 'The authors combine the mixed-curvature product formalism that uses products of Euclidean, hyperbolic, and spherical spaces for embeddings, but use these for GCN operations.',\n",
       " 'The insights (e.g. Figure 5) are interesting and show the study is not just trying to be a benchmark paper for some top-level results, but actually cares about understanding a problem and fixing it.',\n",
       " '- page 2: \"network\\'s type to be class\" -> \"to be a class\"',\n",
       " 'The purpose of the public set is explained only in section 5.2.',\n",
       " '- The quantitative evaluation in table 1 is interesting and useful.',\n",
       " 'They then proceed to evaluate the predictive uncertainty that CDNs offer on three tasks: a toy regression problem, out-of-distribution example detection on MNIST/notMNIST and adversarial example detection on MNIST and CIFAR 10.',\n",
       " '2. Besides the computational expensive-ness of the three-step approach (vector quantization, BERT, acoustic model training), the combined model complexity is large because these steps do not share neural network architecture.',\n",
       " '*',\n",
       " 'So, I have some doubts about the experimental results.',\n",
       " 'Thereby, allowing the controller to share parameters and reuse information across generations.',\n",
       " '##',\n",
       " 'This paper considers unsupervised (or self-supervised) discrete representation learning of speech using a combination of a recent vector quantized neural network discritization method and future time step prediction.',\n",
       " '2. In my opinion, the idea is elegant.',\n",
       " 'The authors combine two ideas, 1: the training of MF RBMs and 2: the the ability to prevent adversarial attacks.',\n",
       " '1. Are e_{i,t} and lambda_{i,t} vectors?',\n",
       " 'The model is trained to predict whether a rewrite rule can be applied to a formula given its latent representation.',\n",
       " 'The authors acknowledge this connection, but I think they should begin by introducing CE and then explain how Cakewalk generalizes it.',\n",
       " \"- Remark 1 seems trivial, but the selection of baseline x' seems less trivial.\",\n",
       " '[3]\\xa0Zhao, Han, et al. \"Multiple Source Domain Adaptation with Adversarial Learning.\" Advances in Neural Information Processing Systems. 2018.',\n",
       " 'While the fast weights approach is not totally original, its application to this problem is novel and very well-suited to it.',\n",
       " 'It is not clear to me why we need to add MC-Dropout to the ensemble.',\n",
       " 'The authors claim that the model is not convolutional.',\n",
       " 'I did not see any theoretical or empirical support for this in the paper.',\n",
       " 'What happens is that the authors arrive at this solution using a different approach.',\n",
       " 'Several of the other cited tests also use a permutation approach for the test threhsold.',\n",
       " '- The following paper merits a discussion in the related work section:',\n",
       " '- In section 4.3 last paragraph, first sentence: \"with the maximunm number\" should be \"with the maximum number\"',\n",
       " 'In addition to these three main points, the authors could strengthen their results by providing experiments on another dataset (e.g., CIFAR-10) or model architecture (e.g., a ResNet), and by averaging over a larger number of test data points (currently 200).',\n",
       " 'It is shown that having >=1 additional agents improves training of the BLEU score in standard MT and unsupervised MT tasks.',\n",
       " 'The paper, considers methods for solving smooth unconstrained min-max optimization problems.',\n",
       " '- If the information on the hierarchy of sub-categories is not available, it will be an annoying hyperparameters that should be well tuned.',\n",
       " 'Thomas B. Berrett, Richard J. Samworth',\n",
       " '- what prior distributions p(z) and p(u) are used? What is the choice based on?',\n",
       " 'This paper introduces the Scratchpad Encoder, a novel addition to the sequence to sequence (seq2seq) framework and explore its effectiveness in generating natural language questions from a given logical form.',\n",
       " ...]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_labels[0]\n",
    "val_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 true:  3\n",
      "7 true:  7\n",
      "8 true:  4\n",
      "1 true:  0\n",
      "8 true:  1\n",
      "1 true:  1\n",
      "1 true:  7\n",
      "7 true:  7\n",
      "8 true:  8\n",
      "8 true:  7\n",
      "7 true:  6\n",
      "8 true:  8\n",
      "1 true:  8\n",
      "8 true:  7\n",
      "6 true:  0\n",
      "7 true:  8\n",
      "1 true:  8\n",
      "1 true:  6\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "4 true:  4\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "8 true:  4\n",
      "6 true:  7\n",
      "8 true:  8\n",
      "6 true:  8\n",
      "7 true:  6\n",
      "8 true:  7\n",
      "1 true:  1\n",
      "8 true:  2\n",
      "6 true:  6\n",
      "8 true:  6\n",
      "8 true:  8\n",
      "6 true:  6\n",
      "1 true:  1\n",
      "7 true:  7\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "1 true:  1\n",
      "6 true:  6\n",
      "1 true:  7\n",
      "7 true:  1\n",
      "6 true:  6\n",
      "7 true:  2\n",
      "8 true:  7\n",
      "8 true:  1\n",
      "7 true:  8\n",
      "7 true:  8\n",
      "8 true:  1\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "7 true:  7\n",
      "7 true:  2\n",
      "8 true:  6\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "1 true:  6\n",
      "8 true:  8\n",
      "6 true:  6\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "1 true:  1\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "1 true:  7\n",
      "3 true:  3\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "1 true:  8\n",
      "6 true:  8\n",
      "8 true:  2\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "1 true:  1\n",
      "7 true:  1\n",
      "1 true:  1\n",
      "1 true:  1\n",
      "6 true:  7\n",
      "3 true:  4\n",
      "1 true:  1\n",
      "6 true:  6\n",
      "8 true:  8\n",
      "1 true:  1\n",
      "8 true:  6\n",
      "8 true:  8\n",
      "1 true:  1\n",
      "6 true:  6\n",
      "8 true:  8\n",
      "6 true:  7\n",
      "1 true:  3\n",
      "8 true:  3\n",
      "8 true:  8\n",
      "6 true:  6\n",
      "8 true:  8\n",
      "6 true:  6\n",
      "4 true:  4\n",
      "1 true:  1\n",
      "6 true:  6\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "6 true:  6\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "1 true:  8\n",
      "1 true:  1\n",
      "8 true:  8\n",
      "4 true:  4\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "6 true:  6\n",
      "8 true:  2\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "6 true:  7\n",
      "7 true:  7\n",
      "8 true:  8\n",
      "1 true:  8\n",
      "7 true:  7\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "6 true:  7\n",
      "8 true:  7\n",
      "8 true:  7\n",
      "8 true:  8\n",
      "8 true:  7\n",
      "8 true:  7\n",
      "6 true:  5\n",
      "8 true:  8\n",
      "8 true:  6\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "4 true:  4\n",
      "8 true:  6\n",
      "6 true:  8\n",
      "6 true:  5\n",
      "6 true:  6\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "1 true:  5\n",
      "7 true:  6\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "8 true:  4\n",
      "3 true:  4\n",
      "3 true:  8\n",
      "8 true:  7\n",
      "8 true:  8\n",
      "4 true:  4\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "3 true:  3\n",
      "8 true:  8\n",
      "6 true:  3\n",
      "8 true:  2\n",
      "6 true:  1\n",
      "1 true:  1\n",
      "7 true:  8\n",
      "8 true:  2\n",
      "7 true:  7\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "1 true:  0\n",
      "7 true:  7\n",
      "1 true:  8\n",
      "7 true:  4\n",
      "8 true:  8\n",
      "6 true:  6\n",
      "1 true:  1\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "1 true:  1\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "7 true:  7\n",
      "6 true:  3\n",
      "8 true:  7\n",
      "8 true:  8\n",
      "1 true:  5\n",
      "8 true:  8\n",
      "1 true:  0\n",
      "8 true:  8\n",
      "8 true:  6\n",
      "8 true:  8\n",
      "1 true:  1\n",
      "8 true:  7\n",
      "7 true:  7\n",
      "7 true:  7\n",
      "7 true:  7\n",
      "4 true:  8\n",
      "8 true:  8\n",
      "7 true:  0\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "8 true:  0\n",
      "6 true:  5\n",
      "8 true:  8\n",
      "7 true:  8\n",
      "8 true:  8\n",
      "6 true:  2\n",
      "8 true:  8\n",
      "6 true:  7\n",
      "8 true:  8\n",
      "6 true:  8\n",
      "7 true:  7\n",
      "8 true:  8\n",
      "8 true:  7\n",
      "7 true:  4\n",
      "1 true:  1\n",
      "8 true:  8\n",
      "1 true:  2\n",
      "1 true:  6\n",
      "8 true:  8\n",
      "1 true:  1\n",
      "6 true:  1\n",
      "8 true:  8\n",
      "8 true:  7\n",
      "7 true:  2\n",
      "8 true:  8\n",
      "8 true:  3\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "1 true:  8\n",
      "1 true:  8\n",
      "8 true:  8\n",
      "6 true:  6\n",
      "7 true:  6\n",
      "8 true:  8\n",
      "1 true:  1\n",
      "1 true:  1\n",
      "7 true:  7\n",
      "3 true:  3\n",
      "6 true:  1\n",
      "6 true:  7\n",
      "8 true:  3\n",
      "8 true:  8\n",
      "1 true:  6\n",
      "1 true:  8\n",
      "8 true:  7\n",
      "6 true:  1\n",
      "8 true:  1\n",
      "8 true:  8\n",
      "6 true:  6\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "8 true:  6\n",
      "6 true:  6\n",
      "8 true:  7\n",
      "3 true:  7\n",
      "6 true:  0\n",
      "6 true:  6\n",
      "7 true:  7\n",
      "1 true:  1\n",
      "6 true:  6\n",
      "8 true:  8\n",
      "8 true:  4\n",
      "7 true:  7\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "1 true:  1\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "7 true:  6\n",
      "8 true:  7\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "6 true:  6\n",
      "8 true:  1\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "6 true:  6\n",
      "8 true:  3\n",
      "8 true:  8\n",
      "8 true:  7\n",
      "8 true:  1\n",
      "8 true:  8\n",
      "8 true:  7\n",
      "1 true:  1\n",
      "6 true:  7\n",
      "1 true:  1\n",
      "6 true:  6\n",
      "1 true:  7\n",
      "8 true:  2\n",
      "8 true:  8\n",
      "6 true:  3\n",
      "8 true:  8\n",
      "6 true:  7\n",
      "8 true:  4\n",
      "6 true:  7\n",
      "1 true:  1\n",
      "8 true:  8\n",
      "6 true:  6\n",
      "8 true:  8\n",
      "7 true:  2\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "3 true:  3\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "4 true:  4\n",
      "8 true:  8\n",
      "1 true:  1\n",
      "7 true:  1\n",
      "1 true:  5\n",
      "8 true:  8\n",
      "6 true:  8\n",
      "6 true:  7\n",
      "8 true:  8\n",
      "7 true:  3\n",
      "7 true:  7\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "7 true:  6\n",
      "8 true:  8\n",
      "7 true:  1\n",
      "6 true:  7\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "8 true:  2\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "8 true:  7\n",
      "8 true:  8\n",
      "6 true:  7\n",
      "1 true:  6\n",
      "6 true:  8\n",
      "8 true:  8\n",
      "8 true:  2\n",
      "8 true:  8\n",
      "6 true:  7\n",
      "1 true:  1\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "1 true:  7\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "1 true:  1\n",
      "7 true:  4\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "6 true:  7\n",
      "6 true:  6\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "7 true:  7\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "6 true:  7\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "7 true:  7\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "6 true:  7\n",
      "6 true:  8\n",
      "8 true:  8\n",
      "4 true:  2\n",
      "8 true:  0\n",
      "8 true:  7\n",
      "4 true:  4\n",
      "8 true:  7\n",
      "8 true:  8\n",
      "6 true:  7\n",
      "4 true:  6\n",
      "6 true:  6\n",
      "1 true:  1\n",
      "6 true:  6\n",
      "2 true:  2\n",
      "8 true:  7\n",
      "6 true:  8\n",
      "8 true:  8\n",
      "7 true:  8\n",
      "8 true:  8\n",
      "8 true:  4\n",
      "1 true:  5\n",
      "1 true:  8\n",
      "8 true:  7\n",
      "8 true:  8\n",
      "6 true:  6\n",
      "7 true:  8\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "6 true:  6\n",
      "6 true:  5\n",
      "1 true:  1\n",
      "6 true:  7\n",
      "8 true:  8\n",
      "8 true:  7\n",
      "1 true:  8\n",
      "8 true:  8\n",
      "7 true:  7\n",
      "1 true:  1\n",
      "6 true:  6\n",
      "7 true:  6\n",
      "8 true:  8\n",
      "6 true:  8\n",
      "7 true:  7\n",
      "8 true:  8\n",
      "6 true:  5\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "6 true:  1\n",
      "7 true:  6\n",
      "7 true:  3\n",
      "8 true:  8\n",
      "7 true:  2\n",
      "8 true:  7\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "8 true:  6\n",
      "7 true:  7\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "6 true:  0\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "6 true:  1\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "8 true:  7\n",
      "6 true:  1\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "1 true:  5\n",
      "7 true:  3\n",
      "1 true:  1\n",
      "1 true:  1\n",
      "8 true:  7\n",
      "1 true:  6\n",
      "4 true:  2\n",
      "8 true:  6\n",
      "7 true:  7\n",
      "7 true:  2\n",
      "8 true:  4\n",
      "7 true:  8\n",
      "4 true:  6\n",
      "1 true:  1\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "1 true:  1\n",
      "1 true:  1\n",
      "7 true:  7\n",
      "6 true:  7\n",
      "8 true:  8\n",
      "6 true:  2\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "1 true:  8\n",
      "8 true:  7\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "1 true:  0\n",
      "6 true:  8\n",
      "6 true:  3\n",
      "3 true:  8\n",
      "6 true:  6\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "8 true:  7\n",
      "6 true:  6\n",
      "6 true:  6\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "1 true:  7\n",
      "8 true:  8\n",
      "7 true:  1\n",
      "8 true:  8\n",
      "3 true:  3\n",
      "8 true:  8\n",
      "6 true:  6\n",
      "7 true:  7\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "6 true:  8\n",
      "1 true:  1\n",
      "2 true:  2\n",
      "8 true:  8\n",
      "6 true:  6\n",
      "1 true:  6\n",
      "1 true:  8\n",
      "8 true:  2\n",
      "8 true:  8\n",
      "8 true:  0\n",
      "8 true:  6\n",
      "3 true:  7\n",
      "8 true:  4\n",
      "8 true:  8\n",
      "4 true:  8\n",
      "8 true:  8\n",
      "1 true:  1\n",
      "7 true:  2\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "3 true:  3\n",
      "6 true:  7\n",
      "6 true:  7\n",
      "8 true:  8\n",
      "6 true:  7\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "1 true:  0\n",
      "1 true:  1\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "6 true:  6\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "1 true:  1\n",
      "8 true:  4\n",
      "1 true:  6\n",
      "6 true:  7\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "1 true:  6\n",
      "6 true:  6\n",
      "1 true:  1\n",
      "6 true:  8\n",
      "8 true:  8\n",
      "7 true:  7\n",
      "8 true:  4\n",
      "8 true:  8\n",
      "7 true:  7\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "6 true:  6\n",
      "7 true:  7\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "1 true:  1\n",
      "1 true:  1\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "8 true:  7\n",
      "6 true:  8\n",
      "8 true:  1\n",
      "7 true:  7\n",
      "1 true:  1\n",
      "8 true:  8\n",
      "7 true:  3\n",
      "8 true:  4\n",
      "7 true:  8\n",
      "1 true:  1\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "6 true:  7\n",
      "7 true:  7\n",
      "8 true:  6\n",
      "4 true:  4\n",
      "1 true:  1\n",
      "1 true:  1\n",
      "8 true:  8\n",
      "8 true:  7\n",
      "8 true:  8\n",
      "3 true:  3\n",
      "8 true:  8\n",
      "8 true:  7\n",
      "1 true:  5\n",
      "1 true:  2\n",
      "7 true:  7\n",
      "6 true:  6\n",
      "6 true:  8\n",
      "8 true:  3\n",
      "7 true:  7\n",
      "8 true:  8\n",
      "7 true:  7\n",
      "8 true:  8\n",
      "6 true:  1\n",
      "4 true:  4\n",
      "6 true:  6\n",
      "6 true:  7\n",
      "1 true:  7\n",
      "8 true:  8\n",
      "6 true:  8\n",
      "8 true:  8\n",
      "6 true:  8\n",
      "4 true:  4\n",
      "7 true:  7\n",
      "8 true:  8\n",
      "7 true:  7\n",
      "8 true:  2\n",
      "7 true:  6\n",
      "1 true:  7\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "7 true:  7\n",
      "3 true:  3\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "1 true:  8\n",
      "1 true:  1\n",
      "8 true:  8\n",
      "7 true:  4\n",
      "7 true:  7\n",
      "7 true:  7\n",
      "7 true:  4\n",
      "8 true:  8\n",
      "6 true:  6\n",
      "7 true:  5\n",
      "6 true:  8\n",
      "8 true:  8\n",
      "8 true:  7\n",
      "8 true:  7\n",
      "8 true:  8\n",
      "7 true:  7\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "6 true:  6\n",
      "8 true:  8\n",
      "7 true:  5\n",
      "8 true:  8\n",
      "1 true:  1\n",
      "6 true:  6\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "1 true:  1\n",
      "6 true:  7\n",
      "8 true:  7\n",
      "1 true:  1\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "8 true:  7\n",
      "3 true:  3\n",
      "8 true:  8\n",
      "1 true:  8\n",
      "8 true:  8\n",
      "8 true:  7\n",
      "6 true:  7\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "8 true:  6\n",
      "8 true:  1\n",
      "7 true:  1\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "6 true:  6\n",
      "7 true:  7\n",
      "8 true:  6\n",
      "7 true:  7\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "8 true:  2\n",
      "6 true:  8\n",
      "8 true:  1\n",
      "8 true:  4\n",
      "6 true:  6\n",
      "6 true:  7\n",
      "8 true:  6\n",
      "4 true:  8\n",
      "1 true:  1\n",
      "4 true:  4\n",
      "8 true:  1\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "7 true:  2\n",
      "7 true:  4\n",
      "8 true:  8\n",
      "8 true:  6\n",
      "6 true:  7\n",
      "1 true:  1\n",
      "1 true:  1\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "6 true:  6\n",
      "1 true:  6\n",
      "8 true:  8\n",
      "3 true:  3\n",
      "6 true:  8\n",
      "6 true:  6\n",
      "8 true:  8\n",
      "6 true:  7\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "7 true:  8\n",
      "7 true:  8\n",
      "1 true:  0\n",
      "8 true:  8\n",
      "7 true:  8\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "4 true:  4\n",
      "8 true:  4\n",
      "7 true:  6\n",
      "6 true:  7\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "8 true:  1\n",
      "6 true:  6\n",
      "7 true:  6\n",
      "4 true:  4\n",
      "8 true:  1\n",
      "6 true:  7\n",
      "8 true:  2\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "7 true:  7\n",
      "1 true:  3\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "8 true:  5\n",
      "8 true:  7\n",
      "8 true:  8\n",
      "8 true:  4\n",
      "6 true:  3\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "8 true:  6\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "7 true:  7\n",
      "8 true:  8\n",
      "6 true:  1\n",
      "1 true:  1\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "7 true:  6\n",
      "8 true:  8\n",
      "1 true:  6\n",
      "6 true:  7\n",
      "3 true:  3\n",
      "4 true:  4\n",
      "1 true:  7\n",
      "1 true:  2\n",
      "1 true:  0\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "7 true:  7\n",
      "7 true:  7\n",
      "1 true:  7\n",
      "1 true:  1\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "1 true:  6\n",
      "6 true:  8\n",
      "8 true:  8\n",
      "1 true:  1\n",
      "1 true:  1\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "6 true:  8\n",
      "1 true:  8\n",
      "7 true:  7\n",
      "7 true:  2\n",
      "8 true:  6\n",
      "1 true:  6\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "8 true:  3\n",
      "8 true:  8\n",
      "1 true:  8\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "7 true:  8\n",
      "8 true:  8\n",
      "8 true:  7\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "7 true:  7\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "1 true:  1\n",
      "8 true:  8\n",
      "6 true:  8\n",
      "8 true:  8\n",
      "7 true:  6\n",
      "8 true:  8\n",
      "6 true:  8\n",
      "8 true:  8\n",
      "8 true:  6\n",
      "7 true:  7\n",
      "1 true:  6\n",
      "4 true:  4\n",
      "7 true:  3\n",
      "8 true:  8\n",
      "8 true:  7\n",
      "4 true:  4\n",
      "8 true:  8\n",
      "8 true:  2\n",
      "8 true:  7\n",
      "7 true:  6\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "1 true:  8\n",
      "8 true:  8\n",
      "1 true:  1\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "6 true:  1\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "6 true:  6\n",
      "4 true:  8\n",
      "7 true:  7\n",
      "8 true:  1\n",
      "8 true:  8\n",
      "4 true:  4\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "6 true:  7\n",
      "6 true:  0\n",
      "8 true:  8\n",
      "6 true:  7\n",
      "1 true:  8\n",
      "1 true:  1\n",
      "7 true:  7\n",
      "8 true:  8\n",
      "6 true:  1\n",
      "6 true:  3\n",
      "1 true:  1\n",
      "6 true:  6\n",
      "1 true:  1\n",
      "6 true:  1\n",
      "7 true:  6\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "3 true:  4\n",
      "1 true:  8\n",
      "8 true:  8\n",
      "1 true:  1\n",
      "1 true:  5\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "7 true:  6\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "1 true:  1\n",
      "8 true:  8\n",
      "4 true:  4\n",
      "7 true:  7\n",
      "6 true:  7\n",
      "7 true:  2\n",
      "6 true:  7\n",
      "6 true:  7\n",
      "8 true:  7\n",
      "1 true:  1\n",
      "6 true:  6\n",
      "7 true:  3\n",
      "6 true:  7\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "1 true:  7\n",
      "7 true:  6\n",
      "8 true:  6\n",
      "1 true:  5\n",
      "1 true:  5\n",
      "8 true:  8\n",
      "7 true:  7\n",
      "8 true:  7\n",
      "3 true:  3\n",
      "7 true:  7\n",
      "1 true:  1\n",
      "6 true:  6\n",
      "6 true:  7\n",
      "6 true:  6\n",
      "8 true:  8\n",
      "2 true:  2\n",
      "6 true:  6\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "7 true:  7\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "1 true:  1\n",
      "6 true:  7\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "6 true:  6\n",
      "4 true:  4\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "3 true:  3\n",
      "1 true:  5\n",
      "2 true:  7\n",
      "6 true:  1\n",
      "8 true:  4\n",
      "8 true:  8\n",
      "8 true:  4\n",
      "8 true:  4\n",
      "8 true:  8\n",
      "6 true:  7\n",
      "7 true:  7\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "3 true:  4\n",
      "8 true:  2\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "1 true:  6\n",
      "6 true:  6\n",
      "8 true:  8\n",
      "4 true:  4\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "7 true:  8\n",
      "1 true:  1\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "1 true:  5\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "8 true:  6\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "1 true:  6\n",
      "8 true:  8\n",
      "7 true:  8\n",
      "8 true:  8\n",
      "1 true:  1\n",
      "8 true:  8\n",
      "8 true:  7\n",
      "8 true:  8\n",
      "4 true:  7\n",
      "6 true:  7\n",
      "8 true:  8\n",
      "8 true:  7\n",
      "8 true:  7\n",
      "8 true:  8\n",
      "7 true:  3\n",
      "1 true:  1\n",
      "1 true:  1\n",
      "7 true:  3\n",
      "8 true:  8\n",
      "8 true:  6\n",
      "8 true:  8\n",
      "7 true:  2\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "4 true:  3\n",
      "8 true:  8\n",
      "1 true:  1\n",
      "8 true:  8\n",
      "1 true:  2\n",
      "1 true:  1\n",
      "8 true:  8\n",
      "4 true:  4\n",
      "1 true:  6\n",
      "6 true:  8\n",
      "7 true:  6\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "1 true:  1\n",
      "8 true:  7\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "6 true:  7\n",
      "8 true:  8\n",
      "6 true:  7\n",
      "8 true:  8\n",
      "7 true:  1\n",
      "8 true:  8\n",
      "6 true:  6\n",
      "8 true:  8\n",
      "1 true:  1\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "8 true:  7\n",
      "6 true:  6\n",
      "6 true:  6\n",
      "6 true:  7\n",
      "7 true:  7\n",
      "8 true:  8\n",
      "6 true:  7\n",
      "8 true:  8\n",
      "1 true:  1\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "1 true:  0\n",
      "6 true:  8\n",
      "8 true:  8\n",
      "1 true:  3\n",
      "1 true:  7\n",
      "8 true:  8\n",
      "6 true:  7\n",
      "4 true:  4\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "6 true:  3\n",
      "8 true:  0\n",
      "1 true:  1\n",
      "8 true:  8\n",
      "8 true:  8\n",
      "1 true:  6\n",
      "7 true:  5\n",
      "6 true:  1\n",
      "8 true:  6\n",
      "8 true:  7\n",
      "8 true:  2\n",
      "8 true:  8\n",
      "6 true:  6\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "pred = []\n",
    "for v, y in zip(val_texts, val_labels.numpy().tolist()):\n",
    "    print(predict_category(v), \"true: \", y)\n",
    "    pred.append(predict_category(v))\n",
    "\n",
    "\n",
    "confusion = confusion_matrix(val_labels, pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.set(font_scale=1.2)\n",
    "sns.heatmap(confusion, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False, square=True,\n",
    "xticklabels=[\"0\", \"1\", \"2\", \"3 \", \"4\", \"5\", \"6\",  \"7\", \"8\"], yticklabels=[\"0\", \"1\", \"2\", \"3 \", \"4\", \"5\", \"6\",  \"7\", \"8\"])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6187739463601533\n",
      "Precision: 0.6020378228947637\n",
      "Recall: 0.6187739463601533\n",
      "F1-Score: 0.5923581379814531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\carme\\miniconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy = accuracy_score(val_labels, pred)\n",
    "print('Accuracy:', accuracy)\n",
    "\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "precision = precision_score(val_labels, pred, average='weighted')\n",
    "recall = recall_score(val_labels, pred, average='weighted')\n",
    "f1 = f1_score(val_labels, pred, average='weighted')\n",
    "\n",
    "print('Precision:', precision)\n",
    "print('Recall:', recall)\n",
    "print('F1-Score:', f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_directory = r\"C:\\Users\\carme\\OneDrive - Appelt Steuerberatung\\Carmen Appelt\\Master\\Semester 3\\DASP\\data\\jiujitsu\\JitsuPEER_data_and_models_v1\\models\\roberta-base_neg\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.save_pretrained(save_directory)\n",
    "tokenizer.save_pretrained(save_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ROBERTA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaForSequenceClassification: ['roberta.embeddings.position_ids']\n",
      "- This IS expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFRobertaForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer, TFRobertaForSequenceClassification\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained(save_directory, from_pt=True)\n",
    "model = TFRobertaForSequenceClassification.from_pretrained(save_directory, from_pt=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def predict_category(text):\n",
    "    inputs = tokenizer(text, truncation=True, padding=True, return_tensors=\"tf\")\n",
    "    outputs = model(inputs)\n",
    "    logits = outputs.logits\n",
    "    prediction_value = tf.argmax(logits, axis=1).numpy()[0]\n",
    "    return prediction_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 true:  3\n",
      "0 true:  7\n",
      "0 true:  4\n",
      "0 true:  0\n",
      "0 true:  1\n",
      "0 true:  1\n",
      "0 true:  7\n",
      "0 true:  7\n",
      "0 true:  8\n",
      "0 true:  7\n",
      "0 true:  6\n",
      "0 true:  8\n",
      "0 true:  8\n",
      "0 true:  7\n",
      "0 true:  0\n",
      "0 true:  8\n",
      "0 true:  8\n",
      "0 true:  6\n",
      "0 true:  8\n",
      "0 true:  8\n",
      "0 true:  4\n",
      "0 true:  8\n",
      "0 true:  8\n",
      "0 true:  8\n",
      "0 true:  4\n",
      "0 true:  7\n",
      "0 true:  8\n",
      "0 true:  8\n",
      "0 true:  6\n",
      "0 true:  7\n",
      "0 true:  1\n",
      "0 true:  2\n",
      "0 true:  6\n",
      "0 true:  6\n",
      "0 true:  8\n",
      "0 true:  6\n",
      "0 true:  1\n",
      "0 true:  7\n",
      "0 true:  8\n",
      "0 true:  8\n",
      "0 true:  1\n",
      "0 true:  6\n",
      "0 true:  7\n",
      "0 true:  1\n",
      "0 true:  6\n",
      "0 true:  2\n",
      "0 true:  7\n",
      "0 true:  1\n",
      "0 true:  8\n",
      "0 true:  8\n",
      "0 true:  1\n",
      "0 true:  8\n",
      "0 true:  8\n",
      "0 true:  7\n",
      "0 true:  2\n",
      "0 true:  6\n",
      "0 true:  8\n",
      "0 true:  8\n",
      "0 true:  6\n",
      "0 true:  8\n",
      "0 true:  6\n",
      "0 true:  8\n",
      "0 true:  8\n",
      "0 true:  8\n",
      "0 true:  8\n",
      "0 true:  1\n",
      "0 true:  8\n",
      "0 true:  8\n",
      "0 true:  7\n",
      "0 true:  3\n",
      "0 true:  8\n",
      "0 true:  8\n",
      "0 true:  8\n",
      "0 true:  8\n",
      "0 true:  2\n",
      "0 true:  8\n",
      "0 true:  8\n",
      "0 true:  1\n",
      "0 true:  1\n",
      "0 true:  1\n",
      "0 true:  1\n",
      "0 true:  7\n",
      "0 true:  4\n",
      "0 true:  1\n",
      "0 true:  6\n",
      "0 true:  8\n",
      "0 true:  1\n",
      "0 true:  6\n",
      "0 true:  8\n",
      "0 true:  1\n",
      "0 true:  6\n",
      "0 true:  8\n",
      "0 true:  7\n",
      "0 true:  3\n",
      "0 true:  3\n",
      "0 true:  8\n",
      "0 true:  6\n",
      "0 true:  8\n",
      "0 true:  6\n",
      "0 true:  4\n",
      "0 true:  1\n",
      "0 true:  6\n",
      "0 true:  8\n",
      "0 true:  8\n",
      "0 true:  6\n",
      "0 true:  8\n",
      "0 true:  8\n",
      "0 true:  8\n",
      "0 true:  1\n",
      "0 true:  8\n",
      "0 true:  4\n",
      "0 true:  8\n",
      "0 true:  8\n",
      "0 true:  6\n",
      "0 true:  2\n",
      "0 true:  8\n",
      "0 true:  8\n",
      "0 true:  8\n",
      "0 true:  7\n",
      "0 true:  7\n",
      "0 true:  8\n",
      "0 true:  8\n",
      "0 true:  7\n",
      "0 true:  8\n",
      "0 true:  8\n",
      "0 true:  7\n",
      "0 true:  7\n",
      "0 true:  7\n",
      "0 true:  8\n",
      "0 true:  7\n",
      "0 true:  7\n",
      "0 true:  5\n",
      "0 true:  8\n",
      "0 true:  6\n",
      "0 true:  8\n",
      "0 true:  8\n",
      "0 true:  4\n",
      "0 true:  6\n",
      "0 true:  8\n",
      "0 true:  5\n",
      "0 true:  6\n",
      "0 true:  8\n",
      "0 true:  8\n",
      "0 true:  8\n",
      "0 true:  8\n",
      "0 true:  5\n",
      "0 true:  6\n",
      "0 true:  8\n",
      "0 true:  8\n",
      "0 true:  4\n",
      "0 true:  4\n",
      "0 true:  8\n",
      "0 true:  7\n",
      "0 true:  8\n",
      "0 true:  4\n",
      "0 true:  8\n",
      "0 true:  8\n",
      "0 true:  3\n",
      "0 true:  8\n",
      "0 true:  3\n",
      "0 true:  2\n",
      "0 true:  1\n",
      "0 true:  1\n",
      "0 true:  8\n",
      "0 true:  2\n",
      "0 true:  7\n",
      "0 true:  8\n",
      "0 true:  8\n",
      "0 true:  0\n",
      "0 true:  7\n",
      "0 true:  8\n",
      "0 true:  4\n",
      "0 true:  8\n",
      "0 true:  6\n",
      "0 true:  1\n",
      "0 true:  8\n",
      "0 true:  8\n",
      "0 true:  8\n",
      "0 true:  8\n",
      "0 true:  1\n",
      "0 true:  8\n",
      "0 true:  8\n",
      "0 true:  7\n",
      "0 true:  3\n",
      "0 true:  7\n",
      "0 true:  8\n",
      "0 true:  5\n",
      "0 true:  8\n",
      "0 true:  0\n",
      "0 true:  8\n",
      "0 true:  6\n",
      "0 true:  8\n",
      "0 true:  1\n",
      "0 true:  7\n",
      "0 true:  7\n",
      "0 true:  7\n",
      "0 true:  7\n",
      "0 true:  8\n",
      "0 true:  8\n",
      "0 true:  0\n",
      "0 true:  8\n",
      "0 true:  8\n",
      "0 true:  0\n",
      "0 true:  5\n",
      "0 true:  8\n",
      "0 true:  8\n",
      "0 true:  8\n",
      "0 true:  2\n",
      "0 true:  8\n",
      "0 true:  7\n",
      "0 true:  8\n",
      "0 true:  8\n",
      "0 true:  7\n",
      "0 true:  8\n",
      "0 true:  7\n",
      "0 true:  4\n",
      "0 true:  1\n",
      "0 true:  8\n",
      "0 true:  2\n",
      "0 true:  6\n",
      "0 true:  8\n",
      "0 true:  1\n",
      "0 true:  1\n",
      "0 true:  8\n",
      "0 true:  7\n",
      "0 true:  2\n",
      "0 true:  8\n",
      "0 true:  3\n",
      "0 true:  8\n",
      "0 true:  8\n",
      "0 true:  8\n",
      "0 true:  8\n",
      "0 true:  8\n",
      "0 true:  6\n",
      "0 true:  6\n",
      "0 true:  8\n",
      "0 true:  1\n",
      "0 true:  1\n",
      "0 true:  7\n",
      "0 true:  3\n",
      "0 true:  1\n",
      "0 true:  7\n",
      "0 true:  3\n",
      "0 true:  8\n",
      "0 true:  6\n",
      "0 true:  8\n",
      "0 true:  7\n",
      "0 true:  1\n",
      "0 true:  1\n",
      "0 true:  8\n",
      "0 true:  6\n",
      "0 true:  8\n",
      "0 true:  8\n",
      "0 true:  6\n",
      "0 true:  6\n",
      "0 true:  7\n",
      "0 true:  7\n",
      "0 true:  0\n",
      "0 true:  6\n",
      "0 true:  7\n",
      "0 true:  1\n",
      "0 true:  6\n",
      "0 true:  8\n",
      "0 true:  4\n",
      "0 true:  7\n",
      "0 true:  8\n",
      "0 true:  8\n",
      "0 true:  8\n",
      "0 true:  1\n",
      "0 true:  8\n",
      "0 true:  8\n",
      "0 true:  6\n",
      "0 true:  7\n",
      "0 true:  8\n",
      "0 true:  8\n",
      "0 true:  6\n",
      "0 true:  1\n",
      "0 true:  8\n",
      "0 true:  8\n",
      "0 true:  6\n",
      "0 true:  3\n",
      "0 true:  8\n",
      "0 true:  7\n",
      "0 true:  1\n",
      "0 true:  8\n",
      "0 true:  7\n",
      "0 true:  1\n",
      "0 true:  7\n",
      "0 true:  1\n",
      "0 true:  6\n",
      "0 true:  7\n",
      "0 true:  2\n",
      "0 true:  8\n",
      "0 true:  3\n",
      "0 true:  8\n",
      "0 true:  7\n",
      "0 true:  4\n",
      "0 true:  7\n",
      "0 true:  1\n",
      "0 true:  8\n",
      "0 true:  6\n",
      "0 true:  8\n",
      "0 true:  2\n",
      "0 true:  8\n",
      "0 true:  8\n",
      "0 true:  3\n",
      "0 true:  8\n",
      "0 true:  8\n",
      "0 true:  4\n",
      "0 true:  8\n",
      "0 true:  1\n",
      "0 true:  1\n",
      "0 true:  5\n",
      "0 true:  8\n",
      "0 true:  8\n",
      "0 true:  7\n",
      "0 true:  8\n",
      "0 true:  3\n",
      "0 true:  7\n",
      "0 true:  8\n",
      "0 true:  8\n",
      "0 true:  6\n",
      "0 true:  8\n",
      "0 true:  1\n",
      "0 true:  7\n",
      "0 true:  8\n",
      "0 true:  8\n",
      "0 true:  2\n",
      "0 true:  8\n",
      "0 true:  8\n",
      "0 true:  7\n",
      "0 true:  8\n",
      "0 true:  7\n",
      "0 true:  6\n",
      "0 true:  8\n",
      "0 true:  8\n",
      "0 true:  2\n",
      "0 true:  8\n",
      "0 true:  7\n",
      "0 true:  1\n",
      "0 true:  8\n",
      "0 true:  8\n",
      "0 true:  7\n",
      "0 true:  8\n",
      "0 true:  8\n",
      "0 true:  8\n",
      "0 true:  8\n",
      "0 true:  1\n",
      "0 true:  4\n",
      "0 true:  8\n",
      "0 true:  8\n",
      "0 true:  7\n",
      "0 true:  6\n",
      "0 true:  8\n",
      "0 true:  8\n",
      "0 true:  7\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_3708\\2019771364.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_texts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_labels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredict_category\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"true: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mpred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredict_category\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mconfusion\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_3708\\2149254210.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mpredict_category\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"tf\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mprediction_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mprediction_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\carme\\miniconda3\\Lib\\site-packages\\tf_keras\\src\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[1;31m# To get the full stack trace, call:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[1;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m             \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\carme\\miniconda3\\Lib\\site-packages\\tf_keras\\src\\engine\\training.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    584\u001b[0m                 \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mcopied_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcopied_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    586\u001b[0m             \u001b[0mlayout_map_lib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_map_subclass_model_variable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_layout_map\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 588\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\carme\\miniconda3\\Lib\\site-packages\\tf_keras\\src\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[1;31m# To get the full stack trace, call:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[1;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m             \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\carme\\miniconda3\\Lib\\site-packages\\tf_keras\\src\\engine\\base_layer.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1138\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1139\u001b[0m                 with autocast_variable.enable_auto_cast_variables(\n\u001b[0;32m   1140\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1141\u001b[0m                 ):\n\u001b[1;32m-> 1142\u001b[1;33m                     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1144\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1145\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\carme\\miniconda3\\Lib\\site-packages\\tf_keras\\src\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    154\u001b[0m                 \u001b[0mnew_e\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mnew_e\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m             \u001b[1;32mdel\u001b[0m \u001b[0msignature\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 158\u001b[1;33m             \u001b[1;32mdel\u001b[0m \u001b[0mbound_signature\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\carme\\miniconda3\\Lib\\site-packages\\transformers\\modeling_tf_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    433\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    435\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    436\u001b[0m         \u001b[0munpacked_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_processing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfn_args_and_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 437\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0munpacked_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\carme\\miniconda3\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_tf_roberta.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict, labels, training)\u001b[0m\n\u001b[0;32m   1428\u001b[0m             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n\u001b[0;32m   1429\u001b[0m             config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n\u001b[0;32m   1430\u001b[0m             \u001b[0;31m`\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_labels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0ma\u001b[0m \u001b[0mclassification\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mcomputed\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mCross\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mEntropy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1431\u001b[0m         \"\"\"\n\u001b[1;32m-> 1432\u001b[1;33m         outputs = self.roberta(\n\u001b[0m\u001b[0;32m   1433\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1434\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1435\u001b[0m             \u001b[0mtoken_type_ids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\carme\\miniconda3\\Lib\\site-packages\\tf_keras\\src\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[1;31m# To get the full stack trace, call:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[1;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m             \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\carme\\miniconda3\\Lib\\site-packages\\tf_keras\\src\\engine\\base_layer.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1138\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1139\u001b[0m                 with autocast_variable.enable_auto_cast_variables(\n\u001b[0;32m   1140\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1141\u001b[0m                 ):\n\u001b[1;32m-> 1142\u001b[1;33m                     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1144\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1145\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\carme\\miniconda3\\Lib\\site-packages\\tf_keras\\src\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    154\u001b[0m                 \u001b[0mnew_e\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mnew_e\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m             \u001b[1;32mdel\u001b[0m \u001b[0msignature\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 158\u001b[1;33m             \u001b[1;32mdel\u001b[0m \u001b[0mbound_signature\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\carme\\miniconda3\\Lib\\site-packages\\transformers\\modeling_tf_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    433\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    435\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    436\u001b[0m         \u001b[0munpacked_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_processing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfn_args_and_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 437\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0munpacked_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\carme\\miniconda3\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_tf_roberta.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, training)\u001b[0m\n\u001b[0;32m    823\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    824\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    825\u001b[0m             \u001b[0mhead_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    826\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 827\u001b[1;33m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[0;32m    828\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0membedding_output\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    829\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m             \u001b[0mhead_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\carme\\miniconda3\\Lib\\site-packages\\tf_keras\\src\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[1;31m# To get the full stack trace, call:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[1;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m             \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\carme\\miniconda3\\Lib\\site-packages\\tf_keras\\src\\engine\\base_layer.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1138\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1139\u001b[0m                 with autocast_variable.enable_auto_cast_variables(\n\u001b[0;32m   1140\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1141\u001b[0m                 ):\n\u001b[1;32m-> 1142\u001b[1;33m                     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1144\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1145\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\carme\\miniconda3\\Lib\\site-packages\\tf_keras\\src\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    154\u001b[0m                 \u001b[0mnew_e\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mnew_e\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m             \u001b[1;32mdel\u001b[0m \u001b[0msignature\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 158\u001b[1;33m             \u001b[1;32mdel\u001b[0m \u001b[0mbound_signature\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\carme\\miniconda3\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_tf_roberta.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, training)\u001b[0m\n\u001b[0;32m    612\u001b[0m                 \u001b[0mall_hidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mall_hidden_states\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    613\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    614\u001b[0m             \u001b[0mpast_key_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpast_key_values\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mpast_key_values\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    615\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 616\u001b[1;33m             layer_outputs = layer_module(\n\u001b[0m\u001b[0;32m    617\u001b[0m                 \u001b[0mhidden_states\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    618\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    619\u001b[0m                 \u001b[0mhead_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\carme\\miniconda3\\Lib\\site-packages\\tf_keras\\src\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[1;31m# To get the full stack trace, call:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[1;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m             \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\carme\\miniconda3\\Lib\\site-packages\\tf_keras\\src\\engine\\base_layer.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1138\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1139\u001b[0m                 with autocast_variable.enable_auto_cast_variables(\n\u001b[0;32m   1140\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1141\u001b[0m                 ):\n\u001b[1;32m-> 1142\u001b[1;33m                     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1144\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1145\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\carme\\miniconda3\\Lib\\site-packages\\tf_keras\\src\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    154\u001b[0m                 \u001b[0mnew_e\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mnew_e\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m             \u001b[1;32mdel\u001b[0m \u001b[0msignature\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 158\u001b[1;33m             \u001b[1;32mdel\u001b[0m \u001b[0mbound_signature\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\carme\\miniconda3\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_tf_roberta.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions, training)\u001b[0m\n\u001b[0;32m    504\u001b[0m         \u001b[0mtraining\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    505\u001b[0m     ) -> Tuple[tf.Tensor]:\n\u001b[0;32m    506\u001b[0m         \u001b[1;31m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    507\u001b[0m         \u001b[0mself_attn_past_key_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpast_key_value\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mpast_key_value\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 508\u001b[1;33m         self_attention_outputs = self.attention(\n\u001b[0m\u001b[0;32m    509\u001b[0m             \u001b[0minput_tensor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    510\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    511\u001b[0m             \u001b[0mhead_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\carme\\miniconda3\\Lib\\site-packages\\tf_keras\\src\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[1;31m# To get the full stack trace, call:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[1;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m             \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\carme\\miniconda3\\Lib\\site-packages\\tf_keras\\src\\engine\\base_layer.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1138\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1139\u001b[0m                 with autocast_variable.enable_auto_cast_variables(\n\u001b[0;32m   1140\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1141\u001b[0m                 ):\n\u001b[1;32m-> 1142\u001b[1;33m                     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1144\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1145\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\carme\\miniconda3\\Lib\\site-packages\\tf_keras\\src\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    154\u001b[0m                 \u001b[0mnew_e\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mnew_e\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m             \u001b[1;32mdel\u001b[0m \u001b[0msignature\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 158\u001b[1;33m             \u001b[1;32mdel\u001b[0m \u001b[0mbound_signature\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\carme\\miniconda3\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_tf_roberta.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, input_tensor, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions, training)\u001b[0m\n\u001b[0;32m    395\u001b[0m             \u001b[0mpast_key_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpast_key_value\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    396\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    397\u001b[0m             \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    398\u001b[0m         )\n\u001b[1;32m--> 399\u001b[1;33m         attention_output = self.dense_output(\n\u001b[0m\u001b[0;32m    400\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    401\u001b[0m         )\n\u001b[0;32m    402\u001b[0m         \u001b[1;31m# add attentions (possibly with past_key_value) if we output them\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\carme\\miniconda3\\Lib\\site-packages\\tf_keras\\src\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[1;31m# To get the full stack trace, call:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[1;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m             \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\carme\\miniconda3\\Lib\\site-packages\\tf_keras\\src\\engine\\base_layer.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1138\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1139\u001b[0m                 with autocast_variable.enable_auto_cast_variables(\n\u001b[0;32m   1140\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1141\u001b[0m                 ):\n\u001b[1;32m-> 1142\u001b[1;33m                     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1144\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1145\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\carme\\miniconda3\\Lib\\site-packages\\tf_keras\\src\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    154\u001b[0m                 \u001b[0mnew_e\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mnew_e\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m             \u001b[1;32mdel\u001b[0m \u001b[0msignature\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 158\u001b[1;33m             \u001b[1;32mdel\u001b[0m \u001b[0mbound_signature\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\carme\\miniconda3\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_tf_roberta.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, hidden_states, input_tensor, training)\u001b[0m\n\u001b[0;32m    348\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    349\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    350\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 351\u001b[1;33m         \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhidden_states\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    352\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    353\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\carme\\miniconda3\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 155\u001b[1;33m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\carme\\miniconda3\\Lib\\site-packages\\tensorflow\\python\\framework\\override_binary_operator.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;31m# TODO(b/178860388): Figure out why binary_op_wrapper and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m         \u001b[1;31m#   r_binary_op_wrapper use different force_same_dtype values.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmaybe_promote_tensors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m         \u001b[1;31m# Even if dispatching the op failed, the RHS may be a tensor aware\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m         \u001b[1;31m# object that can implement the operator with knowledge of itself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m         \u001b[1;31m# and the tensor.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\carme\\miniconda3\\Lib\\site-packages\\tensorflow\\python\\ops\\tensor_math_operator_overrides.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_add_dispatch_factory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m   \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_add_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\carme\\miniconda3\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 155\u001b[1;33m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\carme\\miniconda3\\Lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1258\u001b[0m       \u001b[1;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1260\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1261\u001b[1;33m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1262\u001b[0m         \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1263\u001b[0m         \u001b[1;31m# TypeError, when given unexpected types.  So we need to catch both.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1264\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop_dispatch_handler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\carme\\miniconda3\\Lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m   1713\u001b[0m     \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype_hint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase_dtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"y\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1714\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1715\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1716\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1717\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_v2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\carme\\miniconda3\\Lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m    507\u001b[0m         _ctx, \"AddV2\", name, x, y)\n\u001b[0;32m    508\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    509\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    510\u001b[0m       \u001b[0m_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 511\u001b[1;33m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    512\u001b[0m       \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    513\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    514\u001b[0m       return add_v2_eager_fallback(\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "pred = []\n",
    "for v, y in zip(val_texts, val_labels.numpy().tolist()):\n",
    "    print(predict_category(v), \"true: \", y)\n",
    "    pred.append(predict_category(v))\n",
    "\n",
    "confusion = confusion_matrix(val_labels, pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.set(font_scale=1.2)\n",
    "sns.heatmap(confusion, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False, square=True,\n",
    "            xticklabels=[\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\"],\n",
    "            yticklabels=[\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\"])\n",
    "plt.xlabel('Vorhergesagt')\n",
    "plt.ylabel('Wahr')\n",
    "plt.title('Konfusionsmatrix')\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
