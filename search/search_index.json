{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Review Overview Generation</p> <p>At Paper Review Aggregator, our mission is to simplify the work of meta-reviewers by providing an AI-driven platform that aggregates and summarizes paper reviews. By reducing manual effort and enhancing decision-making, we aim to streamline the peer-review process for academic research. This application was developed as part of the \"Data Analysis Software Project\" course at Technische Universit\u00e4t Darmstadt, Germany. The project allowed Master\u2019s students to apply their data analysis, software engineering, and machine learning skills to solve real-world problems.</p> <p>Contributions</p> <p>In this project participated Johannes Lemken, Carmen Appelt, Zhijingshui Yang, Philipp Oehler and Jan Werth. We were supervised by Sukannya Purkayastha.</p>"},{"location":"api/attitude_classifier/","title":"Attitude Classification Code Documentation","text":""},{"location":"api/attitude_classifier/#attitude_classifier.model_prediction.attitude_roots_prediction","title":"<code>attitude_roots_prediction(data)</code>","text":"<p>Predicts attitude root categories for given textual data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>A DataFrame containing a 'sentence' column.</p> required <p>Returns:</p> Type Description <p>pd.DataFrame: The input DataFrame with additional columns:           - 'attitude_root_number': The predicted root category index.           - 'attitude_root': The corresponding category label.</p> Source code in <code>attitude_classifier\\model_prediction.py</code> <pre><code>def attitude_roots_prediction(data):\n    \"\"\"\n    Predicts attitude root categories for given textual data.\n\n    Args:\n        data (pd.DataFrame): A DataFrame containing a 'sentence' column.\n\n    Returns:\n        pd.DataFrame: The input DataFrame with additional columns:\n                      - 'attitude_root_number': The predicted root category index.\n                      - 'attitude_root': The corresponding category label.\n    \"\"\"\n    local_path = \"models/attitude_root/\"\n    huggingface_model_path = \"DASP-ROG/AttitudeModel\"\n\n    # Load the tokenizer and model from huggingface if not available locally\n    logger.info(\"Loading tokenizer and model for root category ...\")\n    tokenizer = BertTokenizer.from_pretrained(huggingface_model_path, cache_dir=local_path)\n    model = BertForSequenceClassification.from_pretrained(huggingface_model_path, num_labels=9,\n                                                          cache_dir=local_path)\n    model.eval()\n\n    logger.info(\"Predicting root category\")\n    data['attitude_root_number'] = data['sentence'].apply(lambda x: predict_root_category(x, model, tokenizer))\n    logger.info(f\"Root category prediction done.\")\n\n    label_mapping = {\n        0: 'None',\n        1: 'Substance',\n        2: 'Originality',\n        3: 'Clarity',\n        4: 'Soundness-correctness',\n        5: 'Motivation-impact',\n        6: 'Meaningful-comparison',\n        7: 'Replicabilitye',\n        8: 'Other'\n    }\n\n    data['attitude_root'] = data['attitude_root_number'].map(label_mapping)\n    data = data[data['attitude_root'] != 'None']\n\n    return data\n</code></pre>"},{"location":"api/attitude_classifier/#attitude_classifier.model_prediction.combine_roots_and_themes","title":"<code>combine_roots_and_themes(preprocessed_data)</code>","text":"<p>Combines root category predictions with theme category predictions.</p> <p>Parameters:</p> Name Type Description Default <code>preprocessed_data</code> <code>DataFrame</code> <p>A DataFrame containing sentences to be classified.</p> required <p>Returns:</p> Type Description <p>pd.DataFrame: A DataFrame containing:           - 'Attitude_roots': Combined root and theme clusters.           - 'Frequency': Frequency of each cluster occurrence.           - 'Descriptions': Descriptions of each category.           - 'Comments': Aggregated comments associated with each category.</p> Source code in <code>attitude_classifier\\model_prediction.py</code> <pre><code>def combine_roots_and_themes(preprocessed_data):\n    \"\"\"\n    Combines root category predictions with theme category predictions.\n\n    Args:\n        preprocessed_data (pd.DataFrame): A DataFrame containing sentences to be classified.\n\n    Returns:\n        pd.DataFrame: A DataFrame containing:\n                      - 'Attitude_roots': Combined root and theme clusters.\n                      - 'Frequency': Frequency of each cluster occurrence.\n                      - 'Descriptions': Descriptions of each category.\n                      - 'Comments': Aggregated comments associated with each category.\n    \"\"\"\n    df = attitude_roots_prediction(preprocessed_data)\n\n    # Load the pretrained model and tokenizer\n    local_path = \"models/attitude_theme/\"\n    huggingface_model_path = \"DASP-ROG/ThemeModel\"\n    tokenizer = BertTokenizer.from_pretrained(huggingface_model_path, cache_dir=local_path)\n    model = BertForSequenceClassification.from_pretrained(huggingface_model_path, num_labels=11, cache_dir=local_path,\n                                                          problem_type=\"multi_label_classification\")\n\n    model.eval()\n    # theme prediction\n    logger.info(\"Predicting attitude theme category\")\n    df['attitude_themes'] = df['sentence'].apply(lambda x: predict_theme_category(x, model, tokenizer))  # attitude_themes_prediction\n    logger.info(\"Attitude theme prediction done\")\n\n    # Apply the function to create clusters\n    df.loc[:, \"clusters\"] = df.apply(create_clusters, axis=1)\n    df = df.explode(\"clusters\", ignore_index=True)\n\n    # Count distinct authors\n    distinct_authors_count = df['author'].nunique()\n    # Group by 'author' and 'clusters', and aggregate the sentences into a list\n    aggregated_df = df.groupby(['clusters', 'author'])['sentence'].apply(list).reset_index()\n    final_df = aggregated_df.groupby('clusters').agg(\n        comments=('sentence', lambda x: [[author, sentences] for author, sentences in zip(aggregated_df['author'], x)])\n    ).reset_index()\n    final_df['Frequency'] = final_df['comments'].apply(len) / distinct_authors_count\n    final_df['Descriptions'] = 'none'\n    final_df = final_df.rename(columns={'comments': 'Comments', 'clusters': 'Attitude_roots'})\n    final_df = final_df[['Attitude_roots', 'Frequency', 'Descriptions', 'Comments']]\n    final_df = final_df.sort_values(by='Frequency', ascending=False)\n\n    current_path = os.path.dirname(os.path.abspath(__file__))\n    desc = pd.read_csv(os.path.join(current_path, 'attitudes_desc.csv'))\n\n    merged_df = pd.merge(final_df, desc, on=['Attitude_roots'],\n                         how='left')  # todo:what happens if attitude + theme combi is not known\n    merged_df.rename(columns={'Descriptions_y': 'Descriptions'}, inplace=True)\n\n    # Drop Descriptions_x column\n    merged_df.drop(columns=['Descriptions_x'], inplace=True)\n\n    # generate description if there is no corresponding description\n    if merged_df['Descriptions'].isna().any():\n        logger.info('New attitude clusters appear, generating description...')\n        # Iterate through the rows and generate description for missing descriptions\n        for index, row in merged_df.iterrows():\n            if pd.isna(row['Descriptions']):\n                attitude_root = row['Attitude_roots']\n                logger.info(f'New attitude clusters appear: {attitude_root}')\n                # Generate descriptions if it's missing\n                comments = row['Comments']  \n                input_texts = extract_sentences(comments)\n                logger.info(f'input sentences: {input_texts}')\n                best_sentence = get_most_representative_sentence(input_texts)\n                logger.info(f'desc: {best_sentence}')\n\n                # Set the best sentence as the description\n                merged_df.at[index, 'Descriptions'] = best_sentence\n\n                # Update the corresponding entry in desc DataFrame with the new attitude_root and description\n                new_row = pd.DataFrame({'Attitude_roots': [attitude_root], 'Descriptions': [best_sentence]})\n                desc = pd.concat([desc, new_row], ignore_index=True)\n        # Save the updated desc DataFrame back to CSV\n        logger.info('New attitude clusters appear, update attitudes_desc.csv')\n        desc.to_csv(os.path.join(current_path, 'attitudes_desc.csv'), index=False)\n\n    # change order as expected from frontend\n    merged_df = merged_df[['Attitude_roots', 'Frequency', 'Descriptions', 'Comments']]\n    return merged_df\n</code></pre>"},{"location":"api/attitude_classifier/#attitude_classifier.model_prediction.create_clusters","title":"<code>create_clusters(row)</code>","text":"<p>Combines root category with theme labels to create clusters.</p> <p>Parameters:</p> Name Type Description Default <code>row</code> <code>Series</code> <p>A row from a DataFrame containing 'attitude_root' and 'attitude_themes'.</p> required <p>Returns:</p> Type Description <p>list[str]: A list of combined root and theme clusters.</p> Source code in <code>attitude_classifier\\model_prediction.py</code> <pre><code>def create_clusters(row):\n    \"\"\"\n    Combines root category with theme labels to create clusters.\n\n    Args:\n        row (pd.Series): A row from a DataFrame containing 'attitude_root' and 'attitude_themes'.\n\n    Returns:\n        list[str]: A list of combined root and theme clusters.\n    \"\"\"\n    # Extract root and themes\n    root = row[\"attitude_root\"]\n    themes = row[\"attitude_themes\"]\n\n    # Combine root with each theme\n    clusters = [f\"{root}({theme})\" for theme in themes]\n    return clusters\n</code></pre>"},{"location":"api/attitude_classifier/#attitude_classifier.model_prediction.predict_root_category","title":"<code>predict_root_category(text, model, tokenizer)</code>","text":"<p>Predicts the root category of a given text using a pre-trained BERT model.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The input text for classification.</p> required <code>model</code> <code>BertForSequenceClassification</code> <p>Pre-trained BERT model for root category classification.</p> required <code>tokenizer</code> <code>BertTokenizer</code> <p>Tokenizer corresponding to the BERT model.</p> required <p>Returns:</p> Name Type Description <code>int</code> <p>The predicted category index.</p> Source code in <code>attitude_classifier\\model_prediction.py</code> <pre><code>def predict_root_category(text, model, tokenizer):\n    \"\"\"\n    Predicts the root category of a given text using a pre-trained BERT model.\n\n    Args:\n        text (str): The input text for classification.\n        model (BertForSequenceClassification): Pre-trained BERT model for root category classification.\n        tokenizer (BertTokenizer): Tokenizer corresponding to the BERT model.\n\n    Returns:\n        int: The predicted category index.\n    \"\"\"\n\n    predict_input = tokenizer.encode(\n        text,\n        truncation=True,\n        padding=True,\n        return_tensors=\"pt\"\n    )\n\n    try:\n        with torch.no_grad():\n            outputs = model(predict_input)\n            logits = outputs.logits\n        prediction_value = torch.argmax(logits, dim=1).item()\n    except Exception as e:\n        logger.error(e)\n\n    return prediction_value\n</code></pre>"},{"location":"api/attitude_classifier/#attitude_classifier.model_prediction.predict_theme_category","title":"<code>predict_theme_category(text, model, tokenizer)</code>","text":"<p>Predicts the theme category of a given text using a pre-trained BERT model.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The input text for classification.</p> required <code>model</code> <code>BertForSequenceClassification</code> <p>Pre-trained BERT model for theme classification.</p> required <code>tokenizer</code> <code>BertTokenizer</code> <p>Tokenizer corresponding to the BERT model.</p> required <p>Returns:</p> Type Description <p>list[str]: A list of predicted theme labels.</p> Source code in <code>attitude_classifier\\model_prediction.py</code> <pre><code>def predict_theme_category(text, model, tokenizer):\n    \"\"\"\n    Predicts the theme category of a given text using a pre-trained BERT model.\n\n    Args:\n        text (str): The input text for classification.\n        model (BertForSequenceClassification): Pre-trained BERT model for theme classification.\n        tokenizer (BertTokenizer): Tokenizer corresponding to the BERT model.\n\n    Returns:\n        list[str]: A list of predicted theme labels.\n    \"\"\"\n    threshold = 0.5\n\n    # Tokenize the input text using tokenizer (handles padding, truncation, etc.)\n    inputs = tokenizer(text, truncation=True, padding=\"max_length\", max_length=128, return_tensors=\"pt\")\n\n    # Run the model for prediction\n    with torch.no_grad():  # Disable gradient calculation during inference\n        outputs = model(**inputs)\n        logits = outputs.logits\n\n    # Apply sigmoid to logits (since it's multi-label classification)\n    sigmoid = Sigmoid()\n    probabilities = sigmoid(logits)\n\n    # Apply threshold to get binary predictions (0 or 1)\n    predictions = (probabilities &gt; threshold).int()\n    predictions = predictions.squeeze().tolist()\n\n    labels = ['ANA', 'BIB', 'DAT', 'EXP', 'INT', 'MET', 'OAL', 'PDI', 'RES', 'RWK', 'TNF']\n    # Find all indices of 1\n    indices = [i for i, value in enumerate(predictions) if value == 1]\n    true_labels = [labels[i] for i in indices]\n\n    # Return the binary predictions (as a list)\n    return true_labels\n</code></pre>"},{"location":"api/attitude_classifier/#attitude_classifier.main.predict","title":"<code>predict(request)</code>  <code>async</code>","text":"<p>Endpoint for classifying attitudes based on input data.</p> <p>This function receives a request containing sentence data, processes it using  the <code>combine_roots_and_themes</code> function, and returns a structured classification  result.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>RawInput</code> <p>The input request containing a <code>data</code> attribute with                  sentence data in JSON format.</p> required <p>Returns:</p> Type Description <p>list[dict]: A list of dictionaries where each dictionary represents a          processed sentence with classification results.</p> <p>Raises:</p> Type Description <code>HTTPException</code> <p>If an error occurs during processing, a 500 error is returned            with the error details.</p> Source code in <code>attitude_classifier\\main.py</code> <pre><code>@app.post(\"/classify_attitudes\")\nasync def predict(request: RawInput):\n    \"\"\"\n    Endpoint for classifying attitudes based on input data.\n\n    This function receives a request containing sentence data, processes it using \n    the `combine_roots_and_themes` function, and returns a structured classification \n    result.\n\n    Args:\n        request (RawInput): The input request containing a `data` attribute with \n                            sentence data in JSON format.\n\n    Returns:\n        list[dict]: A list of dictionaries where each dictionary represents a \n                    processed sentence with classification results.\n\n    Raises:\n        HTTPException: If an error occurs during processing, a 500 error is returned\n                       with the error details.\n    \"\"\"\n    try:\n        data = request.data\n        df_sentences = pd.DataFrame(data)\n\n        logger.info(\"Predicting attitudes...\")\n        result = combine_roots_and_themes(df_sentences)\n        # TODO: description\n        result = result.fillna('none')\n        # Convert dataframes to JSON for response\n        result = result.to_dict(orient=\"records\")\n\n        return result\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=f\"Error processing data: {str(e)}\")\n</code></pre>"},{"location":"api/backend/","title":"Backend Code Documentation","text":""},{"location":"api/backend/#backend.data_processing.process_file","title":"<code>process_file(reviews_json)</code>","text":"<p>Processes a list of reviews. This means that each review will be first sentencized and then be given to nlp models to classify the sentences. :param reviews: list of reviews (from frontend) :return: input for request and attitude models and overview for frontend visualization</p> Source code in <code>backend\\data_processing.py</code> <pre><code>def process_file(reviews_json: list[dict]) -&gt; (pd.DataFrame, pd.DataFrame):\n    \"\"\"\n    Processes a list of reviews. This means that each review will be first sentencized and then be given to nlp models\n    to classify the sentences.\n    :param reviews: list of reviews (from frontend)\n    :return: input for request and attitude models and overview for frontend visualization\n    \"\"\"\n\n    reviews = [Review.from_dict(review_dict) for review_dict in reviews_json]\n    text_processer = TextProcessor(reviews=reviews)\n    df_sentences, df_overview = text_processer.process()\n\n    return df_overview, df_sentences\n</code></pre>"},{"location":"api/backend/#backend.text_processing.text_processor.TextProcessor","title":"<code>TextProcessor</code>","text":"<p>A class for processing and analyzing textual review data. This includes preprocessing, segmentation, and generating statistical overviews.</p> Source code in <code>backend\\text_processing\\text_processor.py</code> <pre><code>class TextProcessor:\n    \"\"\"\n    A class for processing and analyzing textual review data. This includes preprocessing, segmentation,\n    and generating statistical overviews.\n    \"\"\"\n    def __init__(self, reviews: list[Review]):\n        \"\"\"\n        Initializes the TextProcessor with a list of Review objects.\n\n        :param reviews: List of Review objects to be processed.\n        \"\"\"\n        self.reviews = reviews\n        self.config = {\"punct_chars\": ['\\n']}\n        self.nlp = spacy.load(\"en_core_web_sm\", exclude=[\"parser\"])\n        self.nlp.add_pipe(\"sentencizer\")\n        self.nlp.add_pipe(\"split_only_at_linebreaks\", before=\"sentencizer\")\n        self.keys_to_extract = [\"summary\", \"strengths\", \"weaknesses\", \"questions\"]\n        self.overview_keys = [\"rating\", \"soundness\", \"presentation\", \"contribution\"]\n\n        self.abbreviations = [\n            \"e.g.\", \"i.e.\", \"Dr.\", \"Mr.\", \"Mrs.\", \"Prof.\", \"etc.\", \"Fig.\", \"Tab.\", \"al.\", \"et al.\", \"vol.\", \"no.\",\n            \"pp.\", \"Ch.\", \"Eq.\", \"Sec.\", \"Ref.\", \"Inc.\", \"Corp.\", \"Ltd.\", \"Co.\", \"Vs.\", \"approx.\", \"esp.\", \"incl.\",\n            \"viz.\", \"resp.\", \"cf.\", \"avg.\", \"max.\", \"min.\", \"std.\", \"dev.\", \"var.\", \"mean.\", \"median.\", \"p-value\",\n            \"s.d.\", \"a.k.a.\", \"b.c.\", \"a.d.\", \"m.a.\", \"n.d.\", \"ca.\"]\n\n    @staticmethod\n    def _protect_abbreviation(abbreviations: list[str], text: str) -&gt; str:\n        \"\"\"\n        Replaces periods in abbreviations with a placeholder to avoid unintended sentence splitting.\n\n        :param abbreviations: List of abbreviations to protect.\n        :param text: Text in which abbreviations should be protected.\n        :return: Processed text with protected abbreviations.\n        \"\"\"\n        for abbr in abbreviations:\n            protected = abbr.replace(\".\", \"&lt;DOT&gt;\")\n            text = text.replace(abbr, protected)\n        return text\n\n    def _preprocess_text(self, reviews: list[Review]) -&gt; list[Review]:\n        \"\"\"\n        Cleans and normalizes text fields within the review objects.\n\n        :param reviews: List of Review objects.\n        :return: List of processed Review objects with cleaned text fields.\n        \"\"\"\n        for review in reviews:\n            for key in self.keys_to_extract:\n                text = getattr(review, key, \"\")\n                text = text.strip()\n                text = \"\\n\".join(\" \".join(line.split()) for line in text.splitlines())\n                text = text.encode(\"utf-8\", \"ignore\").decode(\"utf-8\")\n                text = re.sub(r\"http\\S+\", \"\", text)\n                text = re.sub(r\"\\S+@\\S+\", \"\", text)\n                text = text.replace(\";\", \".\")\n                text = text.replace(\"*\", \"\")\n                text = re.sub(r\"(?&lt;![.!?])\\n\", \" \", text)\n                text = self._protect_abbreviation(self.abbreviations, text)\n                text = text.strip()\n                text = text + \"\\n\"\n                setattr(review, key, text)\n        return reviews\n\n    def _segment_content(self, reviews: list[Review]) -&gt; (list[Review], pd.DataFrame):\n        \"\"\"\n        Segments text content into sentences using SpaCy and stores them in a DataFrame.\n\n        :param reviews: List of Review objects.\n        :return: Tuple containing the updated list of reviews and a DataFrame with segmented sentences.\n        \"\"\"\n        sent_df = []\n        for review in tqdm(reviews, desc=\"Segmenting content\"):\n            for key in self.keys_to_extract:\n                text = getattr(review, key, \"\")\n                doc = self.nlp(text)\n                sentences = [sent.text.replace(\"&lt;DOT&gt;\", \".\") for sent in doc.sents]\n                sentences = [\" \".join(sent.split()) for sent in sentences]\n                sentences = [sent.lstrip(\"-\").strip() for sent in sentences]\n                review.sentences = sentences\n                tmp = pd.DataFrame({\"author\": review.reviewer, \"tag\": key, \"sentence\": sentences})\n                sent_df.append(tmp)\n\n        df = pd.concat(sent_df, ignore_index=True)\n        return reviews, df\n\n    def _get_overview(self, reviews: list[Review]) -&gt; pd.DataFrame:\n        \"\"\"\n        Computes an overview of review scores.\n\n        :param reviews: List of Review objects.\n        :return: DataFrame summarizing average scores per category.\n        \"\"\"\n        df_list = []\n\n        for key in self.overview_keys:\n            score_list = []\n            total = 0\n            for review in reviews:\n                reviewer = review.reviewer\n                tmp = getattr(review, key, \"\")\n                if isinstance(tmp, str):\n                    score = float(tmp.split(\" \")[0].replace(\":\", \"\"))\n                else:\n                    score = float(tmp)\n                total += score\n                list = [reviewer, score]\n                score_list.append(list)\n            avg = total / len(score_list)\n            df_list.append({\"Category\": key.title(), \"Avg_Score\": avg, \"Individual_scores\": score_list})\n\n        return pd.DataFrame(df_list)\n\n    def process(self) -&gt; (pd.DataFrame, pd.DataFrame):\n        \"\"\"\n        Processes and segments the reviews. Creates a sentences dataframe aan overview dataframe of the scores .\n        :return: Sentences dataframe and overview dataframe\n        \"\"\"\n        logger.info(f\"Processing reviews\")\n        preprocessed_reviews = self._preprocess_text(self.reviews)\n        logger.info(\"Segmenting content\")\n        reviews, df_sentences = self._segment_content(preprocessed_reviews)\n        logger.info(\"Got reviews and df_sentences. Creating overview.\")\n        df_overview = self._get_overview(preprocessed_reviews)\n\n        logger.success(f\"Processing reviews done\")\n        return df_sentences, df_overview\n</code></pre>"},{"location":"api/backend/#backend.text_processing.text_processor.TextProcessor.__init__","title":"<code>__init__(reviews)</code>","text":"<p>Initializes the TextProcessor with a list of Review objects.</p> <p>:param reviews: List of Review objects to be processed.</p> Source code in <code>backend\\text_processing\\text_processor.py</code> <pre><code>def __init__(self, reviews: list[Review]):\n    \"\"\"\n    Initializes the TextProcessor with a list of Review objects.\n\n    :param reviews: List of Review objects to be processed.\n    \"\"\"\n    self.reviews = reviews\n    self.config = {\"punct_chars\": ['\\n']}\n    self.nlp = spacy.load(\"en_core_web_sm\", exclude=[\"parser\"])\n    self.nlp.add_pipe(\"sentencizer\")\n    self.nlp.add_pipe(\"split_only_at_linebreaks\", before=\"sentencizer\")\n    self.keys_to_extract = [\"summary\", \"strengths\", \"weaknesses\", \"questions\"]\n    self.overview_keys = [\"rating\", \"soundness\", \"presentation\", \"contribution\"]\n\n    self.abbreviations = [\n        \"e.g.\", \"i.e.\", \"Dr.\", \"Mr.\", \"Mrs.\", \"Prof.\", \"etc.\", \"Fig.\", \"Tab.\", \"al.\", \"et al.\", \"vol.\", \"no.\",\n        \"pp.\", \"Ch.\", \"Eq.\", \"Sec.\", \"Ref.\", \"Inc.\", \"Corp.\", \"Ltd.\", \"Co.\", \"Vs.\", \"approx.\", \"esp.\", \"incl.\",\n        \"viz.\", \"resp.\", \"cf.\", \"avg.\", \"max.\", \"min.\", \"std.\", \"dev.\", \"var.\", \"mean.\", \"median.\", \"p-value\",\n        \"s.d.\", \"a.k.a.\", \"b.c.\", \"a.d.\", \"m.a.\", \"n.d.\", \"ca.\"]\n</code></pre>"},{"location":"api/backend/#backend.text_processing.text_processor.TextProcessor._get_overview","title":"<code>_get_overview(reviews)</code>","text":"<p>Computes an overview of review scores.</p> <p>:param reviews: List of Review objects. :return: DataFrame summarizing average scores per category.</p> Source code in <code>backend\\text_processing\\text_processor.py</code> <pre><code>def _get_overview(self, reviews: list[Review]) -&gt; pd.DataFrame:\n    \"\"\"\n    Computes an overview of review scores.\n\n    :param reviews: List of Review objects.\n    :return: DataFrame summarizing average scores per category.\n    \"\"\"\n    df_list = []\n\n    for key in self.overview_keys:\n        score_list = []\n        total = 0\n        for review in reviews:\n            reviewer = review.reviewer\n            tmp = getattr(review, key, \"\")\n            if isinstance(tmp, str):\n                score = float(tmp.split(\" \")[0].replace(\":\", \"\"))\n            else:\n                score = float(tmp)\n            total += score\n            list = [reviewer, score]\n            score_list.append(list)\n        avg = total / len(score_list)\n        df_list.append({\"Category\": key.title(), \"Avg_Score\": avg, \"Individual_scores\": score_list})\n\n    return pd.DataFrame(df_list)\n</code></pre>"},{"location":"api/backend/#backend.text_processing.text_processor.TextProcessor._preprocess_text","title":"<code>_preprocess_text(reviews)</code>","text":"<p>Cleans and normalizes text fields within the review objects.</p> <p>:param reviews: List of Review objects. :return: List of processed Review objects with cleaned text fields.</p> Source code in <code>backend\\text_processing\\text_processor.py</code> <pre><code>def _preprocess_text(self, reviews: list[Review]) -&gt; list[Review]:\n    \"\"\"\n    Cleans and normalizes text fields within the review objects.\n\n    :param reviews: List of Review objects.\n    :return: List of processed Review objects with cleaned text fields.\n    \"\"\"\n    for review in reviews:\n        for key in self.keys_to_extract:\n            text = getattr(review, key, \"\")\n            text = text.strip()\n            text = \"\\n\".join(\" \".join(line.split()) for line in text.splitlines())\n            text = text.encode(\"utf-8\", \"ignore\").decode(\"utf-8\")\n            text = re.sub(r\"http\\S+\", \"\", text)\n            text = re.sub(r\"\\S+@\\S+\", \"\", text)\n            text = text.replace(\";\", \".\")\n            text = text.replace(\"*\", \"\")\n            text = re.sub(r\"(?&lt;![.!?])\\n\", \" \", text)\n            text = self._protect_abbreviation(self.abbreviations, text)\n            text = text.strip()\n            text = text + \"\\n\"\n            setattr(review, key, text)\n    return reviews\n</code></pre>"},{"location":"api/backend/#backend.text_processing.text_processor.TextProcessor._protect_abbreviation","title":"<code>_protect_abbreviation(abbreviations, text)</code>  <code>staticmethod</code>","text":"<p>Replaces periods in abbreviations with a placeholder to avoid unintended sentence splitting.</p> <p>:param abbreviations: List of abbreviations to protect. :param text: Text in which abbreviations should be protected. :return: Processed text with protected abbreviations.</p> Source code in <code>backend\\text_processing\\text_processor.py</code> <pre><code>@staticmethod\ndef _protect_abbreviation(abbreviations: list[str], text: str) -&gt; str:\n    \"\"\"\n    Replaces periods in abbreviations with a placeholder to avoid unintended sentence splitting.\n\n    :param abbreviations: List of abbreviations to protect.\n    :param text: Text in which abbreviations should be protected.\n    :return: Processed text with protected abbreviations.\n    \"\"\"\n    for abbr in abbreviations:\n        protected = abbr.replace(\".\", \"&lt;DOT&gt;\")\n        text = text.replace(abbr, protected)\n    return text\n</code></pre>"},{"location":"api/backend/#backend.text_processing.text_processor.TextProcessor._segment_content","title":"<code>_segment_content(reviews)</code>","text":"<p>Segments text content into sentences using SpaCy and stores them in a DataFrame.</p> <p>:param reviews: List of Review objects. :return: Tuple containing the updated list of reviews and a DataFrame with segmented sentences.</p> Source code in <code>backend\\text_processing\\text_processor.py</code> <pre><code>def _segment_content(self, reviews: list[Review]) -&gt; (list[Review], pd.DataFrame):\n    \"\"\"\n    Segments text content into sentences using SpaCy and stores them in a DataFrame.\n\n    :param reviews: List of Review objects.\n    :return: Tuple containing the updated list of reviews and a DataFrame with segmented sentences.\n    \"\"\"\n    sent_df = []\n    for review in tqdm(reviews, desc=\"Segmenting content\"):\n        for key in self.keys_to_extract:\n            text = getattr(review, key, \"\")\n            doc = self.nlp(text)\n            sentences = [sent.text.replace(\"&lt;DOT&gt;\", \".\") for sent in doc.sents]\n            sentences = [\" \".join(sent.split()) for sent in sentences]\n            sentences = [sent.lstrip(\"-\").strip() for sent in sentences]\n            review.sentences = sentences\n            tmp = pd.DataFrame({\"author\": review.reviewer, \"tag\": key, \"sentence\": sentences})\n            sent_df.append(tmp)\n\n    df = pd.concat(sent_df, ignore_index=True)\n    return reviews, df\n</code></pre>"},{"location":"api/backend/#backend.text_processing.text_processor.TextProcessor.process","title":"<code>process()</code>","text":"<p>Processes and segments the reviews. Creates a sentences dataframe aan overview dataframe of the scores . :return: Sentences dataframe and overview dataframe</p> Source code in <code>backend\\text_processing\\text_processor.py</code> <pre><code>def process(self) -&gt; (pd.DataFrame, pd.DataFrame):\n    \"\"\"\n    Processes and segments the reviews. Creates a sentences dataframe aan overview dataframe of the scores .\n    :return: Sentences dataframe and overview dataframe\n    \"\"\"\n    logger.info(f\"Processing reviews\")\n    preprocessed_reviews = self._preprocess_text(self.reviews)\n    logger.info(\"Segmenting content\")\n    reviews, df_sentences = self._segment_content(preprocessed_reviews)\n    logger.info(\"Got reviews and df_sentences. Creating overview.\")\n    df_overview = self._get_overview(preprocessed_reviews)\n\n    logger.success(f\"Processing reviews done\")\n    return df_sentences, df_overview\n</code></pre>"},{"location":"api/backend/#backend.text_processing.text_processor.split_only_at_linebreaks","title":"<code>split_only_at_linebreaks(doc)</code>","text":"<p>Custom SpaCy component to split sentences only at line breaks.</p> <p>:param doc: SpaCy Doc object :return: Processed Doc object with sentence boundaries set at line breaks only.</p> Source code in <code>backend\\text_processing\\text_processor.py</code> <pre><code>@Language.component(\"split_only_at_linebreaks\")\ndef split_only_at_linebreaks(doc: Doc) -&gt; Doc:\n    \"\"\"\n    Custom SpaCy component to split sentences only at line breaks.\n\n    :param doc: SpaCy Doc object\n    :return: Processed Doc object with sentence boundaries set at line breaks only.\n    \"\"\" \n    for token in doc:\n        token.is_sent_start = False\n\n    for i, token in enumerate(doc[:-1]):\n        if token.text == \"\\n\" or token.text == \"\\n \":\n            if i + 1 &lt; len(doc):\n                doc[i + 1].is_sent_start = True\n    return doc\n</code></pre>"},{"location":"api/backend/#backend.text_processing.models.Review","title":"<code>Review</code>  <code>dataclass</code>","text":"Source code in <code>backend\\text_processing\\models.py</code> <pre><code>@dataclass\nclass Review:\n    reviewer: str\n    rating: str | int\n    soundness: str | int\n    presentation: str | int\n    contribution: str | int\n    summary: str\n    strengths: str\n    weaknesses: str\n    questions: str\n    confidence: str\n    forum: str | None = None\n    id: str | None = None\n    venue: str | None = None\n    year: str | None = None\n    type: str | None = None\n    date: str | int | None = None\n    flag_for_ethic_review: str | None = None\n    content: str | None = None  # All the text in a single string\n    sentences: list[str] | None = None  # Output of spacy sentencizer =&gt; This is used for the model classification\n\n    @classmethod\n    def from_dict(cls, data: dict):\n        \"\"\"\n        Creates an instance of the `Review` class from a dictionary.\n\n        Args:\n            data (dict): A dictionary containing review data, with keys matching the attributes of the class.\n\n        Returns:\n            Review: An instance of the `Review` class populated with the given data.\n        \"\"\"\n        # Collect field names from the dataclass\n        field_names = {field.name for field in fields(cls)}\n        # Filter the data dict to only include keys that match the class fields\n        init_args = {key: data.get(key) for key in field_names if key in data}\n        # Create an instance of Review with the filtered data\n        return cls(**init_args)\n</code></pre>"},{"location":"api/backend/#backend.text_processing.models.Review.from_dict","title":"<code>from_dict(data)</code>  <code>classmethod</code>","text":"<p>Creates an instance of the <code>Review</code> class from a dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict</code> <p>A dictionary containing review data, with keys matching the attributes of the class.</p> required <p>Returns:</p> Name Type Description <code>Review</code> <p>An instance of the <code>Review</code> class populated with the given data.</p> Source code in <code>backend\\text_processing\\models.py</code> <pre><code>@classmethod\ndef from_dict(cls, data: dict):\n    \"\"\"\n    Creates an instance of the `Review` class from a dictionary.\n\n    Args:\n        data (dict): A dictionary containing review data, with keys matching the attributes of the class.\n\n    Returns:\n        Review: An instance of the `Review` class populated with the given data.\n    \"\"\"\n    # Collect field names from the dataclass\n    field_names = {field.name for field in fields(cls)}\n    # Filter the data dict to only include keys that match the class fields\n    init_args = {key: data.get(key) for key in field_names if key in data}\n    # Create an instance of Review with the filtered data\n    return cls(**init_args)\n</code></pre>"},{"location":"api/frontend/","title":"Frontend Code Documentation","text":"<p>Home Page Module</p> <p>This module defines the <code>home_page</code> function, which renders the Home page of the Paper Review Aggregator application. It applies custom CSS styles and displays home content, teasers, and contact information using shared methods.</p> <p>Landing Page Module</p> <p>This module defines the <code>landing_page</code> function, which renders the Landing page of the Paper Review Aggregator application. It handles user input for providing either a URL or uploading a file containing paper reviews. The module manages user authentication with OpenReview, processes uploaded files, and facilitates navigation to the main dashboard for review aggregation.</p> <p>Main Page Module</p> <p>This module defines the <code>main_page</code> function, which renders the main dashboard of the Paper Review Aggregator application. It handles the retrieval and display of aggregated review data, including overview, request information, attitude roots, and summary. The module interacts with backend APIs to process review data and utilizes various submodules to present the information in an organized and visually appealing manner.</p> <p>Paper Review Generator Streamlit Application</p> <p>This application provides a user interface for aggregating and generating summaries of paper reviews. It includes navigation between different pages such as Home, Review Aggregation, Contact, and About. The application is styled using custom CSS and incorporates logos and other UI elements to enhance user experience.</p> <p>About Page Module</p> <p>This module defines the <code>about_page</code> function, which renders the About page of the Paper Review Aggregator application. It applies custom CSS styles and displays about content along with contact information using shared methods.</p> <p>Contact Module</p> <p>This module defines the function to display contact information within the Paper Review Aggregator application. It utilizes shared methods and imports functionalities from other modules to render contact details on the interface.</p>"},{"location":"api/frontend/#frontend.home_page.home_page","title":"<code>home_page(custom_css)</code>","text":"<p>Render the Home page with custom styling and content.</p> <p>This function applies custom CSS styles to the Streamlit application and displays the home content, teaser section, and contact information by invoking corresponding functions from the <code>home_content</code> and <code>contact_info</code> modules.</p> <p>Parameters:</p> Name Type Description Default <code>custom_css</code> <code>str</code> <p>A string containing CSS styles to customize the appearance               of the Streamlit application.</p> required Source code in <code>frontend\\home_page.py</code> <pre><code>def home_page(custom_css):\n    \"\"\"\n    Render the Home page with custom styling and content.\n\n    This function applies custom CSS styles to the Streamlit application and displays\n    the home content, teaser section, and contact information by invoking corresponding\n    functions from the `home_content` and `contact_info` modules.\n\n    Parameters:\n        custom_css (str): A string containing CSS styles to customize the appearance\n                          of the Streamlit application.\n    \"\"\"\n\n    # Apply custom CSS Styles\n    st.markdown(custom_css, unsafe_allow_html=True)\n\n    # Show home details\n    use_default_container(modules.home_content.show_home_content)\n\n    # Show teaser\n    use_default_container(modules.home_content.show_home_teaser)\n\n    # Show contact details\n    use_default_container(modules.contact_info.show_contact_info)\n</code></pre>"},{"location":"api/frontend/#frontend.landing_page.extract_paper_id","title":"<code>extract_paper_id(url)</code>","text":"<p>Extracts the paper ID from a given OpenReview URL.</p> <p>This function parses the provided URL to extract the paper ID, ensuring that the URL is valid and points to an OpenReview forum.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The OpenReview URL from which to extract the paper ID.</p> required <p>Returns:</p> Type Description <p>str or None: The extracted paper ID if successful, None otherwise.</p> Source code in <code>frontend\\landing_page.py</code> <pre><code>def extract_paper_id(url):\n    \"\"\"\n    Extracts the paper ID from a given OpenReview URL.\n\n    This function parses the provided URL to extract the paper ID, ensuring that\n    the URL is valid and points to an OpenReview forum.\n\n    Parameters:\n        url (str): The OpenReview URL from which to extract the paper ID.\n\n    Returns:\n        str or None: The extracted paper ID if successful, None otherwise.\n    \"\"\"\n    parsed_url = urlparse(url)\n    if parsed_url.netloc != 'openreview.net':\n        st.error(\"Provided URL is not from openreview.net.\")\n        return False\n    if parsed_url.path != '/forum':\n        st.error(\"Invalid OpenReview URL path.\")\n        return False\n    query_params = parse_qs(parsed_url.query)\n    paper_id_list = query_params.get('id', [])\n    if paper_id_list:\n        return paper_id_list[0]\n    else:\n        st.error(\"Paper ID not found in URL.\")\n        return False\n</code></pre>"},{"location":"api/frontend/#frontend.landing_page.get_input","title":"<code>get_input()</code>","text":"<p>Retrieves the user's input from the session state.</p> <p>This function fetches the 'reviews' data stored in the Streamlit session state.</p> <p>Returns:</p> Name Type Description <code>str</code> <p>The user's input, either a URL or file data.</p> Source code in <code>frontend\\landing_page.py</code> <pre><code>def get_input():\n    \"\"\"\n    Retrieves the user's input from the session state.\n\n    This function fetches the 'reviews' data stored in the Streamlit session state.\n\n    Returns:\n        str: The user's input, either a URL or file data.\n    \"\"\"\n    reviews = st.session_state.get(\"reviews\")\n    return reviews\n</code></pre>"},{"location":"api/frontend/#frontend.landing_page.landing_page","title":"<code>landing_page(custom_css)</code>","text":"<p>Renders the Landing page with options to provide a URL or upload a file for review aggregation.</p> <p>This function applies custom CSS styles, displays instructions to the user, and provides interactive elements such as file uploaders and login forms for OpenReview. It handles user authentication, processes uploaded files, and navigates to the main dashboard upon successful input validation.</p> <p>Parameters:</p> Name Type Description Default <code>custom_css</code> <code>str</code> <p>A string containing CSS styles to customize the appearance of the               Streamlit application.</p> required Source code in <code>frontend\\landing_page.py</code> <pre><code>def landing_page(custom_css):\n    \"\"\"\n    Renders the Landing page with options to provide a URL or upload a file for review aggregation.\n\n    This function applies custom CSS styles, displays instructions to the user, and provides\n    interactive elements such as file uploaders and login forms for OpenReview. It handles\n    user authentication, processes uploaded files, and navigates to the main dashboard upon\n    successful input validation.\n\n    Parameters:\n        custom_css (str): A string containing CSS styles to customize the appearance of the\n                          Streamlit application.\n    \"\"\"\n\n    def provide_sample_download():\n        \"\"\"\n        Provides a download button for users to download a sample DOCM file template.\n\n        This function loads a sample DOCM file from the local directory and renders a download\n        button using Streamlit. Users can click the button to download the template for\n        formatting their review files.\n        \"\"\"\n\n        # Base path for images\n        base_path = Path(__file__).parent\n\n        # Path to your .docx file\n        file_path = Path(base_path / \"data/review_template.docm\")\n\n        # Load the .docx file into memory\n        with open(file_path, \"rb\") as file:\n            docx_file = file.read()\n\n        # Provide a download button\n        col1, col2 = st.columns([2.4, 1])\n        with col2:\n            st.download_button(\n                label=\"Download Sample DOCM File\",\n                data=docx_file,\n                file_name=\"review.docm\",\n                mime=\"application/vnd.openxmlformats-officedocument.wordprocessingml.document\"\n            )\n\n    def display_show_analysis_button(key):\n        \"\"\"\n        Displays a \"Show Analysis\" button that triggers navigation to the main dashboard.\n\n        Parameters:\n            key (str): A unique key identifier for the Streamlit button to manage state.\n        \"\"\"\n        col1, col2 = st.columns([4.5, 1])\n        with col2:\n            if st.button(\"Show Analysis\", key=key):\n                print(\"pressed show analysis button\")\n                switch_to_main_page()\n\n    def set_active_tab(tab_name):\n        \"\"\"\n        Updates the active tab in the Streamlit session state.\n\n        Parameters:\n            tab_name (str): The name of the tab to set as active.\n        \"\"\"\n        st.session_state['active_tab'] = tab_name\n\n    def show_openreview_login_and_url_tab():\n        st.write(\"To provide the aggregation of your desired paper, we need your OpenReview login credentials and a valid link to the desired reviews that will be aggregated.\")\n        st.write(\"In case you don't have an OpenReview account, you can either create one at openreview.com or upload a file instead (use tab above).\")\n\n        # Session states\n        if 'logged_in' not in st.session_state:\n            st.session_state['logged_in'] = False\n        if 'client' not in st.session_state:\n            st.session_state['client'] = None\n\n        if not st.session_state['logged_in']:\n            col1, col2 = st.columns([1, 2])\n            with col1:\n\n                #Create username and password field and save states in session state equally named\n                st.text_input(\"Username (OpenReview)\", key=\"username\")\n                st.text_input(\"Password (OpenReview)\", key=\"password\", type=\"password\")\n\n            with col2:\n                st.markdown('&lt;div class=\"invisbible-line-small\"&gt;  &lt;/div&gt;', unsafe_allow_html=True)\n                # Button to submit the credentials\n                if st.button(\"Login\"):\n                    try:\n                        username = st.session_state[\"username\"]\n                        password = st.session_state[\"password\"]\n                        client = OpenReviewClient(username=username, password=password)\n                        st.success(\"Login successful!\")\n                        st.session_state[\"logged_in\"] = True\n                        st.session_state[\"client\"] = client\n                        # Rerun the app to update the UI\n                        st.rerun()\n                    except openreview.OpenReviewException as ore:\n                        error_response = ore.args[0] if ore.args else \"An error occurred with OpenReview.\"\n                        if \"Invalid username or password\" in error_response.get('message', ''):\n                            st.error(\"Invalid username or password. Please try again.\")\n                        else:\n                            st.error(\n                                f\"An unexpected error occurred: {error_response.get('message', 'No details available')}\")\n                    except ConnectionError:\n                        st.error(\n                            \"Connection error: Unable to connect to OpenReview. Please check your internet connection.\")\n                    except requests.exceptions.RequestException as e:\n                        if \"Max retries exceeded\" in str(e) or \"Failed to establish a new connection\" in str(e):\n                            st.error(\n                                \"Connection error: Unable to connect to OpenReview. Please check your internet connection.\")\n                        else:\n                            st.error(f\"An unexpected error occurred: {str(e)}\")\n                    except Exception as e:\n                        st.error(\"An unexpected error occurred. Please try again later.\")\n                        st.error(e)\n\n\n        else:\n            client = st.session_state['client']\n            st.success(f\"Welcome {client.client.user['user']['profile']['fullname']}!\")\n            input_url = st.text_input(\"Enter URL to Paper Reviews\", key=\"entered_url\",\n                                      placeholder=\"https://openreview.net/forum?id=XXXXXXXXX\")\n\n            if input_url:\n                paper_id = extract_paper_id(input_url)\n                if paper_id:\n                    st.session_state[\"paper_id\"] = paper_id\n\n                    try:\n                        paper = client.get_reviews_from_id(paper_id)\n                        # TODO: if we pass url or file later in show_analysis, then we don't need to get reviews right now.\n                        st.session_state[\"reviews\"] = paper.reviews\n                        st.success(f'Reviews extracted from paper: \"{paper.title}\"')\n                    except Exception as e:\n                        st.error(e)\n                else:\n                    st.error(\"Invalid OpenReview URL.\")\n            else:\n                st.info(\"Please enter a valid OpenReview URL to proceed.\")\n\n        # Check conditions to display the \"Show Analysis\" button\n        if ('reviews' in st.session_state and st.session_state['reviews'] and st.session_state[\n            'active_tab'] == \"Enter URL\"):\n            display_show_analysis_button(key=\"show_analysis_button_tab1\")\n\n    def show_word_download_and_upload_tab():\n        st.write(\n            \"In case you cannot provide a URL, you can also upload a DOCX file containing all reviews. \"\n            \"To do so, please download a sample file and provide your data in this format.\"\n        )\n        provide_sample_download()  # template download\n\n        try:\n            uploaded_files = st.file_uploader(\n                \"Select a file or drop it here to upload it.\",\n                type=[\"docm\"],\n                accept_multiple_files=True\n            )\n\n            if uploaded_files:\n                st.session_state[\"uploaded_files\"] = uploaded_files\n                num_files = len(uploaded_files)\n                file_word = \"file\" if num_files == 1 else \"files\"\n                st.success(f\"Uploaded {num_files} {file_word} successfully.\")\n\n                for uploaded_file in uploaded_files:\n                    try:\n                        # Process each uploaded file\n                        upload_processor = UploadedFileProcessor([uploaded_file])\n                        reviews = upload_processor.process()\n                        st.session_state[\"reviews\"] = reviews\n\n                        # Additional success feedback\n                        st.info(f\"Processed {uploaded_file.name} successfully.\")\n                    except ValueError as ve:\n                        st.error(f\"File {uploaded_file.name} has invalid content: {ve}\")\n                    except FileNotFoundError:\n                        st.error(f\"File {uploaded_file.name} could not be found.\")\n                    except Exception as e:\n                        st.error(f\"An unexpected error occurred while processing {uploaded_file.name}.\")\n        except Exception as e:\n            st.error(\"An error occurred while uploading files. Please try again.\")\n\n        # Check conditions to display the \"Show Analysis\" button\n        if ('reviews' in st.session_state and st.session_state['reviews'] and\n                st.session_state['active_tab'] == \"Upload file\"):\n            display_show_analysis_button(\"show_analysis_button_tab2\")\n\n    def content():\n        \"\"\"\n        Defines the main content structure of the Landing page.\n\n        This function applies custom CSS, displays instructional text, creates tabs for URL\n        input and file uploads, handles user authentication with OpenReview, processes\n        uploaded files, and provides navigation buttons for initiating analysis.\n        \"\"\"\n\n        # Apply custom CSS\n        st.markdown(custom_css, unsafe_allow_html=True)\n\n        st.title(\"Paper Review Aggregator\")\n\n        st.write(\n            \"You can either provide a link of an OpenReview thread for the desired paper review aggregation (account of OpenReview login credentials required) or you provide us a file containing all reviews to aggregate. In this case you must use our template format.\"\n        )\n\n        # Create tabs\n        # tab1, tab2 = st.tabs([\"Enter URL\", \"Upload file\"])\n\n        tabs = [\"Enter URL\", \"Upload file\"]\n        tab1, tab2 = st.tabs(tabs)\n\n        # Initialize session state for active tab\n        if 'active_tab' not in st.session_state:\n            st.session_state['active_tab'] = tabs[0]\n\n        # Web API\n        with tab1:\n            set_active_tab(\"Enter URL\")\n\n            show_openreview_login_and_url_tab()\n\n        # File Uploader\n        with tab2:\n            set_active_tab(\"Upload file\")\n\n            show_word_download_and_upload_tab()\n\n        # Optionally, provide an informational message when no reviews are available\n        # if not ('reviews' in st.session_state and st.session_state['reviews']):\n        #     st.info(\"Please provide a valid URL or upload a file to proceed to the analysis.\")\n\n        # st.button(\"Go to analysis\", on_click=lambda: switch_to_main_page(skip_validation=True))\n\n    use_default_container(content)\n\n    use_default_container(modules.contact_info.show_contact_info)\n</code></pre>"},{"location":"api/frontend/#frontend.landing_page.switch_to_main_page","title":"<code>switch_to_main_page(skip_validation=False)</code>","text":"<p>Switches the application to the main dashboard page.</p> Source code in <code>frontend\\landing_page.py</code> <pre><code>def switch_to_main_page(skip_validation=False):\n    \"\"\"\n    Switches the application to the main dashboard page.\n\n    \"\"\"\n    st.session_state.page = \"Meta Reviewer Dashboard\"\n    st.rerun() #prevents requirement to press the button twice.\n</code></pre>"},{"location":"api/frontend/#frontend.main_page.get_classification_with_api","title":"<code>get_classification_with_api()</code>","text":"<p>Retrieve classification data by sending review data to the backend API.</p> <p>This function collects review data from the Streamlit session state, sends it to the backend API endpoint for processing, and retrieves the aggregated classification results including overview, request information, attitude roots, and summary.</p> <p>Returns:</p> Name Type Description <code>tuple</code> <p>A tuple containing four pandas DataFrames: - df_overview: Overview of the paper reviews. - df_requests: Classified request information. - df_attitudes: Classified attitude roots. - df_summary: Generated summary of the reviews.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If an error occurs during the API request or data processing.</p> Source code in <code>frontend\\main_page.py</code> <pre><code>def get_classification_with_api():\n    \"\"\"\n    Retrieve classification data by sending review data to the backend API.\n\n    This function collects review data from the Streamlit session state, sends it to the\n    backend API endpoint for processing, and retrieves the aggregated classification\n    results including overview, request information, attitude roots, and summary.\n\n    Returns:\n        tuple: A tuple containing four pandas DataFrames:\n            - df_overview: Overview of the paper reviews.\n            - df_requests: Classified request information.\n            - df_attitudes: Classified attitude roots.\n            - df_summary: Generated summary of the reviews.\n\n    Raises:\n        Exception: If an error occurs during the API request or data processing.\n    \"\"\"\n    try:\n        # Check if there are reviews to analyze\n        if \"reviews\" not in st.session_state:\n            st.error(\"No reviews to analyze\")\n\n        # Prepare the payload by converting review objects to dictionaries\n        payload = [review.__dict__ for review in st.session_state.reviews]\n        # Send a POST request to the backend API for processing\n        # http://backend:8080/process\n        # http://localhost:8080/process\n        backend_url = os.environ.get(\"BACKEND_URL\", \"http://localhost:8080\")\n        response = requests.post(\n            f\"{backend_url}/process\",\n            json={\"data\": payload}\n        )\n\n        if response.status_code == 200:\n            data = response.json()\n            # df_sentences = pd.DataFrame(data[\"df_sentences\"])\n            df_overview = pd.DataFrame(data[\"overview\"])\n            df_requests = pd.DataFrame(data[\"request_response\"])\n            df_attitudes = pd.DataFrame(data[\"attitude_response\"])\n            df_summary = pd.DataFrame(data[\"summary_response\"])\n        else:\n            st.error(f\"Error: {response.text}\")\n\n        # Return all the dataframes\n        return df_overview, df_requests, df_attitudes, df_summary\n\n    except Exception as e:\n        st.error(f\"An error occurred: {e}\")\n        return pd.DataFrame()  # Return an empty DataFrame\n</code></pre>"},{"location":"api/frontend/#frontend.main_page.main_page","title":"<code>main_page(custom_css)</code>","text":"<p>Render the Main Page with aggregated review data and interactive visualizations.</p> <p>This function applies custom CSS styles, retrieves and manages aggregated review data from the backend, handles fallback to dummy data if necessary, and displays various sections including overview, attitude roots, request information, and summary using dedicated modules. It also incorporates a slideshow component to navigate through different data sections and displays contact information.</p> <p>Parameters:</p> Name Type Description Default <code>custom_css</code> <code>str</code> <p>A string containing CSS styles to customize the appearance               of the Streamlit application.</p> required Source code in <code>frontend\\main_page.py</code> <pre><code>def main_page(custom_css):\n    \"\"\"\n    Render the Main Page with aggregated review data and interactive visualizations.\n\n    This function applies custom CSS styles, retrieves and manages aggregated review data\n    from the backend, handles fallback to dummy data if necessary, and displays various\n    sections including overview, attitude roots, request information, and summary using\n    dedicated modules. It also incorporates a slideshow component to navigate through\n    different data sections and displays contact information.\n\n    Parameters:\n        custom_css (str): A string containing CSS styles to customize the appearance\n                          of the Streamlit application.\n    \"\"\"\n    # Apply custom CSS Styles\n    st.markdown(main_page_css, unsafe_allow_html=True)\n\n    # avoid API recall when using the next buttons by reusing main_page_variables if already set in this session\n    if \"main_page_variables\" not in st.session_state:\n\n        try: # Fetch data and store it in session state\n            overview, request_information, attitude_roots, summary = get_classification_with_api()\n            st.session_state.main_page_variables = {\n                \"overview\": overview,\n                \"request_information\": request_information,\n                \"attitude_roots\": attitude_roots,\n                \"summary\": summary\n            }\n        except: # Fallback to dummy data if anything goes wrong\n            st.session_state.main_page_variables = {\n                \"overview\": None,\n                \"request_information\": None,\n                \"attitude_roots\": None,\n                \"summary\": None,\n            }\n\n    # Access data from session state\n    overview = st.session_state.main_page_variables[\"overview\"]\n    attitude_roots = st.session_state.main_page_variables[\"attitude_roots\"]\n    request_information = st.session_state.main_page_variables[\"request_information\"]\n    summary = st.session_state.main_page_variables[\"summary\"]\n\n    # Fallback to dummy data if any DataFrame is empty\n    base_path = os.getcwd()\n    if overview is None or overview.empty:\n        st.warning(\"No data available for overview. - display dummy data instead!\")\n        with open(os.path.join(base_path, 'dummy_data', 'dummy_overview.pkl'), 'rb') as file:\n            overview = pickle.load(file)\n    if attitude_roots is None or attitude_roots.empty:\n        st.warning(\"No data available for attitudes classification. - display dummy data instead!\")\n        with open(os.path.join(base_path, 'dummy_data', 'dummy_attitude_roots.pkl'), 'rb') as file:\n            attitude_roots = pickle.load(file)\n    if request_information is None or request_information.empty:\n        st.warning(\"No data available for request classification. - display dummy data instead!\")\n        with open(os.path.join(base_path, 'dummy_data', 'dummy_requests.pkl'), 'rb') as file:\n            request_information = pickle.load(file)\n    if summary is None or summary.empty:\n        st.warning(\"No data available for summary. - display dummy data instead!\") \n        summary = pd.read_csv(os.path.join(base_path, \"dummy_data\", \"dummy_summary.csv\"), sep=\";\", encoding=\"utf-8\")\n\n    # Show overview container\n    use_default_container(modules.overview.show_overview, overview)\n\n    # Show slideshow containing attitude roots, request information and summary\n    attitude_root_container = lambda: modules.attitude_roots.show_attitude_roots_data(attitude_roots)\n    request_information_container = lambda: modules.request_information.show_request_information_data(request_information)\n    summary_container = lambda: modules.summary.show_summary_data(summary)\n\n    slideshow = ss.StreamlitSlideshow([attitude_root_container, request_information_container, summary_container],\n                                      [\"Attitude Roots\", \"Request Information\", \"Summary\"])\n    use_default_container(slideshow.show)\n\n    # Show contact info\n    use_default_container(modules.contact_info.show_contact_info)\n</code></pre>"},{"location":"api/frontend/#frontend.app.safe_delete_session_state","title":"<code>safe_delete_session_state(key)</code>","text":"<p>Deletes a key from the Streamlit session state.</p> <p>This function ensures that specific variables are removed from the session state when a user navigates away from a page, preventing unintended data persistence.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The key in the session state to be deleted.</p> required Source code in <code>frontend\\app.py</code> <pre><code>def safe_delete_session_state(key):\n    \"\"\"\n    Deletes a key from the Streamlit session state.\n\n    This function ensures that specific variables are removed from the session state\n    when a user navigates away from a page, preventing unintended data persistence.\n\n    Parameters:\n        key (str): The key in the session state to be deleted.\n    \"\"\"\n    if key in st.session_state:\n        del st.session_state[key]\n</code></pre>"},{"location":"api/frontend/#frontend.app.show_navigation_bar_and_content","title":"<code>show_navigation_bar_and_content()</code>","text":"<p>Displays the navigation sidebar and renders content based on the selected page.</p> <p>This function handles the loading of the logo, setting up the sidebar with navigation buttons, and rendering the appropriate page content based on the user's selection. It also applies custom CSS for styling various UI elements.</p> The supported pages include <ul> <li>Home</li> <li>Review Aggregation</li> <li>Contact</li> <li>About</li> </ul> <p>The function ensures that session state variables are appropriately managed when navigating between pages.</p> Source code in <code>frontend\\app.py</code> <pre><code>def show_navigation_bar_and_content():\n    \"\"\"\n    Displays the navigation sidebar and renders content based on the selected page.\n\n    This function handles the loading of the logo, setting up the sidebar with navigation\n    buttons, and rendering the appropriate page content based on the user's selection.\n    It also applies custom CSS for styling various UI elements.\n\n    The supported pages include:\n        - Home\n        - Review Aggregation\n        - Contact\n        - About\n\n    The function ensures that session state variables are appropriately managed when\n    navigating between pages.\n    \"\"\"\n\n    def load_logo(filepath):\n        \"\"\"\n        Loads and encodes a logo image to Base64.\n\n        This nested function reads an image file from the specified filepath and encodes\n        it in Base64 format, which is suitable for embedding in HTML.\n\n        Parameters:\n            filepath (Path): The path to the logo image file.\n\n        Returns:\n            str: The Base64-encoded string of the logo image.\n        \"\"\"\n        with open(filepath, \"rb\") as logo_file:\n            return base64.b64encode(logo_file.read()).decode(\"utf-8\")\n\n    # Path to the local logo file\n    base_path = Path(__file__).parent\n    logo_base64 = load_logo(base_path / \"images/logo_header.png\")\n\n    # Display the logo in the sidebar\n    st.sidebar.markdown(\n        f\"\"\"\n        &lt;div style=\"text-align: center;\"&gt;\n            &lt;img src=\"data:image/png;base64,{logo_base64}\" alt=\"Logo\" style=\"width: 120px; margin-bottom: 20px;\"&gt;\n        &lt;/div&gt;\n        \"\"\",\n        unsafe_allow_html=True,\n    )\n\n    # Initialize session state for page tracking if not already set\n    if 'page' not in st.session_state:\n        st.session_state.page = 'Home'\n\n    # Apply custom CSS to style buttons in the sidebar\n    st.sidebar.markdown(\"\"\"\n        &lt;style&gt;\n        /* Button styling */\n        section[data-testid=\"stSidebar\"] div.stButton &gt; button {\n        background-color: #3a60b2 !important;\n        color: white !important;\n        width: 100%; /* Make button take full width of sidebar */\n        border: none;\n        border-radius: 5px;\n        padding: 10px 20px;\n        font-size: 16px;      \n        }\n        section[data-testid=\"stSidebar\"] div.stButton &gt; button:hover {\n            background-color: #0056b3 !important;\n        }\n        /* Change sidebar background to white */\n        section[data-testid=\"stSidebar\"] {\n            background-color: white !important;\n        }\n        [data-testid=\"stSidebarCollapseButton\"] button {\n            color: #3a60b2 !important;\n            }\n        [data-testid=\"stSidebarCollapsedControl\"] button {\n            color: #3a60b2 !important;\n            }\n\n        &lt;/style&gt;\n    \"\"\", unsafe_allow_html=True)\n\n    # Sidebar title and navigation buttons\n    st.sidebar.title(\"Paper Review Aggregator\")\n\n    if st.sidebar.button(\"Home\"):\n        st.session_state.page = \"Home\"\n    if st.sidebar.button(\"Review Aggregation\"):\n        st.session_state.page = \"Review Aggregation\"\n    #if st.sidebar.button(\"Meta Reviewer Dashboard\"):\n    #    st.session_state.page = \"Meta Reviewer Dashboard\"\n    if st.sidebar.button(\"Contact\"):\n        st.session_state.page = \"Contact\"\n    if st.sidebar.button(\"About\"):\n        st.session_state.page = \"About\"\n\n    # Render content based on the selected page\n    if st.session_state.page == \"Home\":\n        safe_delete_session_state(\"main_page_variables\")\n        st.session_state.page = home_page.home_page(custom_css)\n    elif st.session_state.page == \"Review Aggregation\":\n        safe_delete_session_state(\"main_page_variables\")\n        landing_page.landing_page(custom_css)\n    elif st.session_state.page == \"Meta Reviewer Dashboard\":\n        main_page.main_page(custom_css)\n    elif st.session_state.page == \"Contact\":\n        safe_delete_session_state(\"main_page_variables\")\n        contact.show_contact_info(custom_css)\n    elif st.session_state.page == \"About\":\n        safe_delete_session_state(\"main_page_variables\")\n        about.about_page(custom_css)\n</code></pre>"},{"location":"api/frontend/#frontend.about.about_page","title":"<code>about_page(custom_css)</code>","text":"<p>Render the About page with custom styling and content.</p> <p>This function applies custom CSS styles to the Streamlit application and displays the about content and contact information by invoking corresponding functions from the <code>about_content</code> and <code>contact_info</code> modules.</p> <p>Parameters:</p> Name Type Description Default <code>custom_css</code> <code>str</code> <p>A string containing CSS styles to customize the appearance               of the Streamlit application.</p> required Source code in <code>frontend\\about.py</code> <pre><code>def about_page(custom_css):\n    \"\"\"\n    Render the About page with custom styling and content.\n\n    This function applies custom CSS styles to the Streamlit application and displays\n    the about content and contact information by invoking corresponding functions from\n    the `about_content` and `contact_info` modules.\n\n    Parameters:\n        custom_css (str): A string containing CSS styles to customize the appearance\n                          of the Streamlit application.\n    \"\"\"\n\n    # Apply custom CSS Styles\n    st.markdown(custom_css, unsafe_allow_html=True)\n\n    # Show about details\n    use_default_container(modules.about_content.show_about_content)\n\n    # Show contact details\n    use_default_container(modules.contact_info.show_contact_info)\n</code></pre>"},{"location":"api/frontend/#frontend.contact.show_contact_info","title":"<code>show_contact_info(custom_css)</code>","text":"<p>Render the contact information section with custom styling.</p> <p>This function applies custom CSS styles to the Streamlit application and displays contact details by invoking the <code>show_contact_info</code> function from the <code>modules.contact_info</code> module.</p> <p>Parameters:</p> Name Type Description Default <code>custom_css</code> <code>str</code> <p>A string containing CSS styles to customize the appearance               of the Streamlit application.</p> required Source code in <code>frontend\\contact.py</code> <pre><code>def show_contact_info(custom_css):\n    \"\"\"\n    Render the contact information section with custom styling.\n\n    This function applies custom CSS styles to the Streamlit application and\n    displays contact details by invoking the `show_contact_info` function from\n    the `modules.contact_info` module.\n\n    Parameters:\n        custom_css (str): A string containing CSS styles to customize the appearance\n                          of the Streamlit application.\n    \"\"\"\n\n    # Apply custom CSS Styles\n    st.markdown(custom_css, unsafe_allow_html=True)\n\n    # Show contact details\n    use_default_container(modules.contact_info.show_contact_info)\n</code></pre>"},{"location":"api/request_classifier/","title":"Request Classifier Code Documentation","text":""},{"location":"api/request_classifier/#request_classifier.classification.request_classifier_pipeline.process_dataframe_request","title":"<code>process_dataframe_request(df, local_dir_request, hf_request_classifier, local_dir_fine_request, hf_fine_request_classifier)</code>","text":"<p>Processes a DataFrame containing sentences for coarse and fine-grained classification.</p> <p>Parameters: df (pd.DataFrame): Input DataFrame with a column <code>sentence</code> containing text to classify. local_dir_request (str): Local directory of the binary classifier. hf_request_classifier (str): Hugging Face model path for the binary request classifier. local_dir_fine_request (str): Local directory of the multiclass classifier. hf_fine_request_classifier (str): Hugging Face model path for the multiclass classifier.</p> <p>Returns: pd.DataFrame: DataFrame with added columns for coarse and fine-grained labels.</p> Source code in <code>request_classifier\\classification\\request_classifier_pipeline.py</code> <pre><code>def process_dataframe_request(df: pd.DataFrame, local_dir_request: str, hf_request_classifier: str,\n                              local_dir_fine_request: str, hf_fine_request_classifier: str) -&gt; pd.DataFrame:\n    \"\"\"\n    Processes a DataFrame containing sentences for coarse and fine-grained classification.\n\n    Parameters:\n    df (pd.DataFrame): Input DataFrame with a column `sentence` containing text to classify.\n    local_dir_request (str): Local directory of the binary classifier.\n    hf_request_classifier (str): Hugging Face model path for the binary request classifier.\n    local_dir_fine_request (str): Local directory of the multiclass classifier.\n    hf_fine_request_classifier (str): Hugging Face model path for the multiclass classifier.\n\n    Returns:\n    pd.DataFrame: DataFrame with added columns for coarse and fine-grained labels.\n    \"\"\"\n    if 'sentence' not in df.columns:\n        raise ValueError(\"DataFrame must include a 'sentence' column.\")\n\n    texts = df['sentence'].tolist()\n\n    # Load the coarse-grained request classifier\n    tokenizer_bert = BertTokenizer.from_pretrained(hf_request_classifier, cache_dir=local_dir_request)\n    model_bert = BertForSequenceClassification.from_pretrained(hf_request_classifier, cache_dir=local_dir_request)\n    # tokenizer_bert = load_BertTokenizer(local_path=local_dir, huggingface_model_path=hf_request_classifier)\n    # model_bert = load_BertForSequenceClassification(local_path=local_dir, huggingface_model_path=hf_request_classifier)\n    model_bert.eval()\n    # device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    device = \"cpu\"\n    model_bert.to(device)\n\n    # Define a prediction function for the coarse-grained classifier\n    def predict_request(texts):\n        \"\"\"\n        Predicts coarse-grained labels for a batch of texts.\n\n        Parameters:\n        texts (list): List of input sentences.\n\n        Returns:\n        np.ndarray: Array of predicted labels.\n        \"\"\"\n        inputs = tokenizer_bert(\n            texts,\n            return_tensors=\"pt\",\n            padding=True,\n            truncation=True,\n            max_length=128\n        )\n        inputs = {k: v.to(device) for k, v in inputs.items()}\n        with torch.no_grad():\n            outputs = model_bert(**inputs)\n        logits = outputs.logits\n        predictions = torch.argmax(logits, dim=-1)\n        return predictions.cpu().numpy()\n\n    # Perform coarse-grained classification\n    logger.info(\"Classifying requests (coarse-grained)\")\n    predictions_bert = []\n    batch_size = 16\n    try:\n        for i in tqdm(range(0, len(texts), batch_size), desc=\"Processing batches\"):\n            batch_texts = texts[i:i + batch_size]\n            preds = predict_request(batch_texts)\n            predictions_bert.extend(preds)\n    except Exception as e:\n        print(f\"Error during prediction: {e}\")\n\n    df['coarse_label_pred'] = predictions_bert\n\n    # Filter rows where coarse-grained prediction indicates a request\n    df_requests = df[df['coarse_label_pred'] == 1].copy()\n    # print(df_requests)\n\n    logger.info(\"Loading fine-grained BERT model\")\n    tokenizer_bert_fine = BertTokenizer.from_pretrained(hf_fine_request_classifier, cache_dir=local_dir_fine_request)\n    model_bert_fine = BertForSequenceClassification.from_pretrained(hf_fine_request_classifier,\n                                                                    cache_dir=local_dir_fine_request)\n    # tokenizer_bert_fine = load_BertTokenizer(local_path=local_dir_fine_request,\n    #                                          huggingface_model_path=hf_fine_request_classifier)\n    # model_bert_fine = load_BertForSequenceClassification(local_path=local_dir_fine_request,\n    #                                                      huggingface_model_path=hf_fine_request_classifier)\n    # tokenizer_bert_fine = BertTokenizer.from_pretrained(local_dir_fine_request)\n    # model_bert_fine = BertForSequenceClassification.from_pretrained(local_dir_fine_request)\n    model_bert_fine.eval()\n    device_fine = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    model_bert_fine.to(device_fine)\n\n    def predict_request_fine(batch_texts):\n        \"\"\"\n        Predicts fine-grained (multi-class) labels for a batch of texts.\n        Returns an array of numeric label IDs.\n        \"\"\"\n        inputs = tokenizer_bert_fine(\n            batch_texts,\n            return_tensors=\"pt\",\n            padding=True,\n            truncation=True,\n            max_length=128\n        )\n        inputs = {k: v.to(device_fine) for k, v in inputs.items()}\n        with torch.no_grad():\n            outputs = model_bert_fine(**inputs)\n        logits = outputs.logits\n        return torch.argmax(logits, dim=-1).cpu().numpy()\n\n    # ------------------------------------------------------------------#\n    # 4) Perform fine-grained classification (only for request rows)\n    # ------------------------------------------------------------------#\n    if not df_requests.empty:\n        logger.info(\"Performing fine-grained classification with second BERT\")\n        texts_fine = df_requests['sentence'].tolist()\n        predictions_fine = []\n        for i in tqdm(range(0, len(texts_fine), batch_size), desc=\"Processing fine batches\"):\n            batch_texts_fine = texts_fine[i:i + batch_size]\n            preds = predict_request_fine(batch_texts_fine)\n            predictions_fine.extend(preds)\n\n        # Assign numeric label IDs\n        df_requests['fine_grained_label'] = predictions_fine\n\n        # Convert numeric label IDs to label names\n        # Example: label_map = {\"Request for Explanation\": 0, \"Request for Improvement\": 1, ...}\n        # So we invert that mapping to {0: \"Request for Explanation\", 1: ...}\n        reverse_label_map = {v: k for k, v in label_map.items()}\n        df_requests['fine_grained_label_name'] = df_requests['fine_grained_label'].map(reverse_label_map)\n\n    else:\n        # If no requests are detected, set fine-grained labels to default values\n        df_requests['fine_grained_label'] = -1\n        df_requests['fine_grained_label_name'] = None\n\n    # ------------------------------------------------------------------#\n    # 5) Summarize results\n    # ------------------------------------------------------------------#\n    df_requests_summarized = summarize_requests_by_authors(df_requests)\n    # print(df_requests_summarized)\n    return df_requests_summarized\n</code></pre>"},{"location":"api/request_classifier/#request_classifier.classification.request_classifier_pipeline.summarize_requests_by_authors","title":"<code>summarize_requests_by_authors(df_requests)</code>","text":"<p>Aggregates fine-grained labels and summarizes them by the number of unique authors  and associated comments, with sentences grouped by their respective categories.</p> <p>Parameters: df_requests (pd.DataFrame): DataFrame containing fine-grained request information.</p> <p>pd.DataFrame: Summary DataFrame with categories, unique author counts,                associated comments, and sentences grouped by their respective labels.</p> Source code in <code>request_classifier\\classification\\request_classifier_pipeline.py</code> <pre><code>def summarize_requests_by_authors(df_requests: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Aggregates fine-grained labels and summarizes them by the number of unique authors \n    and associated comments, with sentences grouped by their respective categories.\n\n    Parameters:\n    df_requests (pd.DataFrame): DataFrame containing fine-grained request information.\n\n    Returns:\n    pd.DataFrame: Summary DataFrame with categories, unique author counts, \n                  associated comments, and sentences grouped by their respective labels.\n    \"\"\"\n    # Count total number of unique authors\n    num_authors = df_requests['author'].nunique()\n\n    # Create a summary grouped by fine-grained label names\n    summary = (\n        df_requests.groupby('fine_grained_label_name')\n        .agg(\n            Frequency=('author', lambda x: x.nunique() / num_authors),  # Relative frequency of unique authors\n        )\n        .reset_index()\n        .rename(columns={'fine_grained_label_name': 'Request Information'})  # Rename columns for clarity\n        .sort_values(by='Frequency', ascending=False)  # Sort by proportion\n    )\n\n    # Add sentences grouped by authors for each fine-grained label\n    def group_sentences_by_label(df: pd.DataFrame) -&gt; dict:\n        \"\"\"\n        Groups sentences by authors for each fine-grained label.\n\n        Parameters:\n        df (pd.DataFrame): DataFrame containing fine-grained labels, authors, and sentences.\n\n        Returns:\n        dict: A dictionary where keys are fine-grained labels, and values are lists of \n              [author, [list of sentences written by the author]] for that label.\n        \"\"\"\n        grouped = {}\n        for label in df['fine_grained_label_name'].unique():\n            label_df = df[df['fine_grained_label_name'] == label]\n            author_sentences = label_df.groupby('author')['sentence'].apply(list).reset_index()\n            grouped[label] = author_sentences.apply(lambda row: [row['author'], row['sentence']], axis=1).tolist()\n        return grouped\n\n    # Generate sentences grouped by authors for each fine-grained label\n    sentences_by_label = group_sentences_by_label(df_requests)\n\n    # Add grouped sentences for each category to the summary\n    summary['Comments'] = summary['Request Information'].map(sentences_by_label)\n    summary['Request Information'] = summary['Request Information'].str.replace(\"Request for \", \"\", regex=False)\n\n    return summary\n</code></pre>"},{"location":"api/summary_generator/","title":"Summary Generator APCodeI Documentation","text":"<p>main.py</p> <p>A FastAPI application that exposes an endpoint for generating summaries from preprocessed data.</p> <p>predict_LLAMA2.py</p> <p>A script that: 1) Loads a LLaMA2 model from Hugging Face 2) Predicts summary of input</p> <p>data_processing.py</p> <p>Provides functions for processing CSV files, converting DataFrames into input text, and exporting data. Also includes functions for generating prompts for overview, attitude roots, and request information.</p>"},{"location":"api/summary_generator/#summary_generator.main.load_LLAMA2_model","title":"<code>load_LLAMA2_model(model_dir='./models/llama2')</code>","text":"<p>Load a LLaMA 2 model from Hugging Face that is supposed to be stored locally at the given path. For a real-world scenario, ensure you have:   - 'transformers&gt;=4.30'   - 'sentencepiece'   - You have accepted the license for LLaMA2 if it's gated.</p> <p>Args:       model_dir (str): path to directory where the model is supposed to be stored.</p> Source code in <code>summary_generator\\main.py</code> <pre><code>@app.on_event(\"startup\")\ndef load_LLAMA2_model(model_dir: str = \"./models/llama2\"):\n    \"\"\"\n    Load a LLaMA 2 model from Hugging Face that is supposed to be stored locally at the given path.\n    For a real-world scenario, ensure you have:\n      - 'transformers&gt;=4.30'\n      - 'sentencepiece'\n      - You have accepted the license for LLaMA2 if it's gated.\n\n      Args:\n          model_dir (str): path to directory where the model is supposed to be stored.\n    \"\"\"\n    global model, tokenizer\n    logger.info(f\"Loading model '{model_dir}' from Hugging Face...\")\n\n    hf_dir = \"DASP-ROG/SummaryModel\"\n    tokenizer = LlamaTokenizer.from_pretrained(hf_dir, cache_dir=model_dir, legacy=True)\n    model = LlamaForCausalLM.from_pretrained(\n        hf_dir,\n        torch_dtype=torch.float16,\n        cache_dir=model_dir,\n        device_map=\"auto\"\n    )\n\n    # Set pad_token to eos_token\n    tokenizer.pad_token = tokenizer.eos_token\n    model.config.pad_token_id = tokenizer.eos_token_id\n    logger.info(\"Model loaded successfully.\")\n</code></pre>"},{"location":"api/summary_generator/#summary_generator.main.predict","title":"<code>predict(overview_df, attitude_df, request_df)</code>  <code>async</code>","text":"<p>Generate a summary using the provided overview, attitude, and request data.</p> <p>Parameters:</p> Name Type Description Default <code>overview_df</code> <code>RawInput</code> <p>Overview data.</p> required <code>attitude_df</code> <code>RawInput</code> <p>Attitude roots data.</p> required <code>request_df</code> <code>RawInput</code> <p>Request information data.</p> required <p>Returns:</p> Type Description <code>list[dict]</code> <p>list[dict]: A list of summary lines formatted as dictionaries.</p> Source code in <code>summary_generator\\main.py</code> <pre><code>@app.post(\"/generate_summary\")\nasync def predict(overview_df: RawInput, attitude_df: RawInput, request_df: RawInput) -&gt; list[dict]:\n    \"\"\"\n    Generate a summary using the provided overview, attitude, and request data.\n\n    Args:\n        overview_df (RawInput): Overview data.\n        attitude_df (RawInput): Attitude roots data.\n        request_df (RawInput): Request information data.\n\n    Returns:\n        list[dict]: A list of summary lines formatted as dictionaries.\n    \"\"\"\n    try:\n        if model is None or tokenizer is None:\n            raise HTTPException(status_code=500, detail=\"Model not loaded yet\")\n\n        # 1) Transform json files into dfs\n        overview = overview_df.data\n        overview_df = pd.DataFrame(overview)\n\n        attitude = attitude_df.data\n        attitude_df = pd.DataFrame(attitude)\n\n        request = request_df.data\n        request_df = pd.DataFrame(request)\n\n        # 2) Transform overview into its final text and extract lists with attributes and comments of attitude and request\n        overview_output, attitude_list, request_list = generate_input_text(overview_df, attitude_df, request_df)\n        logger.info(\"Generating input text from data\")\n\n        # # 3) load model for prediction\n        # model, tokenizer = predict_LLAMA2.load_LLAMA2_model(model_dir=\"models/llama2/\")\n        # logger.info(\"Model loaded successfully.\")\n\n        # 4) just reuse the finalized overview data\n        summary = []\n        summary.append(\"#### Overview\")\n        summary.append(overview_output)\n        #for line in overview_output.splitlines():\n        #    summary.append(line)\n\n        # 5) add given Attitude Roots and their AI generated summary of comments\n        logger.info(\"Processing Attitude Roots...\")\n        summary.append(\"\")\n        summary.append (\"#### Attitude Roots\")\n        attitude_roots, comments = attitude_list\n\n        # case of no return what means that there arent any attitude roots\n        if attitude_roots == [] and comments == []:\n            summary.append(\"No attitude roots were found during analysis.\")\n\n        # case of return what means we can predict!\n        for attitude, comment in zip(attitude_roots, comments):\n            pred = predict_LLAMA2.predict(comment, model=model, tokenizer=tokenizer)\n            summary.append(f\"{attitude}  \\n *AI aggregated comments*: {pred}\")\n\n\n        # 6) add given Reqest Information and their AI generated summary of comments\n        logger.info(\"Processing Request Information...\")\n        summary.append(\"\")\n        summary.append(\"#### Request Information\")\n        requests, comments = request_list\n\n        # case of no return what means that there arent any attitude roots\n        if requests == [] and comments == []:\n            summary.append(\"No request information were found during analysis.\")\n\n        # case of return what means we can predict!\n        for request, comment in zip(requests, comments):\n            pred = predict_LLAMA2.predict(comment, model=model, tokenizer=tokenizer)\n            summary.append(f\"{request}  \\n *AI aggregated comments*: {pred}\")\n\n        summary_df = pd.DataFrame(summary)\n\n        # # TODO: remove this line when we use the model for prediction\n        # # summary_df = pd.DataFrame([\"## Temporary dummy\", \"We currently commented out AI summary generation. Go to DASP_report_template/summary_generator/main.py to use it again.\"])\n\n        summary_df_dict = summary_df.to_dict(orient=\"records\")\n        logger.info(\"Summary generation completed successfully.\")\n\n        # with open(\"summary.json\", \"r\") as f:\n        #     summary_df_dict = json.load(f)\n\n\n        return summary_df_dict\n\n\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=f\"Error processing data: {str(e)}\")\n</code></pre>"},{"location":"api/summary_generator/#summary_generator.predict_LLAMA2.predict","title":"<code>predict(input_text, model=None, tokenizer=None, model_dir='/storage/ukp/shared/shared_model_weights/models--llama-2-hf/7B-Chat', min_new_tokens=20, max_new_tokens_cap=512)</code>","text":"<p>Build the prompt from real data, run generation on LLaMA 2,  and return an output that is capped by <code>max_new_tokens_cap</code>.</p> <p>Parameters:</p> Name Type Description Default <code>input_text</code> <code>str</code> <p>Raw text or data for the prompt builder.</p> required <code>model_dir</code> <code>str</code> <p>Path or HF repo for your LLaMA2 model.</p> <code>'/storage/ukp/shared/shared_model_weights/models--llama-2-hf/7B-Chat'</code> <code>min_new_tokens</code> <code>int</code> <p>Ensures we generate at least this many tokens.</p> <code>20</code> <code>max_new_tokens_cap</code> <code>int</code> <p>Hard upper bound on new tokens generated.</p> <code>512</code> <p>Returns:</p> Name Type Description <code>str</code> <p>The final generated text (prompt echo stripped).</p> Source code in <code>summary_generator\\predict_LLAMA2.py</code> <pre><code>def predict(\n        input_text: str,\n        model=None,\n        tokenizer=None,\n        model_dir: str = \"/storage/ukp/shared/shared_model_weights/models--llama-2-hf/7B-Chat\",\n        min_new_tokens: int = 20,\n        max_new_tokens_cap: int = 512,\n):\n    \"\"\"\n    Build the prompt from real data, run generation on LLaMA 2, \n    and return an output that is capped by `max_new_tokens_cap`.\n\n    Args:\n        input_text (str): Raw text or data for the prompt builder.\n        model_dir (str): Path or HF repo for your LLaMA2 model.\n        min_new_tokens (int): Ensures we generate at least this many tokens.\n        max_new_tokens_cap (int): Hard upper bound on new tokens generated.\n\n    Returns:\n        str: The final generated text (prompt echo stripped).\n    \"\"\"\n    # 1) Load model &amp; tokenizer if needed\n    # if (model is None and tokenizer is None):\n    #     model, tokenizer = load_LLAMA2_model(model_dir)\n\n    # 2) Build the final prompt from the input data\n    prompt = input_to_prompt_converter.build_llama2_prompt(input_text)\n    # logger.info(\"Prompt is ready. Calculating lengths and generating...\")\n\n    # 3) Measure the prompt and input length in tokens\n    prompt_tokens = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False)\n\n    # 4) Move everything to the model's device\n    device = model.device\n    for k, v in prompt_tokens.items():\n        prompt_tokens[k] = v.to(device)\n\n    # 5) Create generation config\n    gen_config = GenerationConfig(\n        max_new_tokens=150,\n        do_sample=True,\n        temperature=0.7,\n        top_p=0.9,\n        repetition_penalty=1.2,  # helps reduce repeated tokens\n        # Additional parameters (e.g., num_beams) if desired\n    )\n\n    # 6) Generate\n    with torch.no_grad():\n        outputs = model.generate(**prompt_tokens, generation_config=gen_config)\n\n    # 7) Decode\n    decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n\n    # 8) LLaMA often re-echos the prompt. Remove it if present:\n    result = decoded[0]\n    if result.startswith(prompt):\n        result = result[len(prompt):]\n\n    # 9) Remove \"Summary: \" prefix if present\n    result = result.lstrip()\n    summary_prefix = \"Summary:\"\n    if result.startswith(summary_prefix):\n        result = result[len(summary_prefix):]\n\n    return result.strip()\n</code></pre>"},{"location":"api/summary_generator/#summary_generator.data_processing.convert_dataframes_to_input","title":"<code>convert_dataframes_to_input(folder_dataframes)</code>","text":"<p>Converts a dictionary of folder DataFrames into input strings for each folder.</p> <p>This function takes the processed DataFrames for each folder and generates input text using the <code>generate_input_text</code> function. It ensures that each folder has exactly three DataFrames before processing.</p> <p>Parameters:</p> Name Type Description Default <code>folder_dataframes</code> <code>dict</code> <p>A dictionary where each key is a folder name and each value                       is a list of three DataFrames (overview, attitude_roots, request_information).</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary where each key is a folder name and each value is the generated input text.</p> Source code in <code>summary_generator\\data_processing.py</code> <pre><code>def convert_dataframes_to_input(folder_dataframes):\n    \"\"\"\n    Converts a dictionary of folder DataFrames into input strings for each folder.\n\n    This function takes the processed DataFrames for each folder and generates input text\n    using the `generate_input_text` function. It ensures that each folder has exactly three\n    DataFrames before processing.\n\n    Parameters:\n        folder_dataframes (dict): A dictionary where each key is a folder name and each value\n                                  is a list of three DataFrames (overview, attitude_roots, request_information).\n\n    Returns:\n        dict: A dictionary where each key is a folder name and each value is the generated input text.\n    \"\"\"\n    # Dictionary to store input strings for each folder\n    folder_inputs = {}\n\n    for folder, dfs in folder_dataframes.items():\n        if len(dfs) == 3:\n            overview_df, attitude_df, request_df = dfs\n            # Generate input text using the imported function\n            input_text = generate_input_text(overview_df, attitude_df, request_df)\n            folder_inputs[folder] = input_text\n\n    return folder_inputs\n</code></pre>"},{"location":"api/summary_generator/#summary_generator.data_processing.export_unlabeled_data","title":"<code>export_unlabeled_data()</code>","text":"<p>Exports aggregated comments from all folders into a JSON Lines file.</p> <p>This function processes CSV files from each folder, generates input text, extracts comments from attitude roots and request information, and writes them as JSON objects to a <code>.jsonl</code> file. Each line in the file represents a JSON object with an \"input\" key containing the comment text.</p> <p>The exported file is named \"real_world_data_unlabeled.jsonl\".</p> Source code in <code>summary_generator\\data_processing.py</code> <pre><code>def export_unlabeled_data():\n    \"\"\"\n    Exports aggregated comments from all folders into a JSON Lines file.\n\n    This function processes CSV files from each folder, generates input text,\n    extracts comments from attitude roots and request information, and writes them\n    as JSON objects to a `.jsonl` file. Each line in the file represents a JSON object\n    with an \"input\" key containing the comment text.\n\n    The exported file is named \"real_world_data_unlabeled.jsonl\".\n    \"\"\"\n    complete_export = \"\"\n    folder_dataframes = process_csv_files()  # return a list [overview_df, attitude_df, request_df]\n    # dfs = [overview_df, attitude_df, request_df]\n    for folder, dfs in folder_dataframes.items():\n        overview_df, attitude_df, request_df = dfs\n        overview_output, attitude_list, request_list = generate_input_text(overview_df, attitude_df, request_df)\n\n        # export each comment\n        attitude_roots, comments = attitude_list\n        for comment in comments:\n            part_string_for_export = '{\"input\": \"' + f\"{comment}\" + '\"}\\n'\n            complete_export += part_string_for_export\n\n        request, comments = request_list\n        for comments in comments:\n            part_string_for_export = '{\"input\": \"' + f\"{comments}\" + '\"}\\n'\n            complete_export += part_string_for_export\n\n    print(complete_export)        \n    with open(\"real_world_data_unlabeled.jsonl\", \"w\", encoding=\"utf-8\") as output_file:\n        output_file.write(complete_export)\n\n    print(\"Predictions have been exported to real_word_data_unlabeled.jsonl\")\n</code></pre>"},{"location":"api/summary_generator/#summary_generator.data_processing.generate_attitude_roots_prompts","title":"<code>generate_attitude_roots_prompts(attitude_df)</code>","text":"<p>Summarizes attitude roots by frequency and aggregates related comments.</p> <p>This function processes the attitude roots DataFrame to generate descriptive lines indicating how often each attitude root appears and includes any associated descriptions. It also aggregates comments related to each attitude root.</p> <p>Parameters:</p> Name Type Description Default <code>attitude_df</code> <code>DataFrame</code> <p>DataFrame containing attitude roots information with columns                         like 'Attitude_roots', 'Frequency', 'Descriptions', and 'Comments'.</p> required <p>Returns:</p> Name Type Description <code>list</code> <p>A list containing two elements:   1. A list of summary lines for each attitude root.   2. A list of aggregated comments for each attitude root.</p> Source code in <code>summary_generator\\data_processing.py</code> <pre><code>def generate_attitude_roots_prompts(attitude_df):\n    \"\"\"\n    Summarizes attitude roots by frequency and aggregates related comments.\n\n    This function processes the attitude roots DataFrame to generate descriptive lines\n    indicating how often each attitude root appears and includes any associated descriptions.\n    It also aggregates comments related to each attitude root.\n\n    Parameters:\n        attitude_df (pd.DataFrame): DataFrame containing attitude roots information with columns\n                                    like 'Attitude_roots', 'Frequency', 'Descriptions', and 'Comments'.\n\n    Returns:\n        list: A list containing two elements:\n              1. A list of summary lines for each attitude root.\n              2. A list of aggregated comments for each attitude root.\n    \"\"\"\n    attitude_df['Frequency_Percent'] = attitude_df['Frequency'] * 100\n    lines = []\n    comments = []\n\n    for _, row in attitude_df.iterrows():\n        root = row['Attitude_roots']\n        freq = row['Frequency_Percent']\n        descr = row['Descriptions']\n        comments_list = row['Comments']\n\n        combined_comments = []\n        for _, comment_chunk in comments_list:\n            combined_comments.append(\" \".join(comment_chunk))\n\n        all_comments = \" \".join(combined_comments)\n        line = f\"- **{root}** was indicated in {freq:.0f}% of responses. {descr}.\"\n        lines.append(line)\n        comments.append(all_comments)\n\n    return [lines, comments]\n</code></pre>"},{"location":"api/summary_generator/#summary_generator.data_processing.generate_input_text","title":"<code>generate_input_text(overview_df, attitude_df, request_df)</code>","text":"<p>Generates structured input text from overview, attitude, and request DataFrames.</p> <p>This function combines the outputs from generating overview prompts, attitude roots prompts, and request information prompts into a single structured input. It organizes the data with clear section headings for further processing or model input.</p> <p>Parameters:</p> Name Type Description Default <code>overview_df</code> <code>DataFrame</code> <p>DataFrame containing overview information.</p> required <code>attitude_df</code> <code>DataFrame</code> <p>DataFrame containing attitude roots information.</p> required <code>request_df</code> <code>DataFrame</code> <p>DataFrame containing request information.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <p>A tuple containing three elements:    1. Overview summary string.    2. List containing attitude roots summary lines and aggregated comments.    3. List containing request information summary lines and aggregated comments.</p> Source code in <code>summary_generator\\data_processing.py</code> <pre><code>def generate_input_text(overview_df, attitude_df, request_df):\n    \"\"\"\n    Generates structured input text from overview, attitude, and request DataFrames.\n\n    This function combines the outputs from generating overview prompts, attitude roots prompts,\n    and request information prompts into a single structured input. It organizes the data\n    with clear section headings for further processing or model input.\n\n    Parameters:\n        overview_df (pd.DataFrame): DataFrame containing overview information.\n        attitude_df (pd.DataFrame): DataFrame containing attitude roots information.\n        request_df (pd.DataFrame): DataFrame containing request information.\n\n    Returns:\n        tuple: A tuple containing three elements:\n               1. Overview summary string.\n               2. List containing attitude roots summary lines and aggregated comments.\n               3. List containing request information summary lines and aggregated comments.\n    \"\"\"\n    # Collect all prompts from the three scripts\n    overview_output = generate_overview_prompts(overview_df)\n    attitude_list = generate_attitude_roots_prompts(attitude_df)\n    request_list = generate_request_information_prompts(request_df)\n\n    # Combine all data into a single prompt with clear section headings\n\n    return overview_output, attitude_list, request_list\n</code></pre>"},{"location":"api/summary_generator/#summary_generator.data_processing.generate_overview_prompts","title":"<code>generate_overview_prompts(overview_df)</code>","text":"<p>Builds summary lines for each category based on average scores and identifies outliers.</p> <p>This function processes the overview DataFrame to generate descriptive sentences summarizing the average scores for each category. It also identifies and notes any outlier scores that deviate significantly from the average.</p> <p>Parameters:</p> Name Type Description Default <code>overview_df</code> <code>DataFrame</code> <p>DataFrame containing overview information with columns                          like 'Category', 'Avg_Score', and 'Individual_scores'.</p> required <p>Returns:</p> Name Type Description <code>str</code> <p>A concatenated string of summary sentences for each category.</p> Source code in <code>summary_generator\\data_processing.py</code> <pre><code>def generate_overview_prompts(overview_df):\n    \"\"\"\n    Builds summary lines for each category based on average scores and identifies outliers.\n\n    This function processes the overview DataFrame to generate descriptive sentences\n    summarizing the average scores for each category. It also identifies and notes any\n    outlier scores that deviate significantly from the average.\n\n    Parameters:\n        overview_df (pd.DataFrame): DataFrame containing overview information with columns\n                                     like 'Category', 'Avg_Score', and 'Individual_scores'.\n\n    Returns:\n        str: A concatenated string of summary sentences for each category.\n    \"\"\"\n    max_scores = {'Rating': 10, 'Soundness': 4, 'Presentation': 4, 'Contribution': 4}\n    threshold = 1.5\n    overview_sentences = []\n\n    for _, row in overview_df.iterrows():\n        category = row['Category']\n        avg_score = row['Avg_Score']\n        individual_scores = row['Individual_scores']\n        max_score = max_scores.get(category, \"Unknown\")\n\n        scores = [score for (_, score) in individual_scores if score is not None]\n        outliers = [sc for sc in scores if abs(sc - avg_score) &gt; threshold]\n\n        sentence = f\"- **{category}** is {avg_score} out of {max_score}.\"\n        if outliers:\n            unique_outliers = sorted(set(outliers))\n            if len(unique_outliers) == 1:\n                outliers_text = f\"a rating at {unique_outliers[0]}\"\n            else:\n                outliers_text = \", \".join([f\"a rating at {sc}\" for sc in unique_outliers[:-1]])\n                outliers_text += f\" and a rating at {unique_outliers[-1]}\"\n            sentence += f\" Outlier was {outliers_text}.\"\n\n        overview_sentences.append(sentence)\n\n    return \"\\n\".join(overview_sentences)\n</code></pre>"},{"location":"api/summary_generator/#summary_generator.data_processing.generate_request_information_prompts","title":"<code>generate_request_information_prompts(request_df)</code>","text":"<p>Summarizes request information by type, frequency, and aggregates related comments.</p> <p>This function processes the request information DataFrame to generate descriptive lines indicating how often each type of request was made. It also aggregates comments related to each request type.</p> <p>Parameters:</p> Name Type Description Default <code>request_df</code> <code>DataFrame</code> <p>DataFrame containing request information with columns                        like 'Request Information', 'Frequency', and 'Comments'.</p> required <p>Returns:</p> Name Type Description <code>list</code> <p>A list containing two elements:   1. A list of summary lines for each request type.   2. A list of aggregated comments for each request type.</p> Source code in <code>summary_generator\\data_processing.py</code> <pre><code>def generate_request_information_prompts(request_df):\n    \"\"\"\n    Summarizes request information by type, frequency, and aggregates related comments.\n\n    This function processes the request information DataFrame to generate descriptive lines\n    indicating how often each type of request was made. It also aggregates comments related\n    to each request type.\n\n    Parameters:\n        request_df (pd.DataFrame): DataFrame containing request information with columns\n                                   like 'Request Information', 'Frequency', and 'Comments'.\n\n    Returns:\n        list: A list containing two elements:\n              1. A list of summary lines for each request type.\n              2. A list of aggregated comments for each request type.\n    \"\"\"\n    request_df['Frequency_Percent'] = request_df['Frequency'] * 100\n    lines = []\n    comments = []\n\n    for _, row in request_df.iterrows():\n        request_type = row['Request Information']\n        freq = row['Frequency_Percent']\n        comments_list = row['Comments']\n\n        combined_comments = []\n        for _, chunk in comments_list:\n            combined_comments.append(\" \".join(chunk))\n\n        all_comments = \" \".join(combined_comments)\n        line = f\"- **{request_type}** was requested in {freq:.0f}% of replies.\"\n        lines.append(line)\n        comments.append(all_comments)\n\n    return [lines, comments]\n</code></pre>"},{"location":"api/summary_generator/#summary_generator.data_processing.predict_data","title":"<code>predict_data(model, tokenizer, overview_df, attitude_df, request_df)</code>","text":"<p>Generates predictions by aggregating summaries using a machine learning model.</p> <p>This function processes the overview, attitude roots, and request information DataFrames to generate summaries. It utilizes the provided model and tokenizer to generate AI-aggregated comments for attitude roots and request information sections.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <p>The machine learning model used to generate predictions.</p> required <code>tokenizer</code> <p>The tokenizer associated with the model for processing input text.</p> required <code>overview_df</code> <code>DataFrame</code> <p>DataFrame containing overview information.</p> required <code>attitude_df</code> <code>DataFrame</code> <p>DataFrame containing attitude roots information.</p> required <code>request_df</code> <code>DataFrame</code> <p>DataFrame containing request information.</p> required <p>Returns:</p> Name Type Description <code>str</code> <p>The aggregated summary containing both original and AI-generated sections.</p> Source code in <code>summary_generator\\data_processing.py</code> <pre><code>def predict_data(model, tokenizer, overview_df, attitude_df, request_df):\n    \"\"\"\n    Generates predictions by aggregating summaries using a machine learning model.\n\n    This function processes the overview, attitude roots, and request information DataFrames\n    to generate summaries. It utilizes the provided model and tokenizer to generate AI-aggregated\n    comments for attitude roots and request information sections.\n\n    Parameters:\n        model: The machine learning model used to generate predictions.\n        tokenizer: The tokenizer associated with the model for processing input text.\n        overview_df (pd.DataFrame): DataFrame containing overview information.\n        attitude_df (pd.DataFrame): DataFrame containing attitude roots information.\n        request_df (pd.DataFrame): DataFrame containing request information.\n\n    Returns:\n        str: The aggregated summary containing both original and AI-generated sections.\n    \"\"\"\n\n    overview_output, attitude_list, request_list = generate_input_text(overview_df, attitude_df, request_df)\n\n    # 1) Write Overview part without AI\n    summary = \"Overview: \\n\"\n    summary += overview_output\n\n    # 2) add AI generated summary of Attitude Roots\n    summary += \"\\n Attitude Roots: \\n\"\n    attitude_roots, comments = attitude_list\n    for attitude, comment in zip(attitude_roots, comments):\n        pred = model.predict(comment, tokenizer)  # TODO: Implement model prediction\n        summary += f\"{attitude} AI aggregated Comments: {pred} \\n\"\n\n    # 3) add AI generated summary of Request Information\n    summary += \"\\n Request Information: \\n\"\n    requests, comments = request_list\n    for request, comment in zip(requests, comments):\n        pred = model.predict(comment)  # TODO: Implement model prediction\n        summary += f\"{request} AI aggregated Comments: {pred} \\n\"\n\n    return summary\n</code></pre>"},{"location":"api/summary_generator/#summary_generator.data_processing.process_csv_files","title":"<code>process_csv_files(base_path='data/real_sample_data')</code>","text":"<p>Processes CSV files within each subdirectory of the specified base path.</p> <p>This function iterates through each folder in the base directory, reads specific CSV files in a defined order, parses certain columns, rounds numerical values, and stores the resulting DataFrames in a dictionary keyed by folder name.</p> <p>Parameters:</p> Name Type Description Default <code>base_path</code> <code>str</code> <p>The path to the base directory containing subfolders with CSV files.              Defaults to \"data/real_sample_data\".</p> <code>'data/real_sample_data'</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary where each key is a folder name and each value is a list of DataFrames   corresponding to the CSV files in that folder.</p> Source code in <code>summary_generator\\data_processing.py</code> <pre><code>def process_csv_files(base_path=\"data/real_sample_data\"):\n    \"\"\"\n    Processes CSV files within each subdirectory of the specified base path.\n\n    This function iterates through each folder in the base directory, reads specific CSV files\n    in a defined order, parses certain columns, rounds numerical values, and stores the resulting\n    DataFrames in a dictionary keyed by folder name.\n\n    Parameters:\n        base_path (str): The path to the base directory containing subfolders with CSV files.\n                         Defaults to \"data/real_sample_data\".\n\n    Returns:\n        dict: A dictionary where each key is a folder name and each value is a list of DataFrames\n              corresponding to the CSV files in that folder.\n    \"\"\"\n    # Dictionary to hold dataframes for each folder, with each folder containing a list of DataFrames\n    folder_dataframes = {}\n\n    # Iterate through folders\n    for folder in os.listdir(base_path):\n        folder_path = os.path.join(base_path, folder)\n\n        # Ensure it's a directory\n        if os.path.isdir(folder_path):\n            # Ensure specific order of CSV files\n            csv_order = [\"overview.csv\", \"attitude_roots.csv\", \"request_information.csv\"]\n\n            folder_csvs = []  # List to store DataFrames for this folder\n\n            for csv_file in csv_order:\n                file_path = os.path.join(folder_path, csv_file)\n\n                if os.path.exists(file_path):\n                    # Read CSV file into DataFrame\n                    df = pd.read_csv(file_path)\n\n                    # Parse the 'Individual_scores' column (if present)\n                    if 'Individual_scores' in df.columns:\n                        df['Individual_scores'] = df['Individual_scores'].apply(lambda x: ast.literal_eval(x))\n\n                    # Parse and adjust 'Comments' column (if present)\n                    if 'Comments' in df.columns:\n                        df['Comments'] = df['Comments'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n\n                    # Round numerical columns (if any)\n                    for col in df.select_dtypes(include=['float', 'int']).columns:\n                        df[col] = df[col].round(2)\n\n                    folder_csvs.append(df)\n\n            folder_dataframes[folder] = folder_csvs\n\n    return folder_dataframes\n</code></pre>"},{"location":"api/test/","title":"Tests Code Documentation","text":""},{"location":"api/test/#tests.test_openreview_textprocessor_integration.test_openreview_textprocessing_integration","title":"<code>test_openreview_textprocessing_integration(username, password)</code>","text":"<p>This integration tests fetches reviews from OpenReview and processes them with TextProcessor.</p> Source code in <code>tests\\test_openreview_textprocessor_integration.py</code> <pre><code>def test_openreview_textprocessing_integration(username, password):\n    \"\"\"\n    This integration tests fetches reviews from OpenReview and processes them with TextProcessor.\n    \"\"\"\n\n    client = OpenReviewClient(username, password)\n\n    paper_id = \"aVh9KRZdRk\"  # openreview.net/forum?id=aVh9KRZdRk\n    paper = client.get_reviews_from_id(paper_id)\n\n    reviews = paper.reviews\n\n    assert paper is not None\n    assert reviews\n\n    processor = TextProcessor(reviews)\n    df_sentences, df_overview = processor.process()\n\n    assert not df_sentences.empty\n    assert not df_overview.empty\n    assert df_sentences is not None\n    assert df_overview is not None\n\n    expected_overview_cols = [\"Category\", \"Avg_Score\", \"Individual_scores\"]\n\n    assert df_overview.columns.tolist() == expected_overview_cols\n\n    expected_first_sentence = (\"The paper studies the emergence of the in-context ability of the GPT-style transformer \"\n                               \"model trained using autoregressive loss and arithmetic modular datasets. It analyzes \"\n                               \"the influence of the number of tasks, number of in-context examples, model capacity, \"\n                               \"etc., on the ICL capability of an appropriately trained model (i.e., using early \"\n                               \"stopping). It also provides a persuasive \u201ctask decomposition hypothesis\u201d, which is \"\n                               \"well supported by the ablation study and various experiments. The white-box analysis \"\n                               \"on the attention heads provides convincing evidence of the proposed explanation. \"\n                               \"Although there is a gap between the grokking settings (i.e., small model and toy \"\n                               \"dataset) and practical systems, the paper does a good job of explaining many \"\n                               \"important trends and concepts related to the emergence of compositional \"\n                               \"in-context ability. I enjoy reading this paper and suggest an acceptance.\")\n\n    assert expected_first_sentence == df_sentences.iloc[0, 2]\n\n    df_overview_expected = pd.DataFrame({\n        \"Category\": [\"Rating\", \"Soundness\", \"Presentation\", \"Contribution\"],\n        \"Avg_Score\": [7.25, 3.25, 3.50, 3.25],\n        \"Individual_scores\": [\n            [\n                [\"Reviewer rwBm\", 7.0],\n                [\"Reviewer PjEW\", 8.0],\n                [\"Reviewer Jerg\", 7.0],\n                [\"Reviewer CrUb\", 7.0],\n            ],\n            [\n                [\"Reviewer rwBm\", 3.0],\n                [\"Reviewer PjEW\", 4.0],\n                [\"Reviewer Jerg\", 3.0],\n                [\"Reviewer CrUb\", 3.0],\n            ],\n            [\n                [\"Reviewer rwBm\", 4.0],\n                [\"Reviewer PjEW\", 4.0],\n                [\"Reviewer Jerg\", 3.0],\n                [\"Reviewer CrUb\", 3.0],\n            ],\n            [\n                [\"Reviewer rwBm\", 3.0],\n                [\"Reviewer PjEW\", 4.0],\n                [\"Reviewer Jerg\", 3.0],\n                [\"Reviewer CrUb\", 3.0],\n            ],\n        ]\n    })\n\n    pdt.assert_frame_equal(df_overview, df_overview_expected, check_like=True)\n</code></pre>"},{"location":"api/test/#tests.test_textprocessor_classifiy_attitudes_integration.test_textprocessor_classifiy_attitudes_integration","title":"<code>test_textprocessor_classifiy_attitudes_integration(username, password)</code>","text":"<p>Integration test for combine_roots_and_themes</p> Source code in <code>tests\\test_textprocessor_classifiy_attitudes_integration.py</code> <pre><code>def test_textprocessor_classifiy_attitudes_integration(username, password):\n    \"\"\"\n    Integration test for combine_roots_and_themes\n    \"\"\"\n    openreview_client = OpenReviewClient(username, password)\n\n    paper_id = \"aVh9KRZdRk\"  # openreview.net/forum?id=aVh9KRZdRk\n    paper = openreview_client.get_reviews_from_id(paper_id)\n\n    reviews = paper.reviews\n\n    assert paper is not None\n    assert reviews\n\n    processor = TextProcessor(reviews)\n    df_sentences, df_overview = processor.process()\n\n    assert not df_sentences.empty\n    assert not df_overview.empty\n    assert df_sentences is not None\n    assert df_overview is not None\n\n    sentences = df_sentences.to_dict(orient='records')\n    response = requests.post(\"http://localhost:8082/classify_attitudes\", json={\"data\": sentences})\n\n    assert response.status_code == 200\n\n    df = pd.DataFrame(response.json())\n\n    assert df is not None\n    assert df.shape == (27, 4)\n    assert df.columns.tolist() == ['Attitude_roots', 'Frequency', 'Descriptions', 'Comments']\n</code></pre>"},{"location":"api/test/#tests.test_textprocessor_request_classifier_integration.test_textprocessor_request_classifier_integration","title":"<code>test_textprocessor_request_classifier_integration(username, password)</code>","text":"<p>Integration test for process_dataframe_request in the request_classifier module</p> Source code in <code>tests\\test_textprocessor_request_classifier_integration.py</code> <pre><code>def test_textprocessor_request_classifier_integration(username, password):\n    \"\"\"\n    Integration test for process_dataframe_request in the request_classifier module\n    \"\"\"\n    openreview_client = OpenReviewClient(username, password)\n\n    paper_id = \"aVh9KRZdRk\"  # openreview.net/forum?id=aVh9KRZdRk\n    paper = openreview_client.get_reviews_from_id(paper_id)\n\n    reviews = paper.reviews\n\n    assert paper is not None\n    assert reviews\n\n    processor = TextProcessor(reviews)\n    df_sentences, df_overview = processor.process()\n\n    assert not df_sentences.empty\n    assert not df_overview.empty\n    assert df_sentences is not None\n    assert df_overview is not None\n\n    sentences = df_sentences.to_dict(orient=\"records\")\n    response = requests.post(\"http://localhost:8081/classify_request\", json={\"data\": sentences})\n\n    assert response.status_code == 200\n\n    df = pd.DataFrame(response.json())\n\n    assert df is not None\n    assert df.shape == (5, 3)\n    assert df.columns.tolist() == [\"Request Information\", \"Frequency\", \"Comments\"]\n    assert df[\"Request Information\"].values.tolist() == [\"Improvement\", \"Explanation\", \"Clarification\", \"Experiment\",\n                                                         \"Typo Fix\"]\n</code></pre>"},{"location":"api/test/#tests.test_uploadedfileprocessor_textprocessor_integration.sample_uploaded_review","title":"<code>sample_uploaded_review()</code>","text":"<p>Loads a sample .docm file from 'tests/resources/sample_review_form.docm' and returns it as a Streamlit UploadedFile object.</p> Source code in <code>tests\\test_uploadedfileprocessor_textprocessor_integration.py</code> <pre><code>@pytest.fixture\ndef sample_uploaded_review():\n    \"\"\"\n    Loads a sample .docm file from 'tests/resources/sample_review_form.docm' and returns it as a Streamlit UploadedFile\n    object.\n    \"\"\"\n    try:\n        docm_path = Path(\"tests/resources/sample_review.docm\")\n        file_bytes = open(docm_path, \"rb\").read()\n    except FileNotFoundError:\n        # if started with PyCharm debugger the project path is already one deeper\n        docm_path = Path(\"resources/sample_review.docm\")\n        file_bytes = open(docm_path, \"rb\").read()\n\n    file_record = UploadedFileRec(file_id=\"2c6cff9b-8266-4d91-9353-a2c1d5a8e2a8\",\n                                  name=\"sample_review.docm\",\n                                  type=\"application/vnd.ms-word.document.macroEnabled.12\",\n                                  data=file_bytes\n                                  )\n\n    file_urls = FileURLsProto(file_id=\"2c6cff9b-8266-4d91-9353-a2c1d5a8e2a8\",\n                              upload_url=\"/_stcore/upload_file/c075ce9f-812e-4c50-82ac-afac73bb26d6/2c6cff9b-8266-4d91-9353-a2c1d5a8e2a8\",\n                              delete_url=\"/_stcore/upload_file/c075ce9f-812e-4c50-82ac-afac73bb26d6/2c6cff9b-8266-4d91-9353-a2c1d5a8e2a8\"\n                              )\n\n    uploaded_file = UploadedFile(file_record, file_urls)\n    return uploaded_file\n</code></pre>"},{"location":"api/test/#tests.test_uploadedfileprocessor_textprocessor_integration.test_uploadedfileprocessor_textprocessor_integration","title":"<code>test_uploadedfileprocessor_textprocessor_integration(sample_uploaded_review)</code>","text":"<p>This integration tests mocks a streamlit uploaded file and passes it to the UploadedFileProcessor and extracts review objects from it. The reviews then get passet to the TextProcessor.</p> Source code in <code>tests\\test_uploadedfileprocessor_textprocessor_integration.py</code> <pre><code>def test_uploadedfileprocessor_textprocessor_integration(sample_uploaded_review):\n    \"\"\"\n    This integration tests mocks a streamlit uploaded file and passes it to the UploadedFileProcessor and extracts\n    review objects from it. The reviews then get passet to the TextProcessor.\n    \"\"\"\n\n    processor = UploadedFileProcessor(uploaded_files=[sample_uploaded_review])\n\n    reviews = processor.process()\n\n    assert isinstance(reviews, list)\n    assert len(reviews) == 1\n    for r in reviews:\n        assert isinstance(r, Review)\n        # Check that form fields are populated\n        assert hasattr(r, \"summary\")\n\n    processor = TextProcessor(reviews)\n    df_sentences, df_overview = processor.process()\n\n    assert not df_sentences.empty\n    assert not df_overview.empty\n\n    expected_overview_cols = [\"Category\", \"Avg_Score\", \"Individual_scores\"]\n    assert df_overview.columns.tolist() == expected_overview_cols\n\n    expected_first_sentence = (\"This paper presents a method of learning dense 3D correspondence between shapes in a \"\n                               \"self-supervised manner. Specifically, it is built on an existing SO(3)-equivariant \"\n                               \"representation. The input point clouds are independently encoded to SO(3)-equivariant \"\n                               \"global shape descriptor Z and dynamic SO(3)-invariant point-wise local shape \"\n                               \"transforms. Then the network is trained via penalizing errors in self- and cross- \"\n                               \"reconstructions via the decoder. The experiment validates the effectiveness of \"\n                               \"the proposed method.\")\n\n    assert expected_first_sentence == df_sentences.iloc[0, 2]\n\n    df_overview_expected = pd.DataFrame({\n        \"Category\": [\"Rating\", \"Soundness\", \"Presentation\", \"Contribution\"],\n        \"Avg_Score\": [5.0, 3.0, 3.0, 2.0],\n        \"Individual_scores\": [\n            [[\"Reviewer 1\", 5.0]],\n            [[\"Reviewer 1\", 3.0]],\n            [[\"Reviewer 1\", 3.0]],\n            [[\"Reviewer 1\", 2.0]]\n        ]\n    })\n\n    pdt.assert_frame_equal(df_overview, df_overview_expected, check_like=True)\n</code></pre>"},{"location":"api/model_training/attitude/","title":"Attitude Classification Model Training Code Documentation","text":""},{"location":"api/model_training/attitude/#model_training.nlp.attitude_classifier.functions_for_docu.CustomDataset","title":"<code>CustomDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Einfaches Dataset f\u00fcr den Trainer.</p> <p>Parameters:</p> Name Type Description Default <code>encodings</code> <code>dict</code> <p>Tokenisierte Textdaten.</p> required <code>labels</code> <code>Tensor</code> <p>Labels in Tensor-Form.</p> required Source code in <code>model_training\\nlp\\attitude_classifier\\functions_for_docu.py</code> <pre><code>class CustomDataset(Dataset):\n    \"\"\"\n    Einfaches Dataset f\u00fcr den Trainer.\n\n    Parameters:\n        encodings (dict): Tokenisierte Textdaten.\n        labels (Tensor): Labels in Tensor-Form.\n    \"\"\"\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __getitem__(self, idx):\n        \"\"\"\n        Gibt ein einzelnes Item (Encoding + Label) zur\u00fcck.\n\n        Parameters:\n            idx (int): Index des gew\u00fcnschten Eintrags.\n\n        Returns:\n            dict: Tokenisierte Eingaben und zugeh\u00f6riges Label.\n        \"\"\"\n        item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}  # Convert encodings to tensor\n        item[\"labels\"] = self.labels[idx]  # Add label to the dictionary\n        return item\n\n    def __len__(self):\n        \"\"\"\n        Gibt die Gesamtzahl der Eintr\u00e4ge im Dataset zur\u00fcck.\n\n        Returns:\n            int: Anzahl der Datenpunkte.\n        \"\"\"\n        return len(self.labels)\n</code></pre>"},{"location":"api/model_training/attitude/#model_training.nlp.attitude_classifier.functions_for_docu.CustomDataset.__getitem__","title":"<code>__getitem__(idx)</code>","text":"<p>Gibt ein einzelnes Item (Encoding + Label) zur\u00fcck.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int</code> <p>Index des gew\u00fcnschten Eintrags.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>Tokenisierte Eingaben und zugeh\u00f6riges Label.</p> Source code in <code>model_training\\nlp\\attitude_classifier\\functions_for_docu.py</code> <pre><code>def __getitem__(self, idx):\n    \"\"\"\n    Gibt ein einzelnes Item (Encoding + Label) zur\u00fcck.\n\n    Parameters:\n        idx (int): Index des gew\u00fcnschten Eintrags.\n\n    Returns:\n        dict: Tokenisierte Eingaben und zugeh\u00f6riges Label.\n    \"\"\"\n    item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}  # Convert encodings to tensor\n    item[\"labels\"] = self.labels[idx]  # Add label to the dictionary\n    return item\n</code></pre>"},{"location":"api/model_training/attitude/#model_training.nlp.attitude_classifier.functions_for_docu.CustomDataset.__len__","title":"<code>__len__()</code>","text":"<p>Gibt die Gesamtzahl der Eintr\u00e4ge im Dataset zur\u00fcck.</p> <p>Returns:</p> Name Type Description <code>int</code> <p>Anzahl der Datenpunkte.</p> Source code in <code>model_training\\nlp\\attitude_classifier\\functions_for_docu.py</code> <pre><code>def __len__(self):\n    \"\"\"\n    Gibt die Gesamtzahl der Eintr\u00e4ge im Dataset zur\u00fcck.\n\n    Returns:\n        int: Anzahl der Datenpunkte.\n    \"\"\"\n    return len(self.labels)\n</code></pre>"},{"location":"api/model_training/attitude/#model_training.nlp.attitude_classifier.functions_for_docu.compute_metrics","title":"<code>compute_metrics(pred)</code>","text":"<p>Berechnet die Accuracy als Beispielmetrik.</p> <p>Parameters:</p> Name Type Description Default <code>pred</code> <p>Predictions-Objekt des Modells.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>Dictionary mit dem Schl\u00fcssel 'accuracy' und dem berechneten Wert.</p> Source code in <code>model_training\\nlp\\attitude_classifier\\functions_for_docu.py</code> <pre><code>def compute_metrics(pred):\n    \"\"\"\n    Berechnet die Accuracy als Beispielmetrik.\n\n    Parameters:\n        pred: Predictions-Objekt des Modells.\n\n    Returns:\n        dict: Dictionary mit dem Schl\u00fcssel 'accuracy' und dem berechneten Wert.\n    \"\"\"\n    labels = pred.label_ids  # True labels\n    preds = np.argmax(pred.predictions, axis=1)  # Predicted labels\n    accuracy = (preds == labels).mean()  # Calculate accuracy\n    return {\"accuracy\": accuracy}\n</code></pre>"},{"location":"api/model_training/attitude/#model_training.nlp.attitude_classifier.functions_for_docu.create_trainer","title":"<code>create_trainer(model, tokenizer, train_df, val_df, num_epochs=4, output_dir='./results')</code>","text":"<p>Erstellt einen Hugging Face Trainer mit spezifiziertem Modell, Tokenizer und Daten.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <p>Pretrained Modell-Instanz.</p> required <code>tokenizer</code> <p>Tokenizer-Instanz f\u00fcr Textverarbeitung.</p> required <code>train_df</code> <code>DataFrame</code> <p>Trainingsdaten.</p> required <code>val_df</code> <code>DataFrame</code> <p>Validierungsdaten.</p> required <code>num_epochs</code> <code>int</code> <p>Anzahl der Trainings-Epochen. Standard ist 4.</p> <code>4</code> <code>output_dir</code> <code>str</code> <p>Speicherort f\u00fcr Trainingsergebnisse. Standard ist \"./results\".</p> <code>'./results'</code> <p>Returns:</p> Name Type Description <code>Trainer</code> <p>Hugging Face Trainer-Objekt.</p> Source code in <code>model_training\\nlp\\attitude_classifier\\functions_for_docu.py</code> <pre><code>def create_trainer(model, tokenizer, train_df, val_df, num_epochs=4, output_dir=\"./results\"):\n    \"\"\"\n    Erstellt einen Hugging Face Trainer mit spezifiziertem Modell, Tokenizer und Daten.\n\n    Parameters:\n        model: Pretrained Modell-Instanz.\n        tokenizer: Tokenizer-Instanz f\u00fcr Textverarbeitung.\n        train_df (DataFrame): Trainingsdaten.\n        val_df (DataFrame): Validierungsdaten.\n        num_epochs (int): Anzahl der Trainings-Epochen. Standard ist 4.\n        output_dir (str): Speicherort f\u00fcr Trainingsergebnisse. Standard ist \"./results\".\n\n    Returns:\n        Trainer: Hugging Face Trainer-Objekt.\n    \"\"\"\n    # Tokenize and encode training and validation datasets\n    train_encodings, train_labels = encode_data(tokenizer, train_df)\n    val_encodings, val_labels = encode_data(tokenizer, val_df)\n\n    # Create PyTorch Datasets\n    train_dataset = CustomDataset(train_encodings, train_labels)\n    val_dataset = CustomDataset(val_encodings, val_labels)\n\n    # Define training arguments\n    training_args = TrainingArguments(\n        output_dir=output_dir,                 # Output directory for results\n        evaluation_strategy=\"epoch\",          # Evaluate after each epoch\n        save_strategy=\"epoch\",                # Save model after each epoch\n        learning_rate=5e-5,                   # Learning rate\n        per_device_train_batch_size=16,       # Batch size for training\n        per_device_eval_batch_size=64,        # Batch size for evaluation\n        num_train_epochs=num_epochs,          # Number of epochs\n        weight_decay=0.01,                    # Weight decay for regularization\n        logging_dir='./logs',                 # Directory for logs\n        load_best_model_at_end=True,          # Load the best model at the end of training\n        metric_for_best_model=\"accuracy\",     # Metric to determine the best model\n        greater_is_better=True,               # Higher metric values indicate better performance\n        save_total_limit=2                    # Keep only the last 2 model checkpoints\n    )\n\n    # Define early stopping callback\n    early_stopping = EarlyStoppingCallback(\n        early_stopping_patience=3,            # Stop training after 3 epochs with no improvement\n        early_stopping_threshold=0.0         # Minimum improvement threshold\n    )\n\n    # Create the Trainer object\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=val_dataset,\n        tokenizer=tokenizer,\n        compute_metrics=compute_metrics,      # Use custom metrics\n        callbacks=[early_stopping]            # Add early stopping callback\n    )\n\n    return trainer\n</code></pre>"},{"location":"api/model_training/attitude/#model_training.nlp.attitude_classifier.functions_for_docu.encode_data","title":"<code>encode_data(tokenizer, df)</code>","text":"<p>Tokenisiert die Texte und gibt Encodings sowie Labels als Torch Tensor zur\u00fcck.</p> <p>Parameters:</p> Name Type Description Default <code>tokenizer</code> <p>Tokenizer-Instanz f\u00fcr die Textverarbeitung.</p> required <code>df</code> <code>DataFrame</code> <p>Daten mit 'data' (Text) und 'encoded_cat' (Integer-Labels).</p> required <p>Returns:</p> Name Type Description <code>Tuple</code> <p>(Encodings, Labels-Tensor).</p> Source code in <code>model_training\\nlp\\attitude_classifier\\functions_for_docu.py</code> <pre><code>def encode_data(tokenizer, df):\n    \"\"\"\n    Tokenisiert die Texte und gibt Encodings sowie Labels als Torch Tensor zur\u00fcck.\n\n    Parameters:\n        tokenizer: Tokenizer-Instanz f\u00fcr die Textverarbeitung.\n        df (DataFrame): Daten mit 'data' (Text) und 'encoded_cat' (Integer-Labels).\n\n    Returns:\n        Tuple: (Encodings, Labels-Tensor).\n    \"\"\"\n    texts = df[\"data\"].tolist()  # Extract text data\n    labels = df[\"encoded_cat\"].tolist()  # Extract encoded labels\n\n    # Tokenize the text data with truncation and padding\n    encodings = tokenizer(texts, truncation=True, padding=True)\n    labels_tensor = torch.tensor(labels, dtype=torch.long)  # Convert labels to Torch tensor\n\n    return encodings, labels_tensor\n</code></pre>"},{"location":"api/model_training/attitude/#model_training.nlp.attitude_classifier.functions_for_docu.oversample_dataframe","title":"<code>oversample_dataframe(input_df, skip_label='None')</code>","text":"<p>Einfache Oversampling-Funktion, die jede Klasse (au\u00dfer skip_label) um den Faktor 2 erh\u00f6ht. Passe dies nach Bedarf an (z.B. max_count, SMOTE etc.).</p> <p>Parameters:</p> Name Type Description Default <code>input_df</code> <code>DataFrame</code> <p>Eingabedaten mit 'labels'-Spalte.</p> required <code>skip_label</code> <code>str</code> <p>Label, das vom Oversampling ausgeschlossen wird. Standard ist \"None\".</p> <code>'None'</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <p>Oversampled DataFrame mit verdoppelten Eintr\u00e4gen f\u00fcr alle relevanten Klassen.</p> Source code in <code>model_training\\nlp\\attitude_classifier\\functions_for_docu.py</code> <pre><code>def oversample_dataframe(input_df, skip_label=\"None\"):\n    \"\"\"\n    Einfache Oversampling-Funktion, die jede Klasse (au\u00dfer skip_label) um den Faktor 2 erh\u00f6ht.\n    Passe dies nach Bedarf an (z.B. max_count, SMOTE etc.).\n\n    Parameters:\n        input_df (DataFrame): Eingabedaten mit 'labels'-Spalte.\n        skip_label (str): Label, das vom Oversampling ausgeschlossen wird. Standard ist \"None\".\n\n    Returns:\n        DataFrame: Oversampled DataFrame mit verdoppelten Eintr\u00e4gen f\u00fcr alle relevanten Klassen.\n    \"\"\"\n    aspects = [lbl for lbl in all_labels_list if lbl != skip_label]  # Exclude specific label from oversampling\n    output_df = input_df.copy()\n\n    for aspect in aspects:\n        subset = output_df[output_df[\"labels\"] == aspect]  # Filter rows for the specific label\n        count = len(subset)  # Count occurrences of the label\n        if count &gt; 0:\n            # Duplicate samples for oversampling\n            resampled = subset.sample(n=count, replace=True, random_state=42)\n            output_df = pd.concat([output_df, resampled], ignore_index=True)\n    return output_df\n</code></pre>"},{"location":"api/model_training/attitude/#model_training.nlp.attitude_classifier.functions_for_docu.plot_confusion_matrix","title":"<code>plot_confusion_matrix(model, tokenizer, test_df, title='Confusion Matrix')</code>","text":"<p>Generates and displays a confusion matrix for the model's predictions on the test dataset.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <p>The trained model used for predictions.</p> required <code>tokenizer</code> <p>The tokenizer corresponding to the model.</p> required <code>test_df</code> <code>DataFrame</code> <p>The test dataset containing 'data' (text) and 'encoded_cat' (integer labels).</p> required <code>title</code> <code>str</code> <p>Title for the confusion matrix plot. Default is \"Confusion Matrix\".</p> <code>'Confusion Matrix'</code> Source code in <code>model_training\\nlp\\attitude_classifier\\functions_for_docu.py</code> <pre><code>def plot_confusion_matrix(model, tokenizer, test_df, title=\"Confusion Matrix\"):\n    \"\"\"\n    Generates and displays a confusion matrix for the model's predictions on the test dataset.\n\n    Parameters:\n        model: The trained model used for predictions.\n        tokenizer: The tokenizer corresponding to the model.\n        test_df (DataFrame): The test dataset containing 'data' (text) and 'encoded_cat' (integer labels).\n        title (str): Title for the confusion matrix plot. Default is \"Confusion Matrix\".\n    \"\"\"\n    preds = []  # List to store predicted labels\n    test_texts = test_df[\"data\"].tolist()  # Extract test texts\n    test_labels_list = test_df[\"encoded_cat\"].tolist()  # Extract true labels\n\n    # Iterate over test texts and generate predictions\n    for txt in test_texts:\n        # Tokenize the text input and convert to tensor format\n        inputs = tokenizer(txt, truncation=True, padding=True, return_tensors=\"pt\")\n        inputs = {k: v.to(model.device) for k, v in inputs.items()}  # Move inputs to the model's device (CPU/GPU)\n\n        # Perform inference without gradient computation\n        with torch.no_grad():\n            outputs = model(**inputs)  # Get model predictions\n\n        logits = outputs.logits  # Extract logits from model output\n        pred_label = torch.argmax(logits, dim=1).item()  # Get the predicted class label\n        preds.append(pred_label)\n\n    # Compute the confusion matrix\n    cm = confusion_matrix(test_labels_list, preds)\n\n    # Define class names for the plot\n    class_names = [\n        \"none\", \"substance\", \"originality\", \"clarity\", \n        \"soundness-correctness\", \"motivation-impact\", \n        \"meaningful-comparison\", \"replicability\", \"other\"\n    ]\n\n    # Plot the confusion matrix as a heatmap\n    plt.figure(figsize=(10, 8))  # Set figure size\n    sns.heatmap(\n        cm, annot=True, fmt=\"d\", cmap=\"Blues\", \n        xticklabels=class_names, yticklabels=class_names\n    )  # Display the matrix with annotations\n    plt.title(title)  # Set the title\n    plt.ylabel(\"True Label\")  # Label for y-axis\n    plt.xlabel(\"Predicted Label\")  # Label for x-axis\n    plt.show()  # Display the plot\n\n    # Print the classification report\n    print(classification_report(test_labels_list, preds, target_names=class_names))\n</code></pre>"},{"location":"api/model_training/attitude/#model_training.nlp.attitude_classifier.functions_for_docu.train_approach_1","title":"<code>train_approach_1(model_class, tokenizer_class, model_path, model_name)</code>","text":"<p>Ansatz (1): Train for 2 epochs without oversampling.</p> <p>Parameters:</p> Name Type Description Default <code>model_class</code> <p>The class for the Hugging Face model.</p> required <code>tokenizer_class</code> <p>The class for the Hugging Face tokenizer.</p> required <code>model_path</code> <p>Path to the pretrained model.</p> required <code>model_name</code> <p>Name of the model (used for saving results).</p> required <p>Returns:</p> Name Type Description <code>model</code> <p>The trained model.</p> <code>tokenizer</code> <p>The tokenizer used for training.</p> <code>trainer</code> <p>The Hugging Face Trainer instance.</p> Source code in <code>model_training\\nlp\\attitude_classifier\\functions_for_docu.py</code> <pre><code>def train_approach_1(model_class, tokenizer_class, model_path, model_name):\n    \"\"\"\n    Ansatz (1): Train for 2 epochs without oversampling.\n\n    Parameters:\n        model_class: The class for the Hugging Face model.\n        tokenizer_class: The class for the Hugging Face tokenizer.\n        model_path: Path to the pretrained model.\n        model_name: Name of the model (used for saving results).\n\n    Returns:\n        model: The trained model.\n        tokenizer: The tokenizer used for training.\n        trainer: The Hugging Face Trainer instance.\n    \"\"\"\n    out_dir = f\"./results_approach1_{model_name}\"  # Dynamic folder for saving results\n\n    # Load the pretrained model and tokenizer\n    model = model_class.from_pretrained(\n        model_path,\n        num_labels=len(all_labels_list),                 # Number of classes for classification\n        problem_type=\"single_label_classification\",      # Single-label classification problem\n        ignore_mismatched_sizes=True                    # Allow head size mismatch\n    )\n    tokenizer = tokenizer_class.from_pretrained(model_path)\n\n    # Create a Trainer with specified output directory\n    trainer = create_trainer(model, tokenizer, train_df, val_df, num_epochs=4, output_dir=out_dir)\n    trainer.train()  # Train the model\n\n    # Evaluate on the test set\n    test_encodings, test_labels = encode_data(tokenizer, test_df)\n    test_dataset = CustomDataset(test_encodings, test_labels)\n    final_eval_results = trainer.evaluate(test_dataset)\n    print(\"Ansatz (1) - Final Test Results:\", final_eval_results)\n\n    # Save the final model and tokenizer\n    trainer.save_model(out_dir)\n    tokenizer.save_pretrained(out_dir)\n\n    return model, tokenizer, trainer\n</code></pre>"},{"location":"api/model_training/attitude/#model_training.nlp.attitude_classifier.functions_for_docu.train_approach_2","title":"<code>train_approach_2(model_class, tokenizer_class, model_path, model_name)</code>","text":"<p>Ansatz (2): Train for 2 epochs with oversampling.</p> <p>Parameters:</p> Name Type Description Default <code>model_class</code> <p>The class for the Hugging Face model.</p> required <code>tokenizer_class</code> <p>The class for the Hugging Face tokenizer.</p> required <code>model_path</code> <p>Path to the pretrained model.</p> required <code>model_name</code> <p>Name of the model (used for saving results).</p> required <p>Returns:</p> Name Type Description <code>model</code> <p>The trained model.</p> <code>tokenizer</code> <p>The tokenizer used for training.</p> <code>trainer</code> <p>The Hugging Face Trainer instance.</p> Source code in <code>model_training\\nlp\\attitude_classifier\\functions_for_docu.py</code> <pre><code>def train_approach_2(model_class, tokenizer_class, model_path, model_name):\n    \"\"\"\n    Ansatz (2): Train for 2 epochs with oversampling.\n\n    Parameters:\n        model_class: The class for the Hugging Face model.\n        tokenizer_class: The class for the Hugging Face tokenizer.\n        model_path: Path to the pretrained model.\n        model_name: Name of the model (used for saving results).\n\n    Returns:\n        model: The trained model.\n        tokenizer: The tokenizer used for training.\n        trainer: The Hugging Face Trainer instance.\n    \"\"\"\n    # Oversample the training dataset\n    oversampled_train_df = oversample_dataframe(train_df)\n    out_dir = f\"./results_approach2_{model_name}\"\n\n    # Load the pretrained model and tokenizer\n    model = model_class.from_pretrained(\n        model_path,\n        num_labels=len(all_labels_list),\n        problem_type=\"single_label_classification\",\n        ignore_mismatched_sizes=True\n    )\n    tokenizer = tokenizer_class.from_pretrained(model_path)\n\n    # Create a Trainer with the oversampled dataset\n    trainer = create_trainer(model, tokenizer, oversampled_train_df, val_df, num_epochs=4, output_dir=out_dir)\n    trainer.train()  # Train the model\n\n    # Evaluate on the test set\n    test_encodings, test_labels = encode_data(tokenizer, test_df)\n    test_dataset = CustomDataset(test_encodings, test_labels)\n    final_eval_results = trainer.evaluate(test_dataset)\n    print(\"Ansatz (2) - Final Test Results:\", final_eval_results)\n\n    # Save the final model and tokenizer\n    trainer.save_model(out_dir)\n    tokenizer.save_pretrained(out_dir)\n\n    return model, tokenizer, trainer\n</code></pre>"},{"location":"api/model_training/attitude/#model_training.nlp.attitude_classifier.functions_for_docu.train_approach_3","title":"<code>train_approach_3(model_class, tokenizer_class, model_path, model_name)</code>","text":"<p>Ansatz (3): Train for 1 epoch with oversampling followed by 1 epoch without oversampling.</p> <p>Parameters:</p> Name Type Description Default <code>model_class</code> <p>The class for the Hugging Face model.</p> required <code>tokenizer_class</code> <p>The class for the Hugging Face tokenizer.</p> required <code>model_path</code> <p>Path to the pretrained model.</p> required <code>model_name</code> <p>Name of the model (used for saving results).</p> required <p>Returns:</p> Name Type Description <code>model</code> <p>The trained model after both phases.</p> <code>tokenizer</code> <p>The tokenizer used for training.</p> <code>trainer</code> <p>The Hugging Face Trainer instance after phase B.</p> Source code in <code>model_training\\nlp\\attitude_classifier\\functions_for_docu.py</code> <pre><code>def train_approach_3(model_class, tokenizer_class, model_path, model_name):\n    \"\"\"\n    Ansatz (3): Train for 1 epoch with oversampling followed by 1 epoch without oversampling.\n\n    Parameters:\n        model_class: The class for the Hugging Face model.\n        tokenizer_class: The class for the Hugging Face tokenizer.\n        model_path: Path to the pretrained model.\n        model_name: Name of the model (used for saving results).\n\n    Returns:\n        model: The trained model after both phases.\n        tokenizer: The tokenizer used for training.\n        trainer: The Hugging Face Trainer instance after phase B.\n    \"\"\"\n    out_dir_stepA = f\"./results_approach3_stepA_{model_name}\"  # Folder for Step A results\n    out_dir_stepB = f\"./results_approach3_stepB_{model_name}\"  # Folder for Step B results\n\n    # Step A: Train with oversampled dataset\n    oversampled_train_df = oversample_dataframe(train_df)\n    model_stepA = model_class.from_pretrained(\n        model_path,\n        num_labels=len(all_labels_list),\n        problem_type=\"single_label_classification\",\n        ignore_mismatched_sizes=True\n    )\n    tokenizer_stepA = tokenizer_class.from_pretrained(model_path)\n\n    trainer_stepA = create_trainer(model_stepA, tokenizer_stepA, oversampled_train_df, val_df, num_epochs=2, output_dir=out_dir_stepA)\n    trainer_stepA.train()  # Train the model for Step A\n\n    # Step B: Continue training with the original (non-oversampled) dataset\n    trainer_stepB = create_trainer(model_stepA, tokenizer_stepA, train_df, val_df, num_epochs=2, output_dir=out_dir_stepB)\n    trainer_stepB.train()  # Train the model for Step B\n\n    # Evaluate on the test set\n    test_encodings, test_labels = encode_data(tokenizer_stepA, test_df)\n    test_dataset = CustomDataset(test_encodings, test_labels)\n    final_eval_results = trainer_stepB.evaluate(test_dataset)\n    print(\"Ansatz (3) - Final Test Results:\", final_eval_results)\n\n    # Save the final model and tokenizer after Step B\n    trainer_stepB.save_model(out_dir_stepB)\n    tokenizer_stepA.save_pretrained(out_dir_stepB)\n\n    return model_stepA, tokenizer_stepA, trainer_stepB\n</code></pre>"},{"location":"api/model_training/request_classifier/","title":"Request Classification Model Training Code Documentation","text":""},{"location":"api/model_training/request_classifier/#model_training.nlp.request_classifier.dataset_DISAPERE.add_index_to_jsonl","title":"<code>add_index_to_jsonl(input_file, output_file)</code>","text":"<p>Reads a JSONL file and adds a sequential 'index' field to each JSON object.</p> <p>Parameters:</p> Name Type Description Default <code>input_file</code> <code>str</code> <p>The path to the input JSONL file.</p> required <code>output_file</code> <code>str</code> <p>The path for the output JSONL file with index fields.</p> required <p>Returns:</p> Type Description <p>None</p> Source code in <code>model_training\\nlp\\request_classifier\\dataset_DISAPERE.py</code> <pre><code>def add_index_to_jsonl(input_file, output_file):\n    \"\"\"\n    Reads a JSONL file and adds a sequential 'index' field to each JSON object.\n\n    Args:\n        input_file (str): The path to the input JSONL file.\n        output_file (str): The path for the output JSONL file with index fields.\n\n    Returns:\n        None\n    \"\"\"\n    try:\n        with open(input_file, 'r', encoding='utf-8') as infile, \\\n             open(output_file, 'w', encoding='utf-8') as outfile:\n\n            for i, line in enumerate(infile):\n                obj = json.loads(line.strip())\n                obj[\"index\"] = i\n                outfile.write(json.dumps(obj, ensure_ascii=False) + '\\n')\n\n        print(f\"Added 'index' field to each entry in {input_file} -&gt; {output_file}\")\n    except Exception as e:\n        print(f\"Error adding index to JSONL {input_file}: {e}\")\n</code></pre>"},{"location":"api/model_training/request_classifier/#model_training.nlp.request_classifier.dataset_DISAPERE.add_unique_target_index_to_nested_list","title":"<code>add_unique_target_index_to_nested_list(input_file, output_file)</code>","text":"<p>Adds a unique 'target_index' field to each sentence in 'review_sentences', based on the 'review_action'. Each unique action is mapped to a unique index.</p> <p>Parameters:</p> Name Type Description Default <code>input_file</code> <code>str</code> <p>The input JSON file containing objects with 'review_sentences'.</p> required <code>output_file</code> <code>str</code> <p>The output JSON file to write the modified data.</p> required <p>Returns:</p> Type Description <p>None</p> Source code in <code>model_training\\nlp\\request_classifier\\dataset_DISAPERE.py</code> <pre><code>def add_unique_target_index_to_nested_list(input_file, output_file):\n    \"\"\"\n    Adds a unique 'target_index' field to each sentence in 'review_sentences',\n    based on the 'review_action'. Each unique action is mapped to a unique index.\n\n    Args:\n        input_file (str): The input JSON file containing objects with 'review_sentences'.\n        output_file (str): The output JSON file to write the modified data.\n\n    Returns:\n        None\n    \"\"\"\n    with open(input_file, \"r\", encoding=\"utf-8\") as infile:\n        data = json.load(infile)\n\n    action_to_index = {}\n    current_index = 0\n\n    for item in data:\n        for sentence in item.get(\"review_sentences\", []):\n            action = sentence.get(\"review_action\")\n            if action not in action_to_index:\n                action_to_index[action] = current_index\n                current_index += 1\n            sentence[\"target_index\"] = action_to_index[action]\n\n    try:\n        with open(output_file, \"w\", encoding=\"utf-8\") as outfile:\n            json.dump(data, outfile, ensure_ascii=False, indent=4)\n        print(f\"Added unique target indexes and saved to {output_file}\")\n    except Exception as e:\n        print(f\"Error writing to {output_file}: {e}\")\n</code></pre>"},{"location":"api/model_training/request_classifier/#model_training.nlp.request_classifier.dataset_DISAPERE.convert_json_to_jsonl","title":"<code>convert_json_to_jsonl(input_file, output_folder, target_action)</code>","text":"<p>Converts a JSON list file into a JSON Lines (.jsonl) file.</p> <p>Parameters:</p> Name Type Description Default <code>input_file</code> <code>str</code> <p>The path to the input JSON file (list of objects).</p> required <code>output_folder</code> <code>str</code> <p>The directory path to store the resulting JSONL file.</p> required <code>target_action</code> <code>str</code> <p>A label (e.g., \"train\" or \"arg_request\") to name the JSONL file.</p> required <p>Returns:</p> Type Description <p>None</p> Source code in <code>model_training\\nlp\\request_classifier\\dataset_DISAPERE.py</code> <pre><code>def convert_json_to_jsonl(input_file, output_folder, target_action):\n    \"\"\"\n    Converts a JSON list file into a JSON Lines (.jsonl) file.\n\n    Args:\n        input_file (str): The path to the input JSON file (list of objects).\n        output_folder (str): The directory path to store the resulting JSONL file.\n        target_action (str): A label (e.g., \"train\" or \"arg_request\") to name the JSONL file.\n\n    Returns:\n        None\n    \"\"\"\n    try:\n        with open(input_file, 'r', encoding='utf-8') as json_file:\n            data = json.load(json_file)\n\n            if not isinstance(data, list):\n                print(f\"Data in {input_file} is not a list. Conversion to JSONL aborted.\")\n                return\n\n            output_file = os.path.join(output_folder, f\"filtered_{target_action}.jsonl\")\n            with open(output_file, 'w', encoding='utf-8') as jsonl_file:\n                for item in data:\n                    jsonl_file.write(json.dumps(item, ensure_ascii=False) + '\\n')\n\n            print(f\"Converted {input_file} to JSONL: {output_file}\")\n    except Exception as e:\n        print(f\"Error converting {input_file} to JSONL: {e}\")\n</code></pre>"},{"location":"api/model_training/request_classifier/#model_training.nlp.request_classifier.dataset_DISAPERE.filter_instances_by_review_action","title":"<code>filter_instances_by_review_action(input_file, output_folder, target_action)</code>","text":"<p>Filters review sentences matching a specific 'review_action' value from a JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>input_file</code> <code>str</code> <p>The path to the merged JSON file.</p> required <code>output_folder</code> <code>str</code> <p>The directory to save the filtered output.</p> required <code>target_action</code> <code>str</code> <p>The 'review_action' value to filter (e.g., \"arg_request\").</p> required <p>Returns:</p> Type Description <p>None</p> Source code in <code>model_training\\nlp\\request_classifier\\dataset_DISAPERE.py</code> <pre><code>def filter_instances_by_review_action(input_file, output_folder, target_action):\n    \"\"\"\n    Filters review sentences matching a specific 'review_action' value from a JSON file.\n\n    Args:\n        input_file (str): The path to the merged JSON file.\n        output_folder (str): The directory to save the filtered output.\n        target_action (str): The 'review_action' value to filter (e.g., \"arg_request\").\n\n    Returns:\n        None\n    \"\"\"\n    filtered_instances = []\n\n    try:\n        with open(input_file, 'r', encoding='utf-8') as json_file:\n            merged_data = json.load(json_file)\n            for obj in merged_data:\n                for sentence in obj.get(\"review_sentences\", []):\n                    if sentence.get(\"review_action\") == target_action:\n                        filtered_instances.append(sentence)\n    except Exception as e:\n        print(f\"Error reading {input_file}: {e}\")\n        return\n\n    output_file = os.path.join(output_folder, f\"filtered_{target_action}.json\")\n\n    try:\n        with open(output_file, 'w', encoding='utf-8') as json_output_file:\n            json.dump(filtered_instances, json_output_file, ensure_ascii=False, indent=4)\n        print(f\"Filtered instances for action '{target_action}' saved to {output_file}\")\n    except Exception as e:\n        print(f\"Error writing {output_file}: {e}\")\n</code></pre>"},{"location":"api/model_training/request_classifier/#model_training.nlp.request_classifier.dataset_DISAPERE.filter_review_sentences","title":"<code>filter_review_sentences(input_file, output_file)</code>","text":"<p>Extracts all 'review_sentences' from each object in the JSON and writes them into a single flattened list.</p> <p>Parameters:</p> Name Type Description Default <code>input_file</code> <code>str</code> <p>The input JSON file containing review sentence objects.</p> required <code>output_file</code> <code>str</code> <p>The output JSON file where filtered data will be written.</p> required <p>Returns:</p> Type Description <p>None</p> Source code in <code>model_training\\nlp\\request_classifier\\dataset_DISAPERE.py</code> <pre><code>def filter_review_sentences(input_file, output_file):\n    \"\"\"\n    Extracts all 'review_sentences' from each object in the JSON and writes them\n    into a single flattened list.\n\n    Args:\n        input_file (str): The input JSON file containing review sentence objects.\n        output_file (str): The output JSON file where filtered data will be written.\n\n    Returns:\n        None\n    \"\"\"\n    with open(input_file, \"r\", encoding=\"utf-8\") as infile:\n        data = json.load(infile)\n\n    review_sentences = []\n    for item in data:\n        review_sentences.extend(item.get(\"review_sentences\", []))\n\n    try:\n        with open(output_file, \"w\", encoding=\"utf-8\") as outfile:\n            json.dump(review_sentences, outfile, ensure_ascii=False, indent=4)\n        print(f\"Filtered review sentences saved to {output_file}\")\n    except Exception as e:\n        print(f\"Error writing {output_file}: {e}\")\n</code></pre>"},{"location":"api/model_training/request_classifier/#model_training.nlp.request_classifier.dataset_DISAPERE.merge_json_files","title":"<code>merge_json_files(folder_path, output_file)</code>","text":"<p>Merges all JSON files within the specified folder into a single JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>folder_path</code> <code>str</code> <p>The directory path containing JSON files to merge.</p> required <code>output_file</code> <code>str</code> <p>The output JSON file path.</p> required <p>Returns:</p> Type Description <p>None</p> Source code in <code>model_training\\nlp\\request_classifier\\dataset_DISAPERE.py</code> <pre><code>def merge_json_files(folder_path, output_file):\n    \"\"\"\n    Merges all JSON files within the specified folder into a single JSON file.\n\n    Args:\n        folder_path (str): The directory path containing JSON files to merge.\n        output_file (str): The output JSON file path.\n\n    Returns:\n        None\n    \"\"\"\n    merged_data = []\n\n    for file_name in os.listdir(folder_path):\n        file_path = os.path.join(folder_path, file_name)\n        try:\n            with open(file_path, 'r', encoding='utf-8') as json_file:\n                data = json.load(json_file)\n                merged_data.append(data)\n        except Exception as e:\n            print(f\"Error reading {file_name}: {e}\")\n\n    try:\n        with open(output_file, 'w', encoding='utf-8') as output_json_file:\n            json.dump(merged_data, output_json_file, ensure_ascii=False, indent=4)\n        print(f\"Merged JSON file created: {output_file}\")\n    except Exception as e:\n        print(f\"Error writing {output_file}: {e}\")\n</code></pre>"},{"location":"api/model_training/request_classifier/#model_training.nlp.request_classifier.fine_request_classifier_llama.create_few_shot_prompt","title":"<code>create_few_shot_prompt(query, few_shot_examples)</code>","text":"<p>Creates a few-shot learning prompt for the T5 model.</p> <p>Parameters: query (str): The input sentence to classify. few_shot_examples (dict): Dictionary containing few-shot examples for each category.</p> <p>Returns: str: The generated few-shot learning prompt.</p> Source code in <code>model_training\\nlp\\request_classifier\\fine_request_classifier_llama.py</code> <pre><code>def create_few_shot_prompt(query, few_shot_examples):\n    \"\"\"\n    Creates a few-shot learning prompt for the T5 model.\n\n    Parameters:\n    query (str): The input sentence to classify.\n    few_shot_examples (dict): Dictionary containing few-shot examples for each category.\n\n    Returns:\n    str: The generated few-shot learning prompt.\n    \"\"\"\n    prompt = (\n        \"As an assistant, analyze the sentence and determine its category based on the reasoning.\\n\"\n        \"Select the area which is the subject or the type of request.\\n\\n\"\n        \"Categories:\\n\"\n        \"Request for Improvement\\n\"\n        \"Request for Explanation\\n\"\n        \"Request for Experiment\\n\"\n        \"Request for Typo Fix\\n\"\n        \"Request for Clarification\\n\"\n        \"Request for Result\\n\\n\"\n    )\n\n    # Add examples to the prompt\n    for label, sentences in few_shot_examples.items():\n        number = label_map[label] + 1\n        reasoning = \"Reasoning: \" + \" \".join(sentences[0].split()[:10]) + \"...\"\n        prompt += f\"Sentence: \\\"{sentences[0]}\\\"\\n{reasoning}\\nCategory Number: {number}\\n\\n\"\n\n    # Add the query sentence\n    prompt += f\"Sentence: \\\"{query}\\\"\\nReasoning:\"\n    return prompt\n</code></pre>"},{"location":"api/model_training/request_classifier/#model_training.nlp.request_classifier.fine_request_classifier_llama.evaluate_model","title":"<code>evaluate_model(dataset, predictions, label_map)</code>","text":"<p>Evaluates the model's predictions against true labels and displays results.</p> <p>Parameters: dataset (Dataset): Dataset containing true labels. predictions (list): List of predicted labels. label_map (dict): Mapping of categories to label indices.</p> <p>Returns: None</p> Source code in <code>model_training\\nlp\\request_classifier\\fine_request_classifier_llama.py</code> <pre><code>def evaluate_model(dataset, predictions, label_map):\n    \"\"\"\n    Evaluates the model's predictions against true labels and displays results.\n\n    Parameters:\n    dataset (Dataset): Dataset containing true labels.\n    predictions (list): List of predicted labels.\n    label_map (dict): Mapping of categories to label indices.\n\n    Returns:\n    None\n    \"\"\"\n    true_labels = dataset['fine_review_action'].map(lambda x: label_map[fine_to_category_map[x]]).tolist()\n    predicted_labels = [map_prediction_to_label(pred, label_map) for pred in predictions]\n\n    # Calculate metrics\n    accuracy = accuracy_score(true_labels, predicted_labels)\n    f1 = f1_score(true_labels, predicted_labels, average=\"weighted\", zero_division=0)\n    precision = precision_score(true_labels, predicted_labels, average=\"weighted\", zero_division=0)\n    recall = recall_score(true_labels, predicted_labels, average=\"weighted\", zero_division=0)\n\n    print(\"\\n--- Evaluation Results ---\")\n    print(f\"Accuracy: {accuracy:.4f}\")\n    print(f\"F1 Score: {f1:.4f}\")\n    print(f\"Precision: {precision:.4f}\")\n    print(f\"Recall: {recall:.4f}\")\n\n    # Display confusion matrix\n    cm = confusion_matrix(true_labels, predicted_labels, labels=list(label_map.values()))\n    display_labels = list(label_map.keys())\n    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=display_labels)\n    disp.plot(xticks_rotation='vertical', cmap='Blues')\n    plt.title('Confusion Matrix')\n    plt.show()\n</code></pre>"},{"location":"api/model_training/request_classifier/#model_training.nlp.request_classifier.fine_request_classifier_llama.generate_predictions_from_dataset","title":"<code>generate_predictions_from_dataset(dataset, few_shot_examples, tokenizer, model, max_new_tokens=50)</code>","text":"<p>Generates predictions for each sentence in the dataset using few-shot prompts.</p> <p>Parameters: dataset (Dataset): Dataset containing sentences to classify. few_shot_examples (dict): Dictionary of few-shot examples. tokenizer (T5Tokenizer): Tokenizer for the T5 model. model (T5ForConditionalGeneration): Pretrained T5 model. max_new_tokens (int): Maximum tokens to generate for each prediction.</p> <p>Returns: list: List of generated predictions.</p> Source code in <code>model_training\\nlp\\request_classifier\\fine_request_classifier_llama.py</code> <pre><code>def generate_predictions_from_dataset(dataset, few_shot_examples, tokenizer, model, max_new_tokens=50):\n    \"\"\"\n    Generates predictions for each sentence in the dataset using few-shot prompts.\n\n    Parameters:\n    dataset (Dataset): Dataset containing sentences to classify.\n    few_shot_examples (dict): Dictionary of few-shot examples.\n    tokenizer (T5Tokenizer): Tokenizer for the T5 model.\n    model (T5ForConditionalGeneration): Pretrained T5 model.\n    max_new_tokens (int): Maximum tokens to generate for each prediction.\n\n    Returns:\n    list: List of generated predictions.\n    \"\"\"\n    predictions = []\n    for query in tqdm(dataset[\"sentence\"], desc=\"Generating predictions\"):\n        few_shot_prompt = create_few_shot_prompt(query, few_shot_examples)\n        inputs = tokenizer(\n            few_shot_prompt,\n            return_tensors=\"pt\",\n            padding=True,\n            truncation=True,\n            max_length=512,\n        )\n        input_ids = inputs[\"input_ids\"].to(model.device)\n        attention_mask = inputs[\"attention_mask\"].to(model.device)\n\n        outputs = model.generate(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            max_new_tokens=max_new_tokens,\n            temperature=0.8,\n            top_p=0.9,\n            do_sample=True,\n            pad_token_id=tokenizer.pad_token_id,\n            eos_token_id=tokenizer.eos_token_id,\n        )\n\n        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n        predictions.append(generated_text.strip())\n    return predictions\n</code></pre>"},{"location":"api/model_training/request_classifier/#model_training.nlp.request_classifier.fine_request_classifier_llama.load_model_and_tokenizer","title":"<code>load_model_and_tokenizer(save_directory)</code>","text":"<p>Loads a saved model and tokenizer from the specified directory.</p> <p>Parameters: save_directory (str): Directory from where the model and tokenizer will be loaded.</p> <p>Returns: tuple: Loaded model and tokenizer.</p> Source code in <code>model_training\\nlp\\request_classifier\\fine_request_classifier_llama.py</code> <pre><code>def load_model_and_tokenizer(save_directory):\n    \"\"\"\n    Loads a saved model and tokenizer from the specified directory.\n\n    Parameters:\n    save_directory (str): Directory from where the model and tokenizer will be loaded.\n\n    Returns:\n    tuple: Loaded model and tokenizer.\n    \"\"\"\n    print(f\"Loading model and tokenizer from {save_directory}...\")\n    tokenizer = T5Tokenizer.from_pretrained(save_directory)\n    model = T5ForConditionalGeneration.from_pretrained(save_directory)\n    print(\"Loading successful.\")\n    return model, tokenizer\n</code></pre>"},{"location":"api/model_training/request_classifier/#model_training.nlp.request_classifier.fine_request_classifier_llama.map_prediction_to_label","title":"<code>map_prediction_to_label(pred, label_map)</code>","text":"<p>Maps model prediction to a corresponding label in the label map.</p> <p>Parameters: pred (str): The prediction text generated by the model. label_map (dict): Dictionary mapping categories to their label indices.</p> <p>Returns: int: Mapped label index or -1 if not found.</p> Source code in <code>model_training\\nlp\\request_classifier\\fine_request_classifier_llama.py</code> <pre><code>def map_prediction_to_label(pred, label_map):\n    \"\"\"\n    Maps model prediction to a corresponding label in the label map.\n\n    Parameters:\n    pred (str): The prediction text generated by the model.\n    label_map (dict): Dictionary mapping categories to their label indices.\n\n    Returns:\n    int: Mapped label index or -1 if not found.\n    \"\"\"\n    pred = pred.strip().lower()\n    pred = pred.strip('.\"\\'&lt;&gt;/ ').lower()\n\n    # Exact match with label\n    for label in label_map:\n        if pred == label.lower():\n            return label_map[label]\n\n    # Partial match\n    for label in label_map:\n        if label.lower() in pred:\n            return label_map[label]\n\n    # Match with numeric category\n    for idx, label in enumerate(label_map.keys(), 1):\n        if pred == str(idx) or pred == f\"{idx}.\":\n            return label_map[label]\n\n    return -1  # Default for unmatched predictions\n</code></pre>"},{"location":"api/model_training/request_classifier/#model_training.nlp.request_classifier.fine_request_classifier_llama.save_model_and_tokenizer","title":"<code>save_model_and_tokenizer(model, tokenizer, save_directory)</code>","text":"<p>Saves the model and tokenizer to the specified directory.</p> <p>Parameters: model (T5ForConditionalGeneration): Trained T5 model. tokenizer (T5Tokenizer): Tokenizer for the T5 model. save_directory (str): Directory to save the model and tokenizer.</p> <p>Returns: None</p> Source code in <code>model_training\\nlp\\request_classifier\\fine_request_classifier_llama.py</code> <pre><code>def save_model_and_tokenizer(model, tokenizer, save_directory):\n    \"\"\"\n    Saves the model and tokenizer to the specified directory.\n\n    Parameters:\n    model (T5ForConditionalGeneration): Trained T5 model.\n    tokenizer (T5Tokenizer): Tokenizer for the T5 model.\n    save_directory (str): Directory to save the model and tokenizer.\n\n    Returns:\n    None\n    \"\"\"\n    print(f\"Saving model and tokenizer in {save_directory}...\")\n    model.save_pretrained(save_directory)\n    tokenizer.save_pretrained(save_directory)\n    print(\"Model and tokenizer saved.\")\n</code></pre>"},{"location":"api/model_training/request_classifier/#model_training.nlp.request_classifier.fine_request_classifier_model_selection.MetricsLogger","title":"<code>MetricsLogger</code>","text":"<p>               Bases: <code>TrainerCallback</code></p> <p>Custom callback to log evaluation metrics after each epoch.</p> Source code in <code>model_training\\nlp\\request_classifier\\fine_request_classifier_model_selection.py</code> <pre><code>class MetricsLogger(TrainerCallback):\n    \"\"\"\n    Custom callback to log evaluation metrics after each epoch.\n    \"\"\"\n    def on_evaluate(self, args, state: TrainerState, control: TrainerControl, metrics, **kwargs):\n        print(f\"Metrics after epoch {state.epoch}: {metrics}\")\n</code></pre>"},{"location":"api/model_training/request_classifier/#model_training.nlp.request_classifier.fine_request_classifier_model_selection.apply_thresholds","title":"<code>apply_thresholds(logits, class_thresholds)</code>","text":"<p>Applies class-specific thresholds to predictions. For each example:   1) Convert logits -&gt; softmax probabilities.   2) For each class, check if p &gt;= class_thresholds[class_idx].   3) If multiple classes meet their threshold, pick the highest-probability one.   4) If none meet their threshold, pick the argmax class.</p> <p>Parameters:</p> Name Type Description Default <code>logits</code> <code>Tensor</code> <p>Model output logits.</p> required <code>class_thresholds</code> <code>dict</code> <p>A dictionary mapping class_idx -&gt; threshold (e.g. {0: 0.5, 1:0.4, ...}).</p> required <p>Returns:</p> Name Type Description <code>list</code> <p>A list of predicted class indices.</p> Source code in <code>model_training\\nlp\\request_classifier\\fine_request_classifier_model_selection.py</code> <pre><code>def apply_thresholds(logits, class_thresholds):\n    \"\"\"\n    Applies class-specific thresholds to predictions. For each example:\n      1) Convert logits -&gt; softmax probabilities.\n      2) For each class, check if p &gt;= class_thresholds[class_idx].\n      3) If multiple classes meet their threshold, pick the highest-probability one.\n      4) If none meet their threshold, pick the argmax class.\n\n    Args:\n        logits (torch.Tensor): Model output logits.\n        class_thresholds (dict): A dictionary mapping class_idx -&gt; threshold (e.g. {0: 0.5, 1:0.4, ...}).\n\n    Returns:\n        list: A list of predicted class indices.\n    \"\"\"\n    probabilities = torch.softmax(torch.tensor(logits), dim=-1)\n    predictions = []\n\n    for prob in probabilities:\n        valid_classes = []\n        for class_idx, p in enumerate(prob):\n            # Default threshold is 0.5 if not provided\n            if p &gt;= class_thresholds.get(class_idx, 0.5):\n                valid_classes.append((class_idx, p))\n\n        if valid_classes:\n            predicted_class = max(valid_classes, key=lambda x: x[1])[0]\n        else:\n            predicted_class = torch.argmax(prob).item()\n\n        predictions.append(predicted_class)\n    return predictions\n</code></pre>"},{"location":"api/model_training/request_classifier/#model_training.nlp.request_classifier.fine_request_classifier_model_selection.clean_dataset","title":"<code>clean_dataset(dataset)</code>","text":"Cleans and filters a Hugging Face Dataset <ul> <li>Removes sentences shorter than 10 characters or with fewer than 1 whitespace.</li> <li>Converts all text to lowercase.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Dataset</code> <p>A Hugging Face Dataset object.</p> required <p>Returns:</p> Name Type Description <code>Dataset</code> <p>A new dataset with cleaned sentences.</p> Source code in <code>model_training\\nlp\\request_classifier\\fine_request_classifier_model_selection.py</code> <pre><code>def clean_dataset(dataset):\n    \"\"\"\n    Cleans and filters a Hugging Face Dataset:\n      - Removes sentences shorter than 10 characters or with fewer than 1 whitespace.\n      - Converts all text to lowercase.\n\n    Args:\n        dataset (Dataset): A Hugging Face Dataset object.\n\n    Returns:\n        Dataset: A new dataset with cleaned sentences.\n    \"\"\"\n    def clean_sentence(example):\n        sentence = example['sentence']\n        if isinstance(sentence, str):\n            sentence = sentence.strip().lower()\n            if len(sentence) &gt;= 10 and sentence.count(\" \") &gt;= 1:\n                return sentence\n        return None\n\n    # Filter out invalid sentences\n    dataset = dataset.filter(lambda example: clean_sentence(example) is not None)\n    dataset = dataset.map(lambda example: {\"sentence\": clean_sentence(example)})\n    return dataset\n</code></pre>"},{"location":"api/model_training/request_classifier/#model_training.nlp.request_classifier.fine_request_classifier_model_selection.clean_special_characters","title":"<code>clean_special_characters(dataset, column='sentence')</code>","text":"<p>Removes special characters (e.g., leading/trailing quotes) from the specified column in the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Dataset</code> <p>A Hugging Face Dataset object.</p> required <code>column</code> <code>str</code> <p>The column name to clean. Defaults to \"sentence\".</p> <code>'sentence'</code> <p>Returns:</p> Name Type Description <code>Dataset</code> <p>A new dataset with cleaned values in the specified column.</p> Source code in <code>model_training\\nlp\\request_classifier\\fine_request_classifier_model_selection.py</code> <pre><code>def clean_special_characters(dataset, column=\"sentence\"):\n    \"\"\"\n    Removes special characters (e.g., leading/trailing quotes) from the specified column in the dataset.\n\n    Args:\n        dataset (Dataset): A Hugging Face Dataset object.\n        column (str, optional): The column name to clean. Defaults to \"sentence\".\n\n    Returns:\n        Dataset: A new dataset with cleaned values in the specified column.\n    \"\"\"\n    def clean_example(example):\n        if isinstance(example[column], str):\n            example[column] = example[column].strip().replace('\"', '')\n        return example\n\n    return dataset.map(clean_example)\n</code></pre>"},{"location":"api/model_training/request_classifier/#model_training.nlp.request_classifier.fine_request_classifier_model_selection.compute_metrics","title":"<code>compute_metrics(eval_pred)</code>","text":"<p>Computes evaluation metrics for a classification model.</p> <p>Parameters:</p> Name Type Description Default <code>eval_pred</code> <code>tuple</code> <p>A tuple containing (logits, labels).</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>Dictionary of metric names mapped to their scores.</p> Source code in <code>model_training\\nlp\\request_classifier\\fine_request_classifier_model_selection.py</code> <pre><code>def compute_metrics(eval_pred):\n    \"\"\"\n    Computes evaluation metrics for a classification model.\n\n    Args:\n        eval_pred (tuple): A tuple containing (logits, labels).\n\n    Returns:\n        dict: Dictionary of metric names mapped to their scores.\n    \"\"\"\n    logits, labels = eval_pred\n    predictions = torch.argmax(torch.tensor(logits), dim=-1)\n    return {\n        \"accuracy\": accuracy_score(labels, predictions),\n        \"f1\": f1_score(labels, predictions, average=\"weighted\"),\n        \"precision\": precision_score(labels, predictions, average=\"weighted\"),\n        \"recall\": recall_score(labels, predictions, average=\"weighted\"),\n    }\n</code></pre>"},{"location":"api/model_training/request_classifier/#model_training.nlp.request_classifier.fine_request_classifier_model_selection.load_data","title":"<code>load_data(do_cleaning=True)</code>","text":"<p>Loads the dataset from CSV files for train, validation, and test splits, and optionally cleans each split.</p> <p>Parameters:</p> Name Type Description Default <code>do_cleaning</code> <code>bool</code> <p>Whether to apply data cleaning. Defaults to True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>DatasetDict</code> <p>A Hugging Face DatasetDict with train, validation, and test splits.</p> Source code in <code>model_training\\nlp\\request_classifier\\fine_request_classifier_model_selection.py</code> <pre><code>def load_data(do_cleaning=True):\n    \"\"\"\n    Loads the dataset from CSV files for train, validation, and test splits,\n    and optionally cleans each split.\n\n    Args:\n        do_cleaning (bool, optional): Whether to apply data cleaning. Defaults to True.\n\n    Returns:\n        DatasetDict: A Hugging Face DatasetDict with train, validation, and test splits.\n    \"\"\"\n    data_files = {\n        \"train\": \"model_training/nlp/request_classifier/DISAPERE/final_dataset/fine_request/train.csv\",\n        \"validation\": \"model_training/nlp/request_classifier/DISAPERE/final_dataset/fine_request/test.csv\",\n        \"test\": \"model_training/nlp/request_classifier/DISAPERE/final_dataset/fine_request/dev.csv\",\n    }\n\n    data = load_dataset(\"csv\", data_files=data_files)\n\n    if do_cleaning:\n        for split in [\"train\", \"validation\", \"test\"]:\n            data[split] = clean_dataset(data[split])\n            data[split] = clean_special_characters(data[split], \"sentence\")\n\n    return data\n</code></pre>"},{"location":"api/model_training/request_classifier/#model_training.nlp.request_classifier.fine_request_classifier_model_selection.main","title":"<code>main()</code>","text":"<p>Main function to orchestrate training over multiple configurations. Writes results (metrics and confusion matrices) to a specified file.</p> Source code in <code>model_training\\nlp\\request_classifier\\fine_request_classifier_model_selection.py</code> <pre><code>def main():\n    \"\"\"\n    Main function to orchestrate training over multiple configurations.\n    Writes results (metrics and confusion matrices) to a specified file.\n    \"\"\"\n    results_file_path = \"metrics_results.txt\"\n\n    # Start fresh results file\n    with open(results_file_path, \"w\", encoding=\"utf-8\") as f:\n        current_time = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n        f.write(f\"Metrics log started at {current_time}\\n\\n\")\n\n    # List of model names or paths to evaluate\n    model_names = [\n        \"model_training/nlp/request_classifier/models/classification/sciBERT_neg/\"\n    ]\n\n    # Example thresholds for 6 classes\n    class_thresholds = {\n        0: 0.35,\n        1: 0.35,\n        2: 0.25,\n        3: 0.20,\n        4: 0.25,\n        5: 0.35\n    }\n\n    # Define different parameter combinations to test\n    missing_combos = [\n        (False, False, True),   # cleaning=True, oversampling=False, thresholding=True\n    ]\n\n    for model_name in model_names:\n        for (cleaning, oversampling, thresholding) in missing_combos:\n            print(\"=\" * 60)\n            print(f\"Running {model_name}\")\n            print(f\"  Cleaning:     {cleaning}\")\n            print(f\"  Oversampling: {oversampling}\")\n            print(f\"  Thresholding: {thresholding}\")\n            print(\"=\" * 60)\n\n            train_and_evaluate_model(\n                model_name=model_name,\n                do_cleaning=cleaning,\n                do_oversampling=oversampling,\n                num_epochs=5,\n                batch_size=16,\n                results_file_path=results_file_path,\n                do_thresholding=thresholding,\n                class_thresholds=class_thresholds,\n                num_labels=6\n            )\n</code></pre>"},{"location":"api/model_training/request_classifier/#model_training.nlp.request_classifier.fine_request_classifier_model_selection.oversample_minority_class","title":"<code>oversample_minority_class(dataset, label_col='target')</code>","text":"<p>Oversamples minority classes in the dataset to balance class distribution.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Dataset</code> <p>A Hugging Face Dataset.</p> required <code>label_col</code> <code>str</code> <p>The column name containing labels. Defaults to \"target\".</p> <code>'target'</code> <p>Returns:</p> Name Type Description <code>Dataset</code> <p>A new Hugging Face Dataset with balanced class distribution.</p> Source code in <code>model_training\\nlp\\request_classifier\\fine_request_classifier_model_selection.py</code> <pre><code>def oversample_minority_class(dataset, label_col=\"target\"):\n    \"\"\"\n    Oversamples minority classes in the dataset to balance class distribution.\n\n    Args:\n        dataset (Dataset): A Hugging Face Dataset.\n        label_col (str, optional): The column name containing labels. Defaults to \"target\".\n\n    Returns:\n        Dataset: A new Hugging Face Dataset with balanced class distribution.\n    \"\"\"\n    df = dataset.to_pandas()\n    class_counts = df[label_col].value_counts()\n    max_count = class_counts.max()\n\n    balanced_dfs = []\n    for cls in class_counts.index:\n        df_cls = df[df[label_col] == cls]\n        if len(df_cls) &lt; max_count:\n            df_cls = df_cls.sample(max_count, replace=True, random_state=42)\n        balanced_dfs.append(df_cls)\n\n    balanced_df = pd.concat(balanced_dfs).sample(frac=1.0, random_state=42).reset_index(drop=True)\n    return Dataset.from_pandas(balanced_df, preserve_index=False)\n</code></pre>"},{"location":"api/model_training/request_classifier/#model_training.nlp.request_classifier.fine_request_classifier_model_selection.predict","title":"<code>predict(texts, model_path='./bert_request_classifier_model')</code>","text":"<p>Predicts labels for a list of texts using a trained BERT model.</p> <p>Parameters:</p> Name Type Description Default <code>texts</code> <code>list</code> <p>A list of text inputs to classify.</p> required <code>model_path</code> <code>str</code> <p>Path or name of the model checkpoint. Defaults to \"./bert_request_classifier_model\".</p> <code>'./bert_request_classifier_model'</code> <p>Returns:</p> Type Description <p>np.ndarray: An array of predicted label indices.</p> Source code in <code>model_training\\nlp\\request_classifier\\fine_request_classifier_model_selection.py</code> <pre><code>def predict(texts, model_path=\"./bert_request_classifier_model\"):\n    \"\"\"\n    Predicts labels for a list of texts using a trained BERT model.\n\n    Args:\n        texts (list): A list of text inputs to classify.\n        model_path (str, optional): Path or name of the model checkpoint. Defaults to \"./bert_request_classifier_model\".\n\n    Returns:\n        np.ndarray: An array of predicted label indices.\n    \"\"\"\n    tokenizer = BertTokenizer.from_pretrained(model_path)\n    model = BertForSequenceClassification.from_pretrained(model_path)\n    model.eval()\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n\n    inputs = tokenizer(\n        texts,\n        return_tensors=\"pt\",\n        padding=True,\n        truncation=True,\n        max_length=128,\n    )\n    inputs = {k: v.to(device) for k, v in inputs.items()}\n\n    with torch.no_grad():\n        outputs = model(**inputs)\n    logits = outputs.logits\n    return torch.argmax(logits, dim=-1).cpu().numpy()\n</code></pre>"},{"location":"api/model_training/request_classifier/#model_training.nlp.request_classifier.fine_request_classifier_model_selection.prepare_datasets","title":"<code>prepare_datasets(tokenizer, do_cleaning=True, do_oversampling=True)</code>","text":"<p>Loads, optionally cleans, optionally oversamples, then tokenizes the train, validation, and test datasets.</p> <p>Parameters:</p> Name Type Description Default <code>tokenizer</code> <code>BertTokenizer</code> <p>A Hugging Face tokenizer.</p> required <code>do_cleaning</code> <code>bool</code> <p>Apply data cleaning if True. Defaults to True.</p> <code>True</code> <code>do_oversampling</code> <code>bool</code> <p>Oversample minority classes if True. Defaults to True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>tuple</code> <p>tokenized_train, tokenized_val, tokenized_test (Dataset objects)</p> Source code in <code>model_training\\nlp\\request_classifier\\fine_request_classifier_model_selection.py</code> <pre><code>def prepare_datasets(tokenizer, do_cleaning=True, do_oversampling=True):\n    \"\"\"\n    Loads, optionally cleans, optionally oversamples, then tokenizes\n    the train, validation, and test datasets.\n\n    Args:\n        tokenizer (BertTokenizer): A Hugging Face tokenizer.\n        do_cleaning (bool, optional): Apply data cleaning if True. Defaults to True.\n        do_oversampling (bool, optional): Oversample minority classes if True. Defaults to True.\n\n    Returns:\n        tuple: tokenized_train, tokenized_val, tokenized_test (Dataset objects)\n    \"\"\"\n    # 1) Load data\n    data = load_data(do_cleaning=do_cleaning)\n\n    # 2) Oversampling only on training split\n    if do_oversampling:\n        train_dataset = oversample_minority_class(data[\"train\"], label_col=\"target\")\n    else:\n        train_dataset = data[\"train\"]\n\n    val_dataset = data[\"validation\"]\n    test_dataset = data[\"test\"]\n\n    # 3) Tokenize\n    def map_fn(x): \n        return tokenize_function(x, tokenizer)\n\n    tokenized_train = train_dataset.map(map_fn, batched=True)\n    tokenized_val = val_dataset.map(map_fn, batched=True)\n    tokenized_test = test_dataset.map(map_fn, batched=True)\n\n    # 4) Rename the label column to \"labels\" and set the format to PyTorch\n    tokenized_train = tokenized_train.rename_column(\"target\", \"labels\")\n    tokenized_train.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n\n    tokenized_val = tokenized_val.rename_column(\"target\", \"labels\")\n    tokenized_val.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n\n    tokenized_test = tokenized_test.rename_column(\"target\", \"labels\")\n    tokenized_test.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n\n    return tokenized_train, tokenized_val, tokenized_test\n</code></pre>"},{"location":"api/model_training/request_classifier/#model_training.nlp.request_classifier.fine_request_classifier_model_selection.tokenize_function","title":"<code>tokenize_function(example, tokenizer)</code>","text":"<p>Tokenizes text data for BERT input.</p> <p>Parameters:</p> Name Type Description Default <code>example</code> <code>dict</code> <p>A dictionary with key \"sentence\" for the text to be tokenized.</p> required <code>tokenizer</code> <code>BertTokenizer</code> <p>A Hugging Face tokenizer instance.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary containing input_ids, attention_mask, etc.</p> Source code in <code>model_training\\nlp\\request_classifier\\fine_request_classifier_model_selection.py</code> <pre><code>def tokenize_function(example, tokenizer):\n    \"\"\"\n    Tokenizes text data for BERT input.\n\n    Args:\n        example (dict): A dictionary with key \"sentence\" for the text to be tokenized.\n        tokenizer (BertTokenizer): A Hugging Face tokenizer instance.\n\n    Returns:\n        dict: A dictionary containing input_ids, attention_mask, etc.\n    \"\"\"\n    return tokenizer(\n        example[\"sentence\"],\n        padding=\"max_length\",\n        truncation=True,\n        max_length=128,\n        return_attention_mask=True,\n    )\n</code></pre>"},{"location":"api/model_training/request_classifier/#model_training.nlp.request_classifier.fine_request_classifier_model_selection.train_and_evaluate_model","title":"<code>train_and_evaluate_model(model_name, do_cleaning=False, do_oversampling=False, num_epochs=5, batch_size=16, results_file_path='metrics_results.txt', do_thresholding=False, class_thresholds=None, num_labels=6)</code>","text":"<p>Trains a BERT-based model for sequence classification.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Model identifier, either on Hugging Face Hub (e.g., \"bert-base-uncased\")               or a local path (e.g., \"classification/custom_model\").</p> required <code>do_cleaning</code> <code>bool</code> <p>Whether to clean the data. Defaults to False.</p> <code>False</code> <code>do_oversampling</code> <code>bool</code> <p>Whether to oversample minority classes. Defaults to False.</p> <code>False</code> <code>num_epochs</code> <code>int</code> <p>Number of training epochs. Defaults to 5.</p> <code>5</code> <code>batch_size</code> <code>int</code> <p>Training/evaluation batch size. Defaults to 16.</p> <code>16</code> <code>results_file_path</code> <code>str</code> <p>Path to write evaluation metrics. Defaults to \"metrics_results.txt\".</p> <code>'metrics_results.txt'</code> <code>do_thresholding</code> <code>bool</code> <p>Whether to apply class-specific thresholds. Defaults to False.</p> <code>False</code> <code>class_thresholds</code> <code>dict</code> <p>Threshold values per class index. Defaults to None.</p> <code>None</code> <code>num_labels</code> <code>int</code> <p>Number of output classes. Defaults to 6.</p> <code>6</code> Source code in <code>model_training\\nlp\\request_classifier\\fine_request_classifier_model_selection.py</code> <pre><code>def train_and_evaluate_model(\n    model_name,\n    do_cleaning=False,\n    do_oversampling=False,\n    num_epochs=5,\n    batch_size=16,\n    results_file_path=\"metrics_results.txt\",\n    do_thresholding=False,\n    class_thresholds=None,\n    num_labels=6\n):\n    \"\"\"\n    Trains a BERT-based model for sequence classification.\n\n    Args:\n        model_name (str): Model identifier, either on Hugging Face Hub (e.g., \"bert-base-uncased\")\n                          or a local path (e.g., \"classification/custom_model\").\n        do_cleaning (bool, optional): Whether to clean the data. Defaults to False.\n        do_oversampling (bool, optional): Whether to oversample minority classes. Defaults to False.\n        num_epochs (int, optional): Number of training epochs. Defaults to 5.\n        batch_size (int, optional): Training/evaluation batch size. Defaults to 16.\n        results_file_path (str, optional): Path to write evaluation metrics. Defaults to \"metrics_results.txt\".\n        do_thresholding (bool, optional): Whether to apply class-specific thresholds. Defaults to False.\n        class_thresholds (dict, optional): Threshold values per class index. Defaults to None.\n        num_labels (int, optional): Number of output classes. Defaults to 6.\n    \"\"\"\n    # 1) Load tokenizer and model (distinguish local path vs. HF Hub)\n    if model_name.startswith(\"model_training/\"):\n        tokenizer = BertTokenizer.from_pretrained(model_name, local_files_only=True)\n        model = BertForSequenceClassification.from_pretrained(model_name, local_files_only=True, num_labels=num_labels)\n    else:\n        tokenizer = BertTokenizer.from_pretrained(model_name)\n        model = BertForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n\n    # 2) Prepare datasets\n    train_ds, val_ds, test_ds = prepare_datasets(\n        tokenizer,\n        do_cleaning=do_cleaning,\n        do_oversampling=do_oversampling\n    )\n\n    # 3) Training Arguments\n    training_args = TrainingArguments(\n        output_dir=\"./results\",\n        evaluation_strategy=\"epoch\",\n        save_strategy=\"epoch\",\n        logging_dir=\"./logs\",\n        learning_rate=2e-5,\n        per_device_train_batch_size=batch_size,\n        per_device_eval_batch_size=batch_size,\n        num_train_epochs=num_epochs,\n        weight_decay=0.01,\n        logging_steps=50,\n        load_best_model_at_end=True,\n    )\n\n    # 4) Instantiate the Trainer\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_ds,\n        eval_dataset=val_ds,\n        compute_metrics=compute_metrics,\n        callbacks=[MetricsLogger()],\n    )\n\n    # 5) Train the model\n    trainer.train()\n\n    # 6) Evaluate on validation and test sets\n    val_metrics = trainer.evaluate(eval_dataset=val_ds)\n    test_metrics = trainer.evaluate(eval_dataset=test_ds)\n\n    # 7) Generate Predictions and Confusion Matrix\n    test_output = trainer.predict(test_ds)\n    pred_logits = test_output.predictions\n    true_labels = test_output.label_ids\n\n    if do_thresholding and class_thresholds is not None:\n        # Use custom thresholding\n        pred_labels = apply_thresholds(pred_logits, class_thresholds)\n        cm = confusion_matrix(true_labels, pred_labels)\n    else:\n        pred_labels = np.argmax(pred_logits, axis=-1)\n        cm = confusion_matrix(true_labels, pred_labels)\n\n    # 8) Write results to file\n    with open(results_file_path, \"a\", encoding=\"utf-8\") as f:\n        f.write(\"==============================================\\n\")\n        f.write(f\"Model Name: {model_name}\\n\")\n        f.write(f\"Data Cleaning: {do_cleaning}\\n\")\n        f.write(f\"Oversampling: {do_oversampling}\\n\")\n        f.write(f\"Thresholding: {do_thresholding}\\n\")\n        f.write(f\"Epochs: {num_epochs}\\n\\n\")\n\n        f.write(\"Validation Metrics:\\n\")\n        f.write(f\"  Accuracy:  {val_metrics['eval_accuracy']:.4f}\\n\")\n        f.write(f\"  Precision: {val_metrics['eval_precision']:.4f}\\n\")\n        f.write(f\"  Recall:    {val_metrics['eval_recall']:.4f}\\n\")\n        f.write(f\"  F1-Score:  {val_metrics['eval_f1']:.4f}\\n\")\n\n        f.write(\"\\nTest Metrics:\\n\")\n        f.write(f\"  Accuracy:  {test_metrics['eval_accuracy']:.4f}\\n\")\n        f.write(f\"  Precision: {test_metrics['eval_precision']:.4f}\\n\")\n        f.write(f\"  Recall:    {test_metrics['eval_recall']:.4f}\\n\")\n        f.write(f\"  F1-Score:  {test_metrics['eval_f1']:.4f}\\n\\n\")\n\n        f.write(\"Confusion Matrix (Test):\\n\")\n        f.write(str(cm) + \"\\n\")\n        f.write(\"==============================================\\n\\n\")\n\n    save_path = \"model_training/nlp/request_classifier/models/classification/sciBERT_neg_finetuned/\"\n    os.makedirs(save_path, exist_ok=True)  # Erstelle den Zielordner, falls er nicht existiert\n    model.save_pretrained(save_path)  # Speichere das Modell\n    tokenizer.save_pretrained(save_path)  # Speichere auch den Tokenizer\n    print(f\"Model and tokenizer saved to {save_path}\")\n</code></pre>"},{"location":"api/model_training/request_classifier/#model_training.nlp.request_classifier.fine_request_prompt_selection.create_few_shot_prompt","title":"<code>create_few_shot_prompt(query, few_shot_examples)</code>","text":"<p>Builds a prompt using the given few_shot_examples (which can come from v1, v2, v3, etc.).</p> Source code in <code>model_training\\nlp\\request_classifier\\fine_request_prompt_selection.py</code> <pre><code>def create_few_shot_prompt(query, few_shot_examples):\n    \"\"\"\n    Builds a prompt using the given few_shot_examples (which can come from v1, v2, v3, etc.).\n    \"\"\"\n    prompt = (\n        \"As an assistant, analyze the sentence and determine its category based on the reasoning.\\n\"\n        \"Select the area which is the subject or the type of request.\\n\\n\"\n        \"Categories:\\n\"\n        \"Request for Improvement\\n\"\n        \"Request for Explanation\\n\"\n        \"Request for Experiment\\n\"\n        \"Request for Typo Fix\\n\"\n        \"Request for Clarification\\n\"\n        \"Request for Result\\n\\n\"\n    )\n    for label, sentences in few_shot_examples.items():\n        number = label_map[label] + 1\n        reasoning = \"Reasoning: \" + \" \".join(sentences[0].split()[:10]) + \"...\"\n        prompt += f\"Sentence: \\\"{sentences[0]}\\\"\\n{reasoning}\\nCategory Number: {number}\\n\\n\"\n    prompt += f\"Sentence: \\\"{query}\\\"\\nReasoning:\"\n    return prompt\n</code></pre>"},{"location":"api/model_training/request_classifier/#model_training.nlp.request_classifier.fine_request_prompt_selection.evaluate_model","title":"<code>evaluate_model(dataset, predictions, label_map)</code>","text":"<p>Calculates accuracy, F1, precision, and recall, and returns a confusion matrix.</p> Source code in <code>model_training\\nlp\\request_classifier\\fine_request_prompt_selection.py</code> <pre><code>def evaluate_model(dataset, predictions, label_map):\n    \"\"\"\n    Calculates accuracy, F1, precision, and recall, and returns a confusion matrix.\n    \"\"\"\n    true_labels = dataset['fine_review_action'].map(lambda x: label_map[fine_to_category_map[x]]).tolist()\n    predicted_labels = [map_prediction_to_label(pred, label_map) for pred in predictions]\n\n    accuracy = accuracy_score(true_labels, predicted_labels)\n    f1 = f1_score(true_labels, predicted_labels, average=\"weighted\", zero_division=0)\n    precision = precision_score(true_labels, predicted_labels, average=\"weighted\", zero_division=0)\n    recall = recall_score(true_labels, predicted_labels, average=\"weighted\", zero_division=0)\n    cm = confusion_matrix(true_labels, predicted_labels, labels=list(label_map.values()))\n    return accuracy, f1, precision, recall, cm\n</code></pre>"},{"location":"api/model_training/request_classifier/#model_training.nlp.request_classifier.fine_request_prompt_selection.generate_predictions_from_dataset","title":"<code>generate_predictions_from_dataset(dataset, few_shot_examples, tokenizer, model, max_new_tokens=50)</code>","text":"<p>Generates model predictions based on provided few_shot_examples.</p> Source code in <code>model_training\\nlp\\request_classifier\\fine_request_prompt_selection.py</code> <pre><code>def generate_predictions_from_dataset(dataset, few_shot_examples, tokenizer, model, max_new_tokens=50):\n    \"\"\"\n    Generates model predictions based on provided few_shot_examples.\n    \"\"\"\n    predictions = []\n    for query in tqdm(dataset[\"sentence\"], desc=\"Generating predictions\"):\n        few_shot_prompt = create_few_shot_prompt(query, few_shot_examples)\n        inputs = tokenizer(\n            few_shot_prompt,\n            return_tensors=\"pt\",\n            padding=True,\n            truncation=True,\n            max_length=512,\n        )\n        input_ids = inputs[\"input_ids\"].to(model.device)\n        attention_mask = inputs[\"attention_mask\"].to(model.device)\n\n        outputs = model.generate(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            max_new_tokens=max_new_tokens,\n            temperature=0.8,\n            top_p=0.9,\n            do_sample=True,\n            pad_token_id=tokenizer.pad_token_id,\n            eos_token_id=tokenizer.eos_token_id,\n        )\n        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n        predictions.append(generated_text.strip())\n    return predictions\n</code></pre>"},{"location":"api/model_training/request_classifier/#model_training.nlp.request_classifier.fine_request_prompt_selection.load_model_and_tokenizer","title":"<code>load_model_and_tokenizer(save_directory)</code>","text":"<p>Loads a saved model and tokenizer from the specified directory.</p> Source code in <code>model_training\\nlp\\request_classifier\\fine_request_prompt_selection.py</code> <pre><code>def load_model_and_tokenizer(save_directory):\n    \"\"\"\n    Loads a saved model and tokenizer from the specified directory.\n    \"\"\"\n    print(f\"Loading model and tokenizer from {save_directory}...\")\n    tokenizer = T5Tokenizer.from_pretrained(save_directory)\n    model = T5ForConditionalGeneration.from_pretrained(save_directory)\n    print(\"Loading successful.\")\n    return model, tokenizer\n</code></pre>"},{"location":"api/model_training/request_classifier/#model_training.nlp.request_classifier.fine_request_prompt_selection.map_prediction_to_label","title":"<code>map_prediction_to_label(pred, label_map)</code>","text":"<p>Attempts to match the generated text (pred) to an entry in label_map. Returns an integer label or -1 if no match is found.</p> Source code in <code>model_training\\nlp\\request_classifier\\fine_request_prompt_selection.py</code> <pre><code>def map_prediction_to_label(pred, label_map):\n    \"\"\"\n    Attempts to match the generated text (pred) to an entry in label_map.\n    Returns an integer label or -1 if no match is found.\n    \"\"\"\n    pred = pred.strip().lower().strip('.\"\\'&lt;&gt;/ ')\n\n    for label in label_map:\n        if pred == label.lower():\n            return label_map[label]\n    for label in label_map:\n        if label.lower() in pred:\n            return label_map[label]\n    for idx, label in enumerate(label_map.keys(), 1):\n        if pred == str(idx) or pred == f\"{idx}.\":\n            return label_map[label]\n    return -1\n</code></pre>"},{"location":"api/model_training/request_classifier/#model_training.nlp.request_classifier.fine_request_classifier.CustomLossWithPenalties","title":"<code>CustomLossWithPenalties</code>","text":"<p>               Bases: <code>Module</code></p> <p>Custom loss function that combines cross-entropy with a penalty matrix.</p> <p>Parameters:</p> Name Type Description Default <code>penalty_matrix</code> <code>Tensor</code> <p>A matrix specifying misclassification penalties.</p> required <code>class_weights</code> <code>Tensor</code> <p>Class weights for handling imbalance.</p> required Source code in <code>model_training\\nlp\\request_classifier\\fine_request_classifier.py</code> <pre><code>class CustomLossWithPenalties(torch.nn.Module):\n    \"\"\"\n    Custom loss function that combines cross-entropy with a penalty matrix.\n\n    Args:\n        penalty_matrix (torch.Tensor): A matrix specifying misclassification penalties.\n        class_weights (torch.Tensor): Class weights for handling imbalance.\n    \"\"\"\n    def __init__(self, penalty_matrix, class_weights):\n        super().__init__()\n        self.penalty_matrix = penalty_matrix\n        self.class_weights = class_weights\n\n    def forward(self, logits, labels):\n        ce_loss = torch.nn.functional.cross_entropy(logits, labels, weight=self.class_weights, reduction=\"none\")\n        penalties = self.penalty_matrix[labels, torch.argmax(logits, dim=-1)]\n        loss = ce_loss * penalties\n        return loss.mean()\n</code></pre>"},{"location":"api/model_training/request_classifier/#model_training.nlp.request_classifier.fine_request_classifier.PenalizedDebertaForSequenceClassification","title":"<code>PenalizedDebertaForSequenceClassification</code>","text":"<p>               Bases: <code>DebertaForSequenceClassification</code></p> <p>Custom DeBERTa model that incorporates a penalty matrix into the loss calculation.</p> <p>Parameters:</p> Name Type Description Default <code>penalty_matrix</code> <code>Tensor</code> <p>Matrix specifying misclassification penalties.</p> required <code>class_weights</code> <code>Tensor</code> <p>Class weights for handling imbalance.</p> required Source code in <code>model_training\\nlp\\request_classifier\\fine_request_classifier.py</code> <pre><code>class PenalizedDebertaForSequenceClassification(DebertaForSequenceClassification):\n    \"\"\"\n    Custom DeBERTa model that incorporates a penalty matrix into the loss calculation.\n\n    Args:\n        penalty_matrix (torch.Tensor): Matrix specifying misclassification penalties.\n        class_weights (torch.Tensor): Class weights for handling imbalance.\n    \"\"\"\n    def __init__(self, config, penalty_matrix, class_weights):\n        super().__init__(config)\n        self.penalty_matrix = penalty_matrix\n        self.class_weights = class_weights\n\n    def forward(self, input_ids=None, attention_mask=None, labels=None, **kwargs):\n        outputs = super().forward(input_ids=input_ids, attention_mask=attention_mask, labels=labels, **kwargs)\n        logits = outputs.logits\n\n        if labels is not None:\n            loss_fct = CustomLossWithPenalties(self.penalty_matrix, self.class_weights)\n            loss = loss_fct(logits, labels)\n            outputs = (loss, logits) + outputs[2:]\n\n        return outputs\n</code></pre>"},{"location":"api/model_training/request_classifier/#model_training.nlp.request_classifier.fine_request_classifier.apply_thresholds","title":"<code>apply_thresholds(logits, class_thresholds)</code>","text":"<p>Applies class-specific thresholds to predictions.</p> <p>Parameters:</p> Name Type Description Default <code>logits</code> <code>ndarray</code> <p>The model's output logits.</p> required <code>class_thresholds</code> <code>dict</code> <p>Threshold values for each class index.</p> required <p>Returns:</p> Name Type Description <code>list</code> <p>A list of predicted class labels after thresholding.</p> Source code in <code>model_training\\nlp\\request_classifier\\fine_request_classifier.py</code> <pre><code>def apply_thresholds(logits, class_thresholds):\n    \"\"\"\n    Applies class-specific thresholds to predictions.\n\n    Args:\n        logits (numpy.ndarray): The model's output logits.\n        class_thresholds (dict): Threshold values for each class index.\n\n    Returns:\n        list: A list of predicted class labels after thresholding.\n    \"\"\"\n    probabilities = torch.softmax(torch.tensor(logits), dim=-1)\n    pred_labels = []\n\n    for prob in probabilities:\n        valid_classes = []\n        for class_idx, p in enumerate(prob):\n            if p &gt;= class_thresholds.get(class_idx, 0.5):\n                valid_classes.append((class_idx, p))\n\n        if valid_classes:\n            predicted_class = max(valid_classes, key=lambda x: x[1])[0]\n        else:\n            predicted_class = torch.argmax(prob).item()\n        pred_labels.append(predicted_class)\n\n    return pred_labels\n</code></pre>"},{"location":"api/model_training/request_classifier/#model_training.nlp.request_classifier.fine_request_classifier.compute_metrics","title":"<code>compute_metrics(eval_pred)</code>","text":"<p>Computes evaluation metrics for the model.</p> <p>Parameters:</p> Name Type Description Default <code>eval_pred</code> <code>tuple</code> <p>A tuple containing (logits, labels).</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary with accuracy, f1, precision, and recall.</p> Source code in <code>model_training\\nlp\\request_classifier\\fine_request_classifier.py</code> <pre><code>def compute_metrics(eval_pred):\n    \"\"\"\n    Computes evaluation metrics for the model.\n\n    Args:\n        eval_pred (tuple): A tuple containing (logits, labels).\n\n    Returns:\n        dict: A dictionary with accuracy, f1, precision, and recall.\n    \"\"\"\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=-1)\n    return {\n        \"accuracy\": accuracy_score(labels, predictions),\n        \"f1\": f1_score(labels, predictions, average=\"weighted\", zero_division=0),\n        \"precision\": precision_score(labels, predictions, average=\"weighted\", zero_division=0),\n        \"recall\": recall_score(labels, predictions, average=\"weighted\", zero_division=0),\n    }\n</code></pre>"},{"location":"api/model_training/request_classifier/#model_training.nlp.request_classifier.fine_request_classifier.tokenize_function","title":"<code>tokenize_function(example)</code>","text":"<p>Tokenizes input text for the DeBERTa model.</p> <p>Parameters:</p> Name Type Description Default <code>example</code> <code>dict</code> <p>A dictionary containing text data under the key \"text\".</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>Tokenized output with padding, truncation, and attention masks.</p> Source code in <code>model_training\\nlp\\request_classifier\\fine_request_classifier.py</code> <pre><code>def tokenize_function(example):\n    \"\"\"\n    Tokenizes input text for the DeBERTa model.\n\n    Args:\n        example (dict): A dictionary containing text data under the key \"text\".\n\n    Returns:\n        dict: Tokenized output with padding, truncation, and attention masks.\n    \"\"\"\n    return tokenizer(\n        example[\"text\"],\n        padding=\"max_length\",\n        truncation=True,\n        max_length=128,\n        return_attention_mask=True,\n    )\n</code></pre>"},{"location":"api/model_training/request_classifier/#model_training.nlp.request_classifier.request_classifier_model_selection.MetricsLogger","title":"<code>MetricsLogger</code>","text":"<p>               Bases: <code>TrainerCallback</code></p> <p>Logs evaluation metrics after each epoch.</p> Source code in <code>model_training\\nlp\\request_classifier\\request_classifier_model_selection.py</code> <pre><code>class MetricsLogger(TrainerCallback):\n    \"\"\"\n    Logs evaluation metrics after each epoch.\n    \"\"\"\n    def on_evaluate(self, args, state: TrainerState, control: TrainerControl, metrics, **kwargs):\n        print(f\"Metrics after epoch {state.epoch}: {metrics}\")\n</code></pre>"},{"location":"api/model_training/request_classifier/#model_training.nlp.request_classifier.request_classifier_model_selection.clean_dataset","title":"<code>clean_dataset(dataset)</code>","text":"Cleans and filters a Hugging Face Dataset <ul> <li>Removes sentences shorter than 10 characters or with fewer than 1 whitespace.</li> <li>Converts text to lowercase.</li> </ul> Source code in <code>model_training\\nlp\\request_classifier\\request_classifier_model_selection.py</code> <pre><code>def clean_dataset(dataset):\n    \"\"\"\n    Cleans and filters a Hugging Face Dataset:\n      - Removes sentences shorter than 10 characters or with fewer than 1 whitespace.\n      - Converts text to lowercase.\n    \"\"\"\n    def clean_sentence(example):\n        sentence = example['sentence']\n        if isinstance(sentence, str):\n            sentence = sentence.strip().lower()\n            if len(sentence) &gt;= 10 and sentence.count(\" \") &gt;= 1:\n                return sentence\n        return None\n\n    dataset = dataset.filter(lambda example: clean_sentence(example) is not None)\n    dataset = dataset.map(lambda example: {\"sentence\": clean_sentence(example)})\n    return dataset\n</code></pre>"},{"location":"api/model_training/request_classifier/#model_training.nlp.request_classifier.request_classifier_model_selection.clean_special_characters","title":"<code>clean_special_characters(dataset, column='sentence')</code>","text":"<p>Removes special characters such as leading/trailing quotes from the specified column.</p> Source code in <code>model_training\\nlp\\request_classifier\\request_classifier_model_selection.py</code> <pre><code>def clean_special_characters(dataset, column=\"sentence\"):\n    \"\"\"\n    Removes special characters such as leading/trailing quotes from the specified column.\n    \"\"\"\n    def clean_example(example):\n        if isinstance(example[column], str):\n            example[column] = example[column].strip().replace('\"', '')\n        return example\n\n    return dataset.map(clean_example)\n</code></pre>"},{"location":"api/model_training/request_classifier/#model_training.nlp.request_classifier.request_classifier_model_selection.compute_metrics","title":"<code>compute_metrics(eval_pred)</code>","text":"<p>Computes classification metrics (accuracy, precision, recall, F1).</p> Source code in <code>model_training\\nlp\\request_classifier\\request_classifier_model_selection.py</code> <pre><code>def compute_metrics(eval_pred):\n    \"\"\"\n    Computes classification metrics (accuracy, precision, recall, F1).\n    \"\"\"\n    logits, labels = eval_pred\n    predictions = torch.argmax(torch.tensor(logits), dim=-1)\n    return {\n        \"accuracy\": accuracy_score(labels, predictions),\n        \"f1\": f1_score(labels, predictions, average=\"weighted\"),\n        \"precision\": precision_score(labels, predictions, average=\"weighted\"),\n        \"recall\": recall_score(labels, predictions, average=\"weighted\"),\n    }\n</code></pre>"},{"location":"api/model_training/request_classifier/#model_training.nlp.request_classifier.request_classifier_model_selection.load_data","title":"<code>load_data(do_cleaning=True)</code>","text":"<p>Loads train/validation/test splits from CSV files and optionally cleans them.</p> Source code in <code>model_training\\nlp\\request_classifier\\request_classifier_model_selection.py</code> <pre><code>def load_data(do_cleaning=True):\n    \"\"\"\n    Loads train/validation/test splits from CSV files and optionally cleans them.\n    \"\"\"\n    data_files = {\n        \"train\": \"model_training/nlp/request_classifier/DISAPERE/final_dataset/Request/train.csv\",\n        \"validation\": \"model_training/nlp/request_classifier/DISAPERE/final_dataset/Request/dev.csv\",\n        \"test\": \"model_training/nlp/request_classifier/DISAPERE/final_dataset/Request/test.csv\",\n    }\n    data = load_dataset(\"csv\", data_files=data_files)\n\n    if do_cleaning:\n        for split in [\"train\", \"validation\", \"test\"]:\n            data[split] = clean_dataset(data[split])\n            data[split] = clean_special_characters(data[split], \"sentence\")\n\n    return data\n</code></pre>"},{"location":"api/model_training/request_classifier/#model_training.nlp.request_classifier.request_classifier_model_selection.main","title":"<code>main()</code>","text":"Runs multiple training sessions for different settings <ul> <li>Varies model name, data cleaning, and oversampling parameters.</li> </ul> Source code in <code>model_training\\nlp\\request_classifier\\request_classifier_model_selection.py</code> <pre><code>def main():\n    \"\"\"\n    Runs multiple training sessions for different settings:\n      - Varies model name, data cleaning, and oversampling parameters.\n    \"\"\"\n    results_file_path = \"metrics_results.txt\"\n    with open(results_file_path, \"w\", encoding=\"utf-8\") as f:\n        current_time = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n        f.write(f\"Metrics log started at {current_time}\\n\\n\")\n\n    model_names = [\n\n        \"model_training/nlp/request_classifier/models/classification/sciBERT_neg\"\n    ]\n\n    for model_name in model_names:\n        for cleaning in [False, True]:\n            for oversampling in [False, True]:\n                train_and_evaluate_model(\n                    model_name=model_name,\n                    do_cleaning=cleaning,\n                    do_oversampling=oversampling,\n                    num_epochs=5,\n                    batch_size=16,\n                    results_file_path=results_file_path\n                )\n</code></pre>"},{"location":"api/model_training/request_classifier/#model_training.nlp.request_classifier.request_classifier_model_selection.oversample_minority_class","title":"<code>oversample_minority_class(dataset, label_col='target')</code>","text":"<p>Oversamples minority classes to balance class distribution.</p> Source code in <code>model_training\\nlp\\request_classifier\\request_classifier_model_selection.py</code> <pre><code>def oversample_minority_class(dataset, label_col=\"target\"):\n    \"\"\"\n    Oversamples minority classes to balance class distribution.\n    \"\"\"\n    df = dataset.to_pandas()\n    class_counts = df[label_col].value_counts()\n    max_count = class_counts.max()\n\n    balanced_dfs = []\n    for cls in class_counts.index:\n        df_cls = df[df[label_col] == cls]\n        if len(df_cls) &lt; max_count:\n            df_cls = df_cls.sample(max_count, replace=True, random_state=42)\n        balanced_dfs.append(df_cls)\n\n    balanced_df = pd.concat(balanced_dfs).sample(frac=1.0, random_state=42).reset_index(drop=True)\n    return Dataset.from_pandas(balanced_df, preserve_index=False)\n</code></pre>"},{"location":"api/model_training/request_classifier/#model_training.nlp.request_classifier.request_classifier_model_selection.predict","title":"<code>predict(texts, model_path='./bert_request_classifier_model')</code>","text":"<p>Predicts labels for a list of texts using a trained BERT model.</p> Source code in <code>model_training\\nlp\\request_classifier\\request_classifier_model_selection.py</code> <pre><code>def predict(texts, model_path=\"./bert_request_classifier_model\"):\n    \"\"\"\n    Predicts labels for a list of texts using a trained BERT model.\n    \"\"\"\n    tokenizer = BertTokenizer.from_pretrained(model_path)\n    model = BertForSequenceClassification.from_pretrained(model_path)\n    model.eval()\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n\n    inputs = tokenizer(\n        texts,\n        return_tensors=\"pt\",\n        padding=True,\n        truncation=True,\n        max_length=128,\n    )\n    inputs = {k: v.to(device) for k, v in inputs.items()}\n\n    with torch.no_grad():\n        outputs = model(**inputs)\n    logits = outputs.logits\n    return torch.argmax(logits, dim=-1).cpu().numpy()\n</code></pre>"},{"location":"api/model_training/request_classifier/#model_training.nlp.request_classifier.request_classifier_model_selection.prepare_datasets","title":"<code>prepare_datasets(tokenizer, do_cleaning=True, do_oversampling=True)</code>","text":"<p>Loads and optionally cleans/oversamples data, then tokenizes train, validation, and test sets. Returns (train_ds, val_ds, test_ds) with \"input_ids\",\"attention_mask\",\"labels\".</p> Source code in <code>model_training\\nlp\\request_classifier\\request_classifier_model_selection.py</code> <pre><code>def prepare_datasets(tokenizer, do_cleaning=True, do_oversampling=True):\n    \"\"\"\n    Loads and optionally cleans/oversamples data, then tokenizes train, validation, and test sets.\n    Returns (train_ds, val_ds, test_ds) with \"input_ids\",\"attention_mask\",\"labels\".\n    \"\"\"\n    data = load_data(do_cleaning=do_cleaning)\n\n    if do_oversampling:\n        train_dataset = oversample_minority_class(data[\"train\"], label_col=\"target\")\n    else:\n        train_dataset = data[\"train\"]\n\n    val_dataset = data[\"validation\"]\n    test_dataset = data[\"test\"]\n\n    def map_fn(x):\n        return tokenize_function(x, tokenizer)\n\n    tokenized_train = train_dataset.map(map_fn, batched=True)\n    tokenized_val = val_dataset.map(map_fn, batched=True)\n    tokenized_test = test_dataset.map(map_fn, batched=True)\n\n    tokenized_train = tokenized_train.rename_column(\"target\", \"labels\")\n    tokenized_train.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n\n    tokenized_val = tokenized_val.rename_column(\"target\", \"labels\")\n    tokenized_val.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n\n    tokenized_test = tokenized_test.rename_column(\"target\", \"labels\")\n    tokenized_test.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n\n    return tokenized_train, tokenized_val, tokenized_test\n</code></pre>"},{"location":"api/model_training/request_classifier/#model_training.nlp.request_classifier.request_classifier_model_selection.tokenize_function","title":"<code>tokenize_function(example, tokenizer)</code>","text":"<p>Tokenizes text data for BERT.</p> Source code in <code>model_training\\nlp\\request_classifier\\request_classifier_model_selection.py</code> <pre><code>def tokenize_function(example, tokenizer):\n    \"\"\"\n    Tokenizes text data for BERT.\n    \"\"\"\n    return tokenizer(\n        example[\"sentence\"],\n        padding=\"max_length\",\n        truncation=True,\n        max_length=128,\n        return_attention_mask=True,\n    )\n</code></pre>"},{"location":"api/model_training/request_classifier/#model_training.nlp.request_classifier.request_classifier_model_selection.train_and_evaluate_model","title":"<code>train_and_evaluate_model(model_name, do_cleaning=False, do_oversampling=False, num_epochs=5, batch_size=16, results_file_path='metrics_results.txt')</code>","text":"<p>Trains a BERT model for binary classification and writes performance metrics to a file.</p> Source code in <code>model_training\\nlp\\request_classifier\\request_classifier_model_selection.py</code> <pre><code>def train_and_evaluate_model(\n    model_name,\n    do_cleaning=False,\n    do_oversampling=False,\n    num_epochs=5,\n    batch_size=16,\n    results_file_path=\"metrics_results.txt\"\n):\n    \"\"\"\n    Trains a BERT model for binary classification and writes performance metrics to a file.\n    \"\"\"\n    if model_name.startswith(\"model_training/\"):\n        tokenizer = BertTokenizer.from_pretrained(model_name, local_files_only=True)\n        model = BertForSequenceClassification.from_pretrained(model_name, local_files_only=True, num_labels=2)\n    else:\n        tokenizer = BertTokenizer.from_pretrained(model_name)\n        model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n\n    train_ds, val_ds, test_ds = prepare_datasets(\n        tokenizer,\n        do_cleaning=do_cleaning,\n        do_oversampling=do_oversampling\n    )\n\n    training_args = TrainingArguments(\n        output_dir=\"./results\",\n        evaluation_strategy=\"epoch\",\n        save_strategy=\"epoch\",\n        logging_dir=\"./logs\",\n        learning_rate=2e-5,\n        per_device_train_batch_size=batch_size,\n        per_device_eval_batch_size=batch_size,\n        num_train_epochs=num_epochs,\n        weight_decay=0.01,\n        logging_steps=50,\n        load_best_model_at_end=True,\n    )\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_ds,\n        eval_dataset=val_ds,\n        compute_metrics=compute_metrics,\n        callbacks=[MetricsLogger()],\n    )\n\n    trainer.train()\n\n    val_metrics = trainer.evaluate(eval_dataset=val_ds)\n    test_metrics = trainer.evaluate(eval_dataset=test_ds)\n\n    test_output = trainer.predict(test_ds)\n    test_preds = np.argmax(test_output.predictions, axis=1)\n    cm = confusion_matrix(test_output.label_ids, test_preds)\n\n    with open(results_file_path, \"a\", encoding=\"utf-8\") as f:\n        f.write(\"==============================================\\n\")\n        f.write(f\"Model Name: {model_name}\\n\")\n        f.write(f\"Data Cleaning: {do_cleaning}\\n\")\n        f.write(f\"Oversampling: {do_oversampling}\\n\")\n        f.write(f\"Epochs: {num_epochs}\\n\")\n        f.write(\"Validation Metrics:\\n\")\n        f.write(f\"  Accuracy:  {val_metrics['eval_accuracy']:.4f}\\n\")\n        f.write(f\"  Precision: {val_metrics['eval_precision']:.4f}\\n\")\n        f.write(f\"  Recall:    {val_metrics['eval_recall']:.4f}\\n\")\n        f.write(f\"  F1-Score:  {val_metrics['eval_f1']:.4f}\\n\")\n        f.write(\"Test Metrics:\\n\")\n        f.write(f\"  Accuracy:  {test_metrics['eval_accuracy']:.4f}\\n\")\n        f.write(f\"  Precision: {test_metrics['eval_precision']:.4f}\\n\")\n        f.write(f\"  Recall:    {test_metrics['eval_recall']:.4f}\\n\")\n        f.write(f\"  F1-Score:  {test_metrics['eval_f1']:.4f}\\n\")\n        f.write(\"Confusion Matrix (Test):\\n\")\n        f.write(str(cm) + \"\\n\")\n        f.write(\"==============================================\\n\\n\")\n</code></pre>"},{"location":"api/model_training/request_classifier/#model_training.nlp.request_classifier.request_classifier.MetricsLogger","title":"<code>MetricsLogger</code>","text":"<p>               Bases: <code>TrainerCallback</code></p> <p>Custom callback to log evaluation metrics after each epoch.</p> Source code in <code>model_training\\nlp\\request_classifier\\request_classifier.py</code> <pre><code>class MetricsLogger(TrainerCallback):\n    \"\"\"\n    Custom callback to log evaluation metrics after each epoch.\n    \"\"\"\n    def on_evaluate(self, args, state: TrainerState, control: TrainerControl, metrics, **kwargs):\n        print(f\"Metrics after epoch {state.epoch}: {metrics}\")\n</code></pre>"},{"location":"api/model_training/request_classifier/#model_training.nlp.request_classifier.request_classifier.clean_dataset","title":"<code>clean_dataset(dataset)</code>","text":"<p>Cleans and filters a Hugging Face Dataset: - Removes sentences shorter than 10 characters or with fewer than 1 whitespace. - Converts all text to lowercase.</p> <p>Parameters: dataset (Dataset): A Hugging Face Dataset object.</p> <p>Returns: Dataset: Cleaned dataset with filtered and processed sentences.</p> Source code in <code>model_training\\nlp\\request_classifier\\request_classifier.py</code> <pre><code>def clean_dataset(dataset):\n    \"\"\"\n    Cleans and filters a Hugging Face Dataset:\n    - Removes sentences shorter than 10 characters or with fewer than 1 whitespace.\n    - Converts all text to lowercase.\n\n    Parameters:\n    dataset (Dataset): A Hugging Face Dataset object.\n\n    Returns:\n    Dataset: Cleaned dataset with filtered and processed sentences.\n    \"\"\"\n    def clean_sentence(example):\n        sentence = example['sentence']\n        if isinstance(sentence, str):\n            sentence = sentence.strip().lower()\n            if len(sentence) &gt;= 10 and sentence.count(\" \") &gt;= 1:\n                return sentence\n        return None\n\n    # Filter out invalid sentences\n    dataset = dataset.filter(lambda example: clean_sentence(example) is not None)\n    dataset = dataset.map(lambda example: {\"sentence\": clean_sentence(example)})\n    return dataset\n</code></pre>"},{"location":"api/model_training/request_classifier/#model_training.nlp.request_classifier.request_classifier.clean_special_characters","title":"<code>clean_special_characters(dataset, column='sentence')</code>","text":"<p>Removes special characters like leading/trailing quotes from the specified column in the dataset.</p> <p>Parameters: dataset (Dataset): A Hugging Face Dataset object. column (str): The column to clean (default: \"sentence\").</p> <p>Returns: Dataset: Cleaned dataset with special characters removed from the column.</p> Source code in <code>model_training\\nlp\\request_classifier\\request_classifier.py</code> <pre><code>def clean_special_characters(dataset, column=\"sentence\"):\n    \"\"\"\n    Removes special characters like leading/trailing quotes from the specified column in the dataset.\n\n    Parameters:\n    dataset (Dataset): A Hugging Face Dataset object.\n    column (str): The column to clean (default: \"sentence\").\n\n    Returns:\n    Dataset: Cleaned dataset with special characters removed from the column.\n    \"\"\"\n    def clean_example(example):\n        example[column] = example[column].strip().replace('\"', '') if isinstance(example[column], str) else example[column]\n        return example\n\n    # Apply the cleaning function to each example in the dataset\n    return dataset.map(clean_example)\n</code></pre>"},{"location":"api/model_training/request_classifier/#model_training.nlp.request_classifier.request_classifier.compute_metrics","title":"<code>compute_metrics(eval_pred)</code>","text":"<p>Computes evaluation metrics for a classification model.</p> <p>Parameters: eval_pred (tuple): A tuple containing logits and labels.</p> <p>Returns: dict: A dictionary with accuracy, F1 score, precision, and recall.</p> Source code in <code>model_training\\nlp\\request_classifier\\request_classifier.py</code> <pre><code>def compute_metrics(eval_pred):\n    \"\"\"\n    Computes evaluation metrics for a classification model.\n\n    Parameters:\n    eval_pred (tuple): A tuple containing logits and labels.\n\n    Returns:\n    dict: A dictionary with accuracy, F1 score, precision, and recall.\n    \"\"\"\n    logits, labels = eval_pred\n    predictions = torch.argmax(torch.tensor(logits), dim=-1)\n    return {\n        \"accuracy\": accuracy_score(labels, predictions),\n        \"f1\": f1_score(labels, predictions, average=\"weighted\"),\n        \"precision\": precision_score(labels, predictions, average=\"weighted\"),\n        \"recall\": recall_score(labels, predictions, average=\"weighted\"),\n    }\n</code></pre>"},{"location":"api/model_training/request_classifier/#model_training.nlp.request_classifier.request_classifier.load_and_clean_data","title":"<code>load_and_clean_data()</code>","text":"<p>Loads and cleans the dataset from CSV files.</p> <p>Returns: DatasetDict: A dictionary containing train, validation, and test datasets.</p> Source code in <code>model_training\\nlp\\request_classifier\\request_classifier.py</code> <pre><code>def load_and_clean_data():\n    \"\"\"\n    Loads and cleans the dataset from CSV files.\n\n    Returns:\n    DatasetDict: A dictionary containing train, validation, and test datasets.\n    \"\"\"\n    data_files = {\n        \"train\": \"model_training/nlp/request_classifier/DISAPERE/final_dataset/Request/train.csv\",\n        \"validation\": \"model_training/nlp/request_classifier/DISAPERE/final_dataset/Request/dev.csv\",\n        \"test\": \"model_training/nlp/request_classifier/DISAPERE/final_dataset/Request/test.csv\",\n    }\n\n    # Load datasets\n    data = load_dataset(\"csv\", data_files=data_files)\n\n    # Clean datasets\n    for split in [\"train\", \"validation\", \"test\"]:\n        data[split] = clean_dataset(data[split])\n        data[split] = clean_special_characters(data[split], \"sentence\")\n\n    return data\n</code></pre>"},{"location":"api/model_training/request_classifier/#model_training.nlp.request_classifier.request_classifier.oversample_minority_class","title":"<code>oversample_minority_class(dataset, label_col='target')</code>","text":"<p>Oversamples the minority classes in the dataset to balance the class distribution.</p> <p>Parameters: dataset (Dataset): A Hugging Face Dataset object. label_col (str): The column containing class labels (default: \"target\").</p> <p>Returns: Dataset: A balanced dataset with oversampled minority classes.</p> Source code in <code>model_training\\nlp\\request_classifier\\request_classifier.py</code> <pre><code>def oversample_minority_class(dataset, label_col=\"target\"):\n    \"\"\"\n    Oversamples the minority classes in the dataset to balance the class distribution.\n\n    Parameters:\n    dataset (Dataset): A Hugging Face Dataset object.\n    label_col (str): The column containing class labels (default: \"target\").\n\n    Returns:\n    Dataset: A balanced dataset with oversampled minority classes.\n    \"\"\"\n    df = dataset.to_pandas()\n    class_counts = df[label_col].value_counts()\n    max_count = class_counts.max()\n\n    # Oversample each class to match the majority class size\n    balanced_dfs = []\n    for cls in class_counts.index:\n        df_cls = df[df[label_col] == cls]\n        if len(df_cls) &lt; max_count:\n            df_cls = df_cls.sample(max_count, replace=True, random_state=42)\n        balanced_dfs.append(df_cls)\n\n    balanced_df = pd.concat(balanced_dfs).sample(frac=1.0, random_state=42).reset_index(drop=True)\n    return Dataset.from_pandas(balanced_df, preserve_index=False)\n</code></pre>"},{"location":"api/model_training/request_classifier/#model_training.nlp.request_classifier.request_classifier.predict","title":"<code>predict(texts, model_path='./bert_request_classifier_model')</code>","text":"<p>Predicts labels for a list of texts using a trained BERT model.</p> <p>Parameters: texts (list of str): List of input texts to classify. model_path (str): Path to the trained model.</p> <p>Returns: numpy.ndarray: Predicted labels for the input texts.</p> Source code in <code>model_training\\nlp\\request_classifier\\request_classifier.py</code> <pre><code>def predict(texts, model_path=\"./bert_request_classifier_model\"):\n    \"\"\"\n    Predicts labels for a list of texts using a trained BERT model.\n\n    Parameters:\n    texts (list of str): List of input texts to classify.\n    model_path (str): Path to the trained model.\n\n    Returns:\n    numpy.ndarray: Predicted labels for the input texts.\n    \"\"\"\n    tokenizer = BertTokenizer.from_pretrained(model_path)\n    model = BertForSequenceClassification.from_pretrained(model_path)\n    model.eval()\n    # device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    device = \"cpu\"\n    model.to(device)\n\n    inputs = tokenizer(\n        texts,\n        return_tensors=\"pt\",\n        padding=True,\n        truncation=True,\n        max_length=128,\n    )\n    inputs = {k: v.to(device) for k, v in inputs.items()}\n\n    with torch.no_grad():\n        outputs = model(**inputs)\n    logits = outputs.logits\n    return torch.argmax(logits, dim=-1).cpu().numpy()\n</code></pre>"},{"location":"api/model_training/request_classifier/#model_training.nlp.request_classifier.request_classifier.prepare_datasets","title":"<code>prepare_datasets(tokenizer)</code>","text":"<p>Prepares and tokenizes datasets for training, validation, and testing.</p> <p>Parameters: tokenizer (BertTokenizer): A tokenizer object.</p> <p>Returns: tuple: Tokenized train, validation, and test datasets.</p> Source code in <code>model_training\\nlp\\request_classifier\\request_classifier.py</code> <pre><code>def prepare_datasets(tokenizer):\n    \"\"\"\n    Prepares and tokenizes datasets for training, validation, and testing.\n\n    Parameters:\n    tokenizer (BertTokenizer): A tokenizer object.\n\n    Returns:\n    tuple: Tokenized train, validation, and test datasets.\n    \"\"\"\n    data = load_and_clean_data()\n\n    # Oversample the training dataset\n    train_dataset = oversample_minority_class(data[\"train\"], label_col=\"target\")\n    validation_dataset = data[\"validation\"]\n    test_dataset = data[\"test\"]\n\n    # Tokenize datasets\n    tokenized_train = train_dataset.map(lambda x: tokenize_function(x, tokenizer), batched=True)\n    tokenized_validation = validation_dataset.map(lambda x: tokenize_function(x, tokenizer), batched=True)\n    tokenized_test = test_dataset.map(lambda x: tokenize_function(x, tokenizer), batched=True)\n\n    # Rename label column for compatibility\n    for dataset in [tokenized_train, tokenized_validation, tokenized_test]:\n        dataset = dataset.rename_column(\"target\", \"labels\")\n        dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n\n    return tokenized_train, tokenized_validation, tokenized_test\n</code></pre>"},{"location":"api/model_training/request_classifier/#model_training.nlp.request_classifier.request_classifier.tokenize_function","title":"<code>tokenize_function(example, tokenizer)</code>","text":"<p>Tokenizes text data for BERT input.</p> <p>Parameters: example (dict): A dictionary containing text data. tokenizer (BertTokenizer): A tokenizer object.</p> <p>Returns: dict: Tokenized data.</p> Source code in <code>model_training\\nlp\\request_classifier\\request_classifier.py</code> <pre><code>def tokenize_function(example, tokenizer):\n    \"\"\"\n    Tokenizes text data for BERT input.\n\n    Parameters:\n    example (dict): A dictionary containing text data.\n    tokenizer (BertTokenizer): A tokenizer object.\n\n    Returns:\n    dict: Tokenized data.\n    \"\"\"\n    return tokenizer(\n        example[\"sentence\"],\n        padding=\"max_length\",\n        truncation=True,\n        max_length=128,\n        return_attention_mask=True,\n    )\n</code></pre>"},{"location":"api/model_training/request_classifier/#model_training.nlp.request_classifier.request_classifier.train_model","title":"<code>train_model()</code>","text":"<p>Trains a BERT-based model for sequence classification.</p> <ul> <li>Loads and tokenizes datasets.</li> <li>Configures training arguments and starts training.</li> <li>Evaluates the model on validation and test datasets.</li> <li>Saves the trained model and tokenizer.</li> </ul> <p>Returns: None</p> Source code in <code>model_training\\nlp\\request_classifier\\request_classifier.py</code> <pre><code>def train_model():\n    \"\"\"\n    Trains a BERT-based model for sequence classification.\n\n    - Loads and tokenizes datasets.\n    - Configures training arguments and starts training.\n    - Evaluates the model on validation and test datasets.\n    - Saves the trained model and tokenizer.\n\n    Returns:\n    None\n    \"\"\"\n    model_name = \"bert-base-uncased\"\n    tokenizer = BertTokenizer.from_pretrained(model_name)\n    model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n\n    train_dataset, validation_dataset, test_dataset = prepare_datasets(tokenizer)\n\n    training_args = TrainingArguments(\n        output_dir=\"./results\",\n        evaluation_strategy=\"epoch\",\n        save_strategy=\"epoch\",\n        logging_dir=\"./logs\",\n        learning_rate=2e-5,\n        per_device_train_batch_size=16,\n        per_device_eval_batch_size=16,\n        num_train_epochs=2,\n        weight_decay=0.01,\n        logging_steps=50,\n        load_best_model_at_end=True,\n    )\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=validation_dataset,\n        compute_metrics=compute_metrics,\n        callbacks=[MetricsLogger()],\n    )\n\n    # Train the model\n    trainer.train()\n\n    # Evaluate on validation and test datasets\n    print(\"Validation results:\", trainer.evaluate(eval_dataset=validation_dataset))\n    print(\"Test results:\", trainer.evaluate(eval_dataset=test_dataset))\n\n    # Generate confusion matrix\n    test_output = trainer.predict(test_dataset)\n    test_preds = np.argmax(test_output.predictions, axis=1)\n    cm = confusion_matrix(test_output.label_ids, test_preds)\n    print(\"Confusion Matrix:\\n\", cm)\n\n    # Save the model and tokenizer\n    save_directory = os.path.join(os.path.dirname(__file__), \"../../../../../backend/models/request_classifier/request_classifier\")\n    trainer.save_model(save_directory)\n    tokenizer.save_pretrained(save_directory)\n</code></pre>"},{"location":"api/model_training/summary/","title":"Summary Generation Model Training Code Documentation","text":"<p>main.py</p> <p>This file  (1) checks wheather cuda is availibile  (2) executes the model training for all models we want to investigate and require training,  (3.1) computes evaluation measures for each model using a test-set and saves it into a model_comparison.csv (3.2) makes a prediciton for a dummy data set for each model to make it more human readable and saves the results into output.txt</p> <p>prediction_BART.py</p> <p>A script that: 1) Loads a previously fine-tuned BART model from ./models/bart 2) Predicts summary of input</p> <p>prediction_BLOOM.py</p> <p>A script that: 1) Loads a previously fine-tuned BLOOM model from ./models/bloom 2) Prints ONLY the newly generated text (omitting the prompt)</p> <p>predict_LLAMA2.py</p> <p>A script that: 1) Loads a LLaMA2 model from Hugging Face 2) Predicts summary of input</p>"},{"location":"api/model_training/summary/#model_training.nlp.summary.compute_metrics.compute_metrics","title":"<code>compute_metrics(eval_pred, tokenizer, remove_prompt_portion=False, prompt_delimiter=None)</code>","text":"<p>Compute BERT-Score between predictions and labels. Includes detailed debug logs to trace data shapes and types.</p> <p>Parameters:</p> Name Type Description Default <code>eval_pred</code> <code>Tuple</code> <p>The (predictions, labels) tuple from Trainer.</p> required <code>tokenizer</code> <code>PreTrainedTokenizer</code> <p>The tokenizer used for decoding.</p> required <code>remove_prompt_portion</code> <code>bool</code> <p>If True, tries to remove the prompt portion in each decoded string before scoring. Defaults to False.</p> <code>False</code> <code>prompt_delimiter</code> <code>str</code> <p>If provided, is used to split out the prompt portion. For example, if your text is \"PROMPT\\nOUTPUT\", then prompt_delimiter=\"\\n\" will remove everything up to the first newline.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>A dictionary containing \"bertscore_precision\", \"bertscore_recall\",   and \"bertscore_f1\".</p> Source code in <code>model_training\\nlp\\summary\\compute_metrics.py</code> <pre><code>def compute_metrics(\n    eval_pred,\n    tokenizer,\n    remove_prompt_portion: bool = False,\n    prompt_delimiter: str = None,\n):\n    \"\"\"\n    Compute BERT-Score between predictions and labels.\n    Includes detailed debug logs to trace data shapes and types.\n\n    Args:\n        eval_pred (Tuple): The (predictions, labels) tuple from Trainer.\n        tokenizer (PreTrainedTokenizer): The tokenizer used for decoding.\n        remove_prompt_portion (bool): If True, tries to remove the prompt portion\n            in each decoded string before scoring. Defaults to False.\n        prompt_delimiter (str): If provided, is used to split out the prompt portion.\n            For example, if your text is \"PROMPT\\\\nOUTPUT\", then prompt_delimiter=\"\\\\n\"\n            will remove everything up to the first newline.\n\n    Returns:\n        dict: A dictionary containing \"bertscore_precision\", \"bertscore_recall\",\n              and \"bertscore_f1\".\n    \"\"\"\n    logger.debug(\"[compute_metrics] Entering compute_metrics function.\")\n\n    predictions, labels = eval_pred\n\n    # Debug: Check types\n    logger.debug(f\"[compute_metrics] Type of predictions: {type(predictions)}\")\n    logger.debug(f\"[compute_metrics] Type of labels: {type(labels)}\")\n\n    # Handle predictions\n    if isinstance(predictions, tuple):\n        predictions = predictions[0]\n        logger.debug(\"[compute_metrics] Predictions were a tuple; took first element.\")\n\n    if isinstance(predictions, torch.Tensor):\n        logger.debug(f\"[compute_metrics] Predictions is a torch.Tensor with shape {predictions.shape}\")\n        if predictions.ndim == 3:\n            logger.debug(\"[compute_metrics] Predictions have logits; applying argmax over last dimension.\")\n            predictions = predictions.argmax(dim=-1).cpu().numpy()\n        else:\n            predictions = predictions.cpu().numpy()\n    elif isinstance(predictions, np.ndarray):\n        logger.debug(f\"[compute_metrics] Predictions is a numpy.ndarray with shape {predictions.shape}\")\n        if predictions.ndim == 3:\n            logger.debug(\"[compute_metrics] Predictions have logits; applying argmax over last dimension.\")\n            predictions = np.argmax(predictions, axis=-1)\n    elif isinstance(predictions, list):\n        logger.debug(\"[compute_metrics] Predictions is a list; converting to numpy array.\")\n        predictions = np.array(predictions)\n    else:\n        logger.error(f\"[compute_metrics] Unexpected type for predictions: {type(predictions)}\")\n        raise ValueError(f\"Unexpected type for predictions: {type(predictions)}\")\n\n    # Handle labels\n    if isinstance(labels, torch.Tensor):\n        logger.debug(f\"[compute_metrics] Labels is a torch.Tensor with shape {labels.shape}\")\n        labels = labels.cpu().numpy()\n    elif isinstance(labels, np.ndarray):\n        logger.debug(f\"[compute_metrics] Labels is a numpy.ndarray with shape {labels.shape}\")\n    elif isinstance(labels, list):\n        logger.debug(\"[compute_metrics] Labels is a list; converting to numpy array.\")\n        labels = np.array(labels)\n    else:\n        logger.error(f\"[compute_metrics] Unexpected type for labels: {type(labels)}\")\n        raise ValueError(f\"Unexpected type for labels: {type(labels)}\")\n\n    # Replace -100 with pad_token_id\n    pad_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id\n    logger.debug(f\"[compute_metrics] Using pad_token_id={pad_id} to replace -100 in labels.\")\n    labels = np.where(labels == -100, pad_id, labels)\n\n    # Ensure predictions and labels are 2D arrays\n    if predictions.ndim != 2:\n        logger.error(f\"[compute_metrics] Predictions should be a 2D array, but got shape {predictions.shape}\")\n        raise ValueError(f\"Predictions should be a 2D array, but got shape {predictions.shape}\")\n    if labels.ndim != 2:\n        logger.error(f\"[compute_metrics] Labels should be a 2D array, but got shape {labels.shape}\")\n        raise ValueError(f\"Labels should be a 2D array, but got shape {labels.shape}\")\n\n    # Decode predictions and labels\n    try:\n        decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n        logger.debug(\"[compute_metrics] Successfully decoded predictions and labels.\")\n    except Exception as e:\n        logger.error(f\"[compute_metrics] Error during batch_decode: {e}\")\n        raise e\n\n    # Optionally remove prompt portion from decoded strings\n    if remove_prompt_portion and prompt_delimiter:\n        logger.debug(\"[compute_metrics] Removing prompt portion using the given delimiter.\")\n        for i in range(len(decoded_preds)):\n            if prompt_delimiter in decoded_preds[i]:\n                # Split at the first occurrence of the delimiter, keep only the 'output' portion\n                parts = decoded_preds[i].split(prompt_delimiter, 1)\n                if len(parts) &gt; 1:\n                    decoded_preds[i] = parts[1]  # Keep text after the delimiter\n            if prompt_delimiter in decoded_labels[i]:\n                parts = decoded_labels[i].split(prompt_delimiter, 1)\n                if len(parts) &gt; 1:\n                    decoded_labels[i] = parts[1]\n\n    # Debug sample decoded strings\n    if len(decoded_preds) &gt; 0 and len(decoded_labels) &gt; 0:\n        logger.debug(f\"[compute_metrics] Sample prediction: '{decoded_preds[0]}...'\")\n        logger.debug(f\"[compute_metrics] Sample label: '{decoded_labels[0][:60]}...'\")\n\n    # Compute BERT-Score\n    try:\n        logger.debug(\"[compute_metrics] Computing BERT-Score.\")\n        bert_results = bertscore_metric.compute(\n            predictions=decoded_preds,\n            references=decoded_labels,\n            model_type=\"bert-base-uncased\"\n        )\n        logger.debug(\"[compute_metrics] BERT-Score computation successful.\")\n    except Exception as e:\n        logger.error(f\"[compute_metrics] Error during BERT-Score computation: {e}\")\n        raise e\n\n    # Aggregate scores\n    precision_arr = bert_results[\"precision\"]\n    recall_arr = bert_results[\"recall\"]\n    f1_arr = bert_results[\"f1\"]\n\n    precision = np.mean(precision_arr) * 100\n    recall = np.mean(recall_arr) * 100\n    f1 = np.mean(f1_arr) * 100\n\n    metrics = {\n        \"bertscore_precision\": round(precision, 2),\n        \"bertscore_recall\": round(recall, 2),\n        \"bertscore_f1\": round(f1, 2)\n    }\n\n    logger.debug(f\"[compute_metrics] BERT-Score Metrics: {metrics}\")\n    return metrics\n</code></pre>"},{"location":"api/model_training/summary/#model_training.nlp.summary.dfs_to_input_converter.generate_attitude_roots_prompts","title":"<code>generate_attitude_roots_prompts(attitude_df)</code>","text":"<p>Summarize the attitude roots, frequencies, and comments.</p> Source code in <code>model_training\\nlp\\summary\\dfs_to_input_converter.py</code> <pre><code>def generate_attitude_roots_prompts(attitude_df):\n    \"\"\"\n    Summarize the attitude roots, frequencies, and comments.\n    \"\"\"\n    attitude_df['Frequency_Percent'] = attitude_df['Frequency'] * 100\n    lines = []\n\n    for _, row in attitude_df.iterrows():\n        root = row['Attitude_roots']\n        freq = row['Frequency_Percent']\n        descr = row['Descriptions']\n        comments_list = row['Comments']\n\n        combined_comments = []\n        for _, comment_chunk in comments_list:\n            combined_comments.append(\" \".join(comment_chunk))\n\n        all_comments = \" \".join(combined_comments)\n        line = f\"- {root} appears {freq:.0f}% of the time. {descr}. Comments: {all_comments}\"\n        lines.append(line)\n\n    return \"\\n\".join(lines)\n</code></pre>"},{"location":"api/model_training/summary/#model_training.nlp.summary.dfs_to_input_converter.generate_dummy_input_text","title":"<code>generate_dummy_input_text()</code>","text":"<p>Returns dummy input data string for debug purposes</p> Source code in <code>model_training\\nlp\\summary\\dfs_to_input_converter.py</code> <pre><code>def generate_dummy_input_text():\n    \"\"\"\n    Returns dummy input data string for debug purposes\n    \"\"\"\n    import dummy_data\n\n    # Import dummy data from respective module\n    overview_df, attitude_df, request_df = dummy_data.get_dummy_data()\n\n    # Translate dummy data into input string expected\n    dummy_input_data = generate_input_text(overview_df, attitude_df, request_df)\n\n    return dummy_input_data\n</code></pre>"},{"location":"api/model_training/summary/#model_training.nlp.summary.dfs_to_input_converter.generate_overview_prompts","title":"<code>generate_overview_prompts(overview_df)</code>","text":"<p>Build lines summarizing rating, soundness, etc.</p> Source code in <code>model_training\\nlp\\summary\\dfs_to_input_converter.py</code> <pre><code>def generate_overview_prompts(overview_df):\n    \"\"\"\n    Build lines summarizing rating, soundness, etc.\n    \"\"\"\n    max_scores = {'Rating': 10, 'Soundness': 4, 'Presentation': 4, 'Contribution': 4}\n    threshold = 1.5\n    overview_sentences = []\n\n    for _, row in overview_df.iterrows():\n        category = row['Category']\n        avg_score = row['Avg_Score']\n        individual_scores = row['Individual_scores']\n        max_score = max_scores.get(category, \"Unknown\")\n\n        scores = [score for (_, score) in individual_scores if score is not None]\n        outliers = [sc for sc in scores if abs(sc - avg_score) &gt; threshold]\n\n        sentence = f\"- {category} is {avg_score} out of {max_score}.\"\n        if outliers:\n            unique_outliers = sorted(set(outliers))\n            if len(unique_outliers) == 1:\n                outliers_text = f\"a rating at {unique_outliers[0]}\"\n            else:\n                outliers_text = \", \".join([f\"a rating at {sc}\" for sc in unique_outliers[:-1]])\n                outliers_text += f\" and a rating at {unique_outliers[-1]}\"\n            sentence += f\" Outlier was {outliers_text}.\"\n\n        overview_sentences.append(sentence)\n\n    return \"\\n\".join(overview_sentences)\n</code></pre>"},{"location":"api/model_training/summary/#model_training.nlp.summary.dfs_to_input_converter.generate_request_information_prompts","title":"<code>generate_request_information_prompts(request_df)</code>","text":"<p>Summarize requests (like \"Typo\" or \"Clarification\"), freq, and comments.</p> Source code in <code>model_training\\nlp\\summary\\dfs_to_input_converter.py</code> <pre><code>def generate_request_information_prompts(request_df):\n    \"\"\"\n    Summarize requests (like \"Typo\" or \"Clarification\"), freq, and comments.\n    \"\"\"\n    request_df['Frequency_Percent'] = request_df['Frequency'] * 100\n    lines = []\n\n    for _, row in request_df.iterrows():\n        request_type = row['Request Information']\n        freq = row['Frequency_Percent']\n        comments_list = row['Comments']\n\n        combined_comments = []\n        for _, chunk in comments_list:\n            combined_comments.append(\" \".join(chunk))\n\n        all_comments = \" \".join(combined_comments)\n        line = f\"- {request_type} was requested {freq:.0f}% of the time. Comments: {all_comments}\"\n        lines.append(line)\n\n    return \"\\n\".join(lines)\n</code></pre>"},{"location":"api/model_training/summary/#model_training.nlp.summary.predict_BART.load_BART_model","title":"<code>load_BART_model(model_dir='./models/bart')</code>","text":"<p>Load BART model previously trained by train_BART.py</p> <p>Args:       model_dir (str): path to directory where the model is supposed to be stored.</p> Source code in <code>model_training\\nlp\\summary\\predict_BART.py</code> <pre><code>def load_BART_model(model_dir: str = \"./models/bart\"):\n    \"\"\"\n    Load BART model previously trained by train_BART.py\n\n      Args:\n          model_dir (str): path to directory where the model is supposed to be stored.\n    \"\"\"\n\n    logger.info(f\"Loading BART model from: {model_dir}\")\n    tokenizer = BartTokenizer.from_pretrained(model_dir)\n    model = BartForConditionalGeneration.from_pretrained(model_dir)\n\n    return model, tokenizer\n</code></pre>"},{"location":"api/model_training/summary/#model_training.nlp.summary.predict_BART.predict","title":"<code>predict(input_text, model=None, tokenizer=None, model_dir='./models/bart', min_new_tokens=20)</code>","text":"<p>Generates a BART prediction such that the output is around two-thirds the token length of the input text.</p> <p>Parameters:</p> Name Type Description Default <code>input_text</code> <code>str</code> <p>The raw input text to be summarized/transformed.</p> required <code>model_dir</code> <code>str</code> <p>Path to the directory where the BART model is stored.</p> <code>'./models/bart'</code> <code>min_new_tokens</code> <code>int</code> <p>A minimum number of tokens to generate                   (avoids extremely short outputs).</p> <code>20</code> <p>Returns:</p> Type Description <code>str</code> <p>A string containing the BART model's output.</p> Source code in <code>model_training\\nlp\\summary\\predict_BART.py</code> <pre><code>def predict(\n    input_text: str,\n    model = None,\n    tokenizer = None,\n    model_dir: str = \"./models/bart\",\n    min_new_tokens: int = 20,\n) -&gt; str:\n    \"\"\"\n    Generates a BART prediction such that the output is around\n    two-thirds the token length of the input text.\n\n    Args:\n        input_text (str): The raw input text to be summarized/transformed.\n        model_dir (str): Path to the directory where the BART model is stored.\n        min_new_tokens (int): A minimum number of tokens to generate\n                              (avoids extremely short outputs).\n\n    Returns:\n        A string containing the BART model's output.\n    \"\"\"\n\n    # 1) Load tokenizer and model\n    if (model == None and tokenizer == None):\n        model, tokenizer = load_BART_model(model_dir)\n\n    # Move model to GPU if available\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n\n    # 2) Tokenize the input\n    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True)\n    #input_length = inputs[\"input_ids\"].shape[1]\n\n    # 3) Calculate desired output length (~2/3 of input length)\n    #desired_output_length = int(round((2.0 / 3.0) * input_length))\n    # Ensure it's at least `min_new_tokens` to avoid producing almost nothing\n    #desired_output_length = max(desired_output_length, min_new_tokens)\n\n    #logger.info(f\"Detected input length (tokens): {input_length}\")\n    #logger.info(f\"Desired output length (tokens): {desired_output_length}\")\n\n    # 4) Generate\n    input_ids = inputs[\"input_ids\"].to(device)\n    attention_mask = inputs[\"attention_mask\"].to(device)\n\n    # We use `max_new_tokens=desired_output_length` to limit generation\n    outputs = model.generate(\n        input_ids=input_ids,\n        attention_mask=attention_mask,\n        max_new_tokens=150,\n        num_beams=4,               # optional: beam search\n        no_repeat_ngram_size=2     # optional: avoid repeating phrases\n    )\n\n    # 5) Decode\n    prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    return prediction\n</code></pre>"},{"location":"api/model_training/summary/#model_training.nlp.summary.predict_BLOOM.load_BLOOM_model","title":"<code>load_BLOOM_model(model_dir='./models/bloom')</code>","text":"<p>Load BLOOM model previously trained by train_BLOOM.py</p> <p>Args:       model_dir (str): path to directory where the model is supposed to be stored.</p> Source code in <code>model_training\\nlp\\summary\\predict_BLOOM.py</code> <pre><code>def load_BLOOM_model(model_dir: str = \"./models/bloom\"):\n    \"\"\"\n    Load BLOOM model previously trained by train_BLOOM.py\n\n      Args:\n          model_dir (str): path to directory where the model is supposed to be stored.\n    \"\"\"\n\n    logger.info(f\"Loading BLOOM model from: {model_dir}\")\n    tokenizer = BloomTokenizerFast.from_pretrained(model_dir)\n    model = BloomForCausalLM.from_pretrained(model_dir)\n\n    # Option 1: Set pad_token to eos_token\n    tokenizer.pad_token = tokenizer.eos_token\n    model.config.pad_token_id = tokenizer.eos_token_id\n\n    # Option 2: Add a new pad_token (if preferred)\n    # tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n    # model.resize_token_embeddings(len(tokenizer))\n    # model.config.pad_token_id = tokenizer.pad_token_id\n\n    return model, tokenizer\n</code></pre>"},{"location":"api/model_training/summary/#model_training.nlp.summary.predict_BLOOM.predict","title":"<code>predict(input_text, model=None, tokenizer=None, model_dir='./models/bloom', min_new_tokens=20)</code>","text":"<p>Generates a BLOOM prediction such that the output is around two-thirds the token length of the (prompted) input text, and returns ONLY the newly generated tokens (excluding the prompt).</p> Source code in <code>model_training\\nlp\\summary\\predict_BLOOM.py</code> <pre><code>def predict(\n    input_text: str,\n    model=None,\n    tokenizer=None,\n    model_dir: str = \"./models/bloom\",\n    min_new_tokens: int = 20,\n) -&gt; str:\n    \"\"\"\n    Generates a BLOOM prediction such that the output is around\n    two-thirds the token length of the (prompted) input text,\n    and returns ONLY the newly generated tokens (excluding the prompt).\n    \"\"\"\n\n    # 1) Load tokenizer and model if not provided\n    if (model is None and tokenizer is None):\n        model, tokenizer = load_BLOOM_model(model_dir)\n\n    # Move model to GPU if available\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n\n    # 2) Build and tokenize the input\n    prompt_tokens = tokenizer(input_text, return_tensors=\"pt\", add_special_tokens=False)\n    prompt_length = prompt_tokens[\"input_ids\"].shape[1]\n\n\n    # 4) Generate text\n    input_ids = prompt_tokens[\"input_ids\"].to(device)\n    outputs = model.generate(\n        input_ids=input_ids,\n        max_new_tokens=150,\n        num_beams=4,\n        no_repeat_ngram_size=2\n    )\n\n    # 5) Decode only the newly generated tokens\n    full_sequence = outputs[0]                     # shape: [total_length]\n    new_tokens = full_sequence[prompt_length:]     # slice off the prompt\n    prediction = tokenizer.decode(new_tokens, skip_special_tokens=True)\n\n    return prediction.strip()\n</code></pre>"},{"location":"api/model_training/summary/#model_training.nlp.summary.predict_compare.main","title":"<code>main()</code>","text":"<p>Main function to load test data, generate predictions using each model, compute evaluation metrics, and compare the performance of each model.</p> Source code in <code>model_training\\nlp\\summary\\predict_compare.py</code> <pre><code>def main():\n    \"\"\"\n    Main function to load test data, generate predictions using each model,\n    compute evaluation metrics, and compare the performance of each model.\n    \"\"\"\n    test_file = \"data/test.jsonl\"\n    logger.info(f\"Loading test data from {test_file}\")\n    test_data = load_jsonl(test_file)\n\n    if not test_data:\n        logger.error(\"No test data found. Exiting.\")\n        return\n\n    # Extract inputs and labels\n    inputs = [entry['input'] for entry in test_data]\n    labels = [entry['output'] for entry in test_data]\n\n    # Initialize a dictionary to store predictions for each model\n    model_predictions: Dict[str, List[str]] = {\n        \"T5\": [],\n        \"BART\": [],\n        \"BLOOM\": [],\n        \"LLaMA2\": []\n    }\n\n    # Load tokenizers and each model\n    model_T5, tokenizer_T5 = load_T5_model(model_dir = \"./models/t5\")\n    model_BART, tokenizer_BART = load_BART_model(model_dir = \"./models/bart\")\n    model_BLOOM, tokenizer_BLOOM = load_BLOOM_model(model_dir = \"./models/bloom\")\n    model_LLAMA2, tokenizer_LLAMA2 = load_LLAMA2_model()\n\n    # Generate predictions for each model\n    for idx, input_text in enumerate(inputs):\n        logger.info(f\"Processing sample {idx+1}/{len(inputs)}\")\n        try:\n            pred_t5 = predict_T5(input_text=input_text, model=model_T5, tokenizer=tokenizer_T5)\n            model_predictions[\"T5\"].append(pred_t5)\n        except Exception as e:\n            logger.error(f\"T5 prediction failed for sample {idx+1}: {e}\")\n            model_predictions[\"T5\"].append(\"\")\n\n        try:\n            pred_bart = predict_BART(input_text=input_text, model=model_BART, tokenizer=tokenizer_BART)\n            model_predictions[\"BART\"].append(pred_bart)\n        except Exception as e:\n            logger.error(f\"BART prediction failed for sample {idx+1}: {e}\")\n            model_predictions[\"BART\"].append(\"\")\n\n        try:\n            pred_bloom = predict_BLOOM(input_text=input_text, model=model_BLOOM, tokenizer=tokenizer_BLOOM)\n            model_predictions[\"BLOOM\"].append(pred_bloom)\n        except Exception as e:\n            logger.error(f\"BLOOM prediction failed for sample {idx+1}: {e}\")\n            model_predictions[\"BLOOM\"].append(\"\")\n\n        try:\n            pred_llama2 = predict_LLAMA2(input_text=input_text, model=model_LLAMA2, tokenizer=tokenizer_LLAMA2)\n            model_predictions[\"LLaMA2\"].append(pred_llama2)\n        except Exception as e:\n            logger.error(f\"LLaMA2 prediction failed for sample {idx+1}: {e}\")\n            model_predictions[\"LLaMA2\"].append(\"\")\n\n    # Encode predictions and labels for compute_metrics\n    model_metrics: Dict[str, Dict[str, float]] = {}\n\n    for model_name, predictions in model_predictions.items():\n        logger.info(f\"Computing metrics for model: {model_name}\")\n        try:\n            if model_name == \"T5\":\n                tokenizer = tokenizer_T5\n            elif model_name == \"BART\":\n                tokenizer = tokenizer_BART\n            elif model_name == \"BLOOM\":\n                tokenizer = tokenizer_BLOOM\n            elif model_name == \"LLaMA2\":\n                tokenizer = tokenizer_LLAMA2\n            else:\n                logger.warning(f\"No tokenizer found for model: {model_name}\")\n                tokenizer = None\n\n            if tokenizer is None:\n                raise ValueError(f\"Tokenizer for model {model_name} is not loaded.\")\n\n            # Encode predictions and labels\n            # Replace empty predictions with tokenizer.pad_token or appropriate token\n            encoded_predictions = [\n                pred if pred else tokenizer.pad_token for pred in predictions\n            ]\n            encoded_labels = [\n                label if label else tokenizer.pad_token for label in labels\n            ]\n\n            # Tokenize the predictions and labels to get token IDs\n            tokenized_predictions = tokenizer(\n                encoded_predictions,\n                padding=True,\n                truncation=True,\n                max_length=tokenizer.model_max_length,\n                return_tensors=\"np\"\n            )[\"input_ids\"]\n\n            tokenized_labels = tokenizer(\n                encoded_labels,\n                padding=True,\n                truncation=True,\n                max_length=tokenizer.model_max_length,\n                return_tensors=\"np\"\n            )[\"input_ids\"]\n\n            # Compute metrics\n            metrics = compute_metrics((tokenized_predictions, tokenized_labels), tokenizer=tokenizer)\n            model_metrics[model_name] = metrics\n            logger.info(f\"Metrics for {model_name}: {metrics}\")\n        except Exception as e:\n            logger.error(f\"Error computing metrics for model {model_name}: {e}\")\n            model_metrics[model_name] = {}\n\n    # Display the comparison of metrics\n    print(\"\\n=== Model Performance Comparison ===\\n\")\n    for model_name, metrics in model_metrics.items():\n        print(f\"Model: {model_name}\")\n        if metrics:\n            for metric_name, value in metrics.items():\n                print(f\"  {metric_name}: {value}\")\n        else:\n            print(\"  Metrics not available due to errors.\")\n        print()\n\n    # Save the comparison to a JSON file\n    comparison_file = \"model_performance_comparison.json\"\n    with open(comparison_file, \"w\") as f:\n        json.dump(model_metrics, f, indent=4)\n    logger.info(f\"Model performance comparison saved to {comparison_file}\")\n</code></pre>"},{"location":"api/model_training/summary/#model_training.nlp.summary.predict_LLAMA2.load_LLAMA2_model","title":"<code>load_LLAMA2_model(model_dir='/models/llama2')</code>","text":"<p>Load a LLaMA 2 model from Hugging Face that is supposed to be stored locally at the given path. For a real-world scenario, ensure you have:   - 'transformers&gt;=4.30'   - 'sentencepiece'   - You have accepted the license for LLaMA2 if it's gated.</p> <p>Args:       model_dir (str): path to directory where the model is supposed to be stored.</p> Source code in <code>model_training\\nlp\\summary\\predict_LLAMA2.py</code> <pre><code>def load_LLAMA2_model(model_dir: str = \"/models/llama2\"):\n    \"\"\"\n    Load a LLaMA 2 model from Hugging Face that is supposed to be stored locally at the given path.\n    For a real-world scenario, ensure you have:\n      - 'transformers&gt;=4.30'\n      - 'sentencepiece'\n      - You have accepted the license for LLaMA2 if it's gated.\n\n      Args:\n          model_dir (str): path to directory where the model is supposed to be stored.\n    \"\"\"\n    logger.info(f\"Loading model '{model_dir}' from Hugging Face...\")\n\n    # 'legacy=True' is sometimes needed for older model conversions,\n    # but if you see deprecation warnings, remove it or try without it.\n    tokenizer = LlamaTokenizer.from_pretrained(model_dir, legacy=True)\n    model = LlamaForCausalLM.from_pretrained(\n        model_dir,\n        device_map=\"auto\",\n    )\n\n    # Set pad_token to eos_token\n    tokenizer.pad_token = tokenizer.eos_token\n    model.config.pad_token_id = tokenizer.eos_token_id\n\n    return model, tokenizer\n</code></pre>"},{"location":"api/model_training/summary/#model_training.nlp.summary.predict_LLAMA2.predict","title":"<code>predict(input_text, model=None, tokenizer=None, model_dir='/storage/ukp/shared/shared_model_weights/models--llama-2-hf/7B-Chat', min_new_tokens=20, max_new_tokens_cap=512)</code>","text":"<p>Build the prompt from real data, run generation on LLaMA 2,  and return an output that is ~2/3 of the prompt length (with a min of <code>min_new_tokens</code>), but also capped by <code>max_new_tokens_cap</code>.</p> <p>Parameters:</p> Name Type Description Default <code>input_text</code> <code>str</code> <p>Raw text or data for the prompt builder.</p> required <code>model_dir</code> <code>str</code> <p>Path or HF repo for your LLaMA2 model.</p> <code>'/storage/ukp/shared/shared_model_weights/models--llama-2-hf/7B-Chat'</code> <code>min_new_tokens</code> <code>int</code> <p>Ensures we generate at least this many tokens.</p> <code>20</code> <code>max_new_tokens_cap</code> <code>int</code> <p>Hard upper bound on new tokens generated.</p> <code>512</code> <p>Returns:</p> Name Type Description <code>str</code> <p>The final generated text (prompt echo stripped).</p> Source code in <code>model_training\\nlp\\summary\\predict_LLAMA2.py</code> <pre><code>def predict(\n    input_text: str,\n    model=None,\n    tokenizer=None,\n    model_dir: str = \"/storage/ukp/shared/shared_model_weights/models--llama-2-hf/7B-Chat\",\n    min_new_tokens: int = 20,\n    max_new_tokens_cap: int = 512,\n):\n    \"\"\"\n    Build the prompt from real data, run generation on LLaMA 2, \n    and return an output that is ~2/3 of the prompt length (with a min\n    of `min_new_tokens`), but also capped by `max_new_tokens_cap`.\n\n    Args:\n        input_text (str): Raw text or data for the prompt builder.\n        model_dir (str): Path or HF repo for your LLaMA2 model.\n        min_new_tokens (int): Ensures we generate at least this many tokens.\n        max_new_tokens_cap (int): Hard upper bound on new tokens generated.\n\n    Returns:\n        str: The final generated text (prompt echo stripped).\n    \"\"\"\n    # 1) Load model &amp; tokenizer if needed\n    if (model is None and tokenizer is None):\n        model, tokenizer = load_LLAMA2_model(model_dir)\n\n    # 2) Build the final prompt from the input data\n    prompt = input_to_prompt_converter.build_llama2_prompt(input_text)\n    #logger.info(\"Prompt is ready. Calculating lengths and generating...\")\n\n    # 3) Measure the prompt and input length in tokens\n    prompt_tokens = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False)\n    #input_tokens = tokenizer(input_text, return_tensors=\"pt\", add_special_tokens=False)\n    #input_length = input_tokens[\"input_ids\"].shape[1]\n\n    #logger.debug(f\"Detected input length (tokens): {input_length}\")\n\n    # 4) Desired generation length: ~2/3 of prompt length, but at least min_new_tokens\n    #desired_length = int(round((2.0 / 3.0) * input_length))\n    #desired_length = max(desired_length, min_new_tokens)\n\n    #logger.info(f\"Desired output length (tokens): {desired_length}\")\n\n    # 5) Move everything to the model's device\n    device = model.device\n    for k, v in prompt_tokens.items():\n        prompt_tokens[k] = v.to(device)\n\n    # 6) Create generation config\n    gen_config = GenerationConfig(\n        max_new_tokens=150,\n        do_sample=True,\n        temperature=0.7,\n        top_p=0.9,\n        repetition_penalty=1.2,  # helps reduce repeated tokens\n        # Additional parameters (e.g., num_beams) if desired\n    )\n\n    # 7) Generate\n    with torch.no_grad():\n        outputs = model.generate(**prompt_tokens, generation_config=gen_config)\n\n    # 8) Decode\n    decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n\n    # LLaMA often re-echos the prompt. Remove it if present:\n    result = decoded[0]\n    if result.startswith(prompt):\n        result = result[len(prompt):]\n\n    return result.strip()\n</code></pre>"},{"location":"api/model_training/theme/","title":"Theme Classification Model Training Code Documentation","text":""},{"location":"api/model_training/theme/#model_training.nlp.review_to_theme.functions_docu.read_epoch_lr_json","title":"<code>read_epoch_lr_json(base_dirs, df)</code>","text":"<p>Navigates through subfolders to read JSON files and extract model evaluation metrics.</p> <p>This function iterates through a list of base directories, searches for subdirectories  representing epoch numbers and learning rates, and reads <code>all_results.json</code> files to  extract evaluation metrics.</p> <p>Parameters:</p> Name Type Description Default <code>base_dirs</code> <code>list[str]</code> <p>List of base directories, each corresponding to a model name.</p> required <code>df</code> <code>DataFrame</code> <p>A DataFrame to which the extracted data will be appended.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <ul> <li>pd.DataFrame: Updated DataFrame with extracted data.</li> <li>str: Last found epoch value.</li> <li>float: Last found learning rate value.</li> </ul> The DataFrame will have the following columns <ul> <li>'model_name': Name of the model (base directory).</li> <li>'epoch': Epoch number extracted from folder names.</li> <li>'learning_rate': Learning rate extracted from folder names.</li> <li>'eval_accuracy': Accuracy metric from JSON file.</li> <li>'eval_f1': F1-score from JSON file.</li> <li>'eval_precision': Precision metric from JSON file.</li> <li>'eval_recall': Recall metric from JSON file.</li> </ul> Source code in <code>model_training\\nlp\\review_to_theme\\functions_docu.py</code> <pre><code>def read_epoch_lr_json(base_dirs, df):\n    \"\"\"\n    Navigates through subfolders to read JSON files and extract model evaluation metrics.\n\n    This function iterates through a list of base directories, searches for subdirectories \n    representing epoch numbers and learning rates, and reads `all_results.json` files to \n    extract evaluation metrics.\n\n    Args:\n        base_dirs (list[str]): List of base directories, each corresponding to a model name.\n        df (pd.DataFrame): A DataFrame to which the extracted data will be appended.\n\n    Returns:\n        tuple:\n            - pd.DataFrame: Updated DataFrame with extracted data.\n            - str: Last found epoch value.\n            - float: Last found learning rate value.\n\n    The DataFrame will have the following columns:\n        - 'model_name': Name of the model (base directory).\n        - 'epoch': Epoch number extracted from folder names.\n        - 'learning_rate': Learning rate extracted from folder names.\n        - 'eval_accuracy': Accuracy metric from JSON file.\n        - 'eval_f1': F1-score from JSON file.\n        - 'eval_precision': Precision metric from JSON file.\n        - 'eval_recall': Recall metric from JSON file.\n    \"\"\"\n    epoch_value = None\n    lr_value = 0\n    for base_dir in base_dirs:\n        # Iterate through subfolders A, B, C, D, E, F\n        if os.path.exists(base_dir):\n            for epoch_folder in os.listdir(base_dir):\n                epoch_folder_path = os.path.join(base_dir, epoch_folder)\n\n                # Check if the folder name is an integer (epoch value)\n                if os.path.isdir(epoch_folder_path):\n                    epoch_value = epoch_folder\n\n                    for lr_folder in os.listdir(epoch_folder_path):\n                        lr_folder_path = os.path.join(epoch_folder_path, lr_folder)\n\n                        # Check if the folder name is a valid learning rate (float value)\n                        try:\n                            lr_value = float(lr_folder)\n                        except ValueError:\n                            continue  # Skip folders that do not represent learning rate\n\n                        # Now, look for the json file in the learning rate folder\n                        json_file_path = os.path.join(lr_folder_path, 'all_results.json')\n\n                        if os.path.isfile(json_file_path):\n                            # Read the JSON file\n                            with open(json_file_path, 'r') as f:\n                                json_data = json.load(f)\n\n                            # Store the result with epoch, learning rate, and json content\n                            # Extract necessary values\n                            eval_accuracy = json_data.get('eval_accuracy', None)\n                            eval_f1 = json_data.get('eval_f1', None)\n                            eval_precision = json_data.get('eval_precision', None)\n                            eval_recall = json_data.get('eval_recall', None)\n\n                            # Create a new row with the current values\n                            new_row = pd.DataFrame([{\n                                'model_name': base_dir,  # model_name corresponds to the base_dir\n                                'epoch': epoch_value,\n                                'learning_rate': lr_value,\n                                'eval_accuracy': eval_accuracy,\n                                'eval_f1': eval_f1,\n                                'eval_precision': eval_precision,\n                                'eval_recall': eval_recall\n                            }])\n                            # Append the new row to the DataFrame\n                            df = pd.concat([df, new_row], ignore_index=True)\n\n    return df, epoch_value, lr_value\n</code></pre>"},{"location":"api/model_training/theme_to_desc/","title":"Description Model Training Code Documentation","text":"<p>Fine-tuning the library models for sequence to sequence.</p>"},{"location":"api/model_training/theme_to_desc/#model_training.nlp.e2e_review_to_desc.main.DataTrainingArguments","title":"<code>DataTrainingArguments</code>  <code>dataclass</code>","text":"<p>Arguments pertaining to what data we are going to input our model for training and eval.</p> Source code in <code>model_training\\nlp\\e2e_review_to_desc\\main.py</code> <pre><code>@dataclass\nclass DataTrainingArguments:\n    \"\"\"\n    Arguments pertaining to what data we are going to input our model for training and eval.\n    \"\"\"\n\n    lang: Optional[str] = field(default=None, metadata={\"help\": \"Language id for summarization.\"})\n\n    dataset_name: Optional[str] = field(\n        default=None, metadata={\"help\": \"The name of the dataset to use (via the datasets library).\"}\n    )\n    dataset_config_name: Optional[str] = field(\n        default=None, metadata={\"help\": \"The configuration name of the dataset to use (via the datasets library).\"}\n    )\n    text_column: Optional[str] = field(\n        default=None,\n        metadata={\"help\": \"The name of the column in the datasets containing the full texts (for summarization).\"},\n    )\n    summary_column: Optional[str] = field(\n        default=None,\n        metadata={\"help\": \"The name of the column in the datasets containing the summaries (for summarization).\"},\n    )\n    train_file: Optional[str] = field(\n        default=None, metadata={\"help\": \"The input training data file (a jsonlines or csv file).\"}\n    )\n    validation_file: Optional[str] = field(\n        default=None,\n        metadata={\n            \"help\": (\n                \"An optional input evaluation data file to evaluate the metrics (rouge) on (a jsonlines or csv file).\"\n            )\n        },\n    )\n    test_file: Optional[str] = field(\n        default=None,\n        metadata={\n            \"help\": \"An optional input test data file to evaluate the metrics (rouge) on (a jsonlines or csv file).\"\n        },\n    )\n    overwrite_cache: bool = field(\n        default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n    )\n    preprocessing_num_workers: Optional[int] = field(\n        default=None,\n        metadata={\"help\": \"The number of processes to use for the preprocessing.\"},\n    )\n    max_source_length: Optional[int] = field(\n        default=1024,\n        metadata={\n            \"help\": (\n                \"The maximum total input sequence length after tokenization. Sequences longer \"\n                \"than this will be truncated, sequences shorter will be padded.\"\n            )\n        },\n    )\n    max_target_length: Optional[int] = field(\n        default=128,\n        metadata={\n            \"help\": (\n                \"The maximum total sequence length for target text after tokenization. Sequences longer \"\n                \"than this will be truncated, sequences shorter will be padded.\"\n            )\n        },\n    )\n    val_max_target_length: Optional[int] = field(\n        default=None,\n        metadata={\n            \"help\": (\n                \"The maximum total sequence length for validation target text after tokenization. Sequences longer \"\n                \"than this will be truncated, sequences shorter will be padded. Will default to `max_target_length`.\"\n                \"This argument is also used to override the ``max_length`` param of ``model.generate``, which is used \"\n                \"during ``evaluate`` and ``predict``.\"\n            )\n        },\n    )\n    pad_to_max_length: bool = field(\n        default=False,\n        metadata={\n            \"help\": (\n                \"Whether to pad all samples to model maximum sentence length. \"\n                \"If False, will pad the samples dynamically when batching to the maximum length in the batch. More \"\n                \"efficient on GPU but very bad for TPU.\"\n            )\n        },\n    )\n    max_train_samples: Optional[int] = field(\n        default=None,\n        metadata={\n            \"help\": (\n                \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n                \"value if set.\"\n            )\n        },\n    )\n    max_eval_samples: Optional[int] = field(\n        default=None,\n        metadata={\n            \"help\": (\n                \"For debugging purposes or quicker training, truncate the number of evaluation examples to this \"\n                \"value if set.\"\n            )\n        },\n    )\n    max_predict_samples: Optional[int] = field(\n        default=None,\n        metadata={\n            \"help\": (\n                \"For debugging purposes or quicker training, truncate the number of prediction examples to this \"\n                \"value if set.\"\n            )\n        },\n    )\n    num_beams: Optional[int] = field(\n        default=None,\n        metadata={\n            \"help\": (\n                \"Number of beams to use for evaluation. This argument will be passed to ``model.generate``, \"\n                \"which is used during ``evaluate`` and ``predict``.\"\n            )\n        },\n    )\n    ignore_pad_token_for_loss: bool = field(\n        default=True,\n        metadata={\n            \"help\": \"Whether to ignore the tokens corresponding to padded labels in the loss computation or not.\"\n        },\n    )\n    source_prefix: Optional[str] = field(\n        default=\"\", metadata={\"help\": \"A prefix to add before every source text (useful for T5 models).\"}\n    )\n\n    forced_bos_token: Optional[str] = field(\n        default=None,\n        metadata={\n            \"help\": (\n                \"The token to force as the first generated token after the decoder_start_token_id.\"\n                \"Useful for multilingual models like mBART where the first generated token\"\n                \"needs to be the target language token (Usually it is the target language token)\"\n            )\n        },\n    )\n\n    def __post_init__(self):\n        if (\n            self.dataset_name is None\n            and self.train_file is None\n            and self.validation_file is None\n            and self.test_file is None\n        ):\n            raise ValueError(\"Need either a dataset name or a training, validation, or test file.\")\n        else:\n            if self.train_file is not None:\n                extension = self.train_file.split(\".\")[-1]\n                assert extension in [\"csv\", \"json\"], \"`train_file` should be a csv or a json file.\"\n            if self.validation_file is not None:\n                extension = self.validation_file.split(\".\")[-1]\n                assert extension in [\"csv\", \"json\"], \"`validation_file` should be a csv or a json file.\"\n            if self.test_file is not None:\n                extension = self.test_file.split(\".\")[-1]\n                assert extension in [\"csv\", \"json\"], \"`test_file` should be a csv or a json file.\"\n        if self.val_max_target_length is None:\n            self.val_max_target_length = self.max_target_length\n</code></pre>"},{"location":"api/model_training/theme_to_desc/#model_training.nlp.e2e_review_to_desc.main.ModelArguments","title":"<code>ModelArguments</code>  <code>dataclass</code>","text":"<p>Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.</p> Source code in <code>model_training\\nlp\\e2e_review_to_desc\\main.py</code> <pre><code>@dataclass\nclass ModelArguments:\n    \"\"\"\n    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\n    \"\"\"\n\n    model_name_or_path: str = field(\n        metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n    )\n    config_name: Optional[str] = field(\n        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n    )\n    tokenizer_name: Optional[str] = field(\n        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n    )\n    cache_dir: Optional[str] = field(\n        default=None,\n        metadata={\"help\": \"Where to store the pretrained models downloaded from huggingface.co\"},\n    )\n    use_fast_tokenizer: bool = field(\n        default=True,\n        metadata={\"help\": \"Whether to use one of the fast tokenizer (backed by the tokenizers library) or not.\"},\n    )\n    model_revision: str = field(\n        default=\"main\",\n        metadata={\"help\": \"The specific model version to use (can be a branch name, tag name or commit id).\"},\n    )\n    use_auth_token: bool = field(\n        default=False,\n        metadata={\n            \"help\": (\n                \"Will use the token generated when running `huggingface-cli login` (necessary to use this script \"\n                \"with private models).\"\n            )\n        },\n    )\n    resize_position_embeddings: Optional[bool] = field(\n        default=None,\n        metadata={\n            \"help\": (\n                \"Whether to automatically resize the position embeddings if `max_source_length` exceeds \"\n                \"the model's position embeddings.\"\n            )\n        },\n    )\n</code></pre>"},{"location":"api/model_training/theme_to_desc/#model_training.nlp.e2e_review_to_desc.function_docu.compute_edit_distance","title":"<code>compute_edit_distance(prediction, label)</code>","text":"<p>Computes a normalized similarity score based on Levenshtein (edit) distance.</p> The score is computed as <p>similarity = 1 - (edit_distance / max_length)</p> <p>where <code>max_length</code> is the length of the longer string to normalize the distance.</p> <p>Parameters:</p> Name Type Description Default <code>prediction</code> <code>str</code> <p>The predicted text.</p> required <code>label</code> <code>str</code> <p>The reference label for comparison.</p> required <p>Returns:</p> Name Type Description <code>float</code> <p>Normalized similarity score (range: 0 to 1, where 1 means identical strings).</p> Source code in <code>model_training\\nlp\\e2e_review_to_desc\\function_docu.py</code> <pre><code>def compute_edit_distance(prediction, label):\n    \"\"\"\n    Computes a normalized similarity score based on Levenshtein (edit) distance.\n\n    The score is computed as:\n        similarity = 1 - (edit_distance / max_length)\n    where `max_length` is the length of the longer string to normalize the distance.\n\n    Args:\n        prediction (str): The predicted text.\n        label (str): The reference label for comparison.\n\n    Returns:\n        float: Normalized similarity score (range: 0 to 1, where 1 means identical strings).\n    \"\"\"\n    edit_dist = Levenshtein.distance(prediction, label)\n    max_len = max(len(prediction), len(label))\n    return 1 - (edit_dist / max_len)  # Normalize and return similarity\n</code></pre>"},{"location":"api/model_training/theme_to_desc/#model_training.nlp.e2e_review_to_desc.function_docu.compute_embedding_similarity","title":"<code>compute_embedding_similarity(prediction, label)</code>","text":"<p>Computes the cosine similarity between the embeddings of a predicted text and a reference label.</p> <p>Parameters:</p> Name Type Description Default <code>prediction</code> <code>str</code> <p>The predicted text.</p> required <code>label</code> <code>str</code> <p>The reference label for comparison.</p> required <p>Returns:</p> Name Type Description <code>float</code> <p>Cosine similarity score between the two embeddings (range: -1 to 1).</p> Source code in <code>model_training\\nlp\\e2e_review_to_desc\\function_docu.py</code> <pre><code>def compute_embedding_similarity(prediction, label):\n    \"\"\"\n    Computes the cosine similarity between the embeddings of a predicted text and a reference label.\n\n    Args:\n        prediction (str): The predicted text.\n        label (str): The reference label for comparison.\n\n    Returns:\n        float: Cosine similarity score between the two embeddings (range: -1 to 1).\n    \"\"\"\n    pred_embedding = model.encode([prediction])\n    label_embedding = model.encode([label])\n    return cosine_similarity(pred_embedding, label_embedding)[0][0]\n</code></pre>"},{"location":"api/readme/architecture/","title":"Architecture and Design Notes","text":""},{"location":"api/readme/architecture/#architecture","title":"Architecture","text":""},{"location":"api/readme/architecture/#1-frontend","title":"1 Frontend","text":"<p>The Frontend is the user interface of the system where individuals log in, provide a URL to OpenReview, and optionally download and then upload the filled out templates. The Frontend handles interactions, collects the user\u2019s input (including files and URLs), and displays the resulting classification output once the Backend has processed everything.</p>"},{"location":"api/readme/architecture/#2-backend","title":"2 Backend","text":"<p>Once the Frontend submits data (whether uploaded files or URLs), the Backend starts analyzing the provided data. It first performs formatting and segmentation, breaking the reviews into sentences. From there, the system routes the segments to various prediction modules. The \u201cRequest Prediction\u201d module handles general categorization of the Request, while an \u201cAttitude/Theme Prediction\u201d module determines attitude and corresponding themes and descriptions. After processing these steps, the Backend compiles the outputs\u2014now in the form of classified sentences or structured results\u2014and sends them back to the Frontend to display to the user.</p>"},{"location":"api/readme/architecture/#3-model-training","title":"3 Model Training","text":"<p>As part of our framework, there is the model training. Initially we trained the models used in the Backend to perform the neccesary classification tasks. This training process results in Model Artifacts, such as updated model parameters, which the Backend uses during its prediction steps. If necessary, the existing model and code files can be used to update and improve existing models with new data or better models.</p> <p>Request Classifier</p> <p>For the Request Classifier we used two fine tuned models to firstly label what sentences are requests and then have a fine grained classification for the type of request. The first is a binary classifier which achieved an F1 Score of 91%, we tried oversampling, but the results were worse. For the fine requests we have a multi class prediction for which we also tried different models and approaches. The best results were with the addition of thresholding resulting in a F1 score of 62%.</p> <p>Attitude and Theme Classifier</p> <p>For the Attitude Roots which represent the underlying believes we made a multi class prediction and tried different models and approaches (normal training, oversampling and hybrid with normal training and oversampling). We achieved an F1 Score of 62% with the BERT model which was pretrained on domain knowledge.</p> <p>Summary Generation</p> <p>To create the summary\u2014which aggregates all results sorted by \u201cOverview\u201d, \u201cAttitude Roots\u201d and \u201cRequest Information\u201d\u2014we tried different models and evaluated their performance using the BERT score. More specifically, we pre-structured the summary using Python and then generated predictions only for the collections of comments corresponding to a particular attitude root or request, as determined by our other models.</p> <p>Data Collection and Labeling Process: We manually collected data from nine OpenReview threads to ensure a balanced distribution of overall ratings. Specifically, the selection includes three examples from each rating category: \"low\" with overall score &lt; 4, \"average\" with overall score &gt;=4 but &lt; 7 and \"high\" with overall score &gt;= 7. Instead of treating each individual comment as a separate data point, we clustered sentences so that each cluster represents the set of comments associated with a single paper and corresponds to a specific \"attitude root\" or request as identified by our preliminary models (e.g., all comments complaining about a typo). This clustering resulted in 174 aggregated data points (see model_training/nlp/summary/data/real_world_data_unlabeled.jsonl). Next, each data point was labeled using OpenAI's so far most capable model ChatGPT o1. We then proofread these labels to ensure high quality (see model_training/nlp/summary/data/real_world_data_labeled.jsonl).</p> <p>Prediction: We tried sequence to sequence approaches with BART-large and T5 using an 80%-10%-10% train-validation-test split. The best results were obtained using Llama2 with a 10-shot prompt, achieving an F1-BERT score of 69% on the test data.</p>"},{"location":"api/readme/architecture/#4-communication-flow","title":"4 Communication Flow","text":"<p>Frontend \u2192 Backend</p> <p>The Frontend issues secure API calls to the Backend when users log in, provide URLs, or upload filled templates. The Backend processes these incoming requests\u2014formatting and segmenting the data\u2014and routes them to the appropriate prediction modules. The Backend functions as a API gateway.</p> <p>Backend \u2192 Frontend</p> <p>Once the predictions are complete, the Backend responds via API calls back to the Frontend, delivering classified sentences, sentiment results, or other structured outputs. The Frontend then displays these results to the user in a clear, readable format.</p> <p>Model Training and Backend</p> <p>The models trained in the Backend are stored in the designated containers with are activated when the Backend is called.</p>"},{"location":"api/readme/contact/","title":"Contact","text":"<p>If you have any questions or suggestions regarding this project, feel free to reach out:</p> <ul> <li>Your Name: Johannes Lemken</li> <li>Email: johannes.lemken@stud.tu-darmstadt.de</li> <li>GitHub Profile: Johannes Lemken (https://github.com/JohannesLemken)</li> </ul> <p>Thank you for your interest in this project!</p>"},{"location":"api/readme/data/","title":"Data","text":"<p>For this project, we utilized two primary datasets:</p> <p>JiujitsuPeer Dataset</p> <p>Used for developing the attitude and theme prediction models. The dataset comes pre-labeled by its original creators, providing ground truth annotations for sentiment or stance (attitude) and thematic categories (theme). We selectively extracted and subdivided only those sections most relevant for our model objectives, ensuring training data remained highly focused on the classification tasks at hand.</p> <p>DISAPERE Dataset</p> <p>Used for building and refining our request prediction models. Like the JiujitsuPeer Dataset, DISAPERE was pre-labeled with relevant review actions and requests, allowing us to apply segmentation and filtering specific to the request-classification requirements. We further tailored the dataset by removing or restructuring fields not pertinent to predicting review actions, simplifying integration with our overall pipeline.</p> <p>By leveraging these pre-labeled datasets\u2014and performing only minimal pre-processing to isolate the pertinent fields\u2014we streamlined the model training phase while retaining high-quality annotations for the core prediction tasks.</p>"},{"location":"api/readme/developer_guide/","title":"Developer Guide","text":""},{"location":"api/readme/developer_guide/#clone-the-repository","title":"Clone the Repository","text":"<pre><code>  git clone https://github.com/sukannyapurkayastha/DASP_report_template.git\n  cd your-project\n</code></pre> <p>This is the instruction for developers, who want to debug or directly work with our codes. We have currently 5 components: - frontend - backend (preprocess, segmentation and score calculation) - attitude_classifier - request_classifier - summary_generator</p>"},{"location":"api/readme/developer_guide/#install-environment","title":"Install Environment","text":"<p>Folders have same name as components. In each folder there is a requirements file and conda environment file.  - usage of conda environment file: It contains all information for environment like python verison, list of libs and versions of libs. It's recommended to use this file firstly, if it omits error, try requirements.txt file. - usage of requirements.txt: It's served for docker. But you can also install your environment with this file. Python 3.10 is used for all containers. Versions of libs are not specified in requirements file, in order to prevent version mismatching errors.</p>"},{"location":"api/readme/developer_guide/#run-services-from-terminal","title":"Run Services From Terminal","text":"<pre><code>cd frontend\nstreamlit run app.py\n</code></pre> <p>Following command is valid for all these 4 components, replace {component_folder} with backend, attitude_classifier, request_classifier, summary_generator</p> <pre><code>cd {component_folder}\npython main.py\n</code></pre> <p>After running all 5 commands, the web application is running on localhost:8000. Note that running without docker, it's available at port 8000.</p>"},{"location":"api/readme/files/","title":"Files","text":""},{"location":"api/readme/files/#1-backend","title":"1 <code>backend</code>","text":"<p>The backend is responsible for data collection from OpenReview, preprocessing the data, and managing the workflow between the models and the frontend. It sends preprocessed data to the models for processing, retrieves the results, and communicates the processed data to the frontend for further use.</p> <ul> <li>Subdirectories:</li> <li><code>dataloading</code>: includes scripts for using the open review API</li> <li><code>text_processing</code>: includes file and scripts which are used to preprocess and prepare review for the models</li> <li> <p><code>test</code>: includes test for the text processing</p> </li> <li> <p>Files:</p> </li> <li><code>data_processing.py</code>: calls functions from text processing  </li> <li><code>main.py</code>: creates FastAPI app and runs it</li> <li><code>routers.py</code>: defines a FastAPI endpoint and structures processed data into dictonaries</li> <li><code>backend_env.yaml</code>: contains the packages and dependencies for the backend</li> </ul>"},{"location":"api/readme/files/#2-frontend","title":"2 <code>frontend</code>","text":"<p>The front end of this application is the user-facing interface built with Streamlit and related UI components. It is responsible for displaying the overview over the given reviews, receive the data in form of URL or file, and providing an interactive experience within the application. It is connected via API calls to the backend and includes several features of displaying the data for the user.</p> <ul> <li>Subdirectories:</li> <li><code>images</code>: includes the images needed for displaying the front end </li> <li><code>data</code>: includes the templates for the user if data of reviews is manually input </li> <li><code>modules</code>: includes several scriptsfor features of the frontend like containers for specific parts of the data or elements                of the website like the side bar</li> <li><code>clients</code>: includes the client for excessing OpenReview</li> <li><code>.streamlit</code>: includes a configfile for the custom design</li> <li> <p><code>tests</code>: tests for the frontend</p> </li> <li> <p>Files:</p> </li> <li><code>__init__.py</code>: Initializes the Python package and sets up the module structure (not in use)</li> <li><code>about.py</code>: calls about under modules which contains content and layout logic for the \"About\" page of the front end</li> <li><code>app.py</code>: Coordinates the overall structure and routing of the front end application\u2019s pages</li> <li><code>contact.py</code>: calls about under modules which  implements the \"Contact\" page, providing forms or information for user inquiries</li> <li><code>data.ipynb</code>: jupyper notebook with specifies the right structure and content of the dataframes for the front end </li> <li><code>frontend_env.yaml</code>: Defines the environment dependencies and configurations for running the front end</li> <li><code>home_page.py</code>: Implements the \"Home\" page, showcasing primary content and user entry points</li> <li><code>landing_page.py</code>: Sets up the initial landing view, guiding users to the main sections of the application</li> <li><code>main_page.py</code>: Acts as the core page aggregating or linking to the main functionalities of the site</li> <li><code>README</code>: A documentation file describing the purpose, setup, and usage of the front end</li> <li><code>run.py</code>: A script that launches the Streamlit application and makes the front end accessible</li> <li><code>streamlit_app.py</code>: The primary Streamlit script coordinating UI components, page navigation, and interactions</li> </ul>"},{"location":"api/readme/files/#3-model_training","title":"3 <code>model_training</code>","text":"<p>The model training folder includes the scripts and data which which the models where trained.</p> <ul> <li>Subdirectories:</li> <li><code>attitude_classifier</code>: includes the scripts for fine tuning the a model for multi class classification for attitude roots</li> <li><code>e2e_review_to_desc</code>:  includes the scripts and the data for fine tuning the a model for multi class classification for                            attitude themes and matches them to a description </li> <li><code>request_classifier</code>: includes the data (DISAPERE) and the scripts for binary classication for Review_Action Request vs. All                            and a multi class classification for the fine review actions </li> <li><code>review_to_theme</code>: includes scripts for mapping review sentences to themes</li> <li><code>summary</code>: includes scripts for training of summary generation models. Specifically training of T5-large, BART-large and Llama2.</li> </ul>"},{"location":"api/readme/files/#4-attitude_classifier","title":"4 <code>attitude_classifier</code>","text":"<p>The attitude classifier can contains the model and contains the scripts for the request classifier pipeline.</p> <ul> <li>Files:</li> <li><code>main.py</code>: defines a FastAPI endpoint, creates FastAPI app and runs it</li> <li><code>model_prediction.py</code>:  structures, classifies and transforms processed data into target table</li> <li><code>description_generation.py</code>: generates description for class clusters</li> <li><code>attitude_classifier_env.yaml</code>: contains the packages and dependencies</li> <li><code>attitude_desc.csv</code>: contains mapping information between attitude clusters and description</li> </ul>"},{"location":"api/readme/files/#5-request_classifier","title":"5 <code>request_classifier</code>","text":"<p>The request classifier can contains the model and contains the scripts for the request classifier pipeline.</p> <ul> <li>Subdirectories:</li> <li> <p><code>classification</code>: includes scripts for using the request classifier pipeline</p> </li> <li> <p>Files:</p> </li> <li><code>main.py</code>: creates FastAPI app and runs it</li> <li><code>routers.py</code>: defines a FastAPI endpoint and structures processed data into dictonaries</li> <li><code>request_classifier.yaml</code>: contains the packages and dependencies for the enviroment</li> </ul>"},{"location":"api/readme/files/#6-summary_generator","title":"6 <code>summary_generator</code>","text":"<p>The summyry generator folder can contains the model and contains the scripts for the summary generations.</p> <ul> <li>Subdirectories:</li> <li> <p><code>models/llama2</code>: contains llama2 model</p> </li> <li> <p>Files:</p> </li> <li><code>main.py</code>: creates FastAPI app, runs it and defines a FastAPI endpoint and executes the actual prediction by making each prediction step by step and structures processed data into a list that is turned into a dictonaries</li> <li><code>data_processing.py</code>: Generates structured input text for llama2 from overview, attitude, and request DataFrames.</li> <li><code>input_to_pompt_converter.py</code>: restructures the input from the models into a prompts for the LLM</li> <li><code>predict_LLAMA2.py</code>: makes a prediction for the given prompt</li> <li><code>requirements.txt</code>: contains packages used to execute prediction. Can be used especially for exection on windows</li> <li><code>summary_env.yml</code>: contains the packages and dependencies for the summary generator when running on linux/slurm servers</li> <li><code>slurm_test.py</code>: test function for slurm</li> </ul>"},{"location":"api/readme/main_page/","title":"Main Page","text":""},{"location":"api/readme/main_page/#review-overview-generation","title":"Review Overview Generation","text":"<p>At Paper Review Aggregator, our mission is to simplify the work of meta-reviewers by providing an AI-driven platform that aggregates and summarizes paper reviews. By reducing manual effort and enhancing decision-making, we aim to streamline the peer-review process for academic research. This application was developed as part of the \"Data Analysis Software Project\" course at Technische Universit\u00e4t Darmstadt, Germany. The project allowed Master\u2019s students to apply their data analysis, software engineering, and machine learning skills to solve real-world problems.</p> <p>Contributions</p> <p>In this project participated Johannes Lemken, Carmen Appelt, Zhijingshui Yang, Philipp Oehler and Jan Werth. We were supervised by Sukannya Purkayastha.</p>"},{"location":"api/readme/setup/","title":"Setup and Installation","text":"<p>This guide helps you set up and run the project, which consists of three main parts: - Model Training (NLP models environment) - Backend (APIs connecting frontend and models) - Frontend (UI environment via streamlit)</p>"},{"location":"api/readme/setup/#prerequisites","title":"Prerequisites","text":"<ul> <li>Git for cloning the repository.</li> <li>Docker for containerizing and running the application.</li> <li>Docker Compose for managing multi-container environments.</li> <li>NVIDIA Container Toolkit enables GPU acceleration with nvidia/cuda images. Installing the NVIDIA Container Toolkit</li> </ul>"},{"location":"api/readme/setup/#1-clone-the-repository","title":"1 Clone the Repository","text":"<pre><code>  git clone https://github.com/sukannyapurkayastha/DASP_report_template.git\n  cd your-project\n</code></pre>"},{"location":"api/readme/setup/#2-application-deployment","title":"2 Application Deployment","text":"<p>The application runs as a set of Docker containers orchestrated with Docker Compose. To start the app detached in your local device simply run:</p> <pre><code>docker compose up -d\n</code></pre> <p>Check logs for a specific service (current status or error message):</p> <pre><code>docker compose logs -f &lt;service_name&gt;\n</code></pre> <p>for example  could be: request_classifier <p>If you are hosting the application on your local device, by default the website is published at port 80 on your local machine. For deployment it's highly recommanded to put the application behind a webserver like Caddy, Apache or Nginx.</p> <p>The application is available online at https://reviewoverview.ukp.informatik.tu-darmstadt.de, if you have access to UKP or HRZ VPN. Since the proxy handles SSL termination, currently we don't have webserver in front of Streamlit.</p>"},{"location":"api/readme/testing/","title":"Testing","text":"<p>We adopted a multi-level testing strategy to ensure both reliability and maintainability across our application:</p> <p>Unit Tests</p> <p>Implemented extensively for backend modules and individual model components. Validate each function's correctness, focusing on data loading, preprocessing methods, and core inference logic in isolation.</p> <p>Integration Tests</p> <p>Primarily target the frontend and its interactions with the backend APIs. Assess end-to-end functionality\u2014verifying that data flows correctly from the user interface, through the backend services, and back again with the expected responses and outputs.</p>"}]}