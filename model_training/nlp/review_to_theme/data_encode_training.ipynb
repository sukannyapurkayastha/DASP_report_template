{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['sentence', 'aspect', 'theme', 'description', 'labels', 'ANA', 'BIB',\n",
      "       'DAT', 'EXP', 'INT', 'MET', 'OAL', 'PDI', 'RES', 'RWK', 'TNF'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "df = pd.read_csv('data.csv')\n",
    "df['theme'] = df['theme'].apply(lambda x: x.split('_'))\n",
    "\n",
    "# Step 3: Initialize MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    "\n",
    "# Step 4: One-hot encode the themes\n",
    "encoded_labels = mlb.fit_transform(df['theme'])\n",
    "\n",
    "# Step 5: Create a DataFrame for the encoded labels\n",
    "df_encoded = pd.DataFrame(encoded_labels, columns=mlb.classes_)\n",
    "df_encoded = df_encoded.astype(float)\n",
    "\n",
    "# Step 6: Convert the one-hot encoded columns back to a list of labels\n",
    "df['labels'] = df_encoded.values.tolist()\n",
    "\n",
    "# Step 6: Combine the original dataframe with the encoded labels\n",
    "df = pd.concat([df, df_encoded], axis=1)\n",
    "\n",
    "# Step 7: Print the processed DataFrame\n",
    "print(df.columns)\n",
    "\n",
    "df.to_csv('data_encoded.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANA     45.0\n",
      "BIB     17.0\n",
      "DAT    124.0\n",
      "EXP    261.0\n",
      "INT     13.0\n",
      "MET    937.0\n",
      "OAL    121.0\n",
      "PDI     38.0\n",
      "RES    165.0\n",
      "RWK    227.0\n",
      "TNF    108.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Calculate the frequency of each label\n",
    "label_counts = df_encoded.sum(axis=0)\n",
    "\n",
    "# Print the counts for each label\n",
    "print(label_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nana/anaconda3/envs/model_training/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "df_train = df[['sentence', 'labels']]\n",
    "from datasets import Dataset\n",
    "# Convert DataFrame to Dataset\n",
    "dataset = Dataset.from_pandas(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/nana/EMNLP2023_jiu_jitsu_argumentation_for_rebuttals/JitsuPEER_data_and_models_v1/models/bert-base-uncased_neg and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1811/1811 [00:00<00:00, 2086.23 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "import torch\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "\n",
    "num_themes = 11\n",
    "# Load the pretrained MLM model\n",
    "#\"/home/nana/EMNLP2023_jiu_jitsu_argumentation_for_rebuttals/JitsuPEER_data_and_models_v1/models/bert-base-uncased_neg\"\n",
    "#\"/home/nana/DASP_report_template/model_training/nlp/review_to_theme/results/final_model\"\n",
    "model = BertForSequenceClassification.from_pretrained(\"/home/nana/EMNLP2023_jiu_jitsu_argumentation_for_rebuttals/JitsuPEER_data_and_models_v1/models/bert-base-uncased_neg\", num_labels=num_themes, problem_type=\"multi_label_classification\")\n",
    "# Initialize the tokenizer\n",
    "# /home/nana/DASP_report_template/model_training/nlp/review_to_theme/results/final_model\n",
    "# bert-base-uncased\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Tokenize your dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"sentence\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "# Assuming dataset has columns 'text' and 'labels' (labels should be a binary vector per example)\n",
    "dataset = dataset.map(tokenize_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split_datasets = dataset.train_test_split(test_size=0.4, seed=42)\n",
    "# train_dataset = split_datasets['train']\n",
    "# temp_test_dataset = split_datasets['test']\n",
    "# # Second split: further split temp_test into eval and test\n",
    "# split_temp_test = temp_test_dataset.train_test_split(test_size=0.5, seed=42)  # 50% for eval, 50% for test\n",
    "# eval_dataset = split_temp_test['train']\n",
    "# test_dataset = split_temp_test['test']\n",
    "\n",
    "# # Ensure the dataset is in the correct format (PyTorch or TensorFlow format)\n",
    "# train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "# eval_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "# test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "dataset = dataset.train_test_split(test_size=0.2, seed=42)  # 80% train, 20% test\n",
    "train_dataset = dataset['train']\n",
    "eval_dataset = dataset['test']\n",
    "\n",
    "# Ensure the dataset is in the correct format (PyTorch or TensorFlow format)\n",
    "train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "eval_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate using metrics\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    # Convert logits to predictions for multi-class classification\n",
    "    probs = torch.sigmoid(torch.tensor(predictions))\n",
    "    # Convert probabilities to binary predictions using a threshold of 0.5\n",
    "    threshold = 0.5\n",
    "    binary_preds = (probs > threshold).int()\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(labels, binary_preds)\n",
    "    precision = precision_score(labels, binary_preds, average=\"macro\")\n",
    "    recall = recall_score(labels, binary_preds, average=\"macro\")\n",
    "    f1 = f1_score(labels, binary_preds, average=\"macro\")\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nana/anaconda3/envs/model_training/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer\n",
    "\n",
    "base_output_dir = \"./results\"\n",
    "epoch = 10\n",
    "learning_rate = 5e-5\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"{base_output_dir}/pretrained_model_normal_tokenizer/epoch_{epoch}/{learning_rate}\",           # Directory to save results\n",
    "    evaluation_strategy=\"epoch\",      # Evaluate after each epoch\n",
    "    learning_rate=learning_rate,               # Learning rate\n",
    "    per_device_train_batch_size=8,    # Batch size per device (GPU or CPU)\n",
    "    per_device_eval_batch_size=8,     # Batch size for evaluation\n",
    "    num_train_epochs=epoch,               # Number of epochs\n",
    "    logging_dir=\"./logs\",             # Logging directory\n",
    "    save_strategy=\"epoch\"             # Save model after each epoch\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                      # The model we defined above\n",
    "    args=training_args,               # Training arguments\n",
    "    train_dataset=train_dataset,      # Training dataset\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1810' max='1810' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1810/1810 10:33, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.142441</td>\n",
       "      <td>0.655647</td>\n",
       "      <td>0.533432</td>\n",
       "      <td>0.402010</td>\n",
       "      <td>0.436354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.105106</td>\n",
       "      <td>0.763085</td>\n",
       "      <td>0.667410</td>\n",
       "      <td>0.657074</td>\n",
       "      <td>0.658300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.143000</td>\n",
       "      <td>0.088906</td>\n",
       "      <td>0.763085</td>\n",
       "      <td>0.713142</td>\n",
       "      <td>0.670302</td>\n",
       "      <td>0.688247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.143000</td>\n",
       "      <td>0.085962</td>\n",
       "      <td>0.752066</td>\n",
       "      <td>0.741509</td>\n",
       "      <td>0.728985</td>\n",
       "      <td>0.705833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.143000</td>\n",
       "      <td>0.082211</td>\n",
       "      <td>0.796143</td>\n",
       "      <td>0.822112</td>\n",
       "      <td>0.779199</td>\n",
       "      <td>0.774066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.031200</td>\n",
       "      <td>0.082519</td>\n",
       "      <td>0.804408</td>\n",
       "      <td>0.835256</td>\n",
       "      <td>0.796993</td>\n",
       "      <td>0.802949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.031200</td>\n",
       "      <td>0.085222</td>\n",
       "      <td>0.801653</td>\n",
       "      <td>0.822570</td>\n",
       "      <td>0.802749</td>\n",
       "      <td>0.798308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.031200</td>\n",
       "      <td>0.085971</td>\n",
       "      <td>0.790634</td>\n",
       "      <td>0.825471</td>\n",
       "      <td>0.797795</td>\n",
       "      <td>0.797602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.012700</td>\n",
       "      <td>0.085812</td>\n",
       "      <td>0.807163</td>\n",
       "      <td>0.827012</td>\n",
       "      <td>0.829175</td>\n",
       "      <td>0.819857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.012700</td>\n",
       "      <td>0.085349</td>\n",
       "      <td>0.804408</td>\n",
       "      <td>0.826045</td>\n",
       "      <td>0.800152</td>\n",
       "      <td>0.798976</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nana/anaconda3/envs/model_training/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/nana/anaconda3/envs/model_training/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/nana/anaconda3/envs/model_training/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/nana/anaconda3/envs/model_training/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1810, training_loss=0.053118013545294496, metrics={'train_runtime': 633.9204, 'train_samples_per_second': 22.842, 'train_steps_per_second': 2.855, 'total_flos': 952538986352640.0, 'train_loss': 0.053118013545294496, 'epoch': 10.0})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='46' max='46' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [46/46 00:04]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.08534885197877884, 'eval_accuracy': 0.8044077134986226, 'eval_precision': 0.8260446029099545, 'eval_recall': 0.8001515385063018, 'eval_f1': 0.7989762963808297, 'eval_runtime': 4.4109, 'eval_samples_per_second': 82.296, 'eval_steps_per_second': 10.429, 'epoch': 10.0}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "eval_results = trainer.evaluate(metric_key_prefix=\"eval\")\n",
    "print(eval_results)\n",
    "trainer.save_metrics(\"eval\", eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['sentence', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 363\n",
       "})"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['sentence', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 363\n",
       "})"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict and compute metrics on the test dataset\n",
    "predict_results = trainer.predict(test_dataset, metric_key_prefix=\"predict\")\n",
    "metrics_predict = predict_results.metrics\n",
    "\n",
    "# Save the test metrics without logging\n",
    "# metrics_file_path_predict = f\"{trainer.args.output_dir}/predict_results.json\"\n",
    "trainer.save_metrics(\"predict\", metrics_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0,  ..., 1, 0, 0],\n",
      "        [0, 0, 1,  ..., 1, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "predictions = trainer.predict(eval_dataset)\n",
    "logits = predictions.predictions\n",
    "\n",
    "# Apply sigmoid activation\n",
    "probs = torch.sigmoid(torch.tensor(logits))\n",
    "\n",
    "# Convert probabilities to binary predictions\n",
    "threshold = 0.5\n",
    "binary_preds = (probs > threshold).int()\n",
    "\n",
    "print(binary_preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.7993, Precision: 0.8217, Recall: 0.8032\n"
     ]
    }
   ],
   "source": [
    "# Evaluate using metrics\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "true_labels = predictions.label_ids\n",
    "\n",
    "# Calculate metrics\n",
    "f1 = f1_score(true_labels, binary_preds, average=\"macro\")\n",
    "precision = precision_score(true_labels, binary_preds, average=\"macro\")\n",
    "recall = recall_score(true_labels, binary_preds, average=\"macro\")\n",
    "\n",
    "print(f\"F1 Score: {f1:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./results/final_model/tokenizer_config.json',\n",
       " './results/final_model/special_tokens_map.json',\n",
       " './results/final_model/vocab.txt',\n",
       " './results/final_model/added_tokens.json')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "# Assume `trainer` is your trained Trainer object\n",
    "output_dir = \"./results/final_model\"  # Specify the directory where you want to save the model\n",
    "\n",
    "# Save the model, tokenizer, and config\n",
    "trainer.save_model(output_dir)\n",
    "\n",
    "# Save the tokenizer separately\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1\n",
      "Text: - it wasn't clear how the sparsity percentage on page 3 was defined?\n",
      "True Labels: [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "Predicted Labels: [0 0 0 0 0 0 0 0 1 0 0]\n",
      "--------------------------------------------------\n",
      "Sample 2\n",
      "Text: it is unclear whether the data augmentation techniques is applied only at training time or also at test time.\n",
      "True Labels: [0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "Predicted Labels: [0 0 1 0 0 0 0 0 1 0 0]\n",
      "--------------------------------------------------\n",
      "Sample 3\n",
      "Text: without doing so, it leaves the reader wondering why not simply a standard rbm trained using a standard method ( e. g. contrastive divergence ).\n",
      "True Labels: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "Predicted Labels: [0 0 0 0 0 1 0 0 0 0 0]\n",
      "--------------------------------------------------\n",
      "Sample 4\n",
      "Text: the inference algorithm builds on standard techniques of deep generative models and, also, on previously proposed methods ( wand and blei, 2003 ) for dealing with the complex hierarchical priors involved in this kind of models.\n",
      "True Labels: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "Predicted Labels: [0 0 0 0 0 1 0 0 0 0 0]\n",
      "--------------------------------------------------\n",
      "Sample 5\n",
      "Text: unfortunately this paper offers only weak results.\n",
      "True Labels: [0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Predicted Labels: [0 0 1 1 0 0 0 0 0 0 0]\n",
      "--------------------------------------------------\n",
      "Sample 6\n",
      "Text: 1. the work has limited novelty : the learning of the world model ( recurrent state - space model ) closely follows the prior work of planet.\n",
      "True Labels: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "Predicted Labels: [0 0 0 0 0 1 0 0 0 0 0]\n",
      "--------------------------------------------------\n",
      "Sample 7\n",
      "Text: the results on real datasets are similar to the regular gcn.\n",
      "True Labels: [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "Predicted Labels: [0 0 0 0 0 0 1 0 0 0 0]\n",
      "--------------------------------------------------\n",
      "Sample 8\n",
      "Text: also, i had to go through a large chunk of the paper before coming across the exact setup.\n",
      "True Labels: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "Predicted Labels: [0 0 0 0 0 1 0 0 0 0 0]\n",
      "--------------------------------------------------\n",
      "Sample 9\n",
      "Text: - the writing looks very rushed, and should be improved.\n",
      "True Labels: [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "Predicted Labels: [0 0 0 0 0 0 0 0 1 0 0]\n",
      "--------------------------------------------------\n",
      "Sample 10\n",
      "Text: in general, i feel this section could use some tighter formalism and justifications.\n",
      "True Labels: [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "Predicted Labels: [0 0 0 0 0 1 0 0 0 0 0]\n",
      "--------------------------------------------------\n",
      "Sample 11\n",
      "Text: the proposed approach is very similar to the ce method by rubinstein ( as stated by the authors in the related work section ), limiting the contributions of this paper.\n",
      "True Labels: [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "Predicted Labels: [0 0 0 0 0 1 0 0 1 0 0]\n",
      "--------------------------------------------------\n",
      "Sample 12\n",
      "Text: this paper has problems with clarity / polish and experimental design that are sufficiently severe\n",
      "True Labels: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "Predicted Labels: [0 0 0 0 0 1 0 0 0 0 0]\n",
      "--------------------------------------------------\n",
      "Sample 13\n",
      "Text: methods. there are some scalable property verification methods that can give a\n",
      "True Labels: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "Predicted Labels: [0 0 0 0 0 0 0 0 0 1 0]\n",
      "--------------------------------------------------\n",
      "Sample 14\n",
      "Text: these points remain me puzzled regarding either practical or theoretical application of the result. it would be great if authors could elaborate.\n",
      "True Labels: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "Predicted Labels: [0 0 0 0 0 1 0 0 0 0 0]\n",
      "--------------------------------------------------\n",
      "Sample 15\n",
      "Text: one drawback is that it is highly specific to language models.\n",
      "True Labels: [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Predicted Labels: [0 0 1 0 0 0 0 0 0 0 0]\n",
      "--------------------------------------------------\n",
      "Sample 16\n",
      "Text: for example, how to compute the gradient w. r. t. \\ phi? since the mean policy is used, it is not apparent that how to compute the gradient w. r. t. \\ phi.\n",
      "True Labels: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "Predicted Labels: [0 0 0 0 0 1 0 0 0 0 0]\n",
      "--------------------------------------------------\n",
      "Sample 17\n",
      "Text: in the experiment there is no details on how you set the hyperparameters of cw and ead.\n",
      "True Labels: [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Predicted Labels: [1 0 0 0 0 0 0 0 0 0 0]\n",
      "--------------------------------------------------\n",
      "Sample 18\n",
      "Text: - the definition of g should depend on only \\ theta _ k ^ i and \\ hat { \\ delta } _ k ^ m, not \\ theta _ k ^ k.\n",
      "True Labels: [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "Predicted Labels: [0 0 0 0 0 1 0 0 0 0 0]\n",
      "--------------------------------------------------\n",
      "Sample 19\n",
      "Text: - jointly learning an inference network ( q ) has certainly been done before, and i am not sure authors provide an elaborate enough explanation of what is the difference with adversarially learned inference / adversarial feature learning.\n",
      "True Labels: [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Predicted Labels: [0 0 1 1 0 0 0 0 0 0 0]\n",
      "--------------------------------------------------\n",
      "Sample 20\n",
      "Text: the paper currently only mentions the most related work for the proposed method, using the whole section 2 to describe vcl and use section 3 to describe fsm and half of section 5 to describe ssr.\n",
      "True Labels: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "Predicted Labels: [0 0 0 0 0 1 0 0 0 0 0]\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Decode input_ids back to text using the tokenizer\n",
    "def decode_text(tokenizer, input_ids):\n",
    "    return tokenizer.decode(input_ids, skip_special_tokens=True)\n",
    "\n",
    "# Visualize predictions\n",
    "num_samples = 20  # Number of samples to visualize\n",
    "for i in range(num_samples):\n",
    "    # Decode the text from input_ids\n",
    "    input_ids = test_dataset[i]['input_ids']\n",
    "    text = decode_text(tokenizer, input_ids)\n",
    "\n",
    "    # True labels and predicted labels\n",
    "    true_label = true_labels[i]  # Original label\n",
    "    predicted_label = binary_preds[i].numpy()  # Predicted binary label\n",
    "\n",
    "    # Print results\n",
    "    print(f\"Sample {i + 1}\")\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"True Labels: {true_label}\")\n",
    "    print(f\"Predicted Labels: {predicted_label}\")\n",
    "    print(\"-\" * 50)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "model_training",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
