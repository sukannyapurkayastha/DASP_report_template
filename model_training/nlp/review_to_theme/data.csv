sentence,aspect,theme,description
"With regard to my first negative point above about the lack of discussions, it seems the analysis of Section 4 is disproportionate compared to other places.",arg_other,ANA,"With regard to my first negative point above about the lack of discussions, it seems the analysis of Section 4 is disproportionate compared to other places."
The cited paper 'Learning an adaptive learning rate schedule' does not appear online.,arg_other,BIB,Missing citations in the paper
The above papers are not cited in this paper.,arg_other,BIB,Missing citations in the paper
Missing references - authors may want to consider citing https://arxiv.org/abs/1906.06187 as well in Sec. 2 - it seems very related to this work.,arg_other,BIB,Missing citations in the paper
"Some of the experiments (eg. comparisons involving ShakeShake and ScheduledDropPath, Section 5.2) could also be moved to the appendix in order to make room for a description of LEMONADE in the main paper.",arg_other,EXP,"Some of the experiments (eg. comparisons involving ShakeShake and ScheduledDropPath, Section 5.2) could also be moved to the appendix in order to make room for a description of LEMONADE in the main paper."
"However, you are in a different computational model in which you now have access to an oracle.",arg_other,MET,Theoretical misunderstanding in methodology
- offers minimal (but some) evidence that curiosity-based reward works in more realistic settings,arg_other,MET,Theoretical misunderstanding in methodology
"As a result, constraining the model will alter the mutual information and I think the effect of this should be remarked on.",arg_other,MET,Theoretical misunderstanding in methodology
"In terms of modeling, since the input into the prior network has finite possible discrete values, we do not need a fully connected network to generate $\hat{\mu}_c$ and $\hat{\sigma}_c$. Instead, we can directly optimize $\hat{\mu}_c$ and $\hat{\sigma}_c$ for each $c$ as parameters.",arg_other,MET,Theoretical misunderstanding in methodology
The theory tells the same as in case 2 above but with an additional price of optimizing a different function.,arg_other,MET,Theoretical misunderstanding in methodology
"For example, the original GNN, ChebNet, etc. that leads to GCN can be mentioned.",arg_other,MET,Theoretical misunderstanding in methodology
"- The notion of divergence D(P|G) is not made concrete in section 3 and 3.1, which makes the notation of rather little use.",arg_other,MET,Theoretical misunderstanding in methodology
What are the differences to your approach?,arg_other,MET,Theoretical misunderstanding in methodology
Meanwhile the actual method used in the paper is hidden in Appendices A.1.1-A.2.,arg_other,MET,Theoretical misunderstanding in methodology
"Finally, could you give a more accurate citation (chapter-page number) for the single-channel version of Theorem 2.?",arg_other,MET,Theoretical misunderstanding in methodology
"Also, try using the consistent dimension for x throughout the paper, it confuses the reader.",arg_other,MET,Theoretical misunderstanding in methodology
"hence, there is no connection between the theoretical results on MDL generalization bound and the proposed method MULANN.",arg_other,MET_RES,"hence, there is no connection between the theoretical results on MDL generalization bound and the proposed method MULANN."
"[-] The combination of VAEs and GANs, while new for videos, had already been proposed for image generation as indicated in the Related Work section and its formulation for video prediction is relatively straightforward given existing VAE (Denton & Fergus 2018) and GAN models (Tulyakov et al. 2018).",arg_other,MET_RWK,"[-] The combination of VAEs and GANs, while new for videos, had already been proposed for image generation as indicated in the Related Work section and its formulation for video prediction is relatively straightforward given existing VAE (Denton & Fergus 2018) and GAN models (Tulyakov et al. 2018)."
"Based on the summary, cons, and pros, the current rating I am giving now is weak reject. I would like to discuss the final rating with other reviewers, ACs.",arg_other,OAL,"Reasons for rejection (not fit for conference, contains several weaknesses, etc.)"
"In summary, I'm inclined to reject this paper given the current version.",arg_other,OAL,"Reasons for rejection (not fit for conference, contains several weaknesses, etc.)"
There are a few grammatical/spelling errors that need ironing out.,arg_other,OAL,"Reasons for rejection (not fit for conference, contains several weaknesses, etc.)"
"I believe that the paper should currently be rejected, but I encourage the authors to revise the paper.",arg_other,OAL,"Reasons for rejection (not fit for conference, contains several weaknesses, etc.)"
There are several weaknesses in this paper.,arg_other,OAL,"Reasons for rejection (not fit for conference, contains several weaknesses, etc.)"
"Nevertheless, I believe that it still has to address some points in order to be better suited for publication:",arg_other,OAL,"Reasons for rejection (not fit for conference, contains several weaknesses, etc.)"
I vote for rejection for four major weaknesses explained as follows.,arg_other,OAL,"Reasons for rejection (not fit for conference, contains several weaknesses, etc.)"
"4) There are several typos/grammar issues e.g. ""believed to occurs"", ""important parameters sections"", ""capacity that if efficiently allocated"", etc.).",arg_other,OAL,"Reasons for rejection (not fit for conference, contains several weaknesses, etc.)"
- The paper is over the hard page limit for ICLR so needs to be edit to reduce the length.,arg_other,OAL,"Reasons for rejection (not fit for conference, contains several weaknesses, etc.)"
"I vote to reject the paper at this stage, mainly because of the following three points:",arg_other,OAL,"Reasons for rejection (not fit for conference, contains several weaknesses, etc.)"
The paper would need to be improved substantially in order to appear at a conference like ICLR.,arg_other,OAL,"Reasons for rejection (not fit for conference, contains several weaknesses, etc.)"
I do not think this work is ready for publication.,arg_other,OAL,"Reasons for rejection (not fit for conference, contains several weaknesses, etc.)"
I think the paper does not fit this conference. It is better to be presented in a Demonstration section at a *ACL conference.,arg_other,OAL,"Reasons for rejection (not fit for conference, contains several weaknesses, etc.)"
"Overall, I feel that this paper falls short of what it promises, so I cannot recommend acceptance at this time.",arg_other,OAL,"Reasons for rejection (not fit for conference, contains several weaknesses, etc.)"
"* The paper is almost 9 pages long, but the contribution does not appear more substantial than a standard 8-page submission.",arg_other,OAL,"Reasons for rejection (not fit for conference, contains several weaknesses, etc.)"
I’m also confused by the presentation of the results.,arg_other,RES,I’m also confused by the presentation of the results.
"5. lack of related work:  4D Spatio-Temporal ConvNets: Minkowski Convolutional Neural Networks, CVPR 2019.",arg_other,RWK,"Missing literature review (some literature not included, misses baseline citations, etc)"
2. Missed citation: MnasNet [2] also incorporates the cost of architectures in their search process.,arg_other,RWK,"Missing literature review (some literature not included, misses baseline citations, etc)"
The paper also misses relevant citations of similar questions from the field of (probabilistic) matrix factorization and relational learning.,arg_other,RWK,"Missing literature review (some literature not included, misses baseline citations, etc)"
(3) A large body of graph neural network literature is omitted.,arg_other,RWK,"Missing literature review (some literature not included, misses baseline citations, etc)"
"The reviewer recommends the authors to read some literature review of the topic of GCN and re-organize the references, and use search engines to have a better view on the state of the art of (hyperbolic) geometric theories and graph convolutional networks.",arg_other,RWK,"Missing literature review (some literature not included, misses baseline citations, etc)"
This is a thriving area that requires a careful literature review.,arg_other,RWK,"Missing literature review (some literature not included, misses baseline citations, etc)"
"2) Related work, although extensive in terms of the number of references, do not help to place this work in the literature.",arg_other,RWK,"Missing literature review (some literature not included, misses baseline citations, etc)"
Listing related work is no the same as describing similarities and differences compared to previous methods.,arg_other,RWK,"Missing literature review (some literature not included, misses baseline citations, etc)"
The paper misses the key baseline in Bayesian optimisation using tree structure [1] which can perform the prediction under the tree-structure dependencies.,arg_other,RWK,"Missing literature review (some literature not included, misses baseline citations, etc)"
"There are some well-cited works that the authors may have missed. These are ultimately different approaches, but perhaps the authors can obtain some inspiration from these:",arg_other,RWK,"Missing literature review (some literature not included, misses baseline citations, etc)"
"5, Although there is a notable difference, one may properly mention previous work Yamamoto et al. (2019), which uses GAN as an auxiliary loss within ClariNet and obtains high-fidelity speech ( https://r9y9.github.io/demos/projects/interspeech2019/ ).",arg_other,RWK,"Missing literature review (some literature not included, misses baseline citations, etc)"
"5. The term ""PowerSGD"" seems to have been used by other papers.",arg_other,RWK,"Missing literature review (some literature not included, misses baseline citations, etc)"
Also experiment figures are extremely compact. Try using log scale or other lines to make the gaps wider.,arg_other,TNF,Also experiment figures are extremely compact. Try using log scale or other lines to make the gaps wider.
"This is discussed on p4, but it's unclear to me how keeping $\sigma$ separate is benefitial for the analysis, and this is not picked up again explicitly again?",asp_clarity,ANA,Lack of discussion of analysis
"this analysis is not currently included in the main body of the manuscript, but rather in the appendix, which I find rather annoying.",asp_clarity,ANA,Lack of discussion of analysis
"The paper offers some analysis, suggesting when each of the conditions occurs, upper bounds the smallest singular value of D A (where the example dependent diagonal matrix D incorporates the ReLU activation (shouldn’t this be more clearly introduced and notated?).",asp_clarity,ANA,Lack of discussion of analysis
Here are a few examples: The ICLR citation style needs to use sometimes \citep.,asp_clarity,BIB,Incorrect citation styles
- There is no need for such repetitive citing (esp paragraph 2 on page 2).,asp_clarity,BIB,Incorrect citation styles
Sometimes the same paper is cited 4 times within a few lines.,asp_clarity,BIB,Incorrect citation styles
- Missing references on page 3,asp_clarity,BIB,Incorrect citation styles
"In particular, it is unclear what the assumption on the size of the unlabelled test set is.",asp_clarity,DAT,"Lack of discussion on datasets (size, motivation for use, etc)"
* The issue of possible false positives due to sample dependence in fMRI data has again been ignored in the rebuttal.,asp_clarity,DAT,"Lack of discussion on datasets (size, motivation for use, etc)"
"For example, I don't understand what does it mean in ""However, if training data is complete, ..... handle during missing data during test."" Another example would be the last few paragraphs on page 4; they are very unclear.",asp_clarity,DAT,"Lack of discussion on datasets (size, motivation for use, etc)"
"For example, I have trouble understanding the sentence ""So the existed algorithms should be heuristic or it can get a bad result even we train the neural networks with lots of datasets."" in the introduction.",asp_clarity,DAT,"Lack of discussion on datasets (size, motivation for use, etc)"
"6. Validation data / test sets: Throughout this work, it is unclear what / how validation is performed.",asp_clarity,DAT,"Lack of discussion on datasets (size, motivation for use, etc)"
"You have 3 datasets worth of data for the regression task (it is still unclear, however, what is being used for evaluation), but it doesn't look like this is addressed in the larger scale experiments at all.",asp_clarity,DAT,"Lack of discussion on datasets (size, motivation for use, etc)"
"The gain of using a sufficiently more complicated approach to assess the overall distance between the test and training dataset is not clear, comparing it to the very insightful histograms.",asp_clarity,DAT,"Lack of discussion on datasets (size, motivation for use, etc)"
"However, since quantitative exploration of large real-world datasets may be challenging and expensive to collect, the synthetic experiments could have been more detailed.",asp_clarity,DAT_EXP,"However, since quantitative exploration of large real-world datasets may be challenging and expensive to collect, the synthetic experiments could have been more detailed."
- It is unclear why the results on WikiSQL is presented in Appendix. Combining the results on both datasets in the experiments section would be more convincing.,asp_clarity,DAT_RES_EXP,- It is unclear why the results on WikiSQL is presented in Appendix. Combining the results on both datasets in the experiments section would be more convincing.
"However, the real-world experiments are not necessarily the easiest to read.",asp_clarity,EXP,"Lack of discussion of experimental setup (conclusion from experiments, poor experimental details, etc)"
Authors should clarify the justification behind experimenting only on 'first 500 test images'.,asp_clarity,EXP,"Lack of discussion of experimental setup (conclusion from experiments, poor experimental details, etc)"
It seems there is no conclusion to take away from the experiments in section 5 (convolutions).,asp_clarity,EXP,"Lack of discussion of experimental setup (conclusion from experiments, poor experimental details, etc)"
"The paper is written in a way that makes following it a bit difficult, for example, the experimental setups.",asp_clarity,EXP,"Lack of discussion of experimental setup (conclusion from experiments, poor experimental details, etc)"
In the experiment there is no details on how you set the hyperparameters of CW and EAD.,asp_clarity,EXP,"Lack of discussion of experimental setup (conclusion from experiments, poor experimental details, etc)"
"It is not clear to me what the point of Sec. 5 is, given a trained model, one wants to figure out if an image was present in the training of the model.",asp_clarity,EXP,"Lack of discussion of experimental setup (conclusion from experiments, poor experimental details, etc)"
"- Also, for MISC-r experiments, the weights between the intrinsic reward bonus and the extrinsic reward are not specified in the paper.",asp_clarity,EXP,"Lack of discussion of experimental setup (conclusion from experiments, poor experimental details, etc)"
"In particular, the example in section 3.1 says that a noisy information bottleneck is introduced, but then says that the modified and unmodified models have ""training algorithms that are exactly equivalent."" I think this example needs to be clarified.",asp_clarity,EXP,"Lack of discussion of experimental setup (conclusion from experiments, poor experimental details, etc)"
"5. In fact, I don't know why \omega needs to output p. It's never mentioned in the experiment section.",asp_clarity,EXP,"Lack of discussion of experimental setup (conclusion from experiments, poor experimental details, etc)"
10. Reading the baselines before the experiments is very confusing.,asp_clarity,EXP,"Lack of discussion of experimental setup (conclusion from experiments, poor experimental details, etc)"
"3. This apart, I think that the experiment section is pretty hard to read, given all the metrics and methodology is in the Appendix.",asp_clarity,EXP,"Lack of discussion of experimental setup (conclusion from experiments, poor experimental details, etc)"
The imagenet experiment lacks details.,asp_clarity,EXP,"Lack of discussion of experimental setup (conclusion from experiments, poor experimental details, etc)"
"In summary, I think this is interesting work, but a clearer explanation of the relationship between HRL and MARL, as well as a clearer main argument, supported by experimental evidence, would greatly improve this paper.",asp_clarity,EXP,"Lack of discussion of experimental setup (conclusion from experiments, poor experimental details, etc)"
"The experimental section includes various mistakes (see under minor) and misses to describe figures, leading to the assumption that additional time is required for a more detailed evaluation of the algorithm (including more domains and in particular baselines).",asp_clarity,EXP,"Lack of discussion of experimental setup (conclusion from experiments, poor experimental details, etc)"
"Cons:  unclear transfer learning model, insufficient experiments.",asp_clarity,EXP,"Lack of discussion of experimental setup (conclusion from experiments, poor experimental details, etc)"
"section 6.3, the authors show an experiments in this case, but only on a dense",asp_clarity,EXP,"Lack of discussion of experimental setup (conclusion from experiments, poor experimental details, etc)"
1. the difficult to train the network,asp_clarity,EXP,"Lack of discussion of experimental setup (conclusion from experiments, poor experimental details, etc)"
This paper has problems with clarity/polish and experimental design that are sufficiently severe,asp_clarity,EXP,"Lack of discussion of experimental setup (conclusion from experiments, poor experimental details, etc)"
5. Clarity: The first half of this paper was easy to follow and clear. The experimental section had a couple of areas that left me confused. In particular:,asp_clarity,EXP,"Lack of discussion of experimental setup (conclusion from experiments, poor experimental details, etc)"
"8. ""Beyond fixed schedules, automatically adjusting the training of G and D remains untacked"" -- this is not 100% true.",asp_clarity,EXP,"Lack of discussion of experimental setup (conclusion from experiments, poor experimental details, etc)"
There were some experimental details that were poorly explained but in general the paper was readable.,asp_clarity,EXP,"Lack of discussion of experimental setup (conclusion from experiments, poor experimental details, etc)"
"Also, the sentence, ""We demonstrate the implementation and performance of our pipelined backpropagation in PyTorch on 2 GPUs using ResNet, achieving speedups of up to 1.8X over a 1-GPU baseline, with a small drop in inference accuracy."", is confusing. If I use data parallelization, the gain should be also around 2.",asp_clarity,EXP,"Lack of discussion of experimental setup (conclusion from experiments, poor experimental details, etc)"
"2) It would be great if the paper can clearly define the experiments: ""waypoint"", ""oncoming"", ""mall"", and ""bottleneck"".",asp_clarity,EXP,"Lack of discussion of experimental setup (conclusion from experiments, poor experimental details, etc)"
This needs more elaboration. Is this way of training results expected? What is the lesson learned?,asp_clarity,EXP,"Lack of discussion of experimental setup (conclusion from experiments, poor experimental details, etc)"
The contributions of the method could also be underlined more clearly in the abstract and introduction.,asp_clarity,INT,Unclear intro (eg. contributions)
- The digression at the bottom of the first page about neural architecture search seem out of context and interrupts the flow of the introduction.,asp_clarity,INT,Unclear intro (eg. contributions)
"First, it is somewhat misleading about its contributions: it's not obvious from abstract/introduction that the whole model is the same as DIP except for the proposed architecture.",asp_clarity,INT,Unclear intro (eg. contributions)
The introduction can start at a lower level (such as flat/hyperbolic neural networks).,asp_clarity,INT,Unclear intro (eg. contributions)
10.	The title suggests that the paper studies multiple VQA models but only a single model is studied.,asp_clarity,INT,Unclear intro (eg. contributions)
"I do not understand the ""deep integration of MARL and HRL"" that is claimed in the Introduction.",asp_clarity,INT,Unclear intro (eg. contributions)
"* In the introduction, ""the classical approach"" is mentioned but to be the latter is",asp_clarity,INT,Unclear intro (eg. contributions)
"Specifically, the first contribution listed in the introduction makes it look like this paper introduces the idea of not learning the decoder on the dataset (the one that starts with “The network is not learned and itself incorporates all assumptions on the data.”).",asp_clarity,INT_PDI,"Specifically, the first contribution listed in the introduction makes it look like this paper introduces the idea of not learning the decoder on the dataset (the one that starts with “The network is not learned and itself incorporates all assumptions on the data.”)."
How to understand this phenomenon? Does this mean that the distribution of the layer outputs will not change too much if the layer is deep enough?,asp_clarity,MET,Unclear description of method
"In particular, the node-level pretraining described in section 3.1.1. seems rather complicated to implement as a context graph needs to be computed for each node in the graph.",asp_clarity,MET,Unclear description of method
"- The definition of g should depend on only \theta_k^I and \hat{\delta}_k^M, not \theta_k^k.",asp_clarity,MET,Unclear description of method
"- The equation (1) should hold for any \theta’, not \theta.",asp_clarity,MET,Unclear description of method
"- The equation (1) should contain \rho, not p.",asp_clarity,MET,Unclear description of method
"* The use of the name ""batch-norm"" for the layer wise normalization is both wrong and misleading.",asp_clarity,MET,Unclear description of method
"For example, it is unclear to me why some larger models are not amenable to truncation.",asp_clarity,MET,Unclear description of method
- the notation q appeared in the middle part of page 3 before the definition of q is shown in the last paragraph of p.3.,asp_clarity,MET,Unclear description of method
"At the end of the subsection, the authors argue that 4D convolution is just k times larger than 3D kernels, which sounds like contradicting.",asp_clarity,MET,Unclear description of method
-- It is not clear how the  per-example scalar sigma-i is learned. (for Eq 2),asp_clarity,MET,Unclear description of method
But the authors do not seem to describe how they chose the hyperparameters for the baselines algorithms.,asp_clarity,MET,Unclear description of method
"Since finding an adversarial with smaller perturbation is a harder problem, it is unclear how the algorithms compare for finding adversarial examples with similar distortion.",asp_clarity,MET,Unclear description of method
"In this sense, claims about the low-variance of GO-gradient wrt to other reparametrization baed estimators should be removed, as they are the same.",asp_clarity,MET,Unclear description of method
"On page 4, State update function, what is the meaning of variable ""Epsilon"" in the equation?",asp_clarity,MET,Unclear description of method
"From the supplementary, it seems Epsilon means the environment?",asp_clarity,MET,Unclear description of method
"- In Equation (2), How is P_ij defined exactly, are they parameters? I am quite confused about this part",asp_clarity,MET,Unclear description of method
"Fourth, please make it clear that the proposed method aims to estimate ""causality-in-mean"" because of the formulation in terms of regression.",asp_clarity,MET,Unclear description of method
"While I could imagine human judges preferring them as they are fluent, I think they are wrong as they express a different meaning than the SPARQL query they are supposed to express.",asp_clarity,MET,Unclear description of method
"The maths is not clear, in particular the gradient derivation in equation (8).",asp_clarity,MET,Unclear description of method
"But again, it's not super clear how the paper estimates this derivative.",asp_clarity,MET,Unclear description of method
"1. Are e_{i,t} and lambda_{i,t} vectors?",asp_clarity,MET,Unclear description of method
The description of consistency regularisation methods in section 2.2 is not very clear and I would like to get better understanding of temporal ensembling and SNTG methods here as they play an important role in the experiments.,asp_clarity,MET,Unclear description of method
"Then, the important hyperparameter of the method---threshold---seems to be hard to select (both in sections 4.1 and 4.2).",asp_clarity,MET,Unclear description of method
"Section 2.1 is where the method is proposed, yet most of the descriptions there are unclear. Without these details it is impossible to judge the novelty of the ""rule-based concept extractor"", which is the key technical innovation.",asp_clarity,MET,Unclear description of method
"An example of lack of mathematical rigor is equation 4 in which the same variable name is used to describe the weights before and after pruning, as if it was computer code instead of an equation.",asp_clarity,MET,Unclear description of method
"Also pervasive is the use of the asterisk to denote multiplication, again as if it was code and not math.",asp_clarity,MET,Unclear description of method
- q_theta was introduced in Eq. (8) before it is defined in Eq. (11).,asp_clarity,MET,Unclear description of method
"In the algorithm, the authors need to define the HT function in (3) and (4).",asp_clarity,MET,Unclear description of method
It leaves it unclear to the reader when someone should choose to utilize a spherical method or when the proposed method would then be preferred compared to other spherical methods.,asp_clarity,MET,Unclear description of method
"Also, it is not clear why those are called axioms since they are not use to build anything else.",asp_clarity,MET,Unclear description of method
"- The interchangeable use of the term ""conductance of neuron j"" for equations 2 and 3 is confusing.",asp_clarity,MET,Unclear description of method
"- As said in the my main comments, I am not convinced by the use of the term Axiom. They are not use as building blocks, and are rather used as desirable properties for which the authors prove that only ""path methods"" can satisfy.",asp_clarity,MET,Unclear description of method
I don't have a good sense of whether this is a reasonable theoretical model to explore and a lot of very basic questions remain unanswered for me.,asp_clarity,MET,Unclear description of method
"I take issue with the usage of the phrase ""skill discovery"".",asp_clarity,MET,Unclear description of method
"Here, there is only a single (unconditioned) policy, and the different ""skills"" come from modifications of the environment -- the number of skills is tied to the number of environments.",asp_clarity,MET,Unclear description of method
The language around skills and the extent of prior knowledge still downplays things a bit too much for my liking.,asp_clarity,MET,Unclear description of method
It is in the end plugged into a continual learning algorithm which also performs domain transformation.,asp_clarity,MET,Unclear description of method
The purpose of the public set is explained only in section 5.2.,asp_clarity,MET,Unclear description of method
"-Something is a bit weird with the FGM results. While it is a weaker attack, a 0%/100% disparity between it and every other",asp_clarity,MET,Unclear description of method
"Also, it seems quite strange to me that making the FW overshot and then projecting back would be beneficial.",asp_clarity,MET,Unclear description of method
- I am not sure what you mean in 5.4 “we omit all grid search/ binary search steps,asp_clarity,MET,Unclear description of method
"As is, the counter example proof itself is quite confusing: it would really help if the proof was more formal.",asp_clarity,MET,Unclear description of method
"However, the weakness is that the condition is opaque and it is not entirely clear how broad of class of problems this condition would apply to.",asp_clarity,MET,Unclear description of method
"However, I don't understand the use of $\alpha$ here.",asp_clarity,MET,Unclear description of method
"I believe that's what ""Pred (One Step)"" expresses, but it would maybe be generally helpful to be more precise about the notation",asp_clarity,MET,Unclear description of method
"* Equations (1, 2): z and \phi are not consistently boldfaced",asp_clarity,MET,Unclear description of method
1. It is not clear that training a network to classify a set from another set is necessarily equivalent to ``memorization’’.,asp_clarity,MET,Unclear description of method
I believe that the presentation of the proposed method can be significantly improved.,asp_clarity,MET,Unclear description of method
The method description was a bit confusing and unclear to me.,asp_clarity,MET,Unclear description of method
- the sentence under eq. (2),asp_clarity,MET,Unclear description of method
"- the sentence ""Bacause the identity of the datapoint can never be learned by ..."" What is the identity of a dat point?",asp_clarity,MET,Unclear description of method
It’s not obvious to me how useful the ME bias would be when there is a large number of classes since the probability assigned to the true novel class with a perfect ME bias would be 1 / (# unseen classes).,asp_clarity,MET,Unclear description of method
- It is unclear how MISC-p is performed. Please elaborate on how MISC-p works for prioritization.,asp_clarity,MET,Unclear description of method
"4. I do not understand this: ""to fit well the method overfitting rate"" in Section 3.3.",asp_clarity,MET,Unclear description of method
"(2) Eq. (3): is there a superscription ""(j)"" on z_canon in decoder?",asp_clarity,MET,Unclear description of method
"- Section 3.3: “Different from pruning, which the search space is usually quite limited”. “which” should be “whose”?",asp_clarity,MET,Unclear description of method
- Section 4.4.1: “DSO-NAS can also search architecture [...]”  -> “DSO-NAS can also search for architectures [...]”,asp_clarity,MET,Unclear description of method
"However, it isn’t entirely clear if the primary contribution is showing that ‘curiosity reward’ is a potentially promising approach or if game environments aren’t particularly good testbeds for practical RL algorithms — given the lack of significant results on more realistic domains, my intuition leans toward the later (the ant robot is interesting, but one can come up with ‘simulator artifact’ based explanations).",asp_clarity,MET,Unclear description of method
"While the general description of the model is clear, details are lacking.",asp_clarity,MET,Unclear description of method
"Similarly, you did not indicate what the deterministic version of your model is.",asp_clarity,MET,Unclear description of method
"Also, it is not clear from the discussion on z, whether sampling is performed once for each video of for each frame.",asp_clarity,MET,Unclear description of method
Another somewhat jarring fact about the alt-az convolution is that it is not well defined on the south pole.,asp_clarity,MET,Unclear description of method
"The paragraph motivating the alt-az convolution on page 4 is not very clear, and some claims are questionable.",asp_clarity,MET,Unclear description of method
"I would suggest rewriting this paragraph to make it clearer and less speculative, and acknowledge that although rotating filters might increase computational complexity, it has often been shown very effective.",asp_clarity,MET,Unclear description of method
-Some technical details  are missing.,asp_clarity,MET,Unclear description of method
- Sec 3.4 can you recap all the parameters after eq.11? going through Sec 3.2 and 2.2 to find them is quite annoying.,asp_clarity,MET,Unclear description of method
"Reading it feels like reading about lattices when the work is about general graphs, and lattices provide no intuition about the proposed solution.",asp_clarity,MET,Unclear description of method
It is still not clear why self-modulation stabilizes the generator towards small conditioning values.,asp_clarity,MET,Unclear description of method
The clarity of the presentation (in particular the description of when the method is applicable) and the technical correctness of the paper are somewhat lacking.,asp_clarity,MET,Unclear description of method
Because of the above many discussions about discrete vs. continuous variables are missleading.,asp_clarity,MET,Unclear description of method
Many of the parameters here are also unclear and not properly defined/introduced.,asp_clarity,MET,Unclear description of method
What is the relationship between $\theta$ and $\tilde\theta$ exactly?,asp_clarity,MET,Unclear description of method
"- The end of the 2nd line of lemma 1: P, G should be \mathbb{P}, \mathbb{G}",asp_clarity,MET,Unclear description of method
- The 3rd line of lemma 1: epsilon1 -> epsilon_1,asp_clarity,MET,Unclear description of method
"- Page 14, Eq(14), \lambda should be s",asp_clarity,MET,Unclear description of method
"- Page 14, Eqs(13, 14), w(\mathbb{P}, \mathbb{G}) should appear on the right.",asp_clarity,MET,Unclear description of method
"Reading the Lemmas of Section 4 (Lemma 4.4) you can see that it is the smoothness parameter of function $H$. Thus, is not necessary to have it there (not important for the definition).",asp_clarity,MET,Unclear description of method
"For example, it is not clear the strong emphasis on the robust MDP formalization and the fact that MCTS finds a Nash equilibrium.",asp_clarity,MET,Unclear description of method
"Some statements don't make sense, however, eg. ""HSIC-based estimators tend to have loose confidence intervals due to the need to bound generalization error of kernels f and g on unseen data points.",asp_clarity,MET,Unclear description of method
"The authors do not explicitly define continual learning, incremental learning, and catastrophic forgetting problem.",asp_clarity,MET,Unclear description of method
2. It's very confusing when the authors introduce \sigma and \omega in the beginning of section 4: why would you need two networks predict the same thing?,asp_clarity,MET,Unclear description of method
"4. Even when the authors formally introduce \sigma and \omega in 4.2, it is still not clear that why both of them are used for modelling the success probability.",asp_clarity,MET,Unclear description of method
"For example, in Section 3.2, it can be made clear that the domain of the quantization function is the real and the codomain is a sequence k bits.",asp_clarity,MET,Unclear description of method
This makes the contribution of this paper in terms of the method,asp_clarity,MET,Unclear description of method
"#3) There are several Equations that can be combined, such that to save enough white space in order to discuss further some actual technical details.",asp_clarity,MET,Unclear description of method
- What is dt in Algorithm 1 description?,asp_clarity,MET,Unclear description of method
"--The notation for the proposed parameters theta, theta’, phi, phi’ are not consistent with the notation in the intro section, where phi was used for the encoder and theta for the decoder.",asp_clarity,MET,Unclear description of method
In later sections they use theta and theta’ for encoder/decoder resp.,asp_clarity,MET,Unclear description of method
"Why mention it here, if it's not being defined.",asp_clarity,MET,Unclear description of method
Also it's not clear from the details in the paper what are the architectures for the VAE and RFs (there's a reference to the code but would've been better to have sufficient details in the paper).,asp_clarity,MET,Unclear description of method
I think it is necessary to make it clearer how s_{post_read} and attn_copy are computed with the updated {h^i_t} and what u^i is expected to encode.,asp_clarity,MET,Unclear description of method
"Minor:  Since the action is denoted by ""a"",  it will be more clear if the authors use another symbol to denote the parameter of q(z) instead of ""\alpha"" at Eq 10 and 15.",asp_clarity,MET,Unclear description of method
"Minor, 1/2 is missing in the last line of Eq 19.",asp_clarity,MET,Unclear description of method
"The authors seems interchangeably using “runs” and “iterations”, which makes the concept more confusable.",asp_clarity,MET,Unclear description of method
"- Lack of clarity in the following passage: “In our setting, each point xi in the point cloud can be considered to correspond to single images when we train GANs over images”",asp_clarity,MET,Unclear description of method
"Likewise, it is not well explained what is the value of the learnt relationships, and how uncertainty and errors in the causal learning are relevant to the downstream use of the learnt model.",asp_clarity,MET,Unclear description of method
"Here the end goal is less clear; this is understandable in the sense that the work is positioned as a piece in a grand objective, but it would seem valuable to nevertheless describe some concrete example(s) to elucidate this aspect of the algorithm (use case / error effects downstream).",asp_clarity,MET,Unclear description of method
I don’t understand the claim that “GANs prioritize matching joint distributions of pixels over per-pixel reconstruction” and its implication that VAEs do not prioritize joint distribution matching.,asp_clarity,MET,Unclear description of method
"A minor comment is that the mutual information I(., .) being a function of two variables suddenly became a function of a single variable in Eq. (1) and in the text which precedes it.",asp_clarity,MET,Unclear description of method
3. The key contribution of the paper that the authors could highlight better is that they do not add new hyper-parameters.,asp_clarity,MET,Unclear description of method
It is not clear how smaller RBMs (and their associated parameters) are combined to obtain the larger RBM model.,asp_clarity,MET,Unclear description of method
Perhaps a cleaner definition would just be “A is full rank and there does not exist any X such that Ax < 0?,asp_clarity,MET,Unclear description of method
Also perhaps better to use the curly sign for vector inequality.,asp_clarity,MET,Unclear description of method
|_{y>0} x + b |_{y>0}  <— what is the purpose of the subscripts here? Why is this notation never introduced?,asp_clarity,MET,Unclear description of method
“From a high-level perspective both of these approaches” --> missing comma after “perspective”,asp_clarity,MET,Unclear description of method
"""reverse view on adversarial examples"" --- what this means isn't clear from the preceding text.",asp_clarity,MET,Unclear description of method
"- It would be better to provide the details of the procedure of the proposed method (e.g., Algorithm 1 and each processing of Algorithm 1) in the paper, not in the Appendix.",asp_clarity,MET,Unclear description of method
- The method is very confusingly presented and requires both knowledge of HAT as well as more than one reading to understand.,asp_clarity,MET,Unclear description of method
"The fact that HAT-like masks are used for a generative replay approach is clear, but the actual mechanism of ""growing capacity"" is not made clear at all especially in the beginning of the paper.",asp_clarity,MET,Unclear description of method
"2) Using the word ""task"" in describing ""joint training"" of the generative, discriminative, and classification networks is very confusing (since ""task"" is used for the continual learning description too,",asp_clarity,MET,Unclear description of method
"The method is difficult to understand, missing many details and essential explanation, and generally does not support a significant contribution.",asp_clarity,MET,Unclear description of method
"In equations 1 and 2, should a, b be written in capital? Since they represent random variables.",asp_clarity,MET,Unclear description of method
"Moreover, the way classical models are casted under the InfoNCE principle is badly written: it assumes that readers have a very good knowledge of the models, and the paper does not show well the mapping between the loss function of each model and the InfoNCE criterion.",asp_clarity,MET,Unclear description of method
"It gives technical details that could (in my opinion) get ignored, and I would clearly prefer to catch the main differences between the different models that being flooded by technical details.",asp_clarity,MET,Unclear description of method
"Here again, the article moves from technical details (e.g ""hidden state of the first token (assumed to be a special start of sentence symbol "") without providing formal definitions.",asp_clarity,MET,Unclear description of method
J_MLM is also not clear since x_i is never defined (I assume it is x_{i:i}).,asp_clarity,MET,Unclear description of method
"but the way the paper is written makes it very difficult to follow, and the description of the proposed model is unclear (making the experiments difficult to reproduce) and lacks of a better discussion about the interest of mixing multiple loss.",asp_clarity,MET,Unclear description of method
"There is a little typo in Formula 1 for the STFT spectrogram, I would use the modulus |.| rather than || . ||.",asp_clarity,MET,Unclear description of method
I am a bit unclear about how comparisons are made to other methods that do not optimize for small numbers of parameters? Do you compare against the lowest error network found by LEMONADE? The closest match in # of parameters?,asp_clarity,MET,Unclear description of method
I identify this ambiguity between BERTScore versions as an important weakness of the paper.,asp_clarity,MET,Unclear description of method
"Notably, I'm actually unsure whether you compute IDF over words or word pieces, and how this is applied.",asp_clarity,MET,Unclear description of method
"I found the method section a bit difficult to read though, and even after several readings I cannot get my head around it. Specifically, here are some issues that I hope the Authors could clarify.",asp_clarity,MET,Unclear description of method
- There is a typo in equation 6,asp_clarity,MET,Unclear description of method
"In addition, Procedure 1 is not referenced in the text which makes is hard to understand the utility of the same.",asp_clarity,MET,Unclear description of method
"- I understand the upper bounds on the singular values, but I am not sure how they relate to inverse stability.",asp_clarity,MET,Unclear description of method
The paper seems to lack clarity on certain design/ architecture/ model decisions.,asp_clarity,MET,Unclear description of method
"For example, the authors did not justify why VGMM model was chosen and how does it compare to other density estimators.",asp_clarity,MET,Unclear description of method
"Also, I had to go through a large chunk of the paper before coming across the exact setup.",asp_clarity,MET,Unclear description of method
"- Page 8: ""differntiable methods for NAS."" differentiable is misspelled.",asp_clarity,MET,Unclear description of method
I did not completely follow the arguments towards directed graph deconvolution operators.,asp_clarity,MET,Unclear description of method
"A clearer explanation of the theory here would help, as I think Fourier's theorem nicely supports your claims.",asp_clarity,MET,Unclear description of method
"However, the motivation and benefits of introducing a parent and child variational approximation are not discussed adequately.",asp_clarity,MET,Unclear description of method
I also struggled a little to understand what is the difference between forward interpolate and filtering.,asp_clarity,MET,Unclear description of method
"While the generation of this type of explanations is somewhat novel, from the text it seems that the proposed method may not be able to indicate what part of the image content drove the model to predict class A. Is this indeed the case?",asp_clarity,MET,Unclear description of method
1) There is no clear rationale on why we need a new model based on Transformers for this task.,asp_clarity,MET,Unclear description of method
"Clarification regarding lemma 1: it seems that if the true posterior cannot be expressed by q, a gap will necessarily remain, even in the “limit” of perfect learning. Is this correct?",asp_clarity,MET,Unclear description of method
- Lambda sim and lambda s are used interchangeably. Please make it consistent.,asp_clarity,MET,Unclear description of method
I am not convinced that the measure theoretic perspective is always,asp_clarity,MET,Unclear description of method
"4.4, law of total variation -- define",asp_clarity,MET,Unclear description of method
"The authors do not focus (in the main paper) on GAN variants used currently, and it is not clear if the proposed approach provides improvement relative to the current state of the art implementations (see next paragraph)",asp_clarity,MET,Unclear description of method
"I believe in the second paragraph of 4.1 the authors provide some insight into this matter, however, I have to admit I do not understand this paragraph:",asp_clarity,MET,Unclear description of method
"However, what was not clear to me is how this reduces the non-stationarity of MARL.",asp_clarity,MET,Unclear description of method
"I feel like in the paragraph in questions, a lot of causes and effects are mixed up and more careful descriptions of the benefits of the algorithm would help.",asp_clarity,MET,Unclear description of method
"Apart from global vs. local, can the authors provide more examples of what sort of information this approach can disentangle? (Even for global vs. local, is there a transformation that can remove local information as opposed to global information?)",asp_clarity,MET,Unclear description of method
- What is the choice of beta in the beta-VAE training objective?,asp_clarity,MET,Unclear description of method
"- Why were 30 discrete categories used in the clustering experiment? Is this still comparable to the approaches that use 10, which would correspond to the number of classes?",asp_clarity,MET,Unclear description of method
1. What is M in Algorithm 1 ?,asp_clarity,MET,Unclear description of method
Not sure why Eqns. 2 and 9 need any parentheses,asp_clarity,MET,Unclear description of method
"However, the explanation of the strategy wasn’t very clear for me, and the authors didn’t frame it as a major contribution.",asp_clarity,MET,Unclear description of method
"For the learnable data augmentation it would be great if the authors could provide more insight into the method, how it works, and why is it better than the alternatives.",asp_clarity,MET,Unclear description of method
"This second part is indeed what is used in the proof of the universality of the permutation invariant version of DeepSets, making the connection more visible.",asp_clarity,MET,Unclear description of method
Generalizing this to the multi-channel input as the next step could make the proof more accessible.,asp_clarity,MET,Unclear description of method
1) I do not really understand the emphasis on optimisation while all the proofs are related to the convergence to the stationary distributions.,asp_clarity,MET,Unclear description of method
"There are probably many other ways to trade BLEU score for efficiency, and without showing those other methods (and the point drops they have), it's not clear that K-matrices are a good way to speed up decoding a bit.",asp_clarity,MET,Unclear description of method
- Direct duplication of text between parts of section 5.3 and 8.3 leading to the duplication of the error of describing the value function learning rate as 0.000.,asp_clarity,MET,Unclear description of method
- The egocentric velocity field is not described (section 5),asp_clarity,MET,Unclear description of method
- The wording new paradigm in MARL might be unsuited given existing work on complex domains.,asp_clarity,MET,Unclear description of method
‘Our proposed approach represents the first physics-based simulation of its kind that supports MARL.’ This sentence remains unclear as the authors do not propose a simulation engine.,asp_clarity,MET,Unclear description of method
"However, the  transfer learning model is unclear.",asp_clarity,MET,Unclear description of method
"Detail: section 4 describes the transfer learning model used in the work, but the description is unclear.",asp_clarity,MET,Unclear description of method
It is unknown the used model is a new model or existing model.,asp_clarity,MET,Unclear description of method
I have doubts on applying the proposed method to higher dimensional inputs.,asp_clarity,MET,Unclear description of method
"ReLU network with 2 hidden layers, and it is unknown if it works in general.",asp_clarity,MET,Unclear description of method
positive - it unlikely to be true that an undefended network is predominantly,asp_clarity,MET,Unclear description of method
adversarial examples (or counter-examples for property verification) with L_inf,asp_clarity,MET,Unclear description of method
"This change is noted, but it would be useful to make a least a brief (i.e. one sentence) comment on the motivation for this change.",asp_clarity,MET,Unclear description of method
9.2.	In formula (1.5) “<=>” seems to be used at different levels (?) it would be good to use brackets to make clear which level “<=>” refers to.,asp_clarity,MET,Unclear description of method
"-In describing Eqn 3 there are some weird remarks, e.g. ""N is the sum of all frequencies"". Do you mean that N is the total number of available frequencies? i.e.",asp_clarity,MET,Unclear description of method
- Your F and \tilde{f} are introduced as infinite series.,asp_clarity,MET,Unclear description of method
"- Still it is unclear where 'fj' comes from. You need to state in words eg ""C[b] contains the accumulation of all fj's such that h(j)=b; i.e. for each sequence j \in U, if the hash function h maps the sequence to bin b (ie $h(j)=b$), then we include the *corresponding frequency* in the sum.""",asp_clarity,MET,Unclear description of method
"- The term ""sketch"" is used in Algorithm1, like 10, before 'sketch' is defined!!",asp_clarity,MET,Unclear description of method
"-I'm not going to trudge through the proofs, because I don't think this is self-contained (and I'm clearly not an expert in the area).",asp_clarity,MET,Unclear description of method
"2. While the primary motivation of the work is claimed to be 'mode collapse', it does not turn out from the submission what mode collapse is.",asp_clarity,MET,Unclear description of method
"- The set F in Definition 3.5 looks odd, as it appears to be recursive and might not be unique.",asp_clarity,MET,Unclear description of method
Isn't this just restating the point made in the first sentence?,asp_clarity,MET,Unclear description of method
"3, The notations in Eq. (1) and (2) are messy.",asp_clarity,MET,Unclear description of method
"However, the method, particularly on the weight sharing, lacked a bit of important background on adaptive softmax.",asp_clarity,MET,Unclear description of method
It is unclear how well the proposed method works in general.,asp_clarity,MET,Unclear description of method
"Furthermore, I had some trouble understanding how a SAT instance is solved using algorithm 1.",asp_clarity,MET,Unclear description of method
Specifically the text in Section 3.3 that explains Algorithm 1 is a bit confusing.,asp_clarity,MET,Unclear description of method
"In the start of Section 3, it is not clear why having the projection be sparse is desired.",asp_clarity,MET,Unclear description of method
eqn (8): use something else to denote the function 'U'.,asp_clarity,MET,Unclear description of method
"Additionally, it is unclear how you will have vanishing rewards given the structure of the learned controller.",asp_clarity,MET,Unclear description of method
"This clipping will also introduce bias, this is not discussed, and will probably lower variance.",asp_clarity,MET,Unclear description of method
"This detail is not discussed in this paper, and some details -- such as the learning rate for the meta-optimizer I was unable to find.",asp_clarity,MET,Unclear description of method
It would be better if the authors were a little more careful in their use of terminology here.,asp_clarity,MET,Unclear description of method
*The strategy proposed to introduce weak-supervision is too ad-hoc.,asp_clarity,MET,Unclear description of method
*It is not clear why the latent variables modelling the generative factors are defined using a Gaussian prior.,asp_clarity,MET,Unclear description of method
How the case where two images have a very similar latent factor is avoided while generating pairs of images for the Siamese network?,asp_clarity,MET,Unclear description of method
-The strategy proposed to provide weak-supervision to the model is too ad-hoc and it is not clear how to apply it in general applications,asp_clarity,MET,Unclear description of method
"Although a detailed discussion is provided related to the memory consumption between the proposed method and PipeDream, no detailed discussion is provided with respect to GPipe.",asp_clarity,MET,Unclear description of method
"- (W4) Metrics for ranking of transfer don't make sense (and some are missing) : I also don't understand how ""precision"" and NDCG are used as metrics.",asp_clarity,MET,Unclear description of method
In most cases I would care about how well my model would perform on transfer not just which tasks I should transfer from. I would have wanted to understand something like the correlation of these produced scores with the actual ground truth performance numbers.,asp_clarity,MET,Unclear description of method
"- (top of p. 2) What exactly is the difference between ""implicit feature combinations"" and ""explicit (?), expressive feature combinations""",asp_clarity,MET,Unclear description of method
"- (top of p. 2) ""encourage parameter sharing"" - between what and what? at which level? [reading on, I realized this applies to groups of features; it should maybe be made clear earlier]",asp_clarity,MET,Unclear description of method
- U^m in Eq 1 is undefined and un-discussed.,asp_clarity,MET,Unclear description of method
"What probability term does it correspond to? It is supposed to make probabilities of different cardinalities comparable, but the exact mechanism is unclear.",asp_clarity,MET,Unclear description of method
- The term p(w) disappears on the left hand side of Eq 2.,asp_clarity,MET,Unclear description of method
"- Notation in Sec. 3.2 is very cumbersome, making it hard to follow.",asp_clarity,MET,Unclear description of method
"Furthermore, I found the description ambiguous, preventing me from understanding how exactly the permutation head output is used in Eq 5.",asp_clarity,MET,Unclear description of method
"Specifically, there is some confusion about estimation of w~, which seems based on frequency estimation from past SGD iterations (Eq 3).",asp_clarity,MET,Unclear description of method
"- The network architecture is never described, especially the transition from Conv to Dense and the layer sizes, making the work hard to reproduce.",asp_clarity,MET,Unclear description of method
"In the main text (up to Section 7), there is no mentioning of how the low-level controllers are learned, and how to combine PPO in a MARL partial parameter sharing setting.",asp_clarity,MET,Unclear description of method
Is there a constraint on the CFS and classifiers that ensure the difference between the weights really captures what is suggested?,asp_clarity,MET,Unclear description of method
The paper would read much better if the authors better describe the phenomena based on the gap between the two distribution than using bling-spot.,asp_clarity,MET,Unclear description of method
Is it because the ML solution would be faster to compute with big instances? Is it because with the proposed approach one can curate sophisticated heuristic solutions when provable optimality is out of reach?,asp_clarity,MET,Unclear description of method
- I believe that in particular section 2 goes into too many mathematical details and subtleties that do not really add a lot.,asp_clarity,MET,Unclear description of method
"I think that either the reader already understand those concepts well (which I admit, I don't really, I'm merely curious about GANs and have been following the action from a distance, hence my low confidence rating to this review), or if they does not, it will be very hard to get much out of it.",asp_clarity,MET,Unclear description of method
"- ""the variance of models obtained by Guassian Process regression is handled implicitely so we tran each model once""? I do not understand what this means, and I work with hyper-parameter tuning using gaussian processes daily.",asp_clarity,MET,Unclear description of method
"Although the method the authors suggest is a plausible way to explicitly model the relationship between syntactic pairs and to create a combined embedding for them, their presentation does not make this obvious and it takes effort to reach the conclusion above.",asp_clarity,MET,Unclear description of method
"If this is the case, authors should state it more clearly in the paper that a large proportion of the gap in performance between MixMatch and ReMixMatch is the introduction of stronger transformations (Autoaugment style).",asp_clarity,MET,Unclear description of method
- The discussion/explanation of the differing performance of tying or not tying each part of the embedding weights for the different datasets is confusing; I think it could benefit from tightening up the wording but mostly I just had to read it a couple times. Perhaps all that's complicated is the distinction between embedding and projection weights; it would definitely be helpful to be as explicit about that as possible upfront.,asp_clarity,MET,Unclear description of method
"p2-3, Section 3.1 - I found the equations impossible to read. What",asp_clarity,MET,Unclear description of method
- The importance mixing does not seem to provide a better performance and could therefore be shortened in the paper,asp_clarity,MET,Unclear description of method
It is unclear how the model actually operates and uses attention during execution.,asp_clarity,MET,Unclear description of method
"- typo at the beginning os section 4:  missing 'be' in ""... the algorithm must irrevocably *be* allocated to ...""",asp_clarity,MET,Unclear description of method
-	Talking about depth parametrization use ‘basis’ or ‘bases’ not both and clearly defined the meaning of this important notion.,asp_clarity,MET,Unclear description of method
-	Attention should be given to the notation in formulas (3) and (4).,asp_clarity,MET,Unclear description of method
The projection function there is no longer accepts a 3D point parametrized by 3 variables.,asp_clarity,MET,Unclear description of method
Instead only depth is provided.,asp_clarity,MET,Unclear description of method
"In addition, the subindex ‘1’ of the point ‘q’ is not explained.",asp_clarity,MET,Unclear description of method
I don't understand what is meant by cognitively plausible but not human-like; perhaps an example of a cognitively implausible mechanism would help clarify this issue.,asp_clarity,MET,Unclear description of method
"* Another vague concept that is used without clarification: it is argued that if the network implements something like the Approximate Number System, that shows that it can ""learn and utilize higher-level concepts than mere pattern matching"".",asp_clarity,MET,Unclear description of method
"- My main concern about the analysis is that it shows why several methods (e.g., momentum, multiple update steps) are *not* helpful for stabilising GANs, but does not tell why training with these methods, as well as others such as gradient penalty, *do converge* in practice with properly chosen hyper-parameters?",asp_clarity,MET_ANA,"- My main concern about the analysis is that it shows why several methods (e.g., momentum, multiple update steps) are *not* helpful for stabilising GANs, but does not tell why training with these methods, as well as others such as gradient penalty, *do converge* in practice with properly chosen hyper-parameters?"
"As pointed out by R2, with depth there are a lot more number of possible ways in which one could carve out decision boundaries to separate data points, thus, it is not clear that the loose linear upper bound holds Specifically, as one might expect with depth it could be possible that linear capacity increase is a lower bound (I am not suggesting that it is, but that possibility should be considered and explained in the paper).",asp_clarity,MET_DAT,"As pointed out by R2, with depth there are a lot more number of possible ways in which one could carve out decision boundaries to separate data points, thus, it is not clear that the loose linear upper bound holds Specifically, as one might expect with depth it could be possible that linear capacity increase is a lower bound (I am not suggesting that it is, but that possibility should be considered and explained in the paper)."
If I were to go through the computation of then why not just train a smaller version of that representation technique instead and **directly** see how well it can encode data in k dimensions via that technique / for that task?,asp_clarity,MET_DAT_EXP,If I were to go through the computation of then why not just train a smaller version of that representation technique instead and **directly** see how well it can encode data in k dimensions via that technique / for that task?
"First, many details are missing, especially in the experiments, which makes the proposed method suspicious and non-convincing.",asp_clarity,MET_EXP,-The experimental section do not clarify the benefits of the proposed approach.
"1. While I see the value in designing an automatic exploration mechanism, the complexity of the underlying approach makes the contribution of the bandit-based algorithm difficult to discern from the large number of other bells and whistles in the experiments.",asp_clarity,MET_EXP,-The experimental section do not clarify the benefits of the proposed approach.
-The experimental section do not clarify the benefits of the proposed approach.,asp_clarity,MET_EXP,-The experimental section do not clarify the benefits of the proposed approach.
The details of the approach is not entirely clear and no theoritcal results are provided to support the approach.,asp_clarity,MET_RES,The details of the approach is not entirely clear and no theoritcal results are provided to support the approach.
"* It is unclear to me whether the ""efficient method for SN in convolutional nets"" is more efficient than the power iteration algorithm employed in previous work, such as Miyato et al. 2018, which also used SN in conv nets with different strides.",asp_clarity,MET_RWK,"Unclear connection of method with related work (description of the related work it is based on, why earlier methods fail, etc)"
The difference with the other reference model (SVG) is less clear.,asp_clarity,MET_RWK,"Unclear connection of method with related work (description of the related work it is based on, why earlier methods fail, etc)"
"I agree that local SO(2) invariance is too limiting. But it is not true that rotating filters is not effective in planar/volumetric CNNs, as shown by many recent papers on equivariant networks.",asp_clarity,MET_RWK,"Unclear connection of method with related work (description of the related work it is based on, why earlier methods fail, etc)"
"Since the paper relies so heavily on Lee et al., the authors should make an effort to summarize the approach in a mathematically precise way.",asp_clarity,MET_RWK,"Unclear connection of method with related work (description of the related work it is based on, why earlier methods fail, etc)"
"In particular, the exact difference between the proposed method and the ES baseline is not as clear as it could be.",asp_clarity,MET_RWK,"Unclear connection of method with related work (description of the related work it is based on, why earlier methods fail, etc)"
"Otherwise, it is impossible for a reader to correctly and objectively relate your proposed approach to previous literature.",asp_clarity,MET_RWK,"Unclear connection of method with related work (description of the related work it is based on, why earlier methods fail, etc)"
"- The parameter count of a 2-layer network with $h$ hidden units and input dimension $d$ is $O(dh)$. So perhaps it makes sense to study an asymptotic regime where $dh/n$ approaches $\gamma$, instead of both d and h growing linearly in n. While this issue is hinted at in the discussion section, I don't understand the statement ""the mechanism that provably gives rise to double descent from previous works Hastie et al. (2019); Belkin et al. (2019) might not translate to optimizing two-layer neural networks.""",asp_clarity,MET_RWK,"Unclear connection of method with related work (description of the related work it is based on, why earlier methods fail, etc)"
Without the comparison it’s not clear how much improvement this approach provides compared to existing work that perform stale updates.,asp_clarity,MET_RWK,"Unclear connection of method with related work (description of the related work it is based on, why earlier methods fail, etc)"
This poses a challenge in evaluating this paper.,asp_clarity,OAL,3. The paper is not nicely written or rather easy to follow.
"Finally, the style (font) of the paper does not adhere to the ICLR style template, and must be changed.",asp_clarity,OAL,3. The paper is not nicely written or rather easy to follow.
- some parts of the paper are quite unclear,asp_clarity,OAL,3. The paper is not nicely written or rather easy to follow.
The primary difficulty in reviewing this paper is the poor presentation of the paper.,asp_clarity,OAL,3. The paper is not nicely written or rather easy to follow.
This issues makes reviewing this paper very difficult.,asp_clarity,OAL,3. The paper is not nicely written or rather easy to follow.
There are many typos and grammar errors,asp_clarity,OAL,3. The paper is not nicely written or rather easy to follow.
Weaknesses: Paper could have been written better. I had hard time understanding it.,asp_clarity,OAL,3. The paper is not nicely written or rather easy to follow.
The notations are overall confusing and not explained well.,asp_clarity,OAL,3. The paper is not nicely written or rather easy to follow.
I also had a hard time going through the paper - there aren't many details.,asp_clarity,OAL,3. The paper is not nicely written or rather easy to follow.
"The paper lacks rigor and the writing is of low quality, both in its clarity and its grammar.",asp_clarity,OAL,3. The paper is not nicely written or rather easy to follow.
"However, there are some key issues with the paper that are not clear.",asp_clarity,OAL,3. The paper is not nicely written or rather easy to follow.
- This paper is a slightly difficult read - not because of the,asp_clarity,OAL,3. The paper is not nicely written or rather easy to follow.
"- Minor grammatical mistakes (missing ""a"" or ""the"" in front of some terms, suggest proofread.)",asp_clarity,OAL,3. The paper is not nicely written or rather easy to follow.
2) The writing is poor and hard to follow.,asp_clarity,OAL,3. The paper is not nicely written or rather easy to follow.
"- the paper is poorly written and sentences are generally very hard to parse. For example, section 3.1 is opened by statements such as ""(we use) a multi-task evaluator which trains on the principal and auxiliary tasks, and evaluates the performance of the auxiliary tasks on a meta set""??",asp_clarity,OAL,3. The paper is not nicely written or rather easy to follow.
"- Paper is often hard to follow, and contains a significant number of typos.",asp_clarity,OAL,3. The paper is not nicely written or rather easy to follow.
The paper can benefit from a proofreading.,asp_clarity,OAL,3. The paper is not nicely written or rather easy to follow.
There are a few typos throughout the paper such as:,asp_clarity,OAL,3. The paper is not nicely written or rather easy to follow.
* The text is quite hard to read.,asp_clarity,OAL,3. The paper is not nicely written or rather easy to follow.
There are many typos (see below).,asp_clarity,OAL,3. The paper is not nicely written or rather easy to follow.
"In general, I feel the paper is interesting but would benefit from a major revision which makes the message of the paper more clear, and addresses these and other issues raised in the review phase. Thus I am holding my current rating.",asp_clarity,OAL,3. The paper is not nicely written or rather easy to follow.
"I had trouble to understand some parts of this paper, since some of the sentences do not make sense to me. For example",asp_clarity,OAL,3. The paper is not nicely written or rather easy to follow.
"I think this is a very interesting direction, but the present paper is somewhat unclear.",asp_clarity,OAL,3. The paper is not nicely written or rather easy to follow.
"Overall, I think this paper has some interesting ideas, but those need to be fleshed out and clearly explained in a future revision.",asp_clarity,OAL,3. The paper is not nicely written or rather easy to follow.
"The paper is relatively well-written, although the description of the neural models can be improved.",asp_clarity,OAL,3. The paper is not nicely written or rather easy to follow.
The main criticism I have is that I found the paper harder to read.,asp_clarity,OAL,3. The paper is not nicely written or rather easy to follow.
(4) The writing quality is not satisfactory.,asp_clarity,OAL,3. The paper is not nicely written or rather easy to follow.
3. [Presentation.] The presentation is undesirable. It may make the readers hard to follow the paper.,asp_clarity,OAL,3. The paper is not nicely written or rather easy to follow.
"Overall the paper, while interesting is unacceptably messy.",asp_clarity,OAL,3. The paper is not nicely written or rather easy to follow.
The paper is also littered with typos and vague statements (many enumerated below under *small issues*).,asp_clarity,OAL,3. The paper is not nicely written or rather easy to follow.
3. The paper is not nicely written or rather easy to follow.,asp_clarity,OAL,3. The paper is not nicely written or rather easy to follow.
I recommend adjust the language to be more consistent throughout.,asp_clarity,OAL,3. The paper is not nicely written or rather easy to follow.
Especially given the very specific nature of the topic I miss a strong and clear path through the paper.,asp_clarity,OAL,3. The paper is not nicely written or rather easy to follow.
"In terms of writing, the paper is a bit confusing in terms of motivations and notations.",asp_clarity,OAL,3. The paper is not nicely written or rather easy to follow.
"The paper is well-written but the structure is a bit disconnected; most notably, I didn't see clearly how Section 2 and 3 fit together.",asp_clarity,OAL,3. The paper is not nicely written or rather easy to follow.
I think the paper could benefit from having this in the earlier sections.,asp_clarity,OAL,3. The paper is not nicely written or rather easy to follow.
"First, this paper is not easy to follow.",asp_clarity,OAL,3. The paper is not nicely written or rather easy to follow.
"Fourth, there are some grammar mistakes and typos.",asp_clarity,OAL,3. The paper is not nicely written or rather easy to follow.
"Third, the writing in the paper has some significant lapses in clarity.",asp_clarity,OAL,3. The paper is not nicely written or rather easy to follow.
I found the paper confusing at times.,asp_clarity,OAL,3. The paper is not nicely written or rather easy to follow.
"It is well-written from a syntactical and grammatical point of view, but some key concepts are stated without being explained, which gives the impression that the authors have a clear understanding of the material presented in the paper but communicate only part of the full picture to the reader.",asp_clarity,OAL,3. The paper is not nicely written or rather easy to follow.
Clarity: The clarity is below average.,asp_clarity,OAL,3. The paper is not nicely written or rather easy to follow.
The clarity of this paper needs to be strengthened.,asp_clarity,OAL,3. The paper is not nicely written or rather easy to follow.
2) The main contributions of this paper are not quite clear to me.,asp_clarity,OAL,3. The paper is not nicely written or rather easy to follow.
"In terms of presentation, the notation and statement of theorems are precise, however, the presentation is rather dry, and I think the paper can be significantly more accessible.",asp_clarity,OAL,3. The paper is not nicely written or rather easy to follow.
- Some parts of the paper feel long-winded and aimless.,asp_clarity,OAL,3. The paper is not nicely written or rather easy to follow.
"Honestly, this paper is very difficult to follow.",asp_clarity,OAL,3. The paper is not nicely written or rather easy to follow.
"2) The presentation is not professional, hard to follow and the submission overall looks very rushed:",asp_clarity,OAL,3. The paper is not nicely written or rather easy to follow.
"- The writing looks very rushed, and should be improved.",asp_clarity,OAL,3. The paper is not nicely written or rather easy to follow.
"This paper looks very hastily put together, especially pages 7 and 8.",asp_clarity,OAL,3. The paper is not nicely written or rather easy to follow.
There are many typos and unclear statements.,asp_clarity,OAL,3. The paper is not nicely written or rather easy to follow.
"To change to a firm accept, I think the paper needs some changes in written style mainly, to make it friendlier to newcomers to the area, which can easily be implemented in the camera ready stage of paper preparation.",asp_clarity,OAL,3. The paper is not nicely written or rather easy to follow.
"Overall, the paper is a little confusing.",asp_clarity,OAL,3. The paper is not nicely written or rather easy to follow.
"Unfortunately, it is riddled with grammatical errors and should be proof-read carefully. A lot of singular/plurals are off, and some formulations are odd or downright unclear.",asp_clarity,OAL,3. The paper is not nicely written or rather easy to follow.
- Grammatical errors and odd formulations all over the place,asp_clarity,OAL,3. The paper is not nicely written or rather easy to follow.
Current representation is difficult to read / parse.,asp_clarity,OAL,3. The paper is not nicely written or rather easy to follow.
"The overall presentation could be better, and I would encourage the authors to tidy the paper up in any subsequent submission.",asp_clarity,OAL,3. The paper is not nicely written or rather easy to follow.
"Second, the writing can be greatly improved.",asp_clarity,OAL,3. The paper is not nicely written or rather easy to follow.
"3) The paper needs a thorough proof-reading. There are many grammar mistakes, typos, missing citations. For example,",asp_clarity,OAL,3. The paper is not nicely written or rather easy to follow.
"Therefore, I recommend to accept the paper but after revision because the presentation and explanation of the ideas contain multiple typos and lacking some details (see bellow).",asp_clarity,OAL,3. The paper is not nicely written or rather easy to follow.
- It is extremely hard to follow what exactly is going on; I believe a few illustrative examples would help make the paper much clearer; in fact the idea is not that abstract.,asp_clarity,PDI,Unclear problem definition
"Although the idea behind this paper is fairly simple, the paper is very difficult to understand.",asp_clarity,PDI,Unclear problem definition
- The problem setting description is neither formal nor intuitive which made it very hard for me to understand exactly the problem you are trying to solve.,asp_clarity,PDI,Unclear problem definition
"Since quantitative real-world results are challenging to obtain, improved presentation of the qualitative results would be helpful as well.",asp_clarity,RES,"Improper explanation of results (as in advantages, why are results better, etc)"
"A reader’s most natural question is whether there is a large enough improvement to offset the extra computational cost, so the fact that wall-clock times are relegated to the appendix is a significant weakness.",asp_clarity,RES,"Improper explanation of results (as in advantages, why are results better, etc)"
"Particularly, the authors mixed their observations up with the results of published works, making it hard to identify the contributions of this paper.",asp_clarity,RES,"Improper explanation of results (as in advantages, why are results better, etc)"
"- There are better words than ""decent"" to describe the performance of DARTS, as it's very similar to the results in this work!",asp_clarity,RES,"Improper explanation of results (as in advantages, why are results better, etc)"
This paper presents non-trivial theoretical results that are worth to be published but as I argue below its has a weak relevance to practice and the applicability of the obtained results is unclear.,asp_clarity,RES,"Improper explanation of results (as in advantages, why are results better, etc)"
- Lack of a strong explanation for the results or a solution to the problem,asp_clarity,RES,"Improper explanation of results (as in advantages, why are results better, etc)"
"Since the results are far from state of the art, a clean and neat presentation of the theoretical advantages and contributions is crucial.",asp_clarity,RES,"Improper explanation of results (as in advantages, why are results better, etc)"
It is hard to understand the results without discussing it.,asp_clarity,RES,"Improper explanation of results (as in advantages, why are results better, etc)"
"For instance, the omission of a results discussion section or a conclusion are clearly not reader friendly.",asp_clarity,RES,"Improper explanation of results (as in advantages, why are results better, etc)"
They would have a very low weight difference score though they are ideal representations for each other.,asp_clarity,RES,"Improper explanation of results (as in advantages, why are results better, etc)"
"* Some turns of phrase like ""recently gained a flourishing interest"", ""there is still a wide gap in quality of results"", ""which implies a variety of underlying factors"", ... are vague / do not make much sense and should probably be reformulated to enhance readability.",asp_clarity,RES,"Improper explanation of results (as in advantages, why are results better, etc)"
- Section 1.1 presents results with too many details without introducing the problem.,asp_clarity,RES,"Improper explanation of results (as in advantages, why are results better, etc)"
"However, the theoretical justification and experimental results are not.",asp_clarity,RES_EXP,"Improper Structuring of experimental results on paper (as in on some page, before some section)"
"Regarding the presentation, I found odd having some experimental results (page 5) before the Section on experience even have started.",asp_clarity,RES_EXP,"Improper Structuring of experimental results on paper (as in on some page, before some section)"
Q2: Can the authors structure the experimental results with different sections? Currently it is just a single section which is difficult to read.,asp_clarity,RES_EXP,"Improper Structuring of experimental results on paper (as in on some page, before some section)"
It is also not clear why Table. 3 does not report the Bayes baseline results.,asp_clarity,RES_TNF,Improper presentation of results in tables and figures
"Additionally, in section 6.4, the results in Figure 2 also does not look very",asp_clarity,RES_TNF,Improper presentation of results in tables and figures
"It would be useful to have a table, like the one on the last page, which clearly shows the baseline vs. the random-projection model, with some description of the results in the main body of the text.",asp_clarity,RES_TNF,Improper presentation of results in tables and figures
"This paper is very well written, although very dense and not easy to follows, as many methods are referenced and assume that the reviewer is highly familiar with the related works.",asp_clarity,RWK,"Improper writing of realted work section (as in 1 paragraph rather than multiple for related work, explanation of somepapers, etc)"
Hence it is unclear how large the running time improvement is compared to a well-tuned baseline.,asp_clarity,RWK,"Improper writing of realted work section (as in 1 paragraph rather than multiple for related work, explanation of somepapers, etc)"
"This is not to say that this way of doing things is wrong, but rather that it is misleading in the context of prior work.",asp_clarity,RWK,"Improper writing of realted work section (as in 1 paragraph rather than multiple for related work, explanation of somepapers, etc)"
I find the background on ELBO and GANs unnecessary occluding the clarity at this point.,asp_clarity,RWK,"Improper writing of realted work section (as in 1 paragraph rather than multiple for related work, explanation of somepapers, etc)"
"11. Baseline 2 is actually referred to as ""usage baseline"" but this name is not introduced in the itemized part.",asp_clarity,RWK,"Improper writing of realted work section (as in 1 paragraph rather than multiple for related work, explanation of somepapers, etc)"
"The paper is not very self-contained, and I have to constantly go back to Lee et al. and Xu et al. in order to read through the paper.",asp_clarity,RWK,"Improper writing of realted work section (as in 1 paragraph rather than multiple for related work, explanation of somepapers, etc)"
"Indeed, without referencing the original Pointer Network and (and especially the) Transformer papers, it would not be possible to understand this paper at all.",asp_clarity,RWK,"Improper writing of realted work section (as in 1 paragraph rather than multiple for related work, explanation of somepapers, etc)"
"Also, please place the related work earlier on in the paper.",asp_clarity,RWK,"Improper writing of realted work section (as in 1 paragraph rather than multiple for related work, explanation of somepapers, etc)"
"For example, there are two ""the"" in the end of the third paragraph in Related Work.",asp_clarity,RWK,"Improper writing of realted work section (as in 1 paragraph rather than multiple for related work, explanation of somepapers, etc)"
"In the last paragraph in Related Work, ""provide"" should be ""provides"".",asp_clarity,RWK,"Improper writing of realted work section (as in 1 paragraph rather than multiple for related work, explanation of somepapers, etc)"
"The related work section looks incomplete with some missing related references as mentioned above, and copy of a segment that appears in the introduction.",asp_clarity,RWK,"Improper writing of realted work section (as in 1 paragraph rather than multiple for related work, explanation of somepapers, etc)"
"2. First paragraph in related work is very unrelated to the current subject, please remove.",asp_clarity,RWK,"Improper writing of realted work section (as in 1 paragraph rather than multiple for related work, explanation of somepapers, etc)"
"While it’s great that so much prior work was acknowledged, mentioning a paper once per paragraph is (usually) sufficient and increases readability.",asp_clarity,RWK,"Improper writing of realted work section (as in 1 paragraph rather than multiple for related work, explanation of somepapers, etc)"
"For example, the related works paper would read better if it wasn't one long block (i.e. break it into several paragraphs)",asp_clarity,RWK,"Improper writing of realted work section (as in 1 paragraph rather than multiple for related work, explanation of somepapers, etc)"
9. It is not clear that baseline 1 and 2 correspond to which baselines in later experiments.,asp_clarity,RWK_EXP,9. It is not clear that baseline 1 and 2 correspond to which baselines in later experiments.
"- In Section 3.1 : “Across runs in Table 1, we observe that without Orthogonal Regularization, only 16% of models are amenable to truncation compared to 60% with Orthogonal Regularization.” For me, this is not particularly clear.",asp_clarity,TNF,"Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot)"
"- In Appendix D, the Figures could be slightly clarified by using a colored heatmap to color the curve, with colors corresponding to the threshold values.",asp_clarity,TNF,"Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot)"
"4) Table 1 difference between DNC and DNC (DM) is not clear. I am assuming it's the numbers reported in the paper, vs the author's implementation?",asp_clarity,TNF,"Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot)"
- Figures 1-4 are difficult to interpreted on a printed version.,asp_clarity,TNF,"Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot)"
- figures 2 & 3 should be a lot larger in order to be readable,asp_clarity,TNF,"Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot)"
4. Fig. 3 (right): It is not clear,asp_clarity,TNF,"Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot)"
"On the other hand it is clear that using the confidence of the model to predict the dataset is a useful property, but the right side of the Fig. is very confusing.",asp_clarity,TNF,"Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot)"
"2. Figure 3: it is confusing to call the cumulative distribution of the maximum classification score as the CDF of the model (y-axis fig. 3 left) as CDF means something else generally in such contexts, as the CDF of a predictor.",asp_clarity,TNF,"Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot)"
- Table 3 is a bit confusing as-is (lower is only better when controlling on the quality of the best sample.,asp_clarity,TNF,"Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot)"
- It isn't clear from the tables that OCE_0.1 outperforms REINFORCE_Z (as is mentioned in the discussion).,asp_clarity,TNF,"Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot)"
"2. In the caption of figure 2, there should be a space after `"":"".",asp_clarity,TNF,"Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot)"
"- In figure 3 (c) ""number |T of input"" should be  ""number |T| of input""",asp_clarity,TNF,"Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot)"
"- In figure 5 (a) ""cencept"" should be ""concept""",asp_clarity,TNF,"Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot)"
"- In figure 8 ""Each column corresponds to ..."" should be ""Each row corresponds to ..."".",asp_clarity,TNF,"Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot)"
Fig 4 is very confusing.,asp_clarity,TNF,"Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot)"
"First, it doesn’t label the X axis.",asp_clarity,TNF,"Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot)"
"Second, the caption mentions that early stopping is beneficial for the proposed method, but I can’t see it from the figure.",asp_clarity,TNF,"Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot)"
"Third, I don’t get what is plotted on different subplots.",asp_clarity,TNF,"Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot)"
The input and output types of each block in Figure 1. should be clearly stated.,asp_clarity,TNF,"Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot)"
"The figures are almost useless, because the captions contain very little information.",asp_clarity,TNF,"Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot)"
"For example, the authors should at least say that the ""D"" in Figure 2. stands for delay, and the underline in Figure 4. indicates the bits that are not pruned.",asp_clarity,TNF,"Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot)"
Many more can be said in all the figures.,asp_clarity,TNF,"Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot)"
"Furthermore, the usage of the evaluation method unclear as well, it seems to be designed for evaluating the effectiveness of different adversarial attacks in Figure 2.",asp_clarity,TNF,"Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot)"
-What’s the 3d plot supposed to represent?,asp_clarity,TNF,"Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot)"
=> The results shown in Figure-4 (Section-4.2) seems unclear to me.,asp_clarity,TNF,"Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot)"
* Figure 5 should appear after Figure 4.,asp_clarity,TNF,"Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot)"
The labels of figures are hard to read.,asp_clarity,TNF,"Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot)"
c. Figure 1 is over-complicated.,asp_clarity,TNF,"Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot)"
e. What are the two modalities in Table 2? The author should explain.,asp_clarity,TNF,"Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot)"
2. I don’t understand Figure 4.,asp_clarity,TNF,"Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot)"
"I am confused by Figure 4, and in general with the relative rank metrics.",asp_clarity,TNF,"Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot)"
Weakness: the presentation of the tables/bar charts in the experiment is a bit unclear.,asp_clarity,TNF,"Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot)"
"- Some of the figures your report are compelling but it is a bit unclear to the reader if the results are general (e.g., the examples could have been hand-picked). Are there any quantitative measures you could provide (in addition to Tables 1 and 2 which don't measure the quality of the approach)?",asp_clarity,TNF,"Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot)"
The explanation in Fig 2 on why this is the case seem to me not so clear. Are you trying to show that the Wave-U-Net does not work since there is no 1/f^2 law for clean audio signals?,asp_clarity,TNF,"Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot)"
"Tiny detail: The axes of several of the plots given in the paper mis the lables which makes it hard to read. Straightforward to fix, but worth mentioning nevertheless.",asp_clarity,TNF,"Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot)"
- I am confused what is the fixed reference in Figure 6. It is not explained in the main paper.,asp_clarity,TNF,"Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot)"
"Furthermore, I do not see legend in Figure 3 and thus I cannot figure out what the curves represent.",asp_clarity,TNF,"Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot)"
Typo:. The “Inf” in Tabel 1,asp_clarity,TNF,"Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot)"
"If this is what these graphs show, consider using a different visualization to make it clearer that you're improving the final performance, not just the training process.",asp_clarity,TNF,"Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot)"
"Also, Table 1 is really confused. I would not understand the meaning if I am not familiar with the experiment settings.",asp_clarity,TNF,"Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot)"
- The caption of Table 1 is a little vague. Please clearly state the meaning of the numbers in the table.,asp_clarity,TNF,"Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot)"
"2. In Figure 3, the baseline got different perplexity between 3(a) and 3(b).",asp_clarity,TNF,"Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot)"
"- There are two lines of text between Fig. 4 and Fig. 5, which is confusing.",asp_clarity,TNF,"Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot)"
- Text on experiment figures is much too small.,asp_clarity,TNF,"Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot)"
2. table 2: Dynamic -> Adaptive?,asp_clarity,TNF,"Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot)"
- The aspect ratio in Fig. 5 should be fixed.,asp_clarity,TNF,"Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot)"
-  Figure 2 are hard to read for different M's. It would be better if the authors can show the exact accuracy numbers rather than the overlapped lines,asp_clarity,TNF,"Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot)"
"- Again, Figure 3, it is hard to see the benefits for increasing M from the visualizations for different clusterings.",asp_clarity,TNF,"Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot)"
The plot (Figure2 (d)) is very blurry and people cannot really tell local structure from it.,asp_clarity,TNF,"Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot)"
"Some figures, like Figure 3 and 4, are hard to read.",asp_clarity,TNF,"Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot)"
• Not all of the arrows in Figure 1 are pointing to the right lines.,asp_clarity,TNF,"Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot)"
"With the current table, one has to compare cells in the top half of the table to those in the bottom half of the table, which is quite difficult to do.",asp_clarity,TNF,"Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot)"
The plots in figure 4 are too small.,asp_clarity,TNF,"Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot)"
"- the exposition could be improved, in particular the description of the plots is not very clear, I'm still not sure exactly what they show",asp_clarity,TNF,"Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot)"
The graphs were difficult to parse.,asp_clarity,TNF,"Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot)"
"I was able to make them out, but perhaps separating the top row (FID and diversity graphs) into separate figures, separate lines, or something would have reduced some confusion.",asp_clarity,TNF,"Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot)"
"3) I would suggest a different name other than Neural-LP-N, as it is somewhat underselling this work. Also it makes Table 2 not that easy to read.",asp_clarity,TNF,"Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot)"
-	Figures 1 is way too abstract given the complicated set-up of the proposed architecture.,asp_clarity,TNF,"Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot)"
It also adds capacity and it is not at all made clear whether the comparison is fair since no analysis on number of parameters are shown.,asp_meaningful-comparison,ANA,More experiments needed with related work
"1. Even though K-matrices are aimed at structured matrices, it would be curious either to empirically compare K-matrices to linear transformations in fully-connected networks (i.e. dense matrices) or to provide some theoretical analysis.",asp_meaningful-comparison,ANA,More experiments needed with related work
"Yeh, Raymond A., et al. ""Image Restoration with Deep Generative Models."" 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2018.",asp_meaningful-comparison,BIB,Incorrect citation
"Perhaps I missed it, but I believe Dan Ciresan's paper ""Multi-Column Deep Neural Networks for Image Classification"" should be cited.",asp_meaningful-comparison,BIB,Incorrect citation
"Citations used for Gradcam are wrong -- Sundarajan et al., 2016 should be changed to Selvaraju et al., 2017.",asp_meaningful-comparison,BIB,Incorrect citation
https://openreview.net/references/pdf?id=Sy2fzU9gl,asp_meaningful-comparison,BIB,Incorrect citation
https://arxiv.org/abs/1802.05822,asp_meaningful-comparison,BIB,Incorrect citation
https://arxiv.org/abs/1802.05983,asp_meaningful-comparison,BIB,Incorrect citation
https://arxiv.org/abs/1802.04942,asp_meaningful-comparison,BIB,Incorrect citation
"- I’d also like to see more extensive comparisons between FICM and ICM across different datasets, for example, Super Mario Bros. and the Atari games, instead of only comparing FICM against ICM on ViZDoom.",asp_meaningful-comparison,DAT,"- I’d also like to see more extensive comparisons between FICM and ICM across different datasets, for example, Super Mario Bros. and the Atari games, instead of only comparing FICM against ICM on ViZDoom."
"The real data experiments (sections 4.2 and 4.3) are not very convincing, not only because of the very small size of N, but also because there is no comparison with the other approaches.",asp_meaningful-comparison,DAT_EXP,"The real data experiments (sections 4.2 and 4.3) are not very convincing, not only because of the very small size of N, but also because there is no comparison with the other approaches."
"The MTurk experiment gives a qualitative picture, but it could be improved with comparisons to pairwise distances learned through alternative means using the RGB image itself (given that images would permit such a comparison).",asp_meaningful-comparison,EXP,More experiments needed with related work
"The message of synthetic experiments would be stronger if more of them were available and if the comparison between LOE, TSTE, and OENN was made on more of them.",asp_meaningful-comparison,EXP,More experiments needed with related work
"There should be sufficient details for a reader to implement this model, thought there are some minor details missing regarding the experimental setup, which will be addressed below.",asp_meaningful-comparison,EXP,More experiments needed with related work
The experimental results are not very convincing because many importance baselines are neglected.,asp_meaningful-comparison,EXP,More experiments needed with related work
The comparisons are also absent in experiments.,asp_meaningful-comparison,EXP,More experiments needed with related work
"In numerical experiments, there is no comparison with major competitors besides random sampling in the active learning setup.",asp_meaningful-comparison,EXP,More experiments needed with related work
"3) The experiments lack comparisons to several important baselines from self-supervised learning community, and methods using soft labels for training (as mentioned in 2) above).",asp_meaningful-comparison,EXP,More experiments needed with related work
"While the paper shows experimentally that they aren't as successful as the RFs or IDFs, there's no further discussion on the reasons for poor performance.",asp_meaningful-comparison,EXP,More experiments needed with related work
"Without a proper comparison (formal and experimental) with these lines of work, the paper is incomplete.",asp_meaningful-comparison,EXP,More experiments needed with related work
"So, I have some doubts about the experimental results.",asp_meaningful-comparison,EXP,More experiments needed with related work
"However, there is no comparison with ENAS and DARTS in experiments.",asp_meaningful-comparison,EXP,More experiments needed with related work
"The experiments on synthetic data could be improved: for reproducibility, many works on GANs used the same synthetic data as VEEGAN.",asp_meaningful-comparison,EXP,More experiments needed with related work
"- For the squeezenet and latent permutation experiments, would be nice if there is a comparison to other parameterizations of permutation matrices, e.g. gumbel-sinkhorn.",asp_meaningful-comparison,EXP,More experiments needed with related work
"3. In the introduction, the Authors point that prior methods have trouble dealing with textureless, reflective or transparent approaches, but it's not clear form the paper where it addresses these cases, and if yes, what is the mechanism for that.",asp_meaningful-comparison,INT,"3. In the introduction, the Authors point that prior methods have trouble dealing with textureless, reflective or transparent approaches, but it's not clear form the paper where it addresses these cases, and if yes, what is the mechanism for that."
"Given that object manipulation is the specific application of interest, the comparison with DIAYN and the combined objective with DIAYN is interesting but little motivation or discussion has been provided in the paper.",asp_meaningful-comparison,MET,Missing theoretical comparisons
"As I have mentioned above, actually a lot of methods have been developed to address general image restoration tasks.",asp_meaningful-comparison,MET,Missing theoretical comparisons
Thus I believe authors must compare their method with these state-of-the-art approaches.,asp_meaningful-comparison,MET,Missing theoretical comparisons
The model-based method achieves better validation error than the other baselines that use actual data.,asp_meaningful-comparison,MET,Missing theoretical comparisons
"There are tons of algorithms for image inpainting, denoising, and super-resolution, but the proposed method was not compared with them.",asp_meaningful-comparison,MET,Missing theoretical comparisons
"While the authors say ""attributing a deep network’s prediction to its input is well-studied"" they don't compare directly against these methods.",asp_meaningful-comparison,MET,Missing theoretical comparisons
* Comparison with other methods did not take into account a variety of hyperparameters.,asp_meaningful-comparison,MET,Missing theoretical comparisons
Hence the theoretical sample complexities contributed are not comparable to those of MIME.,asp_meaningful-comparison,MET,Missing theoretical comparisons
It would have been useful to compare the general models here with some specific math problem-focused ones as well.,asp_meaningful-comparison,MET,Missing theoretical comparisons
The proposed approach should compared to other stronger methods such as graph convolution neural network/message passing neural networks/structure2vec.,asp_meaningful-comparison,MET,Missing theoretical comparisons
"However, it’s noteworthy that embedding to a vector could be useful too if the embedding espace is representative of the entire history and the timing of the events.",asp_meaningful-comparison,MET,Missing theoretical comparisons
"For example, in Sections 4 and 5 I was hoping to see comparisons to methods like MAML.",asp_meaningful-comparison,MET,Missing theoretical comparisons
1. Can you clarify why the proposed approach is better than the Meta-RevGrad baseline?,asp_meaningful-comparison,MET,Missing theoretical comparisons
"The authors acknowledge this connection, but I think they should begin by introducing CE and then explain how Cakewalk generalizes it.",asp_meaningful-comparison,MET,Missing theoretical comparisons
"- In active learning, the proposed method should be compared with other methods such as Bayesian DNN using dropout, etc.",asp_meaningful-comparison,MET,Missing theoretical comparisons
"However, the un-standard design of the LSTM models makes it unclear whether the comparisons are solid enough.",asp_meaningful-comparison,MET,Missing theoretical comparisons
My main concerns are about the evaluation and comparison of standard neural models.,asp_meaningful-comparison,MET,Missing theoretical comparisons
I think these issues are against the goal of evaluating standard neural models on the benchmark and will raise doubts about the comparison between different models.,asp_meaningful-comparison,MET,Missing theoretical comparisons
"While the comparison with HSIC is a helpful one, it is also required to compare with the competing method closest to yours, i.e.",asp_meaningful-comparison,MET,Missing theoretical comparisons
"If the authors claim that the proposed MaxConfidence attack method is more powerful than the MaxLoss based attacks, they should provide more comparisons between these methods.",asp_meaningful-comparison,MET,Missing theoretical comparisons
My main concern about this paper is why this algorithm has a better performance than CW attack?,asp_meaningful-comparison,MET,Missing theoretical comparisons
I would suggest comparing with CW attack under different sets of hyper-parameters.,asp_meaningful-comparison,MET,Missing theoretical comparisons
One of the main area which is missing in the paper is the comparison to two other class of RL methods: count-based exploration and novelty search.,asp_meaningful-comparison,MET,Missing theoretical comparisons
"It is fine, but it seems weird that the author still mentioned the setup of MNIST+SVHN in the main text.",asp_meaningful-comparison,MET,Missing theoretical comparisons
"The choice of RBMs is not motivated well and without any comparisons to alternatives, it comes across",asp_meaningful-comparison,MET,Missing theoretical comparisons
"This is true throughout; as stated before it is not clear how many parameters and how much memory these methods need, which makes it impossible to compare.",asp_meaningful-comparison,MET,Missing theoretical comparisons
"In summary, the paper presents what seems like an effective strategy for continual learning, by combining some existing methods together, but does not make it precise what the contributions are and the methodology/analysis make it hard to determine if the comparisons are fair or not.",asp_meaningful-comparison,MET,Missing theoretical comparisons
Why is the second objective log(#params) instead of just #params when the introduction mentions explicitly that tuning the scales between different objectives is not needed in LEMONADE?,asp_meaningful-comparison,MET,Missing theoretical comparisons
"Three of these methods, i.e. Lime, GradCam, PDA, are not designed for producing contrastive explanations, so I am not sure to what extend this comparison is appropriate.",asp_meaningful-comparison,MET,Missing theoretical comparisons
"However, attack in Wasserstein distance and some other methods can also do so.",asp_meaningful-comparison,MET,Missing theoretical comparisons
"Especially if the lower-level component of the hierarchy is pre-trained in a non-MARL setup, it can just be seen as part of the environment from the point of view of the MARL training, offerring limited new insight into MARL.",asp_meaningful-comparison,MET,Missing theoretical comparisons
"Given that you observed that SVHN has higher likelihood on all three model types (PixelCNN, VAE, Glow), why investigate a component specific to just flow-based models (the volume term)?",asp_meaningful-comparison,MET,Missing theoretical comparisons
The paper should compare the proposed work to the InfoGAN work both quantitatively and qualitatively to justify its novelty.,asp_meaningful-comparison,MET,Missing theoretical comparisons
"Finally, the CNN architecture should be compared with modern SAT solvers which have been participating to SAT competitions.",asp_meaningful-comparison,MET,Missing theoretical comparisons
"If the two NNs used in DNN and DNN(resized) are different, I believe it’s still possible to apply the algorithm in DNN(resized) to the NN used in DNN, and get a full trace in the figure as noise and projection changes, which would lead to more fair comparison.",asp_meaningful-comparison,MET,Missing theoretical comparisons
"Though Kaleidoscope include these as special cases, it is not clear whether when given the same resources (either memory or computational cost), Kaleidoscope would outperform them.",asp_meaningful-comparison,MET,Missing theoretical comparisons
"Finally, the paper needs to compare its parameter-reduction approaches against other compression and hyperparameter optimization techniques.",asp_meaningful-comparison,MET,Missing theoretical comparisons
"And while there is an argument to be made for the inherent interestingness of exploring these questions, this angle would be more compelling if multiple encoder architectures were explored and compared.",asp_meaningful-comparison,MET,Missing theoretical comparisons
Have the authors considered to use categorical or binary variables?,asp_meaningful-comparison,MET,Missing theoretical comparisons
"Given that BERT is most robust against their manipulation, it would be good to see a more powerful recurrent model for comparison.",asp_meaningful-comparison,MET,Missing theoretical comparisons
Alternatively why not try using a factorization technique to reduce the rank and then see how well the method does for different ranks?,asp_meaningful-comparison,MET,Missing theoretical comparisons
1a. Comparison to other exploration methods.,asp_meaningful-comparison,MET,Missing theoretical comparisons
"- Since the proposed method starts from Laplace transform, it would be helpful to further discuss the connection between other methods that regularises the eigenvalues of the Jacobian (such as spectral-normalisation), which work in the frequency domain from a different perspective.",asp_meaningful-comparison,MET,Missing theoretical comparisons
Spare few lines of equations to define what are DeepSets and PointNetSeg in the paper and point out the difference since these networks are used throughout the paper extensively without proper mathematical definition.,asp_meaningful-comparison,MET,Missing theoretical comparisons
But the only negative aspect is the basis competitor algorithms are very simple in nature without any form of learning and that are very old.,asp_meaningful-comparison,MET,Missing theoretical comparisons
"Otherwise, it is hard to tell how significant these correlations are. Since the end goal is to determine transferability of tasks and not the methods, it does seem like there are simpler baselines that you could compare against.",asp_meaningful-comparison,MET,Missing theoretical comparisons
"My biggest concern is the lack of comparison with other representation learning methods, which is a very well studied problem.",asp_meaningful-comparison,MET,Missing theoretical comparisons
"However, it looks like authors only compared with VIB which is similar to the proposed method in terms of the objective function.",asp_meaningful-comparison,MET,Missing theoretical comparisons
- The paper mentions the MSVV algorithm twice but no reference or explanation is provided.,asp_meaningful-comparison,MET,Missing theoretical comparisons
"While the authors are using an existing model, they shouldn't assume that the reader has read the paper describing that model.",asp_meaningful-comparison,MET,Missing theoretical comparisons
"2:    The authors should compare against several costs/algorithms (e.g. l_0 with OMP, l_1 with LARS, etc.), and across various N_0/sparsity penalties, and across several datasets.",asp_meaningful-comparison,MET_DAT,"2:    The authors should compare against several costs/algorithms (e.g. l_0 with OMP, l_1 with LARS, etc.), and across various N_0/sparsity penalties, and across several datasets."
"Additionally, I believe the experimental tasks are new, and as a result all implementations of competing techniques are by the paper authors. This makes it difficult to have confidence in the higher reported performance of the proposed techniques.",asp_meaningful-comparison,MET_EXP,Lack of comparison with related works
"The theoretical proof depends on the convexity assumption, I would also suggest comparing the proposed attack with CW and other benchmarks on some simple models that satisfy the assumptions.",asp_meaningful-comparison,MET_EXP,Lack of comparison with related works
"In this light, experiments demonstrating comparisons between GANs and VAEs as the reference generative model for explanations would have made the paper stronger (as the proposed approach relies explicitly on how good the generative model is).",asp_meaningful-comparison,MET_EXP,Lack of comparison with related works
"Furthermore, the experimental section does not compare to other forms of hierarchical approaches for MARL, and generally only provides a single comparison to PPO & MADDPG.",asp_meaningful-comparison,MET_EXP,Lack of comparison with related works
"Besides, in the experiments, the proposed method is not compared to other transfer learning methods.",asp_meaningful-comparison,MET_EXP,Lack of comparison with related works
The proposed method is not appropriately compared with the other methods in experiments.,asp_meaningful-comparison,MET_EXP,Lack of comparison with related works
"4) In the experiments, more comparisons with methods in [1] or [2] should be conducted given they are all parallelizing the backpropagation algorithm and achieve speedup in the training.",asp_meaningful-comparison,MET_EXP,Lack of comparison with related works
"Also, to demonstrate the superiority of the proposed method an appropriate comparison against previous work is needed.",asp_meaningful-comparison,MET_RWK,Incorrect baselines used
"Besides, as the base classifier is different for various baselines, it is hard to compare the methods.",asp_meaningful-comparison,MET_RWK,Incorrect baselines used
2) Applying the model of Hartford et al’18 to problems where interacting sets are identical is similar to applying convolution layer to a feature vector that is not equivariant to translation.,asp_meaningful-comparison,MET_RWK,Incorrect baselines used
"- Jointly learning an inference network (Q) has certainly been done before, and I am not sure authors provide an elaborate enough explanation of what is the difference with adversarially learned inference /  adversarial feature learning.",asp_meaningful-comparison,MET_RWK,Incorrect baselines used
"Furthermore, no comparisons were provided to any baselines/alternative methods.",asp_meaningful-comparison,MET_RWK,Incorrect baselines used
- novelty: the additions proposed are small modifications to existing algorithms and there are other methods of attention on graphs which have been discussed in the paper but not directly compared to (e.g. this method builds heavily on Velickovic et al 2017),asp_meaningful-comparison,MET_RWK,Incorrect baselines used
"Furthermore, the baseline models did not use other almost standard regularisation techniques (weight decay, dropout, batch-norm).",asp_meaningful-comparison,MET_RWK,Incorrect baselines used
But there are no comparisons between the proposed training method and previous related works.,asp_meaningful-comparison,MET_RWK,Incorrect baselines used
"The paper currently only mentions the most related work for the proposed method,  using the whole section 2 to describe VCL and use section 3 to describe FSM and half of section 5 to describe SSR.",asp_meaningful-comparison,MET_RWK,Incorrect baselines used
"While the section 4 has a discussion on related papers, there's no systematic experimental comparison across these methods.",asp_meaningful-comparison,MET_RWK,Incorrect baselines used
RNN-based approaches are with better “complexity” comparing to your sum baseline and “Deepset” approach.,asp_meaningful-comparison,MET_RWK,Incorrect baselines used
"- Relatedly, better baselines should be used; for example, if the memory used by the generative model is merely put to storing randomly chosen instances from the tasks, how will the results compare? Clearly storing instances bypasses the forgetting problem completely (as memory size approaches the dataset size it turns into the joint problem) and it's not clear how many instances are really needed per task, especially for these simpler problems.",asp_meaningful-comparison,MET_RWK,Incorrect baselines used
There is no experiment clearly shows convincing evidence of the correctness of the proposed approach or its utility compared to existing approaches (Xie & Ermon (2019); Kool et al. (2019); PlÂšotz & Roth (2018) ).,asp_meaningful-comparison,MET_RWK,Incorrect baselines used
"Thus, I suggest the authors could show the space and time comparisons with the baseline methods to show effectiveness of the proposed method.",asp_meaningful-comparison,MET_RWK,Incorrect baselines used
"Since this work takes the approach of allowing stale weight updates, the author should also compare with existing distributed training approaches that use asynchronous updates, with or without model parallelism, for example, Dean et al., 2012.",asp_meaningful-comparison,MET_RWK,Incorrect baselines used
- No comparison has been made between their approach and other previous approaches.,asp_meaningful-comparison,MET_RWK,Incorrect baselines used
-  The authors should compare to at least some popular previous approaches that use a feature engineering based methodology such as - IntPred,asp_meaningful-comparison,MET_RWK,Incorrect baselines used
"4. Most importantly, for table4, authors are comparing to the ResNet18 ARNet instead of ResNet50? which is better than the proposed method.",asp_meaningful-comparison,MET_TNF,"Perhaps I missed it, but I believe Dan Ciresan's paper ""Multi-Column Deep Neural Networks for Image Classification"" should be cited."
"In addition, I observe that in Table 1, the proposed method does not outperform the Joint Training in SVHN with A_10.",asp_meaningful-comparison,MET_TNF,"Perhaps I missed it, but I believe Dan Ciresan's paper ""Multi-Column Deep Neural Networks for Image Classification"" should be cited."
"2. The human score of 91.4% is based on majority vote, which should be compared with an ensemble of deep learning prediction.",asp_meaningful-comparison,RES,More comparisons needed with variations of the proposed method
I don't think this is really a fair comparison; I would have liked to have seen results for the unmodified reward function.,asp_meaningful-comparison,RES,More comparisons needed with variations of the proposed method
"The authors also rely mostly on the FID metric, but do not show if and how there is improvement upon visual inspection of the generated images (i.e. is resolution improved, is fraction of images that look clearly 'unnatural' reduced etc.)",asp_meaningful-comparison,RES,More comparisons needed with variations of the proposed method
The experimental sections look rather mechanical. I would have put some results on the learned embedding. Or some demonstration of the embedded history or probability to intuitively convey the idea and how it works.,asp_meaningful-comparison,RES_EXP,Missing interpretation of results due to missing related work
"- Experimental results provided in the paper are only qualitative -- as such, I do not find the comparisons (and improvements) over the existing approaches convincing enough.",asp_meaningful-comparison,RES_EXP,Missing interpretation of results due to missing related work
"It's hard for me to judge of the experimental results of section 5.3, given that there are no other",asp_meaningful-comparison,RES_EXP,Missing interpretation of results due to missing related work
I am not sure if this is the first work in this direction of proving bounds assuming an oracle or if there is some background work that the authors could provide to put this into context.,asp_meaningful-comparison,RWK,Missing baselines
Attacking CRBMs is highly relevant and should be included as a baseline.,asp_meaningful-comparison,RWK,Missing baselines
"However, the idea is very related to Yeh et al.’s work which has already published but not mentioned at all.",asp_meaningful-comparison,RWK,Missing baselines
"For the bAbi task, although there is a significant improvement (43%) in the mean error rate compared to the original DNC, it's important to note that performance in this task has improved a lot since the DNC paper was release. Since this is the only non-toy task in the paper, in my opinion, the authors have to discuss current SOTA on it, and have to cite, for example the universal transformer[1], entnet[2], relational nets [3], among others architectures that shown recent advances on this benchmark.",asp_meaningful-comparison,RWK,Missing baselines
"- For semi-supervised classification, the paper did not report the best results in other baselines.",asp_meaningful-comparison,RWK,Missing baselines
"The paper also misses several powerful baselines of semi-supervised learning, e.g. [1,2].",asp_meaningful-comparison,RWK,Missing baselines
"1. The paper should also talk about the details of ARNet and discuss the difference, as I assume they are the most related work",asp_meaningful-comparison,RWK,Missing baselines
1) Some of the benchmark datasets for the proposed task as well as some well-known methods (see Battaglia et al’18 and references in there) are missing.,asp_meaningful-comparison,RWK,Missing baselines
"However, I think since this is few-shot learning with domain adaptation, there is no domain adaptation baselines being mentioned in comparison.",asp_meaningful-comparison,RWK,Missing baselines
"The reviewer is also interested to see how the the generalization bound introduced in this paper is related to the recent theoretical works [3],[4] on MDL.",asp_meaningful-comparison,RWK,Missing baselines
"- The paper should refer to 1) the reward shaping literature, 2) the growing line of works concerned with control variates for REINFORCE (such as VIMCO, MuProp, REBAR) and 3) the growing line of works concerned about combinatorial optimization with reinforcement learning (Neural Combinatorial Optimization with Reinforcement Learning, etc.)",asp_meaningful-comparison,RWK,Missing baselines
"It would probably help to position the VAE component more precisely w.r.t. one of the two baselines, by indicating the differences.",asp_meaningful-comparison,RWK,Missing baselines
"The tasks are explicitly designed to exploit these additional parameters - so framing the synthetic experiments as, ""here are some simple functions for which we would need the additional parameters that we define"" makes sense; but arguing that Hartford et al. ""fail approximating rather simple functions"" (page 7) is misleading because the functions are precisely the functions on which you would expect Hartford et al. to fail (because it's designed for a different setting).",asp_meaningful-comparison,RWK,Missing baselines
There should be a better discussion of related work on the topic.,asp_meaningful-comparison,RWK,Missing baselines
=> Baselines: The comparison provided in the paper is weak.,asp_meaningful-comparison,RWK,Missing baselines
"Moreover, in spite of the authors writing that their goal is “completely different” from [Lee at al 18a, Ma et al 18a], I found the two cited papers having a similar intent and approach to the problem, but a comparison is completely missing.",asp_meaningful-comparison,RWK,Missing baselines
A proper baseline should have been compared.,asp_meaningful-comparison,RWK,Missing baselines
"Third, the comparison to baseline and “DeepSet” is not fair.",asp_meaningful-comparison,RWK,Missing baselines
4. Comparison with past works.,asp_meaningful-comparison,RWK,Missing baselines
1) The only comparison to another non-fixed sampling baseline is Kool et al. 2019.,asp_meaningful-comparison,RWK,Missing baselines
This baseline was also missing in image reconstruction.,asp_meaningful-comparison,RWK,Missing baselines
This seems like a very weak baseline---there are any number of other reasonable ways to encode this.,asp_meaningful-comparison,RWK,Missing baselines
"Why not compare results to WGAN-GP in this case? Since the proposal of GANs, many papers addressed the mode collapse problem.",asp_meaningful-comparison,RWK,Missing baselines
"For e.g. the results on the oscillating behavior of Dirac-GAN are described in related works (e.g. Mescheder et al. 2018), and in practice, WGAN with no regularization is not used (as well as GAN with momentum, as normally beta1=0 in practice).",asp_meaningful-comparison,RWK,Missing baselines
I am also wondering if the comparison with the baselines is fair.,asp_meaningful-comparison,RWK,Missing baselines
"In other words, although in the present results, the proposed NF-SGAN/WGAN outperforms the baseline, the reported performance of the baselines is worse than in related works on CIFAR10.",asp_meaningful-comparison,RWK,Missing baselines
"On a more minor note, the paper [1] seems to report 4.95 accuracy for MixMatch on CIFAR-10 with 4k labels, while in this paper it’s being reported as 6.24.",asp_meaningful-comparison,RWK,Missing baselines
"However, there are papers empirically analyzing novelty detection using generative model -- should analyze or at least cite:",asp_meaningful-comparison,RWK,Missing baselines
Weakness: It would be good to see some comparison to the state of the art,asp_meaningful-comparison,RWK,Missing baselines
The main issue of this paper is the fair comparisons with other works.,asp_meaningful-comparison,RWK,Missing baselines
- Baseline missing: Random actions from expert,asp_meaningful-comparison,RWK,Missing baselines
- Baseline missing: Simple RNN policies that communicate hidden states.,asp_meaningful-comparison,RWK,Missing baselines
Another baseline could be to simply model the imitator and probing policies as RNNs and let them communicate with each other via the hidden states.,asp_meaningful-comparison,RWK,Missing baselines
"- comparison should be made to the linear composition method in the Arora, Liang, Ma ICLR 2017 paper",asp_meaningful-comparison,RWK,Missing baselines
"However, there is no comparison against existing work.",asp_meaningful-comparison,RWK,Missing baselines
"However, memory overhead is still an issue compared to existing method.",asp_meaningful-comparison,RWK,Missing baselines
It is important to place the contributions in this paper in context of these other works.,asp_meaningful-comparison,RWK,Missing baselines
A number of these references are missing and no experimental comparison to these methods has been made.,asp_meaningful-comparison,RWK,Missing baselines
"* In related work, no reference to previous work on ""statistical"" approaches to NN",asp_meaningful-comparison,RWK,Missing baselines
There should be some kind of comparison with test set results from other state-of-the-art work on these datasets.,asp_meaningful-comparison,RWK_DAT,There should be some kind of comparison with test set results from other state-of-the-art work on these datasets.
"However, the experiments conducted generally show that SAVP offers only a trade-off between the visual quality of GANs and the coverage of VAEs, and does not show a clear advantage over current VAE models (Denton & Fergus, 2018) that with simpler architectures obtain similar results.",asp_meaningful-comparison,RWK_EXP,Missing theoretical comparisons
"With regards to the fMRI experiments, good baselines are missing: DEMINE is compared to Pearson correlation.",asp_meaningful-comparison,RWK_EXP,Missing theoretical comparisons
"E.g., in Table 1 and 2,  the best result of VAT (Miyato et al., 2017) is VAT+Ent, 13.15 for CIFAR-10 (4000 labels) and 4.28 for SVHN (1000 labels).",asp_meaningful-comparison,RWK_RES,"E.g., in Table 1 and 2,  the best result of VAT (Miyato et al., 2017) is VAT+Ent, 13.15 for CIFAR-10 (4000 labels) and 4.28 for SVHN (1000 labels)."
"The author never explains. E.g., link to NRMSE and PFC to the Table.",asp_meaningful-comparison,TNF,Missing explanation of comparsion with related work in tables and figures
"4. From Figure 6 and Figure 8-11, it looks like the bandit is more or less on par with fixed exploration policies.",asp_meaningful-comparison,TNF,Missing explanation of comparsion with related work in tables and figures
- I believe one should not compare the distance shown between the left and right columns of Figure 3 as they are obtained from two different models.,asp_meaningful-comparison,TNF,Missing explanation of comparsion with related work in tables and figures
Unfortunately the paper doesn't provide any qualitative analysis on how modulation is employed by the models after training.,asp_motivation-impact,ANA,Unfortunately the paper doesn't provide any qualitative analysis on how modulation is employed by the models after training.
"In terms of applicability, it seems that many cases where discrete latent variables would be really interesting are not covered (e.g. sigmoid belief networks); the paper demonstrates experiments with discrete images (binary or 4-bit) not particularly motivated in my opinion.",asp_motivation-impact,EXP,Missing explanation of utility of proposed method
"However, it looks to me that the authors need to better explain the motivation of DiVA, the differences of DiVA from existing supervised VAE, and the experimental settings, before the acceptance of this paper.",asp_motivation-impact,EXP,Missing explanation of utility of proposed method
"After reading, the reviewer cannot understand why the user should bother to use hyperbolic representations that are more complex to compute in GCNs, given that the experimental results are roughly the same.",asp_motivation-impact,EXP,Missing explanation of utility of proposed method
"If faster training of dictionary learning models was a bottleneck in practical applications, this might be of interest, but it is not.",asp_motivation-impact,EXP,Missing explanation of utility of proposed method
My main concern with the paper is its limited applicability to robotic manipulation tasks with a clear divide between states of interest vs others.,asp_motivation-impact,MET,Limited insights based on design choices
"Overall, I do not feel the comparisons dramatically change the qualitative understanding the field has of the different optimizers and their tunability.",asp_motivation-impact,MET,Limited insights based on design choices
"While the presentation is clear and the evaluation of the model is thorough, I am unsure of the significance of the proposed method.",asp_motivation-impact,MET,Limited insights based on design choices
"That is, authors assumed that they have a degradation function F and all the inference process is just based on this known function.",asp_motivation-impact,MET,Limited insights based on design choices
Thus we may only apply the proposed model on a few tasks with exactly known F.,asp_motivation-impact,MET,Limited insights based on design choices
If the authors don't discuss a motivation then how will a reader know how to apply the tool?,asp_motivation-impact,MET,Limited insights based on design choices
- My biggest concern is that the technical contributions of the paper are not clear at all.,asp_motivation-impact,MET,Limited insights based on design choices
"While, the paper is a plaisant read, I find difficult to access its importance and the applicability of the ideas presented beyond the analogy with the capacity computation. Perhaps other referee will have a clearer opinion.",asp_motivation-impact,MET,Limited insights based on design choices
"Con: not clear to me how strong and wide the implications are, beyond the analogies and the reinterpretation",asp_motivation-impact,MET,Limited insights based on design choices
This very much limits the utility of the method.,asp_motivation-impact,MET,Limited insights based on design choices
"- VAE, GAN: q is the generative model defined as a mapping of a standard multivariate normal distribution by a NN.",asp_motivation-impact,MET,Limited insights based on design choices
- sigmoid belief networks: q is a Bayesian network where each conditional distribution is a logistic regression model.,asp_motivation-impact,MET,Limited insights based on design choices
"Hence, the effectiveness and advantage of the proposed methods are not clear.",asp_motivation-impact,MET,Limited insights based on design choices
I have no idea that what is the propose of defining a new evaluation method and how this new evaluation method helps in the further design of the MaxConfidence method.,asp_motivation-impact,MET,Limited insights based on design choices
I don’t think it is of any practical use to show that a new algorithm (such at DDGD) provide some defence compared to no defence.,asp_motivation-impact,MET,Limited insights based on design choices
(2) The method is not well motivated.,asp_motivation-impact,MET,Limited insights based on design choices
The model is not well motivated and the optimization algorithm is also not well described.,asp_motivation-impact,MET,Limited insights based on design choices
"I like the area of research the authors are looking into and I think it's an important application. However, the paper doesn't answer key questions about both the application and the models:",asp_motivation-impact,MET,Limited insights based on design choices
One drawback is that it is highly specific to language models.,asp_motivation-impact,MET,Limited insights based on design choices
"In addition, there is not much theoretical justification for it, it seems like a one-off trick.",asp_motivation-impact,MET,Limited insights based on design choices
"The main drawback of the paper is that it seems to be more engineering-focused, and doesn’t provide much insight into semi-supervised learning.",asp_motivation-impact,MET,Limited insights based on design choices
The privacy definition employed in this work is problematic.,asp_motivation-impact,MET,Limited insights based on design choices
Why larger reconstruction error achieves stronger privacy protection? I could not find any formal relationship between reconstruction error and privacy.,asp_motivation-impact,MET,Limited insights based on design choices
"(2) The paper does not do a great job of convincing the reader that the problem it is trying to solve is an important matter, or the proposed method is indeed effective in some applications.",asp_motivation-impact,MET,Limited insights based on design choices
"3. Estimating entropies is a standard practice in statistics and machine learning, with an arsenal of estimators; the motivation of the submission is questionable.",asp_motivation-impact,MET,Limited insights based on design choices
"As pointed out in the weakness section, many design choices are not well motivated, and the effects of those designs are not well studied.",asp_motivation-impact,MET,Limited insights based on design choices
"In my opinion, due to how many researchers are and have been looking into improvements of language modeling, the authors may find it hard to break new ground in this direction.",asp_motivation-impact,MET,Limited insights based on design choices
"However there are a few key weaknesses in my view, not least that the practical utility of these metrics is not obvious, since they require supervision in the target domain.",asp_motivation-impact,MET,Limited insights based on design choices
- The envisioned scenario (and hence utility) of these metrics is a bit unclear to me here.,asp_motivation-impact,MET,Limited insights based on design choices
There are also concerns about the motivations behind parts of the technique.,asp_motivation-impact,MET,Limited insights based on design choices
- (W6) Motivation for CFS: I still don't fully understand the need to understand the density of the representation (especially in the manner proposed in the paper). Why is this an important problem? Perhaps expanding on this would be helpful,asp_motivation-impact,MET,Limited insights based on design choices
Although the paper proposes an interesting framework I would argue that it is a “green apple” in the sense that authors need to motivate the approach better and expand the contribution beyond solving a particular instance.,asp_motivation-impact,MET,Limited insights based on design choices
"- limited amount of new insight, which is limiting as new and better understanding of GANs and practical guidelines are arguably the main contribution of a work of this type",asp_motivation-impact,MET,Limited insights based on design choices
* I found it difficult to follow the theoretical motivation for performing the work.,asp_motivation-impact,MET,Limited insights based on design choices
"In terms of method, the guidance for learning Siamese networks are designed heuristically (e.g. edges, colors, etc.) which limits its applicability over various datasets; I think that designing more principled approach to build such guidances from data should be one of the key contributions of the paper.",asp_motivation-impact,MET_DAT,"In terms of method, the guidance for learning Siamese networks are designed heuristically (e.g. edges, colors, etc.) which limits its applicability over various datasets; I think that designing more principled approach to build such guidances from data should be one of the key contributions of the paper."
1: The authors show no benefit of this scheme except perhaps faster convergence.,asp_motivation-impact,MET_RES,"While this is a new and interesting task, the contribution (as discussed above in “pros” above) is somewhat limited."
The method provides impressive compression ratios (in the order of x20-30) but at the cost of a lower performance.,asp_motivation-impact,MET_RES,"While this is a new and interesting task, the contribution (as discussed above in “pros” above) is somewhat limited."
Significance: It is hard to assess given the current submission.,asp_motivation-impact,OAL,"While sensible, this seems to me to be too minor a contribution to stand alone as a paper."
"In sum, the paper has a very good application but not good enough as a research paper.",asp_motivation-impact,OAL,"While sensible, this seems to me to be too minor a contribution to stand alone as a paper."
"Due to several shortcomings of the paper, most important of which is on presentation of the paper, this manuscript requires a significant revision by the authors to reach the necessary standards for publication, moreover it would be helpful to clarify the modeling choices and consequences of these choices more clearly.",asp_motivation-impact,OAL,"While sensible, this seems to me to be too minor a contribution to stand alone as a paper."
"While sensible, this seems to me to be too minor a contribution to stand alone as a paper.",asp_motivation-impact,OAL,"While sensible, this seems to me to be too minor a contribution to stand alone as a paper."
This could have made the paper much stronger.,asp_motivation-impact,OAL,"While sensible, this seems to me to be too minor a contribution to stand alone as a paper."
As such the paper is not convincing.,asp_motivation-impact,OAL,"While sensible, this seems to me to be too minor a contribution to stand alone as a paper."
"Overall, I am not sure what we could gain from this research direction.",asp_motivation-impact,OAL,"While sensible, this seems to me to be too minor a contribution to stand alone as a paper."
1) The motivation is unclear and overall structure of the paper is confusing.,asp_motivation-impact,OAL,"While sensible, this seems to me to be too minor a contribution to stand alone as a paper."
- Contribution overall may be a bit limited,asp_motivation-impact,OAL,"While sensible, this seems to me to be too minor a contribution to stand alone as a paper."
Significance: Below average,asp_motivation-impact,OAL,"While sensible, this seems to me to be too minor a contribution to stand alone as a paper."
"The writing is understandable for the most part, but the paper seems to lack focus - there is no clear take home message.",asp_motivation-impact,OAL,"While sensible, this seems to me to be too minor a contribution to stand alone as a paper."
The authors do not thoroughly explain the motivation of this paper.,asp_motivation-impact,PDI,Limited contribution in problem definition
It is also not clear to me why these problems are important.,asp_motivation-impact,PDI,Limited contribution in problem definition
"-	While this is a new and interesting task, the contribution (as discussed above in “pros” above) is somewhat limited.",asp_motivation-impact,PDI,Limited contribution in problem definition
2. The paper never clearly demonstrates the problem they are trying to solve (nor well differentiates it from the compressed sensing problem  or sample selection problem),asp_motivation-impact,PDI,Limited contribution in problem definition
"The idea is an interesting one, but",asp_motivation-impact,PDI,Limited contribution in problem definition
"Overall, the idea is simple, the explanation is clear and experimentation is extensive. I would like to see more commentary on why this method might have long-term impact (or not).",asp_motivation-impact,PDI_MET,"Overall, the idea is simple, the explanation is clear and experimentation is extensive. I would like to see more commentary on why this method might have long-term impact (or not)."
The results  are overall not very impressive.,asp_motivation-impact,RES,Limited impact of results
The result does not at all apply to all of them.,asp_motivation-impact,RES,Limited impact of results
"Then to state to which of these cases the results of the paper are applicable, allow for an improvement of the variance and at what additional computational cost (considering the cost of evaluating the discrete derivatives).",asp_motivation-impact,RES,Limited impact of results
Generally speaking it seems like a lot of technicalities for a relatively simple result:,asp_motivation-impact,RES,Limited impact of results
"The results of the paper do not give major insights into what are the preferred techniques for training GANs, and certainly not why and under what circumstances they'll work.",asp_motivation-impact,RES,Limited impact of results
"This is not so interesting, even though results are impressive.",asp_motivation-impact,RES,Limited impact of results
"In general, recent work has found that the raw number of parameters has little to do with the size of the model class or the capacity of a model for deep models, and thus work like [A] has been trying to come up with better complexity measures for models to explain generalization.",asp_motivation-impact,RWK,Limited novelty as compared to related work
"In particular it is not applicable to learning e.g. sigmoid belief networks [Neal, 92] (with conditional Bernoulli units) and many other problems.",asp_motivation-impact,RWK,Limited novelty as compared to related work
"I believe that this paper is not the first one to study question generation from logical form (cf. Guo et al, 2018 as cited), so it is unclear what is the contribution of this paper in that respect.",asp_motivation-impact,RWK,Limited novelty as compared to related work
No baseline comparison with GraphNets.,asp_motivation-impact,RWK,Limited novelty as compared to related work
- little methodological innovation or analytical explanations,asp_originality,ANA,- little methodological innovation or analytical explanations
"For example, “Fuzzy Choquet Integration of Deep Convolutional Neural Networks for Remote Sensing” by Derek T. Anderson et al.",asp_originality,BIB,Suggest missing related work
See http://www.stat.cmu.edu/~ryantibs/convexopt-S15/scribes/08-prox-grad-scribed.pdf for a reference.,asp_originality,BIB,Suggest missing related work
"- Yuri Burda, Roger Grosse, Ruslan Salakhutdinov. Importance Weighted Autoencoders. ICLR 2016",asp_originality,BIB,Suggest missing related work
"If the novelty is in applying to continual learning and new datasets, it is not clear that this is sufficient.",asp_originality,DAT,"In terms of applicability, it seems that many cases where discrete latent variables would be really interesting are not covered (e.g. sigmoid belief networks); the paper demonstrates experiments with discrete images (binary or 4-bit) not particularly motivated in my opinion."
"With lack of clear novel insights, or at least more systematic study on additional datasets of the 'winning' techniques and a sensitivity analysis, the paper does not give a valuable enough contribution to the field to merit publication.",asp_originality,DAT,"In terms of applicability, it seems that many cases where discrete latent variables would be really interesting are not covered (e.g. sigmoid belief networks); the paper demonstrates experiments with discrete images (binary or 4-bit) not particularly motivated in my opinion."
"Overall, the method looks incremental and experimental results are mixed on small datasets so I vote for rejection.",asp_originality,DAT_RES,"Overall, the method looks incremental and experimental results are mixed on small datasets so I vote for rejection."
Simply because for continuous variables similar experiments have been reported before,asp_originality,EXP,Not enough novelty in experiments (seems similar to previous work)
"Even though the proposed approach seems to have significant potential, the experimental",asp_originality,EXP,Not enough novelty in experiments (seems similar to previous work)
"Also, I find the experiments done in section 3 and 4 are similar to previous works and even the conclusions are similar.",asp_originality,EXP,Not enough novelty in experiments (seems similar to previous work)
"The experiments currently demonstrate that optimizing both controller and hardware is better than optimizing just the controller, which is not surprising and is a phenomenon which has been previously studied in the literature.",asp_originality,EXP,Not enough novelty in experiments (seems similar to previous work)
"Originality: Given the existing body of literature, I found the technical novelty of this paper rather weak. However, it seems the experiments are thoroughly conducted.",asp_originality,EXP,Not enough novelty in experiments (seems similar to previous work)
"If this all is indeed the case, it is not surprising that the numbers the authors get in the experiments are so similar to WAE-MMD, because CWAE would be exactly WAE-MMD with a specific choice of the kernel.",asp_originality,EXP,Not enough novelty in experiments (seems similar to previous work)
However the theoretical contribution is poor. And the experiment uses a very classical benchmark providing simulated data.,asp_originality,EXP,Not enough novelty in experiments (seems similar to previous work)
2. Although the experiments are detailed and interesting they support poor theoretical developments and use a very classical benchmark,asp_originality,EXP,Not enough novelty in experiments (seems similar to previous work)
"The novelty of the algorithm itself is limited, since GAN and adversarial training are both minmax problems, and the original algorithm can be carried over easily.",asp_originality,MET,Limited novelty in theoretical contribution
1.	Lack of technical novelty.,asp_originality,MET,Limited novelty in theoretical contribution
It seems to me just a combination of several mature techniques.,asp_originality,MET,Limited novelty in theoretical contribution
"- The technical contribution of the proposed method is not high, because the architecture space of neural network is similar to the prior works.",asp_originality,MET,Limited novelty in theoretical contribution
Although this extension seems to be easily derived using the contributions made at point 2.,asp_originality,MET,Limited novelty in theoretical contribution
"Therefore, I cannot find any advantage in the proposed method, compared with these existing MAP based image restoration approaches.",asp_originality,MET,Limited novelty in theoretical contribution
"The negatives are that the paper does not really show this modified DNC can solve a task that the original DNC could not. As the authors also admit, there have been other DNC improvements that have had more dramatic improvements on bAbI.",asp_originality,MET,Limited novelty in theoretical contribution
"However, as the ""selection network"" uses exactly the same input as ""classification network"", it is hard to imagine how it can learn additional information.",asp_originality,MET,Limited novelty in theoretical contribution
"In the current form of evaluation, it is hard to say if there is any benefit of using the ""selection network"" that is the main novelty of the paper.",asp_originality,MET,Limited novelty in theoretical contribution
"First of all, the proposed SST algorithm alone only performs better than baselines in 1 case, equal to them in 1 case and worse in 1 (table 3).",asp_originality,MET,Limited novelty in theoretical contribution
"The combination of ideas is ok, however, it is unclear how novel or how good is the proposed MF training of the RBMs.",asp_originality,MET,Limited novelty in theoretical contribution
"From this viewpoint, the actor-critic component in Dreamer is an incremental contribution.",asp_originality,MET,Limited novelty in theoretical contribution
"It seems that the proposed algorithm is not very original because its two parts, namely prediction (coefficient estimation) and learning (dictionary update) have been widely used in the literature, using respectively a IHT and a gradient descent.",asp_originality,MET,Limited novelty in theoretical contribution
The authors need to describe in detail the algorithmic novelty of their work.,asp_originality,MET,Limited novelty in theoretical contribution
"The technical approach of using a Lagrangian relaxation is the standard way one goes about handling constrained optimization problems, and thus I do not see any novelty there either.",asp_originality,MET,Limited novelty in theoretical contribution
"Therefore, the paper cannot be qualified for ``meta domain adaptation’’ and has very limited novelty in terms of its contribution to meta-learning; however, the combination of domain adaptation and few-shot learning is fair.",asp_originality,MET,Limited novelty in theoretical contribution
"However, my criticisms remain that the paper is a simple combination of cycle GAN and prototypical networks, and lacks new insights/novelty.",asp_originality,MET,Limited novelty in theoretical contribution
- novelty is low: the proposed algorithm is a heuristic similar to previously proposed algorithms in the transfer learning and auxiliary learning space,asp_originality,MET,Limited novelty in theoretical contribution
The only new result seem to be Theorem 4.7 which is a natural extension to theorem 4.3 to zeroth-order methods.,asp_originality,MET,Limited novelty in theoretical contribution
There exists more principled approaches for selecting out-distribution images that has not considered here like those based on uncertainty estimation or recently proposed direct out-distribution detectors.,asp_originality,MET,Limited novelty in theoretical contribution
1. The work has limited novelty: the learning of the world model (recurrent state-space model) closely follows the prior work of PlaNet.,asp_originality,MET,Limited novelty in theoretical contribution
4. The novelty of the model is relatively limited as it is a combination of previous techniques on a new problem.,asp_originality,MET,Limited novelty in theoretical contribution
I don't think Cakewalk is different enough from the cross-entropy method to warrant acceptance in ICLR.,asp_originality,MET,Limited novelty in theoretical contribution
"While the model seems to perform well, the originality and the improvement w.r.t. baselines are somewhat limited.",asp_originality,MET,Limited novelty in theoretical contribution
"Though the Bayesian inference using GAN is a natural idea, learning algorithms proposed in this paper are simple and are not intensively developed.",asp_originality,MET,Limited novelty in theoretical contribution
The reviewer votes for rejection as the method has limited novelty.,asp_originality,MET,Limited novelty in theoretical contribution
The novelty of this method is minimal.,asp_originality,MET,Limited novelty in theoretical contribution
The theoretical contribution is very limited.,asp_originality,MET,Limited novelty in theoretical contribution
"The main theorem of this paper is an extension of existing methods, so the novelty of theoretical analysis is somewhat limited.",asp_originality,MET,Limited novelty in theoretical contribution
"Overall, I like the approach of the proposed method, especially tuning coefficient during training procedure although novelty in the theoretical analysis is somewhat limited.",asp_originality,MET,Limited novelty in theoretical contribution
"However, the fact that models with less parameters perform better than BERT-based models in the low-resource case is not very surprising.",asp_originality,MET,Limited novelty in theoretical contribution
"It is also not clear how the loss function proposed differs from that of the CDVAE, etc.",asp_originality,MET,Limited novelty in theoretical contribution
"Given that, the novelty of the paper is fairly incremental as it uses NerveNet to evaluate fitness and ES for the main design search.",asp_originality,MET,Limited novelty in theoretical contribution
- Incremental modeling contribution,asp_originality,MET,Limited novelty in theoretical contribution
"While I found the derivation of the Cramer-Wold distance interesting, the final form of this distance (Eq. 2), to me, looks very similar to the MMD with a particular kernel.",asp_originality,MET,Limited novelty in theoretical contribution
"However, I believe this is also the case in the MMD, since if one of the distributions is Gaussian or analytically known, then E[k(x,x')] in the MMD can be analytically computed.",asp_originality,MET,Limited novelty in theoretical contribution
- That learning simple functions and composing them to compute more complex functions would be more data efficient than directly learning the complex functions does not seem very surprising.,asp_originality,MET,Limited novelty in theoretical contribution
"After all, the former approach gets a lot more knowledge about the target function built into it.",asp_originality,MET,Limited novelty in theoretical contribution
Is it a combination of DGR and HAT with some capacity expansion?,asp_originality,MET,Limited novelty in theoretical contribution
The proposed model seems like a straightforward extension of the nCRP with a deep model hanging off the end of it.,asp_originality,MET,Limited novelty in theoretical contribution
"The originality is relative low though, since it is mainly an application of  deep InfoMax to language modeling, not inventing a new algorithm and applying to language modeling.",asp_originality,MET,Limited novelty in theoretical contribution
The remaining components of the proposed method are not very new.,asp_originality,MET,Limited novelty in theoretical contribution
"Overall, I think the novelty of the paper is very limited, as all the weight distortion algorithms in the paper can be formulated as the proximal function in proximal gradient descent.",asp_originality,MET,Limited novelty in theoretical contribution
"In this way, we can easily see that the proposed framework is a stochastic version of proximal gradient descent.",asp_originality,MET,Limited novelty in theoretical contribution
"In summary, I find there is no novelty involved apart from combining the already existing SOTA model in disentangled feature learning (beta-VAE) and image generation (StackGAN).",asp_originality,MET,Limited novelty in theoretical contribution
"(1) My main problem with this paper is that the novel objective proposed by the authors in Eq. 7 is equivalent to the objective of WAEs appearing in Eq. 4 of [1] (up to a heuristic of applying logarithm to the divergence measure, which is not justified but meant to ""improve the balance between two terms"", see footnote 2), where the authors use the newly introduced Cramer-Wold divergence as a choice of the penalty term in Eq. 4 of [1].",asp_originality,MET,Limited novelty in theoretical contribution
"(2) When viewed in this way, CW-distance introduced in Eq (2) closely resembles the unbiased U-statistic estimate of the MMD used in WAE-MMD [1, Algorithm 2].",asp_originality,MET,Limited novelty in theoretical contribution
"-The proposed technique does not seem to be original enough, and it does not handle the problem foundationally well.",asp_originality,MET,Limited novelty in theoretical contribution
The proposed technique is heavily dependent on GBDT (Indeed the algorithm and the learned trees are used at least three times).,asp_originality,MET,Limited novelty in theoretical contribution
"However, I am unable to assess the technical novelty of this work as it seems to heavily rely on prior work which in turn use techniques from random matrix theory.",asp_originality,MET,Limited novelty in theoretical contribution
1) the proof techniques are very standard,asp_originality,MET,Limited novelty in theoretical contribution
"2. I am not convinced this method is sufficiently new, given that there are other methods that try to directly reward visiting new states.",asp_originality,MET,Limited novelty in theoretical contribution
"From this perspective, the paper seems like an application of existing tools (such as CNN, graph convolutional network and binary classification).",asp_originality,MET,Limited novelty in theoretical contribution
"It is unclear, why one should use the proposed duality gap GAN.",asp_originality,MET,Limited novelty in theoretical contribution
"First, the novelty of the approach is limited -- the approach amounts to using a sparse integer layer instead of a floating-point layer within a feed-forward architecture.",asp_originality,MET,Limited novelty in theoretical contribution
1. The regularization techniques in reproducing kernel Hilbert space (RKHS) has,asp_originality,MET,Limited novelty in theoretical contribution
(1) using codes and codebooks to compress weights; and,asp_originality,MET,Limited novelty in theoretical contribution
"Clarification of the connections/differences, and comparison with these related methods should be made to show the efficacy of the proposed method.",asp_originality,MET,Limited novelty in theoretical contribution
"First, the technical contribution is lean. Neither the multi-agent learning or the hierarchical learning of the algorithm is novel.",asp_originality,MET,Limited novelty in theoretical contribution
The combination of these two methods seems straightforward.,asp_originality,MET,Limited novelty in theoretical contribution
"The latter model is similar to RAND-WALK, so it is not surprising that statistical functions there are similarly concentrated.",asp_originality,MET,Limited novelty in theoretical contribution
"- The final method is a mixup of many different techniques, thus, not a strong contribution, but many smaller contributions.",asp_originality,MET,Limited novelty in theoretical contribution
"that (except the minor small section of streaming data), the paper is more like a proper verification of how tree-based learning algorithms work very well in tabular data--which is far from the basis of the paper and does not make the paper novel enough for ICLR.",asp_originality,MET_DAT,"that (except the minor small section of streaming data), the paper is more like a proper verification of how tree-based learning algorithms work very well in tabular data--which is far from the basis of the paper and does not make the paper novel enough for ICLR."
"In the experiments, both the setting and the experimental results show that the proposed W_s will be very close to W_U. As a result, the improvement caused by the proposed method is incremental compared with its variants.",asp_originality,MET_EXP,"In the experiments, both the setting and the experimental results show that the proposed W_s will be very close to W_U. As a result, the improvement caused by the proposed method is incremental compared with its variants."
"Specifically, the policy update in Dreamer resembles that of SVG (Heess et al., 2015), which also backpropagates re-parameterized gradients through a value function and a transition model.",asp_originality,MET_RWK,"Not much novelty in methodology (previous works already studied these methods, similar motiation of methodology)"
"Since the latent models are learned based on existing techniques, the paper presents an incremental contribution.",asp_originality,MET_RWK,"Not much novelty in methodology (previous works already studied these methods, similar motiation of methodology)"
"Though, I still think the contribution is incremental, since back-propagating gradients through values and dynamics has been studied in prior works (albeit with less empirical successes compared to Dreamer).",asp_originality,MET_RWK,"Not much novelty in methodology (previous works already studied these methods, similar motiation of methodology)"
"The proposed approach is very similar to the CE method by Rubinstein (as stated by the authors in the related work section), limiting the contributions of this paper.",asp_originality,MET_RWK,"Not much novelty in methodology (previous works already studied these methods, similar motiation of methodology)"
"[-] It seems to me the motivation is similar to that of Sparsely-Gated MoE (Shazeer et al. 2017), but it is not clear how the proposed two-hierarchy method is superior to the Sparsely-Gated MoE. It would be helpful the paper discuss more about this.",asp_originality,MET_RWK,"Not much novelty in methodology (previous works already studied these methods, similar motiation of methodology)"
"The proposed method adapt a previously presented hierarchical clustering method in the ""standard space"" (Griffiths et al., 2004) to an embedding space defined by a variational autoencoder model.",asp_originality,MET_RWK,"Not much novelty in methodology (previous works already studied these methods, similar motiation of methodology)"
"The inference algorithm builds on standard techniques of deep generative models and, also, on previously proposed methods (Wand and Blei, 2003) for dealing with the complex hierarchical priors involved in this kind of models.",asp_originality,MET_RWK,"Not much novelty in methodology (previous works already studied these methods, similar motiation of methodology)"
"Further the contributions are not clear at all, since a large part of the detailed approach/equations relate to the masking which was taken from previous work.",asp_originality,MET_RWK,"Not much novelty in methodology (previous works already studied these methods, similar motiation of methodology)"
"Furthermore, there are several similar ideas already published, so comparison against those (for example by J. Peng, N. Heess or J. Mere) either as argument or even better as experiment, would be helpful to evaluate the quality of the proposed hierarchy.",asp_originality,MET_RWK,"Not much novelty in methodology (previous works already studied these methods, similar motiation of methodology)"
The proposed method PowerSGD is an extension of the method in Yuan et al. (extended to handle stochastic gradient and momentum).,asp_originality,MET_RWK,"Not much novelty in methodology (previous works already studied these methods, similar motiation of methodology)"
"A weighted k-means solver is also used in [2], though the ""weighted"" in [2] comes from second-order information instead of minimizing reconstruction error.",asp_originality,MET_RWK,"Not much novelty in methodology (previous works already studied these methods, similar motiation of methodology)"
"Once a low-level walking controller is trained, the high-level multi-agent navigation control is not much different from simple environments, e.g. point mass control, used in the previous works.",asp_originality,MET_RWK,"Not much novelty in methodology (previous works already studied these methods, similar motiation of methodology)"
"3. The authors argue in their rebuttal that ""the grid"" is a novel idea that warrants investigation, but remark in figure 5 that likely it isn't the key aspect of their algorithm.",asp_originality,MET_TNF,"3. The authors argue in their rebuttal that ""the grid"" is a novel idea that warrants investigation, but remark in figure 5 that likely it isn't the key aspect of their algorithm."
"However, the paper contains only little novelty and does not provide sufficiently new scientific insights.",asp_originality,OAL,"Overall not original enough (primary claim, limited evaluation, limited novelty)"
Originality: The work is moderately original.,asp_originality,OAL,"Overall not original enough (primary claim, limited evaluation, limited novelty)"
But I'm concerned with the novelty and contributions of this paper.,asp_originality,OAL,"Overall not original enough (primary claim, limited evaluation, limited novelty)"
"In summary, the quality of the paper is poor and the originality of the work is low.",asp_originality,OAL,"Overall not original enough (primary claim, limited evaluation, limited novelty)"
"Overall, this paper is good, but is not novel or important enough for acceptance.",asp_originality,OAL,"Overall not original enough (primary claim, limited evaluation, limited novelty)"
"Overall, I think this paper is not good enough for an ICLR paper and the presentation is confusing in both its contributions and its technical novelty.",asp_originality,OAL,"Overall not original enough (primary claim, limited evaluation, limited novelty)"
I believe the primary claim of this paper is neither surprising nor novel.,asp_originality,OAL,"Overall not original enough (primary claim, limited evaluation, limited novelty)"
"Assessment: Overall, this is a borderline paper, as the task is interesting and novel, but the presentation is lacking in technical detail and there is a lack of novelty on the modeling side.",asp_originality,OAL,"Overall not original enough (primary claim, limited evaluation, limited novelty)"
"The only issue with this paper is its degree of novelty, which is narrow.",asp_originality,OAL,"Overall not original enough (primary claim, limited evaluation, limited novelty)"
"Hence, I am not very sure whether the novelty of the paper is significant.",asp_originality,OAL,"Overall not original enough (primary claim, limited evaluation, limited novelty)"
"Overall, this appears to be a board-line paper with weak novelty.",asp_originality,OAL,"Overall not original enough (primary claim, limited evaluation, limited novelty)"
"This was a fun, albeit incremental paper.",asp_originality,OAL,"Overall not original enough (primary claim, limited evaluation, limited novelty)"
"The paper is overall well written and intuitive but limited in evaluation and novelty (see e.g. [1,2] ) with only limited modifications (sharing low-level controller) for the multi agent case.",asp_originality,OAL,"Overall not original enough (primary claim, limited evaluation, limited novelty)"
My main concern comes from the novelty of this paper.,asp_originality,OAL,"Overall not original enough (primary claim, limited evaluation, limited novelty)"
I do not see much insight into the problem.,asp_originality,PDI,"The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper."
The main problem with this paper is that it is difficult to identify its main and novel contributions.,asp_originality,PDI,"The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper."
"Summarizing, the paper addresses a relevant problem but they do not state which their main contributions are, and reintroduce some ideas previously published in the literature.",asp_originality,PDI,"The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper."
"At the conceptual level, the idea of jointly modeling local video events is not novel, and can date back to at least ten years ago in the paper “Learning realistic human actions from movies”, where the temporal pyramid matching was combined with the bag-of-visual-words framework to capture long-term temporal structure.",asp_originality,PDI,"The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper."
The idea of having a separate class for out-distribution is a very interesting idea but unfortunately previously explored.,asp_originality,PDI,"The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper."
"1) Although the ensemble idea is new, the idea of selective self-training is not novel in self-training or co-training of SSL as in the following survey.",asp_originality,PDI,"The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper."
The idea that introduces labels in VAE is not novel.,asp_originality,PDI,"The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper."
"At a high level, the idea of imposing a mixture of gaussian structure in the feature space of a deep neural network classifier is not new.",asp_originality,PDI,"The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper."
The idea to extend the use of VAE-GANs to the video prediction setting is a pretty natural one and not especially novel.,asp_originality,PDI,"The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper."
- The idea is a simple extension of existing work.,asp_originality,PDI,"The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper."
"Although the idea seems to be interesting, the paper seems to be a bit incremental and is a simple application of existing GAN techniques.",asp_originality,PDI,"The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper."
"The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper.",asp_originality,PDI,"The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper."
"ii) Although the idea of generating contrastive explanations is quite interesting, it is not that novel.",asp_originality,PDI,"The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper."
"Last comment is that, although the concept of `deficiency` in a bottleneck setting is novel, the similar idea for tighter bound of log likelihood has already been pursed in the following paper:",asp_originality,PDI,"The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper."
* the idea of smoothing gradients is not new,asp_originality,PDI,"The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper."
- the idea is trivial and simple. I don't think there's significant novelty here: all the components are existing and combining them seems very trivial to me.,asp_originality,PDI,"The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper."
The idea of using a constrained formulation is not novel either (constrained MDPs have been thoroughly studied since Altman (1999)).,asp_originality,PDI_RWK,The idea of using a constrained formulation is not novel either (constrained MDPs have been thoroughly studied since Altman (1999)).
Although some promising,asp_originality,RES,Not enough originality in results (not surprising)
"Hence, I kindly do not think the outcome is truly a research result.",asp_originality,RES,Not enough originality in results (not surprising)
"As the authors admit, the main result is not especially surprising.",asp_originality,RES,Not enough originality in results (not surprising)
"The results are not strong. And, unfortunately, the model contribution currently is too modest.",asp_originality,RES,Not enough originality in results (not surprising)
"I find the observations interesting, but the contribution is empirical and not entirely new. It would be nice if there were some theoretical results to back up the observations.",asp_originality,RES,Not enough originality in results (not surprising)
"Although I personally enjoyed reading the results that from control theory perspective are inline with GAN literature, the paper does not provide novel surprising results.",asp_originality,RES,Not enough originality in results (not surprising)
"However, this limits the novelty of the results relative to existing literature.",asp_originality,RES,Not enough originality in results (not surprising)
"2) although there must be some small innovations, I thought that all the results had more or less been proven by Dupuis and co-authors:",asp_originality,RES,Not enough originality in results (not surprising)
It was kind of surprising that the authors did not cite this paper given the results are pretty much the same.,asp_originality,RES,Not enough originality in results (not surprising)
"A lot of the times, the result seem to be a derivative of the work by Bietti and Mairal, and looks like the main results in this paper are intertwined with stuff B+M already showed in their paper.",asp_originality,RES,Not enough originality in results (not surprising)
"- Page 7 offers a result (expression for the Dirichlet form), which is hardly more than an exercise for anybody familiar with Markov Chains theory.",asp_originality,RES,Not enough originality in results (not surprising)
"Again, this follows from known results.",asp_originality,RES,Not enough originality in results (not surprising)
Many of the results have been already presented in,asp_originality,RES,Not enough originality in results (not surprising)
"While I wish there were more such studies -- as I believe reproducing past results experimentally is important, and so is providing practical advice for practitioners -- this work in many parts hard to follow, and it is hard to get lot of new insight from the results, or a better understanding of GANs.",asp_originality,RES,Not enough originality in results (not surprising)
"- Gradient starvation, Kaggle experiment: I'm not too convinced about the novelty/usefulness of this result. In the end, even a decision tree stump would stop growing after learning the dark/light feature as a discriminator.",asp_originality,RES_EXP,"- Gradient starvation, Kaggle experiment: I'm not too convinced about the novelty/usefulness of this result. In the end, even a decision tree stump would stop growing after learning the dark/light feature as a discriminator."
"A stronger contribution is 1., which however is somewhat incremental compared to similar comparisons made in the past.",asp_originality,RWK,Incremental novelty of method as compared to related work
"3. Technical contribution: While the authors propose the first bounds for LSTMs and MGUs, most of the analysis seems to be a marginal contribution over the work of Bartlett et al. [3]",asp_originality,RWK,Incremental novelty of method as compared to related work
"However, this task is an instance of natural language generation: given a meaning representation (quite often a database record), generate the natural language text correspoding to it. And previous work on this topic has proposed very similar ideas to the scratchpad proposed here in order to keep track of what the neural decoder has already generated, here are two of them:",asp_originality,RWK,Incremental novelty of method as compared to related work
3) The novelty of this paper is incremental as the theoretical results are extended from Cortes et al (2019) and Zhao et al (2018).,asp_originality,RWK,Incremental novelty of method as compared to related work
"- The paper does not have a significant novel contribution, but rather extends GANs (improved-GAN mostly) with a manifold regularization, which has been explored in many other works Kumar et al. (2017) and Qi et al. (2018).",asp_originality,RWK,Incremental novelty of method as compared to related work
"Compressability is evaluated, but that was already present in the previous work.",asp_originality,RWK,Incremental novelty of method as compared to related work
"For example, the two regimes mentioned in the paper has been identified by a few other works and the contribution of this paper is just to verify them again.",asp_originality,RWK,Incremental novelty of method as compared to related work
"In terms of learning rate scaling, this paper gets similar conclusions as Shallue et al. (2018).",asp_originality,RWK,Incremental novelty of method as compared to related work
"In terms of the difference between vanilla SGD and SGD with momentum, Zhang et al. (2019) already argued that the difference depends on specific batch sizes and SGD with momentum only outperforms SGD in the curvature dominated regime.",asp_originality,RWK,Incremental novelty of method as compared to related work
"However, the novelty is rather limited as similar ideas have been undertaken (e.g., Mescheder et al 2018), but in different contexts.",asp_originality,RWK,Incremental novelty of method as compared to related work
The testbeds all existed previously and this is mostly the effort of pulling then together.,asp_originality,RWK,Incremental novelty of method as compared to related work
"Beyond the core findings, the other settings are less convincingly supported by seem more like work in progress and this paper is really just a scaling-up of [Pathak, et al., ICML17] without generating any strong results regarding questions around representation, what to do about stochasticity (although the discussion regarding something like ‘curiosity honeypots’ is interesting).",asp_originality,RWK,Incremental novelty of method as compared to related work
All of the testbeds have been used previously.,asp_originality,RWK,Incremental novelty of method as compared to related work
Spectrum pooling has been used in the community of computer vision and machine learning.,asp_originality,RWK,Incremental novelty of method as compared to related work
"Taking a random example (there are others by simple searching), in the ECCV paper ""DFT-based Transformation Invariant Pooling Layer for Visual Classification, Ryu et al., 2018"" The DFT magnitude pooling is almost the same as the authors' propositions, where the ""Fourier coefficients are cropped by cutting off high-frequency components"".",asp_originality,RWK,Incremental novelty of method as compared to related work
The work is rather incremental from current state-of-the-art methods.,asp_originality,RWK,Incremental novelty of method as compared to related work
"For example, Narayanaswamy et al. [1] also propose to utilize labels to VAE.",asp_originality,RWK,Incremental novelty of method as compared to related work
"However, there are a few, though not many, works in the literature trying to combine Choquet integral with deep learning.",asp_originality,RWK,Incremental novelty of method as compared to related work
This paper fails to account for a vast amount of literature on modeling natural images that predates the post-AlexNet deep-learning era.,asp_originality,RWK,Incremental novelty of method as compared to related work
These regimes are fairly well covered by previous works (e.g. Belkin et al as well as others).,asp_originality,RWK,Incremental novelty of method as compared to related work
"1) The introduction makes it seem the generative replay is new, without citing approaches such as DGR (which are cited in the related work).",asp_originality,RWK,Incremental novelty of method as compared to related work
"I must probably be missing something, and I encourage the authors to clarify what the main novelties are when compared to the several papers by Dupuis & al.",asp_originality,RWK,Incremental novelty of method as compared to related work
But this is too similar to the cited findings on page 6 (models assign high likelihood to constant images).,asp_originality,RWK,Incremental novelty of method as compared to related work
-- Since CounMin is closely related to Bloom filters the idea of using machine learning to speed it up appears to be noticeably less novel given that for Bloom filters this has already been done by Mitzenmacher’18.,asp_originality,RWK,Incremental novelty of method as compared to related work
"- The experiments seem very similar to Wu et al. 2018, which is considered to be prior work under the ICLR guidelines.",asp_originality,RWK,Incremental novelty of method as compared to related work
"While this paper is acknowledged in the related work, it would be helpful to expand further on the relationship between these works, so the originality and contribution of this paper can be better evaluated.",asp_originality,RWK,Incremental novelty of method as compared to related work
"It's hard to ascertain what exactly the contributions are, and how they might not be a straightforward consequence of prior work (for example, combining results from Bietti and Mairal; and generalization bounds for linear models).",asp_originality,RWK,Incremental novelty of method as compared to related work
"For instance, using codes and codebooks to compress the weights has already been used in [1,2].",asp_originality,RWK,Incremental novelty of method as compared to related work
"In addition, minimizing reconstruction error has already been used in low-rank approximation[3] and network pruning[4].",asp_originality,RWK,Incremental novelty of method as compared to related work
It is thus very hard to know if this new approach brings any improvement to previous work.,asp_originality,RWK,Incremental novelty of method as compared to related work
"2. In experiments, the authors explored many existing methods on improving",asp_originality,RWK_EXP,"2. In experiments, the authors explored many existing methods on improving"
"-The conclusions of the study were suggested by previous papers or are rather expected: adversarial transfer is not symmetric: Deep models less transferable than shallow ones, averaging gradient is better",asp_originality,RWK_RES,"-The conclusions of the study were suggested by previous papers or are rather expected: adversarial transfer is not symmetric: Deep models less transferable than shallow ones, averaging gradient is better"
"When it comes to experiments, constant epoch budget is also fairly well understood and the behavior in Figure 1 is not really surprising (as the eventual training performance gets worse with large batches).",asp_originality,TNF,"When it comes to experiments, constant epoch budget is also fairly well understood and the behavior in Figure 1 is not really surprising (as the eventual training performance gets worse with large batches)."
- Limited analysis of model/architecture design choices,asp_replicability,ANA,Reproducibility of result
"However, I believe the assumptions needed to show this point force the analysis to only characterize learning close to convergence.",asp_replicability,ANA,Reproducibility of result
"Because, the results are only shown on one dataset, it is harder to see how one might extend this work to other form of questions on slightly harder datasets.",asp_replicability,DAT,Lacking details on datasets
"I also wonder if the video data will be released, which could be important for the following comparisons.",asp_replicability,DAT,Lacking details on datasets
"4. How large is the training set of (T, P) pairs? I don't think this is mentioned in the paper.",asp_replicability,DAT,Lacking details on datasets
It is also not clear to me why CIFAR datasets involve two domains and how these domains are relevant in each of the tasks.,asp_replicability,DAT,Lacking details on datasets
"It is necessary to test it on datasets with much more fine classes and much-complicated hierarchy, e.g., ImageNet, MS COCO or their subsets, which have ideal class hierarchy structures.",asp_replicability,DAT,Lacking details on datasets
- Lack of sufficient technical detail on models and dataset,asp_replicability,DAT,Lacking details on datasets
The IDF scores would be stronger if they were computed on a bigger in-domain corpus than the gold test set.,asp_replicability,DAT,Lacking details on datasets
"My concern is, given this is an empirical work,  the number of datasets used in evolution is a bit small.",asp_replicability,DAT,Lacking details on datasets
- Please comment on the extra computation required for obtaining image data for MT sentences and for learning image representations.,asp_replicability,DAT,Lacking details on datasets
"Although I really appreciate the authors' efforts on providing implementational details in the appendix, the code and data do not seem to be publicly available, and I'm expecting that the implementation of this technique is relatively hard due to their complex designs of the generator and discriminator.",asp_replicability,DAT,Lacking details on datasets
While reviewing this paper I went back and read the EN-DE evaluation data for the last few years trying to see how often I could reason that images would help and I came up severely lacking.,asp_replicability,DAT,Lacking details on datasets
It's then hard for me to square that with the +VR gains seen throughout this work on non-grounded datasets.,asp_replicability,DAT,Lacking details on datasets
"- Given the small size of the dataset, I would propose experimenting with non-neural approaches as well, which are also quite common in NLG.",asp_replicability,DAT_EXP,Not enough info on dataset and hyper-parameters
"- experiments are performed using a synthetic setup on a single data set, so it remains unclear if the algorithm would be successful in a real life scenario",asp_replicability,DAT_EXP,Not enough info on dataset and hyper-parameters
"The experimental settings, e.g. how the train:test datasets are split, and hyperparameter settings, are not clearly given.",asp_replicability,DAT_EXP,Not enough info on dataset and hyper-parameters
"Moreover, I  don't think that the data sets in experiments are good enough to cover the importance and the nature of the problem.",asp_replicability,DAT_EXP,Not enough info on dataset and hyper-parameters
- The hyperparameter selection regime (and the experiments used to find them) is not described,asp_replicability,EXP,"Lack of experimental details (no of images, sampling criteria, etc)"
"2. during sampling, either training or testing, how do authors handle temporal overlap or make it overlap?",asp_replicability,EXP,"Lack of experimental details (no of images, sampling criteria, etc)"
3) The paper only conducts comparison experiments with fixed-alpha baselines.,asp_replicability,EXP,"Lack of experimental details (no of images, sampling criteria, etc)"
"Without an empirical example in a realistic setting, it is hard to judge whether the benefits of introducing an ME bias for better classification of new inputs belonging to new classes can outweigh the negatives via a potential increase in misclassification of those belonging to old classes.",asp_replicability,EXP,"Lack of experimental details (no of images, sampling criteria, etc)"
4. Can the authors show concrete examples on how the attacks are generated?,asp_replicability,EXP,"Lack of experimental details (no of images, sampling criteria, etc)"
"- for the experiment with Imagenet images, it is not very clear how many pictures are used. Is this number 2500?",asp_replicability,EXP,"Lack of experimental details (no of images, sampling criteria, etc)"
However it's not clear how is this range used in practice ? Do you sample uniformly $\alpha$ in this range to train the linear interpolation ?,asp_replicability,EXP,"Lack of experimental details (no of images, sampling criteria, etc)"
"3. Shouldn't random indexing produce non-uniform numbers of non-zero entries depending on alpha? Why did you have an exact number of non-zero entries, s, in the experiments?",asp_replicability,EXP,"Lack of experimental details (no of images, sampling criteria, etc)"
It would be nice if the authors pointed to a git repository with their code an experiments.,asp_replicability,EXP,"Lack of experimental details (no of images, sampling criteria, etc)"
2)	It is not clear what the “replicates” refer to in the experiments.,asp_replicability,EXP,"Lack of experimental details (no of images, sampling criteria, etc)"
"Given this, I wonder why all experiments are conducted on sets of two to five images, even for Kitti where standard evaluation protocols would demand predicting entire sequences.",asp_replicability,EXP,"Lack of experimental details (no of images, sampling criteria, etc)"
I think all claims about running time should be corroborated by controlled experiments.,asp_replicability,EXP,"Lack of experimental details (no of images, sampling criteria, etc)"
"-In the introduction, ""it is in general impossible to find an embedding in R^d such that ..."", why do we have to make v and v'(and u, and u') far from each other?",asp_replicability,INT,"-In the introduction, ""it is in general impossible to find an embedding in R^d such that ..."", why do we have to make v and v'(and u, and u') far from each other?"
"-- While the overall claim of the paper in the introduction seems to be to speed up frequency estimation using machine learned advice, results are only given for the Zipfian distribution.",asp_replicability,INT_RES,"-- While the overall claim of the paper in the introduction seems to be to speed up frequency estimation using machine learned advice, results are only given for the Zipfian distribution."
How do we take a limit of M -> ∞ ? Does k also go ∞?,asp_replicability,MET,"Missing details on methodology (eg., use of notation, use on tasks, etc)"
"Cons: While doing this leads to better convergence, each update is still very expensive compared to standard SGD, and for instance on vision tasks the algorithm needs to run for almost double the time to get similar accuracies as an SGD, adam solver.",asp_replicability,MET,"Missing details on methodology (eg., use of notation, use on tasks, etc)"
"Since q_rel require one hot vector as input, how to sample the q_rel given the importance score and how backprob the gradient in this case?",asp_replicability,MET,"Missing details on methodology (eg., use of notation, use on tasks, etc)"
"- In Equation (6), the posterior distribution should be P(X|G) since X is the latent variable to be inferred, right",asp_replicability,MET,"Missing details on methodology (eg., use of notation, use on tasks, etc)"
"How are the lambda and threshold parameters tuned? The authors mention a validation set, are they just exhaustively explored on a 3D grid on the validation set?",asp_replicability,MET,"Missing details on methodology (eg., use of notation, use on tasks, etc)"
How does the proposed method perform in more complicated tasks such as,asp_replicability,MET,"Missing details on methodology (eg., use of notation, use on tasks, etc)"
Dual-1 and Dual-5 are introduced without explanation.,asp_replicability,MET,"Missing details on methodology (eg., use of notation, use on tasks, etc)"
"However, their algorithm--while much less computationally expensive than true full-matrix adaptive preconditioning---is still far more expensive than the usual diagonal version.",asp_replicability,MET,"Missing details on methodology (eg., use of notation, use on tasks, etc)"
It is unclear whether the data augmentation techniques is applied only at training time or also at test time.,asp_replicability,MET,"Missing details on methodology (eg., use of notation, use on tasks, etc)"
"Since each domain may have different number of classes, it is not clear how the number of classes (L) is set in the classification module (maximum number of classes in all domain?).",asp_replicability,MET,"Missing details on methodology (eg., use of notation, use on tasks, etc)"
"- the problem assumptions are too simplistic and unrealistic (feature distributions of target and auxiliary data are identical), so it is questionable if the proposed algorithm has practical importance",asp_replicability,MET,"Missing details on methodology (eg., use of notation, use on tasks, etc)"
- it wasn't clear how the sparsity percentage on page 3 was defined?,asp_replicability,MET,"Missing details on methodology (eg., use of notation, use on tasks, etc)"
- Architecture choice unclear: Why are $\sigma$ and $\omega$ separate networks.,asp_replicability,MET,"Missing details on methodology (eg., use of notation, use on tasks, etc)"
"- page 4, Sect. 4.4: Architecture of $\alpha$ would be nice (more than a linear layer?)",asp_replicability,MET,"Missing details on methodology (eg., use of notation, use on tasks, etc)"
The paper used very restricted Gaussian distributions for the formulation.,asp_replicability,MET,"Missing details on methodology (eg., use of notation, use on tasks, etc)"
"The problem of image classification is considered only, while authors claimed the method can be easily applied to other problems as well.",asp_replicability,MET,"Missing details on methodology (eg., use of notation, use on tasks, etc)"
"Then, there will be another question: how the two networks are trained? Are they trained separately or jointly?",asp_replicability,MET,"Missing details on methodology (eg., use of notation, use on tasks, etc)"
"First of all, the setup for the AE and VAE is not specified.",asp_replicability,MET,"Missing details on methodology (eg., use of notation, use on tasks, etc)"
3. Scenario discussed in Sec. 4 seems somewhat impractical.,asp_replicability,MET,"Missing details on methodology (eg., use of notation, use on tasks, etc)"
"In addition, the paper would also need to show that such a model does not generalize to a validation set of images.",asp_replicability,MET,"Missing details on methodology (eg., use of notation, use on tasks, etc)"
"- in section 2.2, please explain more how gradients w.r.t hyper-parameters are computed.",asp_replicability,MET,"Missing details on methodology (eg., use of notation, use on tasks, etc)"
"- how to tune lambda? it is an important hyper-parameter, but it is set without a good principle, e.g., ""For SGD-APO, we used lambda = 0.001, while for SGDm-APO, we used lambda = 0.01"", ""while for RMSprop-APO, the best lambda was 0.0001",asp_replicability,MET,"Missing details on methodology (eg., use of notation, use on tasks, etc)"
"In Section 3.2.2, the authors only explain how they learn example-based \sigma, but details on how to make graph construction end-to-end trainable are missing.",asp_replicability,MET,"Missing details on methodology (eg., use of notation, use on tasks, etc)"
The proofs are quite dense and I was unable to verify them carefully.,asp_replicability,MET,"Missing details on methodology (eg., use of notation, use on tasks, etc)"
"1. For the evaluation of DBA, I assume that there are 4 adversarial parties, controlling each of the 4 local triggers. When using centralized attacks, are there still 4 adversarial parties, although they share the same global trigger, or if there is only 1 adversarial party?",asp_replicability,MET,"Missing details on methodology (eg., use of notation, use on tasks, etc)"
"Specifically, which features are perturbed, what are the values assigned as the trigger, and what is the corresponding target label?",asp_replicability,MET,"Missing details on methodology (eg., use of notation, use on tasks, etc)"
"- My main concern is reproducibility: the authors employ a number of large architectures, complex loss functions, and regularizers / ""additional improvements"".",asp_replicability,MET,"Missing details on methodology (eg., use of notation, use on tasks, etc)"
"In this simple model, can we not calculate the mutual information directly (i.e., without the bottleneck)?",asp_replicability,MET,"Missing details on methodology (eg., use of notation, use on tasks, etc)"
"Specifically, how can mutual information in this context be formally linked to generalization/overfitting?",asp_replicability,MET,"Missing details on methodology (eg., use of notation, use on tasks, etc)"
"First of all, I don’t understand how the main equation of the compound density network in Equation (3) is different from the general case of a Bayesian neural network? Can the authors please comment on that?",asp_replicability,MET,"Missing details on methodology (eg., use of notation, use on tasks, etc)"
I also find weird the way that the authors arrive to their final objective in Equation (5).,asp_replicability,MET,"Missing details on methodology (eg., use of notation, use on tasks, etc)"
Then they continue to Equation (5) which they present as the combination of the true likelihood with a KL regularisation term.,asp_replicability,MET,"Missing details on methodology (eg., use of notation, use on tasks, etc)"
"However, what the authors implicitly did was to perform variational inference for maximising their likelihood by introducing a variational distribution q(\theta) = p(\theta | g(x_n; \psi).",asp_replicability,MET,"Missing details on methodology (eg., use of notation, use on tasks, etc)"
Is there a reason why the authors do not introduce their objective by following the variational framework?,asp_replicability,MET,"Missing details on methodology (eg., use of notation, use on tasks, etc)"
"My biggest concern in the methodology, however, has to do with the selection of the matrix variate normal prior for the weights and the imposition of diagonal covariances (diag(a) and diag(b)).",asp_replicability,MET,"Missing details on methodology (eg., use of notation, use on tasks, etc)"
What is the purpose then for introducing the matrix variate Gaussian?,asp_replicability,MET,"Missing details on methodology (eg., use of notation, use on tasks, etc)"
"Finally, it is unclear how the authors have picked the best \lambda parameter for their approach? On page 5 they state that they “pick the value that results in a good trade-off between high uncertainty estimates and high prediction accuracy.” Does this mean that you get to observe the performance in the test in order to select the appropriate value for \lambda? If this is the case this is completely undesirable and is considered a bad practice.",asp_replicability,MET,"Missing details on methodology (eg., use of notation, use on tasks, etc)"
You mention that negative mining should improve over this strategy. What does negative mining correspond to in this context? Are there bad rewrites better than others?,asp_replicability,MET,"Missing details on methodology (eg., use of notation, use on tasks, etc)"
I do not understand why making use of labels is important for solving the catastrophic forgetting problem and how the labels are useful in the generative replay process.,asp_replicability,MET,"Missing details on methodology (eg., use of notation, use on tasks, etc)"
It is also not clear to me how domain translation is relevant to continual learning.,asp_replicability,MET,"Missing details on methodology (eg., use of notation, use on tasks, etc)"
I do not understand how the model is trained to solve multiple tasks.,asp_replicability,MET,"Missing details on methodology (eg., use of notation, use on tasks, etc)"
Do the same model is trained for multiple tasks? Is each of the tasks trained sequentially or simultaneously?,asp_replicability,MET,"Missing details on methodology (eg., use of notation, use on tasks, etc)"
6. The rationale of the two tower design (why not combine two) is not clearly explained.,asp_replicability,MET,"Missing details on methodology (eg., use of notation, use on tasks, etc)"
Doesn't the classification loss have a dependency on the input condition?,asp_replicability,MET,"Missing details on methodology (eg., use of notation, use on tasks, etc)"
"--What does a ""heavy classifier"" imply concretely?",asp_replicability,MET,"Missing details on methodology (eg., use of notation, use on tasks, etc)"
"In contrast, the studied learning rates are asymptotic and there is a big discrepancy.",asp_replicability,MET,"Missing details on methodology (eg., use of notation, use on tasks, etc)"
2- Why is a two-stage pre-training (Figure 2) process needed? Why not just a single stage?,asp_replicability,MET,"Missing details on methodology (eg., use of notation, use on tasks, etc)"
"However, it is not obvious that how to move from line 3 to line 4 at Eq 15.",asp_replicability,MET,"Missing details on methodology (eg., use of notation, use on tasks, etc)"
(4) Are \theta and \phi jointly and simultaneously optimized at Eq 12?,asp_replicability,MET,"Missing details on methodology (eg., use of notation, use on tasks, etc)"
"(5) Due to the mean policy approximation, does the mean policy depend on \phi?",asp_replicability,MET,"Missing details on methodology (eg., use of notation, use on tasks, etc)"
The authors should clearly explain how to update \phi when optimizing Eq 12.,asp_replicability,MET,"Missing details on methodology (eg., use of notation, use on tasks, etc)"
"However, the derivations about \phi are missing.",asp_replicability,MET,"Missing details on methodology (eg., use of notation, use on tasks, etc)"
"For example, how to compute the gradient w.r.t. \phi? Since the mean policy is used, it is not apparent that how to compute the gradient w.r.t. \phi.",asp_replicability,MET,"Missing details on methodology (eg., use of notation, use on tasks, etc)"
"Second, the authors claim they are using/motivated by Choquet integral, but do not have any (appendix) sections to explain how this mathematical tool is really integrated into their models.",asp_replicability,MET,"Missing details on methodology (eg., use of notation, use on tasks, etc)"
How do you guarantee that the representation learned by the neural network still obeys the property of Choquet integral? What is your loss or your algorithm?,asp_replicability,MET,"Missing details on methodology (eg., use of notation, use on tasks, etc)"
"It is limited, however, in the sense that it (only) measures auto-encoding capabilities: to what extent can the shape be reproduced given a sample point cloud from the given shape.",asp_replicability,MET,"Missing details on methodology (eg., use of notation, use on tasks, etc)"
"As previously mentioned in public comments on this forum, some points in the paper are not very clear; specifically regarding the loss function, the definition of ""edge-to-edge"" convolutions and generally the architectural choice related to the conditional GAN discriminator.",asp_replicability,MET,"Missing details on methodology (eg., use of notation, use on tasks, etc)"
"- how exactly do you do a L1 loss on graphs? I'd have to assume the topology of the graph is unchanged between Gy and T(Gx) ~ and then maybe take L1 of weight matrix? But then is this general enough ~ given your stated goal of modeling different topologies? Either ways, more explanation / and perhaps equations to clarify this loss would be very helpful.",asp_replicability,MET,"Missing details on methodology (eg., use of notation, use on tasks, etc)"
There is not sufficient detail to reproduce the models based on the paper alone.,asp_replicability,MET,"Missing details on methodology (eg., use of notation, use on tasks, etc)"
"Moreover, it seems strange that significant space was used to give equations describing simple embedding lookups (i.e., matrix multiplications with one-hot vectors), but the basic technical foundations of Transformers were not adequately explained.",asp_replicability,MET,"Missing details on methodology (eg., use of notation, use on tasks, etc)"
"For example, one question is how often a single partial tree has multiple possible completions in the data.",asp_replicability,MET,"Missing details on methodology (eg., use of notation, use on tasks, etc)"
"Why? If we are learning the distribution, would not it make sense to sample all architectures only after training the supernet at our best?",asp_replicability,MET,"Missing details on methodology (eg., use of notation, use on tasks, etc)"
Do they here refer to the gradients with respect to the weights ONLY?,asp_replicability,MET,"Missing details on methodology (eg., use of notation, use on tasks, etc)"
Could we say that the advantage of the Gumbel Softmax technique is two-fold? i) make the loss differentiable with respect to the arch parameters; ii) reduce the variance of the estimate of the loss gradients with respect to the network weights.,asp_replicability,MET,"Missing details on methodology (eg., use of notation, use on tasks, etc)"
"2)	Can the author discuss why the soft sampling procedure in [1] is not enough? I have an intuitive understanding of this, but I think this should be clearly discussed in the manuscript as this is a central aspect of the paper.",asp_replicability,MET,"Missing details on methodology (eg., use of notation, use on tasks, etc)"
3)	The authors use a certain number of warmup steps to train the network weights without updating the architecture parameters to ensure that “the weights are sufficiently trained”. Can the authors discuss the choice on the number of warmup epochs?,asp_replicability,MET,"Missing details on methodology (eg., use of notation, use on tasks, etc)"
- It would greatly benefit the reader if eq. 5 were expanded.,asp_replicability,MET,"Missing details on methodology (eg., use of notation, use on tasks, etc)"
- The approach also seems to add a lot of complexity and heuristics/hyper-parameters.,asp_replicability,MET,"Missing details on methodology (eg., use of notation, use on tasks, etc)"
(c) The authors should present what they mean by a dilated convolution using the notation of the paper.,asp_replicability,MET,"Missing details on methodology (eg., use of notation, use on tasks, etc)"
- the method is not applicable to episodes of different length,asp_replicability,MET,"Missing details on methodology (eg., use of notation, use on tasks, etc)"
"At last, I am pretty sure to not be able to reproduce the model described in the paper (adding a section on that in the supplementary material would help), and many concrete aspects are described too fast (like the way to sample negative pairs).",asp_replicability,MET,"Missing details on methodology (eg., use of notation, use on tasks, etc)"
Is Harmonic Convolution applicable to complex STFT coefficients as well?,asp_replicability,MET,"Missing details on methodology (eg., use of notation, use on tasks, etc)"
If so it would be better to define the operator in a more general notation.,asp_replicability,MET,"Missing details on methodology (eg., use of notation, use on tasks, etc)"
"In Section 4.3 and 4.4, is the x_0 (defined in Section 2.1) complex-valued STFT coefficients or something else?",asp_replicability,MET,"Missing details on methodology (eg., use of notation, use on tasks, etc)"
"What is the L1 loss defined in Section 4.4? To obtain the final separated audio waveform, an inverse STFT is applied on what?",asp_replicability,MET,"Missing details on methodology (eg., use of notation, use on tasks, etc)"
The main problem I see with these approaches is that they rely on sufficiently large batch sizes which could be (currently) problematic for many real-world applications.,asp_replicability,MET,"Missing details on methodology (eg., use of notation, use on tasks, etc)"
"Lastly, if the authors are not planning to release the code, the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.",asp_replicability,MET,"Missing details on methodology (eg., use of notation, use on tasks, etc)"
Also how many steps are required to learn the linear interpolation ? How much the does it influence the quality of the interpolation ?,asp_replicability,MET,"Missing details on methodology (eg., use of notation, use on tasks, etc)"
It is also not clear what are the assumptions made on the connectivity of the input graph and the target graph.,asp_replicability,MET,"Missing details on methodology (eg., use of notation, use on tasks, etc)"
Do we know how does the connectedness of the  input graph affect the translation quality in the case of strongly connected directed graphs? Or what happens if the target graph has a strong connectivity?,asp_replicability,MET,"Missing details on methodology (eg., use of notation, use on tasks, etc)"
"Towards this, how does the computational complexity scale wrt to the connectedness?",asp_replicability,MET,"Missing details on methodology (eg., use of notation, use on tasks, etc)"
A lot of clarity is required on the choice of evaluation metric; for example choice of distance measure ?,asp_replicability,MET,"Missing details on methodology (eg., use of notation, use on tasks, etc)"
What is the L1 norm applied on?,asp_replicability,MET,"Missing details on methodology (eg., use of notation, use on tasks, etc)"
- Trick is specific to LM.,asp_replicability,MET,"Missing details on methodology (eg., use of notation, use on tasks, etc)"
It seems heavily dependent on GBDT.,asp_replicability,MET,"Missing details on methodology (eg., use of notation, use on tasks, etc)"
1. I do not get the point of bringing up NCE. Did you actually use NCE loss? Did you only refer to NCE as a weight tying which can be used in a standard XENT loss [3]? The first paragraph of 3.3 did not help clarify this point either.,asp_replicability,MET,"Missing details on methodology (eg., use of notation, use on tasks, etc)"
"2) Beware of overstating: the argument that the framework is broadly applicable is not that useful, given that it's a lot of work to derive closed-form marginalized estimators.",asp_replicability,MET,"Missing details on methodology (eg., use of notation, use on tasks, etc)"
The method presented is interesting; but it is not clear that it is present with enough detail for it's results to be replicated.,asp_replicability,MET,"Missing details on methodology (eg., use of notation, use on tasks, etc)"
Was crossvalidation used to select the topology?,asp_replicability,MET,"Missing details on methodology (eg., use of notation, use on tasks, etc)"
"If so, what was the methodology.",asp_replicability,MET,"Missing details on methodology (eg., use of notation, use on tasks, etc)"
"Moreover, these bounds are just in the case where the teacher model is linear and while it is claimed that this could be relaxed to a more general class of functions, the specific bounds might change drastically.",asp_replicability,MET,"Missing details on methodology (eg., use of notation, use on tasks, etc)"
"So any insights on the nature of these bounds will be valuable, especially with some comments on how these bounds change if the teacher model is itself realized as a 2-layer neural network.",asp_replicability,MET,"Missing details on methodology (eg., use of notation, use on tasks, etc)"
1 The implementation steps of the proposed method (MoVE) are not clear.,asp_replicability,MET,"Missing details on methodology (eg., use of notation, use on tasks, etc)"
"In 3/4 of the pairs the author tried, this phenomenon is not there. Whether the findings generalize to other situations where the phenomenon appears is uncertain.",asp_replicability,MET,"Missing details on methodology (eg., use of notation, use on tasks, etc)"
"2. I might have missed it, but I couldn't find any motivation on why tanh is used as nonlinearity. Would the method work with relu?",asp_replicability,MET,"Missing details on methodology (eg., use of notation, use on tasks, etc)"
"To be honest, the theoretical contribution of the paper is limited.",asp_replicability,MET,"Missing details on methodology (eg., use of notation, use on tasks, etc)"
More discussions on these questions can be very helpful to further understand the proposed method.,asp_replicability,MET,"Missing details on methodology (eg., use of notation, use on tasks, etc)"
"2. As the problem is formulated as an RL problem, which is well-known for its difficulty in training, did we encounter similar issues? More details in the implementation can be very helpful for reproducibility.",asp_replicability,MET,"Missing details on methodology (eg., use of notation, use on tasks, etc)"
"It should be better motivated why one should use the duality gap as an upper bound for the ""F-distance"".",asp_replicability,MET,"Missing details on methodology (eg., use of notation, use on tasks, etc)"
Minimizing the F-distance as is usually done seems like the more direct and simple approach.,asp_replicability,MET,"Missing details on methodology (eg., use of notation, use on tasks, etc)"
- I have trouble understanding the overall idea behind Algorithm 1 and Eq. (22).,asp_replicability,MET,"Missing details on methodology (eg., use of notation, use on tasks, etc)"
What is the definition of f^* and g^* in Eq. (22)? Some explanatory text would be valuable.,asp_replicability,MET,"Missing details on methodology (eg., use of notation, use on tasks, etc)"
1. The authors should provide more details on how the hand-crafted demonstrator agents were made.,asp_replicability,MET,"Missing details on methodology (eg., use of notation, use on tasks, etc)"
"I assume something similar to an a* algorithm was probably used for the passing task, but what about the maze navigation task?",asp_replicability,MET,"Missing details on methodology (eg., use of notation, use on tasks, etc)"
How do “we choose a specific number of assignments based on prediction probabilities”?,asp_replicability,MET,"Missing details on methodology (eg., use of notation, use on tasks, etc)"
"- The CFS metric depends on a hyperparameter (the ""retention ratio""), which here is arbitrarily set to 80% without any justification.",asp_replicability,MET,"Missing details on methodology (eg., use of notation, use on tasks, etc)"
"13. Reproducibility seems like it would be hard. There are a few parameters (meta-learning rates, meta-optimizers) that I could not find for example and there is a lot of complexity.",asp_replicability,MET,"Missing details on methodology (eg., use of notation, use on tasks, etc)"
Negative points: (1) The authors should provide more justification on equation-3.,asp_replicability,MET,"Missing details on methodology (eg., use of notation, use on tasks, etc)"
Why do the authors directly average different loss for the discriminator and the classifer?,asp_replicability,MET,"Missing details on methodology (eg., use of notation, use on tasks, etc)"
"(2) The function of the discriminator is not very clear, especially for the classification error test.",asp_replicability,MET,"Missing details on methodology (eg., use of notation, use on tasks, etc)"
Why can pGAN lead to the inclination? Is it possible for pGAN to control the specific error tendency?,asp_replicability,MET,"Missing details on methodology (eg., use of notation, use on tasks, etc)"
"However, it is unclear how a similar strategy could be applied if we are interested in learning variables with higher-level semantics such as the expression of a face or its pose.",asp_replicability,MET,"Missing details on methodology (eg., use of notation, use on tasks, etc)"
3. how are predicted labels embedded? Do you learn a vector of each tag of BIOES and then take a weighted sum of these vectors with predicted probabilities as weights?,asp_replicability,MET,"Missing details on methodology (eg., use of notation, use on tasks, etc)"
The main drawback would be the rather incrementality of that paper (basically sample before projecting is a bit better than projecting after sampling) and that this directional setting is quite limited...,asp_replicability,MET,"Missing details on methodology (eg., use of notation, use on tasks, etc)"
2. The learning procedure is confusing.,asp_replicability,MET,"Missing details on methodology (eg., use of notation, use on tasks, etc)"
It is highly recommended to provide the pseudocode of the proposed method.,asp_replicability,MET,"Missing details on methodology (eg., use of notation, use on tasks, etc)"
The reasons for the use of the energy-based formulation are not clear to me.,asp_replicability,MET,"Missing details on methodology (eg., use of notation, use on tasks, etc)"
"Is the energy-based model particularly well-suited to the random-projection setup, or are there other reasons for using it, independent of the use of random projections?",asp_replicability,MET,"Missing details on methodology (eg., use of notation, use on tasks, etc)"
"The index over which the sum happens is n, but n is fixed? So this looks like a sum with just one component in it, namely the first n-gram.",asp_replicability,MET,"Missing details on methodology (eg., use of notation, use on tasks, etc)"
- It is not clear how the initialisation (10) is implemented.,asp_replicability,MET,"Missing details on methodology (eg., use of notation, use on tasks, etc)"
"Particularly the ""fusion"" module remains extremely unclear.",asp_replicability,MET,"Missing details on methodology (eg., use of notation, use on tasks, etc)"
It is not clear to me that the classifier difference metric is well-defined.,asp_replicability,MET,"Missing details on methodology (eg., use of notation, use on tasks, etc)"
"Equation (4). What is d_{k,l}? A pixel-wise target label? Where does it come from?",asp_replicability,MET,"Missing details on methodology (eg., use of notation, use on tasks, etc)"
"2) How are the rules from in Eq (2)? i.e., how is \beta_i selected for each i?",asp_replicability,MET,"Missing details on methodology (eg., use of notation, use on tasks, etc)"
"- Very little details (apart from the optimization algorithm) are given regarding the architecture used (types of input, output, neural units, activation functions, number of hidden layers, loss function, etc...), which makes it very hard to reproduce this approach.",asp_replicability,MET,"Missing details on methodology (eg., use of notation, use on tasks, etc)"
It would be interesting to know the evaluation times for the BA-net and more importantly to have some implementation details to ensure reproducibility.,asp_replicability,MET,"Missing details on methodology (eg., use of notation, use on tasks, etc)"
Overall it would be helpful for reproducibility if authors can visualize all the layers of all the different parts of the network as it is commonly done in the DL papers.,asp_replicability,MET,"Missing details on methodology (eg., use of notation, use on tasks, etc)"
-	How the first camera pose is initialized?,asp_replicability,MET,"Missing details on methodology (eg., use of notation, use on tasks, etc)"
This is all further confused by the semantic topics used for clustering the images which ignores stop words and therefore spatial relations or any grammatical nuances.,asp_replicability,MET,"Missing details on methodology (eg., use of notation, use on tasks, etc)"
"In general, the space that was used to explain the Transformer baselines---which are essentially straightforward ways to adapt transformers to this task---could have been used to give more detail on the dataset.",asp_replicability,MET_DAT,"Missing ablation study (how to generalize findings, what happens if some changes are done to the method, etc)"
"1. What are the key limitations of AutoLoss ? Did we observe some undesirable behavior of the learned optimization schedule, especially when transfer between different datasets or different models ?",asp_replicability,MET_DAT,"Missing ablation study (how to generalize findings, what happens if some changes are done to the method, etc)"
Does the discriminator exclude the poisoning data according to certain rule?,asp_replicability,MET_DAT,"Missing ablation study (how to generalize findings, what happens if some changes are done to the method, etc)"
"But there is no attempt to generalize the findings (e.g. new datasets not from original study, changing other parameters and then evaluating again if these techniques help etc.), not clear",asp_replicability,MET_DAT,"Missing ablation study (how to generalize findings, what happens if some changes are done to the method, etc)"
"- Did the experiments on CIFAR-10 and ImageNet use the cosine learning rate schedule [1]? If or if not, either way, you should specify it in a revised version of this paper, e.g. did you use the cosine schedule in the first 120 steps to train the shared parameters W, did you use it in the retraining from scratch?",asp_replicability,MET_EXP,"Because of this concern, I believe the results in the paper can only really characterize the learning close to convergence, since the network is already able to provide correct classification."
"- Although the authors discussed the experiment setting in detail in supplements, I believe open-sourcing the code / software used to conduct the experiments would be greatly help with the reproducibility of the proposed method for researchers or practitioners.",asp_replicability,MET_EXP,"Because of this concern, I believe the results in the paper can only really characterize the learning close to convergence, since the network is already able to provide correct classification."
"For example, for baseline 1, it is very hard to understand why would we want to use such an unusual baseline, and why it is called a ""random baseline"".",asp_replicability,MET_RWK,"For example, for baseline 1, it is very hard to understand why would we want to use such an unusual baseline, and why it is called a ""random baseline""."
"Clarifications of these points, and more in general the philosophy behind the architectural choices made, would make this paper a much clearer accept.",asp_replicability,OAL,"Because of this concern, I believe the results in the paper can only really characterize the learning close to convergence, since the network is already able to provide correct classification."
My concern for this paper is reproducibility.,asp_replicability,OAL,"Because of this concern, I believe the results in the paper can only really characterize the learning close to convergence, since the network is already able to provide correct classification."
"This sort of two-stage generation is also potentially interesting, I was wondering if the authors had ideas to generalize this idea.",asp_replicability,PDI,"This sort of two-stage generation is also potentially interesting, I was wondering if the authors had ideas to generalize this idea."
"While the authors do report some interesting results, they do a poor job of motivating the proposed extensions.",asp_replicability,RES,Missing details for reproducibility of result
These points remain me puzzled regarding either practical or theoretical application of the result. It would be great if authors could elaborate.,asp_replicability,RES,Missing details for reproducibility of result
"Because of this concern, I believe the results in the paper can only really characterize the learning close to convergence, since the network is already able to provide correct classification.",asp_replicability,RES,Missing details for reproducibility of result
"Can you explain somewhere exactly what you mean when you say ""learning dynamics of deep learning""? Given the specific nature of the results presented in the paper it would be nice to be precise also when it comes to the overall topic under study.",asp_replicability,RES,Missing details for reproducibility of result
3.	Are the authors willing to release the code? Overall the model looks complicated and the appendix is not sufficient to reproduce the results in the paper.,asp_replicability,RES,Missing details for reproducibility of result
I trust that the authors did in fact achieve these results but I cannot figure out how or why.,asp_replicability,RES,Missing details for reproducibility of result
- very incremental improvements on PTB over a very simple baseline (far from SotA),asp_replicability,RWK,Missing implementation details of related work used as baselines
"3. The graph neural networks used in the model are not described in the paper, only a reference to Paliwal et al (2019) is given.",asp_replicability,RWK,Missing implementation details of related work used as baselines
"It would be helpful to have a brief paragraph describing this architecture, for readers not familiar with the referenced paper.",asp_replicability,RWK,Missing implementation details of related work used as baselines
"1) I wonder if the proposed method work for most GAN models, more experiments evaluated on more recent GAN-based  models should be added to verify the superiority claimed in this paper, e.g., TP-GAN [Huang et al., ICCV 2017], PIM [Zhao et al., CVPR 2018], DR-GAN [Tran et al., CVPR 2017], DA-GAN [Zhao et al., NIPS 2017], MH-Parser [Li et al., 2017], 3D-PIM [Zhao et al., IJCAI 2018], SimGAN [Shrivastava et al., CVPR 2016], AIM [Zhao et al., AAAI 2019].",asp_replicability,RWK,Missing implementation details of related work used as baselines
"Some details are missing, which is hardly reproduced by the other researchers.",asp_replicability,RWK,Missing implementation details of related work used as baselines
"- In Table 2 and 3, how are the degree and block information leveraged into the model?",asp_replicability,TNF,"Missing details in tables and figures (how some value as calculated in fig, some notation missing in fig, etc)"
"It is also unclear how the calculation of relative entropy ""D"" was performed in figure 3.",asp_replicability,TNF,"Missing details in tables and figures (how some value as calculated in fig, some notation missing in fig, etc)"
"1. Where is L_da in Figure 2? In Figure 2, what’s the unlabelled data from which testing tasks are drawn? Is it from meta-test data training set?",asp_replicability,TNF,"Missing details in tables and figures (how some value as calculated in fig, some notation missing in fig, etc)"
"- for Figure 6, there is not a clear conclusion.",asp_replicability,TNF,"Missing details in tables and figures (how some value as calculated in fig, some notation missing in fig, etc)"
- in section 4.3 how is the reconstruction built (Figure 3b)?,asp_replicability,TNF,"Missing details in tables and figures (how some value as calculated in fig, some notation missing in fig, etc)"
I'm not sure I understand the PCA figures. Can you please explain how the first principal component was used to generate them?,asp_replicability,TNF,"Missing details in tables and figures (how some value as calculated in fig, some notation missing in fig, etc)"
"(d) In Figure 2, it is unclear to me how the 1/f^2 law is observed in (a) but not in (c) or (e).",asp_replicability,TNF,"Missing details in tables and figures (how some value as calculated in fig, some notation missing in fig, etc)"
It is not clear how the noise is introduced in the graphs.,asp_replicability,TNF,"Missing details in tables and figures (how some value as calculated in fig, some notation missing in fig, etc)"
* \sigma is not given in Figure 3(a),asp_replicability,TNF,"Missing details in tables and figures (how some value as calculated in fig, some notation missing in fig, etc)"
"Therefore, it may be a good idea for the authors to analyze the correlation between FSM changes and accuracy changes.",asp_substance,ANA,Lack of analysis
- no qualitative analysis on how modulation is actually use by the systems.,asp_substance,ANA,Lack of analysis
"-2 Diversity of additional ""agents"" not analyzed (more below).",asp_substance,ANA,Lack of analysis
- Missed opportunity of better analysis of which theorem/rewrite rule properties are more likely to fail,asp_substance,ANA,Lack of analysis
"Presumably, whether the benefit scales well will depend on the loss function - in any case, this warrants some analysis/discussion especially given that there are no experiments to test it.",asp_substance,ANA,Lack of analysis
"- One thing I am confused about is the residual model, which seems quite important for the pipeline but I cannot find details describing it and much analysis on this component.",asp_substance,ANA,Lack of analysis
"4. Mean field analysis, although it lends an insight into the statistics of the activations, needs to connected with empirical observations.",asp_substance,ANA,Lack of analysis
"For the modeling contribution, although it shows some improvements on the benchmarks and some nice analysis, the paper really doesn’t explain well the intuition of this “write” operation/Scratchpad (also the improvement of Scratchpad vs coverage is relatively limited). Is this something tailored to question generation? Why does it expect to improve on the question generation or it can improve any tasks which build on top of seq2seq+att framework (e.g., machine translation, summarization -- if some results can be shown on the most competitive benchmarks, that would be much more convincing)?",asp_substance,ANA,Lack of analysis
\alpha^i_t and u^i are also pretty complex and it would be good to conduct some ablation analysis.,asp_substance,ANA,Lack of analysis
I would have liked to see a bit more analysis as to why some pre-training strategies work over others.,asp_substance,ANA,Lack of analysis
- It would be nice if more network architectures were analysed (such as VGG and DenseNets).,asp_substance,ANA,Lack of analysis
- It would be nice if different stopping criteria were analysed.,asp_substance,ANA,Lack of analysis
"Moreover, said the analysis is in my opinion not sufficiently rigorous, with hand-wavy arguments, no formal proof and unclear terms (e.g. how do you define near-optimal?)",asp_substance,ANA,Lack of analysis
"In that regard, I would like the authors to comment on the worst-case computational complexity of the numerical analysis for determining the volume of a preimage through multiple layers.",asp_substance,ANA,Lack of analysis
"While this assumption seems plausible,  no analysis has been done to verify it in a systematic way.",asp_substance,ANA,Lack of analysis
This may also help to understand some of the limitations of this analysis.,asp_substance,ANA,Lack of analysis
"And the analysis of the ""dynamic range"" of the algorithim is missing.",asp_substance,ANA,Lack of analysis
"- The premises of the analyses are not very convincing, limiting the significance of the paper.",asp_substance,ANA,Lack of analysis
2. The authors should provide ablation study and analysis of their CTAugment.,asp_substance,ANA,Lack of analysis
"4. The authors hypothesize that “stronger augmentation can result in disparate predictions, so their average may not be a meaningful target.” However, they do not show any analysis to support this hypothesis.",asp_substance,ANA,Lack of analysis
-- The analysis is relatively straightforward and boils down to bucketing the error and integration over the buckets.,asp_substance,ANA,Lack of analysis
"-- The machine learned advice is assumed to be flawless at identifying the Heavy Hitters, authors might want to consider incorporating errors in the analysis.",asp_substance,ANA,Lack of analysis
2. It is hard to understand what the model has learned compared to hand-crafted schedule. Are there any analysis other than the results alone?,asp_substance,ANA,Lack of analysis
"If that's the goal, however, a more detailed error analysis would need to be included.",asp_substance,ANA,Lack of analysis
(This seems like something that could easily be done in the empirical analysis as well and would provide richer empirical signals as well),asp_substance,ANA,Lack of analysis
"Recommendations for the authors: Would it be possible to provide an analysis of the cases when TabNN is expected to outperform GBDT by a sizable margin? Or, if not, are there other reasons why using a neural network would make more sense than just simply running GBDT?",asp_substance,ANA,Lack of analysis
"- The analysis has focused on a very simple case of having a linear discriminator which for example in WGAN, forces the first moments to match. How does the analysis extend to more realistic cases?",asp_substance,ANA,Lack of analysis
I feel that the idea deserves a broader analysis beyond just a single choice of disentanglement.,asp_substance,ANA,Lack of analysis
- Lack of theoretical analysis. It could have been nice if the authors could show the observed phenomenon analytically on some simple distribution.,asp_substance,ANA,Lack of analysis
More rigorous experiments and analysis is needed to make this a good ICLR paper.,asp_substance,ANA_EXP,"The advantage of this random encoding was to reduce the number of parameters for an embedding layer, but the results showed we gained much PPL from a 25% reduction in embedding size (Table 1)."
- (W2) Related to the last line: I did not see any experiments / analysis showing how stable these different numbers are across different runs of the representation technique. Nor did I see any error bars in the experiments.,asp_substance,ANA_EXP,"The advantage of this random encoding was to reduce the number of parameters for an embedding layer, but the results showed we gained much PPL from a 25% reduction in embedding size (Table 1)."
which is much smaller than the number of time series usually involved say in gene regulatory network data,asp_substance,DAT,Less datasets used
"2.	Data size is too small, and the baselines",asp_substance,DAT,Less datasets used
"Did the authors also examine the relationship between mutual information and generalization error for CIFAR data sets? Does it not make sense to examine this for all (most of) the setups considered in Table 4, 8, and 9.",asp_substance,DAT,Less datasets used
"Due to the different objects used in the different datasets, some of the experiments have a smaller set of words.",asp_substance,DAT,Less datasets used
It will be also interesting to see how does the proposed method perform on large-scale datasets such as DomainNet and Office-Home dataset:,asp_substance,DAT,Less datasets used
"Weaknesses: The dataset created here is entirely synthetic, and the paper only includes one single small real-world case; it seems like it would be easy to generate a larger and more varied real world dataset as well (possibly from the large literature of extant solved problems in workbooks).",asp_substance,DAT,Less datasets used
"The descriptions of the datasets used are not clear, e.g., the number of classes for each data.",asp_substance,DAT,Less datasets used
"First, the labeled data portion is fixed and is relatively high",asp_substance,DAT,Less datasets used
"For example, the number of labeled examples in Table 1 is fairly large and inconsistent (4K, 1K, 10K for the 3 organic datasets).",asp_substance,DAT,Less datasets used
"BSD 500 only contains 500 images, and it would be good if more diverse set of images are considered.",asp_substance,DAT,Less datasets used
"- Some of the datasets the authors currently test on are quite toy, especially for the image-based MNIST and SVHN datasets.",asp_substance,DAT,Less datasets used
Is it better to decay learning rates for toy data sets?,asp_substance,DAT,Less datasets used
It is difficult to judge the performance of the proposed model based on so small data set.,asp_substance,DAT,Less datasets used
"The first point would be: what's the meaning of synthetically generating training curves other than proving that transformer achieves good performance in modeling discrete distribution? Most practical problems would not have the same distribution as the previously gathered public dataset, thus the data is not representative, and synthetic training curves just does not make sence.",asp_substance,DAT,Less datasets used
* There is still no comparison with competing nonparametric tests on the fMRI data.,asp_substance,DAT,Less datasets used
- How much does the image matter for the single-image data set?,asp_substance,DAT,Less datasets used
"The author can perhaps consider the datasets used by Tsai et al. There are seven datasets, and they can all be modified to the setting of partially-observable multimodal data.",asp_substance,DAT,Less datasets used
"Apart from the assumption H1 of linear separability of the data (which I don't mind), the results require very strong assumptions, in particular hypothesis H2 stating ""at the beginning of training data points from different classes do not activate the same neurons"".",asp_substance,DAT,Less datasets used
"It would have been very interesting to study the quality of interpolations on more models and datasets, and compare their generalization capabilities as well as the bias present in the different datasets.",asp_substance,DAT,Less datasets used
"Third, the datasets used in this paper are rather limited.",asp_substance,DAT,Less datasets used
What is the minimum/maximum size of the data set? Do we really need a large data set or just a subset that covers the data distribution?,asp_substance,DAT,Less datasets used
- Fitting 6 parameters to 42-49 data points raises concerns about overfitting.,asp_substance,DAT,Less datasets used
"Consider doing cross validation over those 42-49 data points, and report the mean of deviations computed on the test folds.",asp_substance,DAT,Less datasets used
My only issue here is that very little information was given about the size of the training sets. Did they use all the samples?,asp_substance,DAT,Less datasets used
"However, in the proposed domain of tabular data, often data sets are significantly more high dimensional in reality and include at least one set of sparse large dimensional features",asp_substance,DAT,Less datasets used
"However, these were done using 1 dataset and also a simple feed-forward network (rather than LSTM).",asp_substance,DAT,Less datasets used
"Thus, it might be helpful to test the result on another dataset (e.g. WikiText).",asp_substance,DAT,Less datasets used
"Comparing to this approach, I wonder if it would be better to provide samples that exhibit a particular factor, or samples that conceal the factor?",asp_substance,DAT,Less datasets used
However image captioning datasets are not mentioned.,asp_substance,DAT,Less datasets used
It would make sense to use image captioning data to create the image lookup.,asp_substance,DAT,Less datasets used
"Given the title of the paper, it would have been nice if this paper explored more than just MNIST vs NotMNIST and SVHN vs CIFAR10, so that the readers can gain a better feel for when generative models will be able to detect outliers.",asp_substance,DAT,Less datasets used
The second order analysis is good but it seems to come down to just a measure of the empirical variances of the datasets.,asp_substance,DAT,Less datasets used
- Lack of an extensive exploration of datasets,asp_substance,DAT,Less datasets used
"- In particular, Section 4 is a series of empirical analyses, based on one dataset pair.",asp_substance,DAT,Less datasets used
"According to the conclusions of the paper, such dataset pairs should be easy to find -- just find a dataset that ""lies within"" another. Did you try e.g. CIFAR-100 train and CIFAR-10 test?",asp_substance,DAT,Less datasets used
"7. The evaluation on two datasets seems to be rather limited, additional comparisons should be included.",asp_substance,DAT,Less datasets used
"First, larger data sets such as Wikitext-2 and Wikitext-103, and/or the billion-word benchmark, are needed to understand how well the approach works in practical LM settings.",asp_substance,DAT,Less datasets used
"* There was some analysis of the augmented IMDB dataset, but none of the SNLI dataset.",asp_substance,DAT,Less datasets used
"That's a very simple kind of question; more generally, I'd like to see more analysis of the new dataset.",asp_substance,DAT,Less datasets used
Q1: Has the authors tried more complicated datasets such as CIFAR-10 to evaluate the pGAN method? It would make the paper more convincing to add results on more complex datasets.,asp_substance,DAT,Less datasets used
o use of the Penn Treebank dataset only;,asp_substance,DAT,Less datasets used
"- it is not really clear to be what data the graphs show: the boxplots show 5% of what data? does it also include the models obtained by gaussian process regression? and what about the line plots, is it the best model so far as you train more and more models? if so, how are those models chosen and ordered? are they the results of single models or average of multiple ones?",asp_substance,DAT,Less datasets used
"However, my concern is that the paper focuses only on a very specific application domain,  and an improvement over the niche dataset with much more supervision (from the extension) is not surprising at all. In the mean time, the notion of ""Neural Stethoscopes"" could be much more  generally applied. Without applications in other domains, it is not immediately clear what the paper's implication is.",asp_substance,DAT,Less datasets used
"1. If we do not have access to the model parameter, the training data, or the artifacts during training like the discriminator, what are some of the real world situations that fit this description? In those cases, is it too much to assume that we can control the random seed input to G?",asp_substance,DAT,Less datasets used
2. Is it reasonable to assume some constraints on how much data we can get from the blackbox generator?,asp_substance,DAT,Less datasets used
- the data sets used in the experiments are very small,asp_substance,DAT_EXP,"Lack of realistic datasets used in experiments (small siz, synthetic)"
"However, given that the datasets used in the experiments were not used in the associated benchmark papers, it is necessary for authors to explain how they trained competing models.",asp_substance,DAT_EXP,"Lack of realistic datasets used in experiments (small siz, synthetic)"
"The experiments use fairly small datasets, where the performance can be largely influenced by how good the feature extractor backbone is (e.g. training on more data and using deeper architecture would warrant better performance, and thus may change the conclusion).",asp_substance,DAT_EXP,"Lack of realistic datasets used in experiments (small siz, synthetic)"
The experimental results presented were all done on small synthetic datasets and it’s hard to evaluate whether the method is practically useful.,asp_substance,DAT_EXP,"Lack of realistic datasets used in experiments (small siz, synthetic)"
"2.	The experimental data set is too small, with only 635 problems.",asp_substance,DAT_EXP,"Lack of realistic datasets used in experiments (small siz, synthetic)"
"However, the selected experiments on the cubic regression toy data (Section 5.1) and the out-of-distribution classification (Section 5.2) are clear examples of system’s noise, i.e. epistemic uncertainty.",asp_substance,DAT_EXP,"Lack of realistic datasets used in experiments (small siz, synthetic)"
I believe that a more challenging experiment should be conducted e.g. using celebA dataset.,asp_substance,DAT_EXP,"Lack of realistic datasets used in experiments (small siz, synthetic)"
"The empirical evaluation is quite weak- one sparsity setting, two baselines, one dataset",asp_substance,DAT_EXP,"Lack of realistic datasets used in experiments (small siz, synthetic)"
Since the paper focuses explicitly on providing contrastive explanations for choosing a class A over another class B -- experiments on datasets which do not have real-images seem insufficient.,asp_substance,DAT_EXP,"Lack of realistic datasets used in experiments (small siz, synthetic)"
"Regarding contrastive explanations, experiments on datasets where distractor classes (y_probe) are present in addition to the class interest (y_true) seem important -- PASCAL VOC, COCO, etc.",asp_substance,DAT_EXP,"Lack of realistic datasets used in experiments (small siz, synthetic)"
Three datasets cannot make the experiments convincing.,asp_substance,DAT_EXP,"Lack of realistic datasets used in experiments (small siz, synthetic)"
"It would also be interesting to see an experiment where the labeled data has a skewed distribution of classes, but we provide the method with information about the true class distribution, and demonstrating that this information helps predictive performance.",asp_substance,DAT_EXP,"Lack of realistic datasets used in experiments (small siz, synthetic)"
"However, only 1 dataset pair is experimented -- there should be more to ensure the findings generalize, since Sections 3 and 4 rely completely on empirical analysis.",asp_substance,DAT_EXP,"Lack of realistic datasets used in experiments (small siz, synthetic)"
More experiments based on other types of data sets with clear global structures such as faces or stop signs will,asp_substance,DAT_EXP,"Lack of realistic datasets used in experiments (small siz, synthetic)"
More experiments on datasets,asp_substance,DAT_EXP,"Lack of realistic datasets used in experiments (small siz, synthetic)"
- It would be nice to have an experiment that varies the size of the external paired sentence-image dataset and tested the impact on performance.,asp_substance,DAT_EXP,"Lack of realistic datasets used in experiments (small siz, synthetic)"
2. Were there experiments that applies your metric to the training datasets like CelebA and FFHQ? In theory your metric should show no gap between N_R_obs and N_R_ref measured on the training dataset since that's the sampled ground truth.,asp_substance,DAT_EXP,"Lack of realistic datasets used in experiments (small siz, synthetic)"
"The same holds for the type of data, since the paper only shows results for image classification benchmarks.",asp_substance,DAT_RES,"In terms of evaluation, the authors only presented a few qualitative results on simple datasets, which is not comprehensive and convincing."
"As for the experimental results, the paper only provides results on the sentimental analysis results and digit datasets, which are small benchmarks.",asp_substance,DAT_RES,"In terms of evaluation, the authors only presented a few qualitative results on simple datasets, which is not comprehensive and convincing."
The results on real datasets are similar to the regular GCN.,asp_substance,DAT_RES,"In terms of evaluation, the authors only presented a few qualitative results on simple datasets, which is not comprehensive and convincing."
They might be more convincing if there were results on a very large corpus such as 1 billion word corpus.,asp_substance,DAT_RES,"In terms of evaluation, the authors only presented a few qualitative results on simple datasets, which is not comprehensive and convincing."
"My overall concern is that the experiment result doesn’t really fully support the claim in the two aspects: 1) the SASNet takes the enriched dataset as input to the neural net but it also uses the complex (validation set) to train the optimal parameters, so strictly it doesn’t really fit in the “transfer” learning scenario.",asp_substance,DAT_RES,"In terms of evaluation, the authors only presented a few qualitative results on simple datasets, which is not comprehensive and convincing."
"With this dataset, it's a bit of a stretch to say there was ""only a 1 point drop in BLEU score"". That's a significant drop, and in fact the DynamicConv paper goes to significant lengths to make a smaller 0.8 point improvement.",asp_substance,DAT_RES,"In terms of evaluation, the authors only presented a few qualitative results on simple datasets, which is not comprehensive and convincing."
"In terms of evaluation, the authors only presented a few qualitative results on simple datasets, which is not comprehensive and convincing.",asp_substance,DAT_RES,"In terms of evaluation, the authors only presented a few qualitative results on simple datasets, which is not comprehensive and convincing."
"The MNIST+SVHN dataset setup is described in detail, yet there is no summary of the experimental results, which are presented in the appendix.",asp_substance,DAT_RES_EXP,"The MNIST+SVHN dataset setup is described in detail, yet there is no summary of the experimental results, which are presented in the appendix."
"The paper does not talk about settings where states of interest are not known, so all of the experiments are based on this strong assumption.",asp_substance,EXP,Experimental study not strong enough
Yet the experiments considered in the paper are limited to very few time series.,asp_substance,EXP,Experimental study not strong enough
It would be good to have some experiments in the sequence to sequence setting to understand if the obtained complexities are in fact what one would expect in practice.,asp_substance,EXP,Experimental study not strong enough
The only set of experiments are comparisons on first 500 MNIST test images.,asp_substance,EXP,Experimental study not strong enough
"Moreover, I don't think some of the presented experiments are necessary.",asp_substance,EXP,Experimental study not strong enough
"Moreover, authors should conduct experiments on state-of-the-art benchmarks, including natural images.",asp_substance,EXP,Experimental study not strong enough
"One could understand the use of ""selection network"" as a way to automatically select a threshold of what to consider confident, however, in this case, the prediction of ""selection network"" should be thresholded at 0.5 (correct prediction or not), but the experiments use complex thresholds.",asp_substance,EXP,Experimental study not strong enough
"I would have been interested in ""false detection"" experiments: comparing estimator in a variety of problems where the mutual information is zero, but for different marginal distribution.",asp_substance,EXP,Experimental study not strong enough
"We think that this is not enough, and more extensive experimental results would provide a better paper.",asp_substance,EXP,Experimental study not strong enough
"However, I am wondering whether this paper is perfectly suited to ICLR conference due to the lack of experiment, practical implication given by the theory, or theory in the non-convex setting (I know that the latter is a huge open question and I am not criticizing the absence of theory in the non-convex-concave setting).",asp_substance,EXP,Experimental study not strong enough
One way to improve to work would be to provide practical takeaways from the theory or to provide experiments in the main paper.,asp_substance,EXP,Experimental study not strong enough
It would be nice to see a better case made for spherical convolutions within the experimental section.,asp_substance,EXP,Experimental study not strong enough
* The connections to deep learning seem arbitrary in some of the experiments.,asp_substance,EXP,Experimental study not strong enough
"For example, what is the number_iterations in the experiments? How are they chosen or what's the specific stopping criteria?",asp_substance,EXP,Experimental study not strong enough
"Given the paper title, the reviewer would have expected more experiments in a multiple domain context.",asp_substance,EXP,Experimental study not strong enough
- The performance gain is not substantial in experiments.,asp_substance,EXP,Experimental study not strong enough
"Furthermore, the claims of the model working for non-MCAR missingness are not substantiated by the experiments.",asp_substance,EXP,Experimental study not strong enough
I think that this is the crux of the paper that should be significantly expanded and experimentally validated.,asp_substance,EXP,Experimental study not strong enough
* The baselines in the experiments could be improved.,asp_substance,EXP,Experimental study not strong enough
"I also remain unconvinced by the response to my issue with the claim “Our experiments show that our networks can remember a large number of images and distinguish them from unseen images”, where the negative images are also seen by the memorization model, so they are not unseen.",asp_substance,EXP,Experimental study not strong enough
3. The experimental study is weak.,asp_substance,EXP,Experimental study not strong enough
"While the experiments across several domains convincingly show the presence of an anti-ME bias in neural network training, I recommend that the paper be rejected because it falls short in demonstrating to what extent performance could be improved by correcting or reversing this bias.",asp_substance,EXP,Experimental study not strong enough
- Detailed experimental setups are missing.,asp_substance,EXP,Experimental study not strong enough
"However, the experimental results are weak in justifying the paper's claims.",asp_substance,EXP,Experimental study not strong enough
"However, I found the contribution of the actor-critic model is insufficient and requires additional experimental validation (see below).",asp_substance,EXP,Experimental study not strong enough
"The main problems come from the experiments, which I would ask for more things.",asp_substance,EXP,Experimental study not strong enough
So how importance of each component to the whole framework? I would ask for the ablation study/additional experiments of using each component.,asp_substance,EXP,Experimental study not strong enough
"This triggers my curiosity of which difficulty is it? It would be nice to see this point more elaborated, and to see ablation study experiments.",asp_substance,EXP,Experimental study not strong enough
"- the authors fix the number of layers of the used network based on ""our experience"". For the sake of completeness, more experiments in this area would be nice.",asp_substance,EXP,Experimental study not strong enough
Regarding the experimental evaluation of the model rather confusing.,asp_substance,EXP,Experimental study not strong enough
"1. As mentioned in the paragraph before Section 4.1, it would be much simpler to consider a single latent embedding space L. In that case, \sigma and \alpha become unnecessary and we only need to train \omega.",asp_substance,EXP,Experimental study not strong enough
"I would imagine that even if an embedding X is a bit noisy, because not exactly equal to gamma(P) where P is the expression it represents, you could consider doing the propagation with gamma(G(X)).",asp_substance,EXP,Experimental study not strong enough
Experimental results itself are fine but not complete.,asp_substance,EXP,Experimental study not strong enough
It would therefore be instructive to add experiments for ResNet to see how well the results generalize to other network architectures.,asp_substance,EXP,Experimental study not strong enough
The second weakness is experimental design.,asp_substance,EXP,Experimental study not strong enough
"In sec. 4, there's a reference to an initial set of experiments with count-based methods without much details.",asp_substance,EXP,Experimental study not strong enough
=> Environment: The experimental section of the paper can be further improved.,asp_substance,EXP,Experimental study not strong enough
1- Why not use a single Boltzmann machine with 128 fully connected latent variables? Could you add this experiment please.,asp_substance,EXP,Experimental study not strong enough
"6, the experimental design of Sec. 4.2 is also a bit unfair.",asp_substance,EXP,Experimental study not strong enough
So the experiments in this paper is also not convincing.,asp_substance,EXP,Experimental study not strong enough
"- The counter example at the bottom of page 2 is limited, in the sense that the oracle assumption seems highly non-realistic, casting doubt on the relevance of the argument.",asp_substance,EXP,Experimental study not strong enough
"-  could you please explain the setting for the “gold standard” experiment. I'd have to assume, for instance, you train a GNN in a supervised way by using both source (non-suspicious) and target (suspicious) behaviour, and label accordingly? That said I am not 100% sure of this problem setting.",asp_substance,EXP,Experimental study not strong enough
"However, my concern is about the experiments.",asp_substance,EXP,Experimental study not strong enough
"- While the current experiments are a good start, I do not think they are extensive enough to count as strong evidence for the  power-law form of \(\epsilon(m,n)\).",asp_substance,EXP,Experimental study not strong enough
The paper’s primary drawback is the restrictive setting under which the experiments are performed.,asp_substance,EXP,Experimental study not strong enough
"Therefore, I am not convinced that the power-law form of the generalization error would hold when the experimental settings are marginally different (like when using the Adam optimizer or a VGG-like architecture).",asp_substance,EXP,Experimental study not strong enough
This needs to be changed: a) you should run all the baselines for each of the current tasks b) you should also expand the experiments evaluated to include tasks where it is not obvious that a hierarchy would help/is necessary c) you should include more baselines.,asp_substance,EXP,Experimental study not strong enough
Please run at least 10 experiments.,asp_substance,EXP,Experimental study not strong enough
The evaluation section lacks experiments that evaluate the computational savings.,asp_substance,EXP,Experimental study not strong enough
The authors could run each WAE training process K times (with random initialization) to see if the closed-form distance enables more stable results.,asp_substance,EXP,Experimental study not strong enough
Additional experiments on at least ImageNet would have made the paper stronger.,asp_substance,EXP,Experimental study not strong enough
"Specifically, since the explanations provided are visual saliency maps the paper would have been stronger if there were experiments suggesting -- what needs to change in a region of an image classified as a ‘cat’ to be classified as a ‘dog’ while there is an instance of the class - ‘dog’ present in the image itself.",asp_substance,EXP,Experimental study not strong enough
"However, the experiments are overall not very useful to the comprehension of the paper and not that illustrative.",asp_substance,EXP,Experimental study not strong enough
2.	The experiments are rather insufficient.,asp_substance,EXP,Experimental study not strong enough
"However, it seems the experiments do not seem to support this.",asp_substance,EXP,Experimental study not strong enough
"In fact, the separate training seems to make this unlikely.",asp_substance,EXP,Experimental study not strong enough
"I can understand the point that training NNLM accelerates the experiments, but the author(s) should consider trying a simply LSTM model after the best settings had been discovered (e.g. Table 1).",asp_substance,EXP,Experimental study not strong enough
I maintain my concerns that the experiments are limited and do not showcase the individual benefit of using explicit information placement.,asp_substance,EXP,Experimental study not strong enough
More experiments based on different transformations that the authors have mentioned would make this a stronger contribution.,asp_substance,EXP,Experimental study not strong enough
Thus the experiment comparison is not really fair.,asp_substance,EXP,Experimental study not strong enough
"A more rigorous setting is to, for N runs, each run splitting the validation and testing set differently.",asp_substance,EXP,Experimental study not strong enough
"Since this paper is an application paper, rather than a theoretical paper that bears theoretical findings, I would expect much more thorough experimental setup and analysis.",asp_substance,EXP,Experimental study not strong enough
2 The experimental settings are not reasonable.,asp_substance,EXP,Experimental study not strong enough
The current experimental settings are not matched with the practice environment.,asp_substance,EXP,Experimental study not strong enough
One set of experiments that I think would be interesting is aimed at understanding the distribution-matching part.,asp_substance,EXP,Experimental study not strong enough
"Section 3 too much redundancy -- it just explains that SVHN has a higher likelihood when trained on CIFAR, and a few variations of the same experiment.",asp_substance,EXP,Experimental study not strong enough
"Thus, the evidence of the experiments is not enough.",asp_substance,EXP,Experimental study not strong enough
"The experiment section needs significant improvement, especially when there is space left.",asp_substance,EXP,Experimental study not strong enough
The experiments of this paper lack comparisons to certified verification,asp_substance,EXP,Experimental study not strong enough
It might be beneficial to include comparison to this approach in the experimental section.,asp_substance,EXP,Experimental study not strong enough
3) The experiments are completely preliminary and not reasonable:,asp_substance,EXP,Experimental study not strong enough
These issues would maybe be excusable if not for the totally inadequate experimental validation.,asp_substance,EXP,Experimental study not strong enough
"1. The experiment uses a single run each of the baseline and DG-GAN, when it's well-known that GAN training runs",asp_substance,EXP,Experimental study not strong enough
3. It would be helpful to show the $\gamma$ value on each experiment with different tasks.,asp_substance,EXP,Experimental study not strong enough
The weight sharing was also needed further investigation and experimental data on sharing different parts.,asp_substance,EXP,Experimental study not strong enough
The rest experiments,asp_substance,EXP,Experimental study not strong enough
- The arguments for why the experimental evidence actually supports the existance of an approximate number system (ANS) could be made more clear.,asp_substance,EXP,Experimental study not strong enough
"- Experiments were conducted only using a few recent results as baselines (ICM, forward dynamics, RND).",asp_substance,EXP,Experimental study not strong enough
Section 2.2 mentioned how different kind of layers would help with the encoder’s utility and privacy. It would be better to back up the argument with some experiments.,asp_substance,EXP,Experimental study not strong enough
"- For the speed processing experiment, did you test what the performance would be if K matrix is replaced by a fully connected layer?",asp_substance,EXP,Experimental study not strong enough
- The setup for the learning to permute experiment is not as general as it would imply in the main text.,asp_substance,EXP,Experimental study not strong enough
- How easy is it to train with K matrices? Did you have to change optimizer hyperparameter compared to existing baselines?,asp_substance,EXP,Experimental study not strong enough
It looks like neither the experiments nor Theorem 3.2 show any benefit to PowerSGDM over PowerSGD.,asp_substance,EXP,Experimental study not strong enough
"(4) For the error-specific attack task, it would be better to provide an ablation experiment.",asp_substance,EXP,Experimental study not strong enough
*The experimental section is too limited.,asp_substance,EXP,Experimental study not strong enough
-I find the experimental studies a bit limited and I would expect larger studies which would have improve the interest of the paper.,asp_substance,EXP,Experimental study not strong enough
- (W5) Multi-task learning: I did not see any mention or experiments of what can be expected when the representations are themselves trained on multiple tasks.,asp_substance,EXP,Experimental study not strong enough
3) The simulation is not convincing.,asp_substance,EXP,Experimental study not strong enough
Experiments are on toy domains with very few goals and sub-task dependencies.,asp_substance,EXP,Experimental study not strong enough
"Authors acknowledge the fact that their experimental setup is rather limited in Appendix C.1, which I agree with and they also claim that there is a representation for a uniform algorithm for any number of advertisers for the AdWords problem, however they leave this as a future work, which I find unfortunate. I would recommend taking this direction rigorously and expand the contribution, which would prove to be a very sound contribution.",asp_substance,EXP,Experimental study not strong enough
"- at the start of section 3: what is an ""experiment""?",asp_substance,EXP,Experimental study not strong enough
"- in 3.1 towards the end of the first paragraph, what is a ""study"", is that the same as experiment or something different?",asp_substance,EXP,Experimental study not strong enough
It is hard to support this motivation when no experiments are done in its favor.,asp_substance,EXP,Experimental study not strong enough
"- The experiments show the learning results, but do not provide a peak ""under the hood"" to understand the way attention evolved and contributed to the results.",asp_substance,EXP,Experimental study not strong enough
The proposed diversity measuring metric is lacking both in terms of experimental proofs and intuitive motivations.,asp_substance,EXP,Experimental study not strong enough
1. Please provide experiments and/or citation for using the face identity as a proxy for face image diversity.,asp_substance,EXP,Experimental study not strong enough
this is important since all your experiments rely on that assumption.,asp_substance,EXP,Experimental study not strong enough
"1. In the experimental study, the authors present the value of their tuning parameters (learning rate, target rate, discount rate…) at the initialisation phase without any justifications. And the experiments are limited to simulated data obtained from MUJOCO physics engine - a very classical benchmark.",asp_substance,EXP,Experimental study not strong enough
- The claim made in the first line of the abstract (applying RL algorithms to continuous control problems often leads to bang-bang control),asp_substance,INT,"5) Last but not least, convergence analysis of the proposed method should be provided given that asynchrony may lead to divergence in the optimization."
"- Given the current form of the paper, the abstract and introduction should be modified to reflect the fact that only limited architectures and optimizers were experimented with, and the claims of the paper are not experimentally validated in general.",asp_substance,INT,"5) Last but not least, convergence analysis of the proposed method should be provided given that asynchrony may lead to divergence in the optimization."
"I vote for weak reject as (1) the paper makes a strong assumption about the availability of both the robot and object states which is not realistic in typical robotic manipulation applications and (2) the objective in the paper will not work if there is no notion of an “object” or object-state e.g.: this algorithm will not learn skills for a robot trying to control itself; hence, it is not truly a general purpose skill discovery algorithm but rather a skill discovery algorithm specifically meant for robot-object manipulation tasks.",asp_substance,MET,Incomplete details on perfromance of the method
"I am also not entirely satisfied by using accuracy as the only metric; how about using something like beam search to build a ""soft"", secondary metric?",asp_substance,MET,Incomplete details on perfromance of the method
"This appears to be very restrictive, since typically the values of time series j at time t-1 are typically depending say of those that time t-2, t-3 etc.",asp_substance,MET,Incomplete details on perfromance of the method
"While training on one image does reduce the burden of number of images, the computational burden remains the same. And as mentioned above, it doesn’t seem likely that *any* image would work for this method.",asp_substance,MET,Incomplete details on perfromance of the method
"Besides visible artifacts, what does it mean? Why does a smoother G reduces those artifacts?",asp_substance,MET,Incomplete details on perfromance of the method
"Additionally, the previously mentioned evaluation steps, i.e., using a fixed attention map as baseline for the evaluation and evaluating the correlation between FSM and accuracy may be informative to illustrate the advantages of the proposed approach.",asp_substance,MET,Incomplete details on perfromance of the method
"It does intuitively makes sense, but more mathematical definition (e.g., dimensionality) may be needed.",asp_substance,MET,Incomplete details on perfromance of the method
"If the goal is to learn representation for low-shot setting, the method needs to be compared with other representation learning methods such as jigsaw[1], colorization[2] and rotation[3].",asp_substance,MET,Incomplete details on perfromance of the method
"The drawback in this model here is that ultimately networks are embedded, but not really generated during test time.",asp_substance,MET,Incomplete details on perfromance of the method
- The proposed method does not outperform the existing state-of-the-art methods in terms of classification accuracy.,asp_substance,MET,Incomplete details on perfromance of the method
Including at least one set of black box attacks is necessary to verify to what degree the vanishing gradient is the case here.,asp_substance,MET,Incomplete details on perfromance of the method
"In addition, I would like to see some discussions whether this technique could be applied to off-policy learning as well.",asp_substance,MET,Incomplete details on perfromance of the method
"(3) FGSM is a quite weak adversarial attack method, which makes evaluating adversarial robustness on FGSM may be misleading.",asp_substance,MET,Incomplete details on perfromance of the method
"5. On relational detection task, is there a way to compare with the STOA method on some specific data split? This will leads to much more convincing results.",asp_substance,MET,Incomplete details on perfromance of the method
- the complexity of the proposed algorithm seems to be very high,asp_substance,MET,Incomplete details on perfromance of the method
"Beyond this simplification, I am not clear if that is actually intended by the authors.",asp_substance,MET,Incomplete details on perfromance of the method
The authors explain how they trained their own model but there is no mention on how they trained benchmark models.,asp_substance,MET,Incomplete details on perfromance of the method
It does not seem sensible to drop X^{j}_{t-1} + \eta_X \cdot \epsilon_X and attain a smaller value of the cost function at the same time.,asp_substance,MET,Incomplete details on perfromance of the method
"Moreover, due to the trace based loss function, the computational cost will also be very high.",asp_substance,MET,Incomplete details on perfromance of the method
3. How do PPO and SimPLe handle partial observability? Is it principled to apply them to partially-observable environments?,asp_substance,MET,Incomplete details on perfromance of the method
4. Why not use continuous actions with a parameterized policy (e.g. Gaussian)?,asp_substance,MET,Incomplete details on perfromance of the method
5. Is it reasonable to assume that the learning dynamics of all deep learning architectures are similar enough that a model trained on one set of deep learning architectures and problems will generalize to new architectures and problems?,asp_substance,MET,Incomplete details on perfromance of the method
- The evaluation of the proposed method is not complete.,asp_substance,MET,Incomplete details on perfromance of the method
In section 4.1 the authors have a good discussion on what is wrong with other methods in order to motivate their approach but then they don't deliver significant evidence in the later part of the section.,asp_substance,MET,Incomplete details on perfromance of the method
- evaluated models (feed-forward NNs and LSTMs) are very basic and far from current SotA architectures,asp_substance,MET,Incomplete details on perfromance of the method
"Furthermore PTB is not a ""challenging"" LM benchmark.",asp_substance,MET,Incomplete details on perfromance of the method
"Lemma 2.4, Point 1: The proof is confusing.",asp_substance,MET,Incomplete details on perfromance of the method
What is G_t in Theorem 2.5. It should be defined in the theorem itself.,asp_substance,MET,Incomplete details on perfromance of the method
"Thus, the theoretical contribution of this paper is limited.",asp_substance,MET,Incomplete details on perfromance of the method
1) The critical issue of this paper is that the algorithm is designed to minimize the upper bound.,asp_substance,MET,Incomplete details on perfromance of the method
It is better to explain the major difference and the motivation of updating the hidden states.,asp_substance,MET,Incomplete details on perfromance of the method
"Without doing so, it leaves the reader wondering why not simply a standard RBM trained using a standard method (e.g. contrastive divergence).",asp_substance,MET,Incomplete details on perfromance of the method
"It would be very interesting to contrast the proposed method with other previously proposed MF based method, in particular using Free energy to approximate the expectation of the model without constraints.",asp_substance,MET,Incomplete details on perfromance of the method
"For example, it is curious to see how denoising Auto encoders would perform.",asp_substance,MET,Incomplete details on perfromance of the method
"- The authors should make a distinction on what kinds of attack is considered: white box, black box or grey box.",asp_substance,MET,Incomplete details on perfromance of the method
"The problem with this strategy is that the rigid composition only works for actions that can be split into consecutive temporal parts with prefixed duration and anchor points in time, which is clearly challenged by many works later when more complicated video events are studied.",asp_substance,MET,Incomplete details on perfromance of the method
Weaknesses: Not all model modifications are studied in all the algorithmic tasks.,asp_substance,MET,Incomplete details on perfromance of the method
"In such a situation, there are better strategies for higher-order tests of independence than estimating mutual information, in particular estimator that give a control p-value.",asp_substance,MET,Incomplete details on perfromance of the method
Making this algorithm not very practical.,asp_substance,MET,Incomplete details on perfromance of the method
- I didn't understand the motivation for testing only very general-purpose models (this is described in Section 3).,asp_substance,MET,Incomplete details on perfromance of the method
"Edit (leaving everything else unchanged for now): After reading R3's assessment, I agree with them that it's worrying that the Deterministic Meta-Dropout performs better than baseline MAML - maybe it's an effect of a larger number of parameters in the model?",asp_substance,MET,Incomplete details on perfromance of the method
"How do we choose a proper beta, and will the algorithm be sensitive to beta?",asp_substance,MET,Incomplete details on perfromance of the method
"compared two schemes of this work, the ones with attentions are “almost” identical with ones",asp_substance,MET,Incomplete details on perfromance of the method
"in practice. For most of newly proposed graph embedding algorithms, it is hard to convince",asp_substance,MET,Incomplete details on perfromance of the method
* The oracle-augmented datasteam model needs to be contextualized better.,asp_substance,MET,Incomplete details on perfromance of the method
What is the benefit of using DL algorithms within the oracle-augmented datastream model?,asp_substance,MET,Incomplete details on perfromance of the method
Is a simple algorithm enough? What algorithms should we ideally use in practice?,asp_substance,MET,Incomplete details on perfromance of the method
What if you used simpler online learning algorithms with formal accuracy guarantees?,asp_substance,MET,Incomplete details on perfromance of the method
- My biggest issue is that there is no clear evaluation of the runtime benefit of the second Viterbi decompressor.,asp_substance,MET,Incomplete details on perfromance of the method
"The explanation for underperforming on SVHM (page 7) may be valid, but you could easily prove it right or wrong by adding an option to SST for ""stratified SSL."" Without this extra work, your claim is just a conjecture.",asp_substance,MET,Incomplete details on perfromance of the method
"All of these pieces are very well-known methods (e.g. VAEs, conditional VAEs, CL, catastrophic forgetting, domain transformation) in the literature and this paper puts them together in a straightforward way.",asp_substance,MET,Incomplete details on perfromance of the method
Why not compare with Sparsely-Gated MoE?,asp_substance,MET,Incomplete details on perfromance of the method
It would be nice to see how a “gold standard” HMC based inference does on at least the small toy problem of Sec 5.1?,asp_substance,MET,Incomplete details on perfromance of the method
Comparisons against these more standard inference algorithms is essential for understanding what advantages are afforded by the objective proposed in the paper.,asp_substance,MET,Incomplete details on perfromance of the method
Another concern is that the evaluation of domain adaptation does not have much varieties.,asp_substance,MET,Incomplete details on perfromance of the method
- there is no attempt to provide a theoretical insight into the performance of the algorithm,asp_substance,MET,Incomplete details on perfromance of the method
-  Why have you chosen the 4 operations at the bottom of page 4? It appears to be a subset of those used in DARTS.,asp_substance,MET,Incomplete details on perfromance of the method
- Consequently why did not you compare simple projected gradient method ?,asp_substance,MET,Incomplete details on perfromance of the method
"- I might have missed that, but are the authors offering an interpretation of their observation that the performance of the self-modulation model performs worse in the combination of spectral normalization and the SNDC architecture?",asp_substance,MET,Incomplete details on perfromance of the method
- I would like to see some more interpretation on why this method works.,asp_substance,MET,Incomplete details on perfromance of the method
I would like to suggest to use the state-of-the-arts classifier for the principal task and to evaluate how much gain your method can get with the help of auxiliary tasks.,asp_substance,MET,Incomplete details on perfromance of the method
"- If the information on the hierarchy of sub-categories is not available, it will be an annoying hyperparameters that should be well tuned.",asp_substance,MET,Incomplete details on perfromance of the method
"1. Since the method is only evaluated on several video sequences, I am not sure how the method will perform on other different scenes.",asp_substance,MET,Incomplete details on perfromance of the method
I wonder how the straightforward regression term plus the smooth term will perform for the mask.,asp_substance,MET,Incomplete details on perfromance of the method
"Here, the straightforward regression term means directly regress the output mask to the target densepose mask. Will the proposed mask term perform better?",asp_substance,MET,Incomplete details on perfromance of the method
This reduces the paper to an empirical exercise rather than a true understanding of their method's advantages and limitations.,asp_substance,MET,Incomplete details on perfromance of the method
"In general, I feel this section could use some tighter formalism and justifications.",asp_substance,MET,Incomplete details on perfromance of the method
Are there any other advantages for F-pooling s.t. people might want to use it as opposed to AA-pooling?,asp_substance,MET,Incomplete details on perfromance of the method
"- also, if the authors can explain more on sec 2.5 it will be helpful. If we simply ignore the imaginary part, although the theory is not applicable, but what would the empirical performance be?",asp_substance,MET,Incomplete details on perfromance of the method
"The method is only evaluated on a single task and many confounding variables (the design of the reward function, factorizing the parametric distribution into marginals, reporting results for a single (non-tuned?) learning rate, etc.) make evaluation difficult.",asp_substance,MET,Incomplete details on perfromance of the method
It’s not obvious to me that ME bias will provide a significant advantage in a one-shot or few-shot learning setting - i.e. how useful is having made a better initial guess (which is random amongst the unseen classes) after you gain some specific information about the target class? Perhaps the benefit is small relative to that of the one-shot or few-shot information gained - I think this needs to be quantified empirically.,asp_substance,MET,Incomplete details on perfromance of the method
"Compared to using just a very simple dense reward (e.g. negative L2 distance between the robot and the object), what would be the advantage of using MI discriminator? It would be great to show the comparison between using simple dense reward and MI discriminator for each Push, Pick&Place, and Slide task.",asp_substance,MET,Incomplete details on perfromance of the method
"1. The focus of this paper is the hyper-parameter, please focus and explain more on the usage with hyper-parameters.",asp_substance,MET,Incomplete details on perfromance of the method
"It is known that SGD with fixed step-size can not find the optimal for convex (perhaps, also simple) problems.",asp_substance,MET,Incomplete details on perfromance of the method
It is thus unclear whether the advantage can be maintained after applying these standard regularsisers.,asp_substance,MET,Incomplete details on perfromance of the method
What happen for Meta-RevGrad + idt or Meta-RevGrad + revMap?,asp_substance,MET,Incomplete details on perfromance of the method
"i.e., omniglot <-> omniglot-M. I wonder whether the proposed approach can still work in large domain shift such as omniglot to fashion-mnist etc.",asp_substance,MET,Incomplete details on perfromance of the method
"In terms of actual technical contributions, I believe much less significant.",asp_substance,MET,Incomplete details on perfromance of the method
"It seems that the discriminator takes a whole sequence as input, but some precision on how this done could be added.",asp_substance,MET,Incomplete details on perfromance of the method
There is however a misunderstanding about the properties of the alt-az convolution that must be cleared up before this paper can be published.,asp_substance,MET,Incomplete details on perfromance of the method
"As noted in the paper, this is a general element of SO(3) (and hence not in the set of alt-az rotations)",asp_substance,MET,Incomplete details on perfromance of the method
So the closure axiom of a group is violated.,asp_substance,MET,Incomplete details on perfromance of the method
"This matters, because the notion of equivariance really only makes sense for a group.",asp_substance,MET,Incomplete details on perfromance of the method
"The paper does not contain an attempted proof of equivariance, and if one tries to give one, the impossibility of doing so will become apparent.",asp_substance,MET,Incomplete details on perfromance of the method
I note that the alt-az convolution *is* equivariant to rotations in the subgroup SO(2) of rotations around the Z-axis.,asp_substance,MET,Incomplete details on perfromance of the method
"This is ultimately due to the fact that the set of alt-az rotations is not the same as the set of points on the sphere, topologically speaking.",asp_substance,MET,Incomplete details on perfromance of the method
3. Can the authors show if the decomposition is also useful for trigger patterns that are not necessarily regular shapes?,asp_substance,MET,Incomplete details on perfromance of the method
"It should be pointed out that the D in the hinge loss represents a neural network output without range restriction, while the D in the non-saturating loss represents sigmoid output, limiting to take in [0,1].",asp_substance,MET,Incomplete details on perfromance of the method
"Though fundamental understanding can happen asynchronously, I reserve my concern that such empirical method is not substantial enough to motivate acceptance in ICLR, especially considering that in (only) 124/144 (86%) of the studied settings, the results are improved. And there is no analysis of the failure settings.",asp_substance,MET,Incomplete details on perfromance of the method
"In what concerns the optimization, the method achieves a better objective value much faster, confirming that it is a lower variance gradient estimator.",asp_substance,MET,Incomplete details on perfromance of the method
Theorem 1 does not take account for the above conditions.,asp_substance,MET,Incomplete details on perfromance of the method
I have some concerns regarding the presentation of the main objective and the lack of justification in certain parts of the methodology.,asp_substance,MET,Incomplete details on perfromance of the method
"While I agree with the authors that it is generally desirable for a model to be more confident when predicting in MNIST (since it has already seen samples of it) compared to when predicting in notMNIST (completely different data), these plots tells us nothing regarding the predictive power of the model.",asp_substance,MET,Incomplete details on perfromance of the method
"There isn't a tested hypothesis, but rather it's a feat of engineering to get this to work.",asp_substance,MET,Incomplete details on perfromance of the method
"This restriction of the number of parameters to be small is only mentioned in the theorem itself, not in the discussion of its implications.",asp_substance,MET,Incomplete details on perfromance of the method
Did you try to have a single network?,asp_substance,MET,Incomplete details on perfromance of the method
"6. Did you consider using an inverse function (say G), that maps an embedding in L / L' back to S (i.e. the inverse function of gamma / gamma').",asp_substance,MET,Incomplete details on perfromance of the method
"Also, G could be used to check whether you obtain the expected formula after 4 steps, which would be a more informative information than the L2 distance between the resulting embedding and the embedding of the final formula.",asp_substance,MET,Incomplete details on perfromance of the method
"The Gamma approximation has no statistical guarantees, as stated explicitly in the HSIC testing paper of NeurIPS 2007.",asp_substance,MET,Incomplete details on perfromance of the method
"The improved training procedures for MI estimation are of interest, however the hypothesis testing parts of the paper could still be improved.",asp_substance,MET,Incomplete details on perfromance of the method
I wonder why the authors didn’t compare or mention optimizers such as ADAM and ADAGRAD which adapt their parameters on-the-fly as well.,asp_substance,MET,Incomplete details on perfromance of the method
I am concerned though why the authors didn’t compare to adaptive optimizers such as ADAM and ADAGRAD and how the performance compares with population based training techniques.,asp_substance,MET,Incomplete details on perfromance of the method
The drawbacks  of the work include the following: (1) There is not much technical contribution.,asp_substance,MET,Incomplete details on perfromance of the method
"When learning rates are in focus, a convincing message would be to show that adaptation of learning rates is more efficient and simpler than their scheduling when tested on state-of-the-art architectures.",asp_substance,MET,Incomplete details on perfromance of the method
It would be interesting to see if the proposed method is competitive for training contemporary networks and w.r.t. simple schedule schemes.,asp_substance,MET,Incomplete details on perfromance of the method
"Although the concept of ""task"" is not explicitly defined in this paper, the authors seem to associate each task with a specific class.",asp_substance,MET,Incomplete details on perfromance of the method
"The KL divergence regularization introduces extra randomness to the auxiliary labels and thus mitigates the problem, but it hardly provides any useful information except randomness.",asp_substance,MET,Incomplete details on perfromance of the method
"Since this paper is about distributed robustness and distributed attacks, it would be very informative to the community to illustrate DBA attack on these methods to have a more compelling message.",asp_substance,MET,Incomplete details on perfromance of the method
"In this case, the paper proves that no careful selection of the learning rate is necessary.",asp_substance,MET,Incomplete details on perfromance of the method
The use of Glorot uniform initializer is somewhat subtle.,asp_substance,MET,Incomplete details on perfromance of the method
For noisy K-FAC and MNF the lambda (which should be fixed to 1 for a correct Bayesian model) was tuned and in general lower than 1 lambdas lead to models that are overconfident and hence underperform on uncertainty tasks.,asp_substance,MET,Incomplete details on perfromance of the method
"Thus I believe that it would be better if you consider the same hyper parameter on all of these methods, e.g. the precision of the Gaussian prior.",asp_substance,MET,Incomplete details on perfromance of the method
It’s probably a bit unfair or misleading to claim neural networks suffering from ME bias.,asp_substance,MET,Incomplete details on perfromance of the method
- what prior distributions p(z) and p(u) are used? What is the choice based on?,asp_substance,MET,Incomplete details on perfromance of the method
"- The model G_theta does not appear in the training objective function (4), how is this module trained precisely?",asp_substance,MET,Incomplete details on perfromance of the method
"Please comment on the choice, and its impact on the behavior of the model.",asp_substance,MET,Incomplete details on perfromance of the method
"Overall, the proposed method is elegant; however, the presentation, the claim, and the experiments suffer from significant flaws.",asp_substance,MET,Incomplete details on perfromance of the method
"I like the directions using surrogate to speed up HPO in general but I feel the learning curve prediction part can be improved. There are already some works, not using deep learning method, for example the following:",asp_substance,MET,Incomplete details on perfromance of the method
"Why these methods are not considered in the beginning? In my opinion, transformer is good for modeling long term dependency and concurrent predictions which is not necessarily the case for learning curves.",asp_substance,MET,Incomplete details on perfromance of the method
How does the transformer based method comparing to others?,asp_substance,MET,Incomplete details on perfromance of the method
"Why, for example, did the authors only perform variational inference with the current and previous frames? Did conditioning on additional frames offer limited further improvement? Can the blurriness instead be attributable to the weak inference model?",asp_substance,MET,Incomplete details on perfromance of the method
"- Theorem 3.2: ""[...] converges at a speed proportional to [...]"". Isn't \bar{u}_t logarithmic (non-linear) in t?",asp_substance,MET,Incomplete details on perfromance of the method
1. The proxy f(z) does not bear any resemblance to LP(z).,asp_substance,MET,Incomplete details on perfromance of the method
2. Can you elaborate more on the metric for measuring the learning progress LP? Why does the myopic metric make sense in spite of the there being plateaus in the training curves?,asp_substance,MET,Incomplete details on perfromance of the method
"Given there is no theoretical justification for the approximation, I believe the paper claims more than what it delivers and should change the presentation, so as not to claim that it is measuring and capturing learning progress to learn faster.",asp_substance,MET,Incomplete details on perfromance of the method
"This is a lot of architectures, which makes me wonder: how would a “cost-aware” random sampling perform with the same number of sampled architectures?",asp_substance,MET,Incomplete details on perfromance of the method
"However, there are significant limitations in demonstrating the effectiveness/impact of the proposed technique:",asp_substance,MET,Incomplete details on perfromance of the method
"As far as I understand, this involves only minor changes in the code since reasonable hyperparameters required for the convergence of Adam have been extensively studied.",asp_substance,MET,Incomplete details on perfromance of the method
- The use of random exploration for the discoverer is underwhelming. Have you tried different approaches? Would more advanced exploration techniques work or improve the performance?,asp_substance,MET,Incomplete details on perfromance of the method
"Otherwise, this choice is incomprehensible.",asp_substance,MET,Incomplete details on perfromance of the method
"It’s also unclear if the proposed method is more memory efficient, since the authors only unroll 4 iterations of it.",asp_substance,MET,Incomplete details on perfromance of the method
I would strongly recommend including the computational cost of each method in the evaluation section.,asp_substance,MET,Incomplete details on perfromance of the method
"On the other hand, the authors give no evidence, empirical or otherwise, that their method is useful on any downstream tasks.",asp_substance,MET,Incomplete details on perfromance of the method
"Plugging this distance into the WAE produces similar performance to existing WAE variants, but does not really advance the existing achievable performance.",asp_substance,MET,Incomplete details on perfromance of the method
"Since the novelty of this work lies in the introduction of the CW distance, I would like to see an independent evaluation of this distance as a  general statistical distance measure (independently of its use in CWAE).",asp_substance,MET,Incomplete details on perfromance of the method
Can you use this distance as a multivariate-Gaussian goodness of fit measure for high-dimensional data drawn from both Gaussian and non-Gaussian distributions and show that it actually outperforms other standard statistical distances (e.g. in two-sample testing power)?,asp_substance,MET,Incomplete details on perfromance of the method
"Besides having closed form in the case of a Gaussian prior (which other statistical distances could potentially also achieve), it would be nice to see some discussion of why the authors believe their CW-distance is conceptually superior to such alternatives.",asp_substance,MET,Incomplete details on perfromance of the method
My main question is that: what is the main advantage of the Cramer-Wold distance to an MMD with a proper kernel?,asp_substance,MET,Incomplete details on perfromance of the method
"My question here is that how is this method better than the standard VAE, where we also have an analytic form for the ELBO when the prior is Gaussian, an no sampling is required.",asp_substance,MET,Incomplete details on perfromance of the method
"Indeed, in VAEs, the prior does not have to be Gaussian, and as long as the density of the prior can be evaluated, we can efficiently optimize the ELBO without sampling the prior; which I don't think is the case for the Cramer-Wold autoencoder.",asp_substance,MET,Incomplete details on perfromance of the method
"- In the case of the search space II, how many GPU days does the proposed method require?",asp_substance,MET,Incomplete details on perfromance of the method
"- About line 10 in Algorithm 1, how does the proposed method update the population P? Please elaborate on this procedure.",asp_substance,MET,Incomplete details on perfromance of the method
"Obviously there is a tradeoff in terms of memory usage, privacy, performance, etc. but since none of these methods currently achieve the best across all of these there is no reason to rule out any of the methods.",asp_substance,MET,Incomplete details on perfromance of the method
This suggests that the strictly incremental training methodology indeed forced the network to learn better generalizations,asp_substance,MET,Incomplete details on perfromance of the method
The proposed approach seems reasonable but it is mostly a work of engineering and provides little insights into the problem nor the proposed model.,asp_substance,MET,Incomplete details on perfromance of the method
- It is a bit unclear to me how the authors propose to obtain independent posteriors over z and c. Is it purely empirical or is there a formal reason that guarantees it?,asp_substance,MET,Incomplete details on perfromance of the method
The structure is obtained by the shared and sparse rows of matrix A. I would like the authors to comment on how the studies will be affected by this property of the common networks.,asp_substance,MET,Incomplete details on perfromance of the method
- the reasoning behind picking VGMM as the density estimator is not fully convincing and (dis)advantages of other candidate density estimators are almost not highlighted.,asp_substance,MET,Incomplete details on perfromance of the method
- it is unclear and possibly could be better explained why one needs to concatenate the goals (what would change if we would not concatenate but estimate state densities rather than trajectories?),asp_substance,MET,Incomplete details on perfromance of the method
"Even if the assumptions aren't satisfied everywhere for typical deep networks, they may be satisfied at most points encountered during training, which would make the contribution even more compelling.",asp_substance,MET,Incomplete details on perfromance of the method
"It seems like LEMONADE would scale poorly to more than 2 objectives, since it effectively requires approximating an #objectves-1 dimensional surface with the population of parents.",asp_substance,MET,Incomplete details on perfromance of the method
- The authors haven't come up with a recommendation for a single configuration of their approach.,asp_substance,MET,Incomplete details on perfromance of the method
"Also, the plus-one smoothing handles unknown words (or word piece?) and I'm not sure why. If we're using the test set to compute IDF, and the sentences we're looking *are* in the test set, then there shouldn't be unknown words and no smoothing is required.",asp_substance,MET,Incomplete details on perfromance of the method
"1. In Section 2, I find the sentence ""We follow a similar intuition but instead of decomposing a 1D signal of time into its components, we transform the time itself and feed its transformation into the model that is to consume the time information"" really unclear. Could you rephrase it?",asp_substance,MET,Incomplete details on perfromance of the method
The paper shows two applications (semi-supervised learning and novelty detection) and it is not clear that the proposed method outperforms existing GAN methods in the classification accuracy in MNIST/SVHN/CIFAR10 (Table 1) and existing sampling methods (Table. 3).,asp_substance,MET,Incomplete details on perfromance of the method
"Apart from combining these to existing ideas, what can be considered as an added novelty to improve the quality of the disentangled features?",asp_substance,MET,Incomplete details on perfromance of the method
- All the variations considered for the behaviour policy performs only myopic exploration and thus provably inefficient in RL.,asp_substance,MET,Incomplete details on perfromance of the method
How this proxy incentives the agent to explore poorly-understood regions?,asp_substance,MET,Incomplete details on perfromance of the method
"6. On a lighter note, I don't believe using a coffe-brewing machine has a 'universally invariant structure' of coffee-making. That's a luxurious way of making coffee :) In the developing world, we still need to boil water, pour coffee powder in it, etc., all without a coffee-brewing machine.",asp_substance,MET,Incomplete details on perfromance of the method
"Similarly, the paper mentions that the ""general approach to solving these combinatorial optimization problems is to recognize the atomic unit necessary to solve the problem"", but at that point the reader has no concrete example of what combinatorial optimization problem would be mapped onto training and inference in RBMS.",asp_substance,MET,Incomplete details on perfromance of the method
"This is a very strong initial assumption, I am not sure how likely this assumption would be satisfied.",asp_substance,MET,Incomplete details on perfromance of the method
"For instance, how deep should a model be for a classification or regression task?",asp_substance,MET,Incomplete details on perfromance of the method
"Repeated equations usually indicate that there is something new happening, but all of these are just restatements of your theta sin(omega tau + phi) term.",asp_substance,MET,Incomplete details on perfromance of the method
"I would introduce the notation for t2v(tau) upfront and use that to define a(tau, k)[j] and f_j",asp_substance,MET,Incomplete details on perfromance of the method
What was wrong with LSTMs/GRUs as they've been used extensively for recursive problems including operations on trees?,asp_substance,MET,Incomplete details on perfromance of the method
2) The evaluation metrics used while borrowed from the language or IR fields doesn't seem to translate to UI design.,asp_substance,MET,Incomplete details on perfromance of the method
I do not think that there is enough justification/demonstration for the fact that a general NN solution for Tabular Data invented.,asp_substance,MET,Incomplete details on perfromance of the method
It might be more fruitful if these linear combinations were learned or sub-senses of words (e.g. [1]).,asp_substance,MET,Incomplete details on perfromance of the method
- Why does temporal correlation reduce the non-stationarity of the MARL problem?,asp_substance,MET,Incomplete details on perfromance of the method
- Why does structured exploration reduce the number of network parameters that need to be learned?,asp_substance,MET,Incomplete details on perfromance of the method
- Why does partial parameter sharing make it easier for each agent to estimate other agents potential changes in behavior?,asp_substance,MET,Incomplete details on perfromance of the method
"- This approach seems very limited, as there must exist a known transformation that removes the desired information.",asp_substance,MET,Incomplete details on perfromance of the method
- Can this approach learn multiple factors as opposed to just two?,asp_substance,MET,Incomplete details on perfromance of the method
- What if the desired factors are not clearly disjoint and collectively exhaustive? (e.g. mustache vs. gender on human faces.),asp_substance,MET,Incomplete details on perfromance of the method
Can the proposed approach perform just as well without a modified objective?,asp_substance,MET,Incomplete details on perfromance of the method
Ablation studies that show the proposed algorithm can improve upon the baseline in all settings would make this a stronger paper.,asp_substance,MET,Incomplete details on perfromance of the method
"The use of beta>1 is fine if it helps alongside the use of this approach, but it would have been better to see the effects of this approach and beta>1 (and other hyperparameters such as k in Table 1) in isolation.",asp_substance,MET,Incomplete details on perfromance of the method
"Also, what will be the performance of a standard image captioning system on the task ?",asp_substance,MET,Incomplete details on perfromance of the method
"Also, the compared methods don’t really use the validation set from the complex data for training at all.",asp_substance,MET,Incomplete details on perfromance of the method
"Currently it is still missing discussion such as, when SASNet would perform better and when it would perform worse, what it is that the state of the art features can’t capture while SASNet can.",asp_substance,MET,Incomplete details on perfromance of the method
"Moreover, it is the prediction performance that matters to such task, but the authors remove the non-structure features from the compared methods.",asp_substance,MET,Incomplete details on perfromance of the method
3)	Some discussion on why the “SASNet ensemble” would yield better performance would be good; could it be overfitting?,asp_substance,MET,Incomplete details on perfromance of the method
"Section 4 seems to lack a high-level idea of what it want to prove -- the hypothesis around the volume term is dismissed shortly after, and it ultimately proves that we do not know what is the reason behind the high SVHN likelihood, making it look like a distracting side-experiment.",asp_substance,MET,Incomplete details on perfromance of the method
This is clearly interested in as far as solving this particular task but does not provide any general insights for the design of (MA)RL algorithms.,asp_substance,MET,Incomplete details on perfromance of the method
"If we define both maximum and minimum thresholds based on the CIFAR-train histogram, it seems like one could detect most SVHN samples just by the virtue that there likelihoods are much higher than even the max threshold determined by the CIFAR-train histogram?",asp_substance,MET,Incomplete details on perfromance of the method
"However, the Pareto front of the proposed method is concentrated on a specific point.",asp_substance,MET,Incomplete details on perfromance of the method
"For example, the proposed method does not achieve high ""privacy"" as ""noisy"" does.",asp_substance,MET,Incomplete details on perfromance of the method
"In this sense, the proposed method is not comparable with ""noisy"".",asp_substance,MET,Incomplete details on perfromance of the method
2) Does the proposed method store immediate activations or recompute the activations in the backward pass?,asp_substance,MET,Incomplete details on perfromance of the method
The authors should try to find a real-world domain which can really demonstrate the effectiveness of the method.,asp_substance,MET,Incomplete details on perfromance of the method
(3) The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks.,asp_substance,MET,Incomplete details on perfromance of the method
"and bounded failure rate, otherwise it is not really a verification method.",asp_substance,MET,Incomplete details on perfromance of the method
methods. There are some scalable property verification methods that can give a,asp_substance,MET,Incomplete details on perfromance of the method
Issues of convergence seem like they deserve some discussion here and potentially could be examined empirically (is CEM-TD3 converging in the swimmer?).,asp_substance,MET,Incomplete details on perfromance of the method
"However, the evaluation of the proposed adaptive kernels is rather limited.",asp_substance,MET,Incomplete details on perfromance of the method
How big is the generalization gap for the tested models when adaptive kernel is used?,asp_substance,MET,Incomplete details on perfromance of the method
"5. Adaptive kernels have only been tested in the first convolutional layer, would the adaptive kernels work well also in different layers?",asp_substance,MET,Incomplete details on perfromance of the method
"In Section 3, the authors compare the game scores of DeepCS proposed in this paper only against to A2C.",asp_substance,MET,Incomplete details on perfromance of the method
"3. The architecture of the neural networks used for the Generator and Discriminator is very non-standard, which",asp_substance,MET,Incomplete details on perfromance of the method
I would like to see a convergence proof where the batch size $B_{t}$ is treated as a small constant like other SGD proofs assume.,asp_substance,MET,Incomplete details on perfromance of the method
It would be good to know how $\gamma$ varies across tasks.,asp_substance,MET,Incomplete details on perfromance of the method
"As proposed in the comments, this should be assessed in the paper by replacing BERT representations by non-contextual representations such as GloVE.",asp_substance,MET,Incomplete details on perfromance of the method
"Since this is for ILCR, I think the authors should have taken a deeper dive into examining those latent representations and potentially visualizing those distances and how they correspond to different policy behaviors.",asp_substance,MET,Incomplete details on perfromance of the method
"The proposed dual-BCFW still contains a hyperparameter (eta) due to the need to introduce a convex subproblem, which makes its number of hyperparameters still the same to SGD.",asp_substance,MET,Incomplete details on perfromance of the method
-- The algorithm proposed in the paper is very straightforward and just removes heavy hitters using oracle advice and then hashes everything else using the standard CountMin sketch.,asp_substance,MET,Incomplete details on perfromance of the method
What would the pairing-based strategy look like in Figure 6 right? Are there not significance tests that could be used to more carefully quantify the level of support for the two alternative strategies?,asp_substance,MET,Incomplete details on perfromance of the method
"1. The formulation uses REINFORCE, which is often known with high variance.",asp_substance,MET,Incomplete details on perfromance of the method
I think it needs to be made clearer how reconstruction error works as a measure of privacy.,asp_substance,MET,Incomplete details on perfromance of the method
"Language modeling is a fast-moving field, so the very latest and greatest techniques are not strictly necessary for this paper, but at least midsize LSTM models that get scores in the ~80 ppl range for Penn Treebank are important, otherwise it becomes very questionable whether the results will provide any practical impact in today's best models.",asp_substance,MET,Incomplete details on perfromance of the method
"As noted by the authors, transfer is most attractive in low-supervision regimes, w.r.t. the target task.",asp_substance,MET,Incomplete details on perfromance of the method
Yet the metrics proposed depend on supervision in the target domain.,asp_substance,MET,Incomplete details on perfromance of the method
- no downstream applications are given which show that these higher order interactions can be useful for downstream tasks.,asp_substance,MET,Incomplete details on perfromance of the method
"- the higher-order features T*a*b are useful only when a is noun and b is an adjective: why not investigate using T to model higher-order interaction for all (a,b) pairs regardless of the syntactic relationships between a and b?",asp_substance,MET,Incomplete details on perfromance of the method
More importantly: (1) It does not show any advantage of replica exchange over standard dynamics; (2) It does not provide any quantitative insight for high-dimensional problems.,asp_substance,MET,Incomplete details on perfromance of the method
"They are an exercise in applying standard formalism to this problem, without really showing any significative advantage of replica exchange.",asp_substance,MET,Incomplete details on perfromance of the method
"As of now, the greedy nature of the algorithm is hidden across a number of sections (not introduced when presenting the main algorithm).",asp_substance,MET,Incomplete details on perfromance of the method
3. The structure of the meta-training loop was unclear to me.,asp_substance,MET,Incomplete details on perfromance of the method
This seems like a limitation of the method if this is the case.,asp_substance,MET,Incomplete details on perfromance of the method
"The WGAN framework is built upon a loss that can be optimized, and should be optimized, until convergence (the discriminator loss is non-saturating) -- not the reverse (more G steps than D steps) as suggested here.",asp_substance,MET,Incomplete details on perfromance of the method
"For example, authors could implement pGAN by ignoring the detectability of the discriminator (i.e. \alpha=0) or typical pGAN when they compare with the label-flip operation.",asp_substance,MET,Incomplete details on perfromance of the method
"However, the authors do not provide an in-depth discussion of this phenomena.",asp_substance,MET,Incomplete details on perfromance of the method
"Given that previous works [1,2,3] have successfully addressed this problem using a completely unsupervised approach, it would be necessary to give more insights about: (i) why the proposed method is failing (ii) why this negative result is interesting and (iii) if the method could be useful in other potential scenarios.",asp_substance,MET,Incomplete details on perfromance of the method
The authors could provide a similar evaluation for their method by using the feature representations learned by the siamese networks in order to evaluate how much information they convey about real factors of variation.,asp_substance,MET,Incomplete details on perfromance of the method
"This does not seem surprising to me, as in the unguided case, the constrative loss seems not strong enough to encourage the latent partitions to be different.",asp_substance,MET,Incomplete details on perfromance of the method
"Further, no proper convergence analysis of the proposed approach is provided and is desired due to the likely divergence in the optimization.",asp_substance,MET,Incomplete details on perfromance of the method
1. Can you comment on the quantization time of the suggested method? Repeatedly solving the EM steps can add up to quite an overhead.,asp_substance,MET,Incomplete details on perfromance of the method
* The BiLSTM they use is very small (embedding and hidden dimension 50).,asp_substance,MET,Incomplete details on perfromance of the method
"While it seems ok to estimate these different metrics using only linear models, my concern with this is that the linear models are only given a subset of the **exact** dimensions of the original representations.",asp_substance,MET,Incomplete details on perfromance of the method
This is very much unlike the learning objectives of most of these representation learning methods and hence is highly biased and dependent on the actual methods and the random seeds used and the rotations it performs. (In many cases the representations are used starting with a fully connected layer bottom layer on top of the representations and hence rotations of the representations do not affect performance),asp_substance,MET,Incomplete details on perfromance of the method
Suppose we rotated these representations. Now the signal from the original dimension is split across multiple dimensions and hence the CFS may be deceivingly high.,asp_substance,MET,Incomplete details on perfromance of the method
To me this is a big concern as different runs of the same representation technique can likely have very different CFS scores based on initializations and random seeds.,asp_substance,MET,Incomplete details on perfromance of the method
(W7) Alternatives to CFS / Computational concerns: A big concern I had was the computational expense of the proposed approach. Unfortunately I did not see any discussion about this in the paper or empirically.,asp_substance,MET,Incomplete details on perfromance of the method
For example using LASSO / LARS like methods you can perhaps figure out a good reduced dimension set more efficiently.,asp_substance,MET,Incomplete details on perfromance of the method
(W7b) Likewise I wonder if we could just measure transfer more directly as well and why we need to go via these CFS sets,asp_substance,MET,Incomplete details on perfromance of the method
"If the paper cannot compare various architecture, it is more convincing to at least use some standard architecture, like DCGAN. Or at least report the parameter tuning effort made for getting the results.",asp_substance,MET,Incomplete details on perfromance of the method
My question is a bit philosophical: The only thing which I was concerned about when reading the paper is projection of the embeddings back to the d-dimensional space.,asp_substance,MET,Incomplete details on perfromance of the method
have you thought about how it would be possible to avoid this step and keep the original variable-size embeddings?,asp_substance,MET,Incomplete details on perfromance of the method
"The mode of language modeling evaluation presented here, without considering an actual language or speech processing task, provides limited insight w.r.t. its utility in actual applications.",asp_substance,MET,Incomplete details on perfromance of the method
"Moreover, the very limited size of the language modeling tasks chosen here is highly advantageous for smoothing/regularization approaches.",asp_substance,MET,Incomplete details on perfromance of the method
Authors should scope the paper to the specific function family these networks can approximate.,asp_substance,MET,Incomplete details on perfromance of the method
"However, the function of interest is limited to a small family of affine equivariant transformations.",asp_substance,MET,Incomplete details on perfromance of the method
"Authors claim adding one layer of DeepSet layer to a PointNet becomes PointNetST, but I see this as a special DeepSet with a single transmission layer.",asp_substance,MET,Incomplete details on perfromance of the method
Lemma 3 is too trivial.,asp_substance,MET,Incomplete details on perfromance of the method
"However, it is not further discussed how the overall KL-based data similarity measure would help in this case since it seems likely that it would also exhibit the same issue.",asp_substance,MET,Incomplete details on perfromance of the method
"- In the empirical results, the performance of the proposed method and Reg-GAN (from the numerics of GAN paper) are quite similar. Are there instances that the proposed approach significantly improves the stability of practical GAN architectures?",asp_substance,MET,Incomplete details on perfromance of the method
"-Is there any reason not comparing the proposed algorithm with other compression schemes? (e.g., network pruning and low-rank approximation)",asp_substance,MET,Incomplete details on perfromance of the method
o feedforward rather than recurrent network;,asp_substance,MET,Incomplete details on perfromance of the method
"The authors use numerous jargon words to describe the techniques studied (e.g. dragon penalty, gradient penalty, spectral normalization, Gaussian process regression in the bandit setting) but they do not explain them,",asp_substance,MET,Incomplete details on perfromance of the method
"Those seem to be quite limiting features of these methods, which is not to say that they are not useful in that realm, but only to clarify my understanding of their possible scope of application.",asp_substance,MET,Incomplete details on perfromance of the method
Although a generic framework that learns to solve online combinatorial optimization problems without domain knowledge is by itself a very motivational goal neither the paper successfully demonstrates that the framework the authors propose achieves this goal nor it explains well enough why one would take the machine learning approach to find good algorithms to such problems.,asp_substance,MET,Incomplete details on perfromance of the method
"This paper should be rejected because proposed method demonstrates that an instance of one class of problems, Fractional Adwords, can be learned to solve without domain expertise, however fails to prove that the approach would be beneficial for any other instances of the same problem.",asp_substance,MET,Incomplete details on perfromance of the method
In particular there is not any theoretical not experimental evidence that the approach would scale to any instances where a pure optimization approach would be slow to provide any meaningful solutions.,asp_substance,MET,Incomplete details on perfromance of the method
"Can one walk the latent distribution of the algorithm agent and draw insights, which might lead into tailoring some algorithms that would be appropriate for some input distribution although in general inferior in terms of worst case guarantees?",asp_substance,MET,Incomplete details on perfromance of the method
2. The main technical contribution claim needs to be elaborated.,asp_substance,MET,Incomplete details on perfromance of the method
I understand how the game theoretic framework is established but how does this manifest itself in the algorithm described in Section 3.1 needs more explanation.,asp_substance,MET,Incomplete details on perfromance of the method
They need to elaborate how their method overcomes these issues better.,asp_substance,MET,Incomplete details on perfromance of the method
Yet their approach is only able to solve the fractional version of the AdWords problem.,asp_substance,MET,Incomplete details on perfromance of the method
"Yet the authors stop at only solving this version with a machine learning approach, which does not hit the bar for me.",asp_substance,MET,Incomplete details on perfromance of the method
I would have expected the authors to at least elaborate on why the current framework is not suitable for the non-relaxed problem.,asp_substance,MET,Incomplete details on perfromance of the method
"However, they state they loosely follow this approach. What does that entail? What kind of theoretical guarantees are given up due to not following this, a better exposition on this topic would help to support the claims.",asp_substance,MET,Incomplete details on perfromance of the method
One would at least expect an after-the-fact interpretation for the weighted tensor term and what this implies with regard to their method and syntactic embedding compositions in general.,asp_substance,MET,Incomplete details on perfromance of the method
"I feel that this discussion devolved into a discussion, again, about normalization rather than the architectural differences in performance.",asp_substance,MET,Incomplete details on perfromance of the method
"- MAAC still requires all observations and actions of all other agents as an input to the value function, which makes this approach not scalable to settings with many agents.",asp_substance,MET,Incomplete details on perfromance of the method
"- The centralized nature is also semantically improbable, as the observations might be high-dimensional in nature, so exchanging these between agents becomes impractical with complex problems.",asp_substance,MET,Incomplete details on perfromance of the method
"One main concern is, how general this approach would be? As it is a good extension for Neural LP, it is not clear that the framework of Neural LP is flexible or powerful enough in general.",asp_substance,MET,Incomplete details on perfromance of the method
"For example, if rules contain quantifiers, how would this be extended?",asp_substance,MET,Incomplete details on perfromance of the method
"While the black-box calibration of a GAN model may be attractive under specific settings, the authors did not consider the restrictions under those situations and their design may be hard to implement as a result.",asp_substance,MET,Incomplete details on perfromance of the method
"As a minor note, were different feature extractors compared?",asp_substance,MET,Incomplete details on perfromance of the method
Is that also true in this domain?,asp_substance,MET,Incomplete details on perfromance of the method
"* The architecture of the particular model is described very briefly, and at multiple points there’s an implication that this is an investigation of “deep learning models” more generally, even though those models may vary widely.",asp_substance,MET,Incomplete details on perfromance of the method
"This assertion is not justified, and seems surprising to me; we have very useful natural language processing systems that do not perform in a way that is comparable to humans (the hedge ""some degree of"" is really neither here nor there).",asp_substance,MET,Incomplete details on perfromance of the method
"I understand why a human who needs to saccade back and forth between the two groups of objects might lose track of the objects that have been paired so far, but I don't understand why that would affect the architecture in question.",asp_substance,MET,Incomplete details on perfromance of the method
"In equation 1, 2 the updates of  \mu_new and \sigma_new uses \lambda_i, however the authors provide common choices for \lambda without any justification or references.",asp_substance,MET,Incomplete details on perfromance of the method
"-3 For image-to-image translation experiments, no quantitative analysis whatsoever is offered so the reader can't really conclude anything about the effect of the proposed method in this domain.",asp_substance,MET_ANA,Lack of analysis of proposed method
"That said, I would like to see more analysis on the behavior of the proposed method under various interesting cases not tested yet.",asp_substance,MET_ANA,Lack of analysis of proposed method
"5) Last but not least, convergence analysis of the proposed method should be provided given that asynchrony may lead to divergence in the optimization.",asp_substance,MET_ANA,Lack of analysis of proposed method
"More precisely, for the digit datasets, the reviewer was interested to see how the proposed MDL performs on jointly adapting SVHN, MNIST, MNIST-M, and USPS or jointly adapting DSLR, Amazon, and Webcam for OFFICE dataset.",asp_substance,MET_DAT,Lack of discussion of performance of method on different datasets
"2. Sec. 3.3, Fig. 3: The capacity (in terms of parameters)of both Resnet-18 and VGG-16 is higher than the capcity for YFCC100M dataset for n=10K images (comes to 161K bits), while the capacity of Resnet-18, with 14.7 million parameters (assuming float32 encoding) has 14.7 * 32 bits = 470.4 million bits, thus capacity alone cannot explain why VGG converges faster than Resnet-18, since both networks exceed the capacity, and capacity does not seem to have an established formal connection to rate of memorization.",asp_substance,MET_DAT,Lack of discussion of performance of method on different datasets
"- Does the MonoGAN exhibit stable training dynamics comparable to training WGAN on CIFAR-10, or do the training dynamics change on the single-image data set?",asp_substance,MET_DAT,Lack of discussion of performance of method on different datasets
"Clearly, replaying data accurately from all tasks will work well, but why is it harder to guard against the generative forgetting problem than the discriminative one?",asp_substance,MET_DAT,Lack of discussion of performance of method on different datasets
Generative replay also brings the time complexity problem since it is time consuming to generate previous data.,asp_substance,MET_DAT,Lack of discussion of performance of method on different datasets
What is the minimum/maximum layers of a deep model? How much data is sufficient for a model to learn?,asp_substance,MET_DAT,Lack of discussion of performance of method on different datasets
"What's the relation between the size of a model and that of a data set? By increasing the depth/width of a neural network, how much new data should be collected for achieving a reasonable performance?",asp_substance,MET_DAT,Lack of discussion of performance of method on different datasets
"* p.4 first paragraph you claim that this method responds well to data which exhibits seasonality, but none of your datasets deal with data that would exhibit seasonality.",asp_substance,MET_DAT,Lack of discussion of performance of method on different datasets
"In particular, if the goal is to use this method to augment the data we use to train NLP systems in order to make them more robust, it seems that the time cost of the process will be prohibitive.",asp_substance,MET_DAT,Lack of discussion of performance of method on different datasets
"These classification datasets are often so close, that I do wonder whether even simpler methods would work just as well.",asp_substance,MET_DAT,Lack of discussion of performance of method on different datasets
3- Is the key that you used only 679 patches containing 98% of occurrences in the dataset as the first stage? What if we vary this percentage? How sensitive are the results? Such experiments could be useful to understand better why your method appears to work well.,asp_substance,MET_DAT_EXP,Lack of ablation on different datasets/ sizes
- Does training the generator and interpolation jointly improve the quality of the generator in general ? It would have been nice to run this method on more complicated dataset like CIFAR10 and see if this method increase the overall FID score.,asp_substance,MET_DAT_EXP,Lack of ablation on different datasets/ sizes
"Although the proposed method does a good job in synthetic experiments, outperforming existing methods by a large margin, its performance on the numerical variants of Freebase/DBPedia dataset does not show consistent significant improvement.",asp_substance,MET_DAT_EXP,Lack of ablation on different datasets/ sizes
In particular I do not think the satement 'all the pre-training methods are at most linear with respect to the number of edges' made in appendix F is correct.,asp_substance,MET_EXP,More variations of experiments needs to be added
"2. I think that the claim that the use of neural networks with discrete inputs can approximately solve NP-hard optimization problems is an exciting one, which likely necessitates more experiments (or theoretical results), but as it stands I don't think it is a fundamentally different conclusion from the fact that this method provides a great scalable solution for the ordinal embedding problem.",asp_substance,MET_EXP,More variations of experiments needs to be added
"Finally, the experimental part is also too weak to evaluate the proposed method.",asp_substance,MET_EXP,More variations of experiments needs to be added
"However, the experiments feel like they are missing motivation as to why this method is being used.",asp_substance,MET_EXP,More variations of experiments needs to be added
While the experiments are only performed on sentimental analysis and digit recognition. How about evaluating the proposed methods on real image recognition such as DomainNet or Office-Home?,asp_substance,MET_EXP,More variations of experiments needs to be added
"Along this direction, it is problematic that, in the synthetic examples, the relative  performance of methods changes significantly from experiment to experiment and there does not seem to be a simple way to control that.",asp_substance,MET_EXP,More variations of experiments needs to be added
The experiments on SHREC17 show all three spherical methods under-performing other approaches.,asp_substance,MET_EXP,More variations of experiments needs to be added
"Also it seems that the regular batch normalization could be very sensitive to domain shifts, and it would be good if the authors can test other normalization schemes such as layer/group normalization as baselines.",asp_substance,MET_EXP,More variations of experiments needs to be added
"However, unlike as advertised, the paper does not address the domain shift issue in meta-learning, and the experiments lack thorough evaluation as the paper considers itself as a meta-learning paper and only compares to other meta-learning approaches without much comparison to domain adaptation papers.",asp_substance,MET_EXP,More variations of experiments needs to be added
However I find the white-box experiments lacking as almost every method has 100% success rate.,asp_substance,MET_EXP,More variations of experiments needs to be added
"- White-box attack experiments don’t really prove the strength of the method, even with imagenet experiments, as almost all attacks get 100% success rate making it hard to compare.",asp_substance,MET_EXP,More variations of experiments needs to be added
I think that the current draft lacks strong experimental results to properly demonstrate the usefulness of the method.,asp_substance,MET_EXP,More variations of experiments needs to be added
In the current experiments there is a comparison only with CO algorithm and SGDA.,asp_substance,MET_EXP,More variations of experiments needs to be added
So overall I have the feeling that the authors have not succeeded to evaluate the model’s power with these two experiments and we cannot draw any strong conclusions regarding the benefit of the proposed mixing approach.,asp_substance,MET_EXP,More variations of experiments needs to be added
"#9) I think that MNIST is almost a toy experiment, since the crucial component of the proposed method is the prior modeling with the GAN.",asp_substance,MET_EXP,More variations of experiments needs to be added
"The MNIST is a relatively simple experiment, and I would like to see how the method works in more challenging problems.",asp_substance,MET_EXP,More variations of experiments needs to be added
I also was not convinced by the experiments which are mostly qualitative. I did not find that this set of experiments provide enough support to the proposed method.,asp_substance,MET_EXP,More variations of experiments needs to be added
"- The paper proposes an interesting experiment to show that the proposed approach is somewhat capable of capturing slightly adversarial biases in the input domain (adding square to the top-left of images of class ‘8’). While I like this experiment, I feel this has not been explored to completion in the sense of experimenting with robustness with respect to structured as well as unstructured perturbations.",asp_substance,MET_EXP,More variations of experiments needs to be added
"Thus my question for the rebuttal period: How would pGAN perform when defense mechanisms are deployed during the learning phase? (ideally, a thorough experiment illustrating the strength of pGAN against a few defense mechanisms would help re-evaluating the score)",asp_substance,MET_EXP,More variations of experiments needs to be added
"4. I think in the comparative experiments, the plain SGD should be added as another reference algorithm.",asp_substance,MET_EXP,More variations of experiments needs to be added
"The method has intuitive appeal and the experimental results are suggestive, but the experiments do not conclusively show that the method achieves something that ordinary machine learning does not.",asp_substance,MET_EXP,More variations of experiments needs to be added
"Second, and more importantly, the experiments need to be re-done to better measure the practical impact of the techniques.",asp_substance,MET_EXP,More variations of experiments needs to be added
"The main appeal is the idea of using T to model syntactic interactions, and the algorithm for learning T. Given that the main attraction of the paper is the potential for more performant word embeddings, I do not believe the work will have wide appeal to ICLR attendees, because no evidence is provided that the features from the learned tensor, say [a, b, T*a*b], are more useful in downstream applications than [a,b] (one experiment in sentiment analysis is tried in the supplementary material with no compelling difference shown).",asp_substance,MET_EXP,More variations of experiments needs to be added
"Since the LM-like approach is the main contribution, and the reported experiments do not show an advantage over GN-like approaches (already taken by previous work), this is my main reason for proposing rejection.",asp_substance,MET_EXP,More variations of experiments needs to be added
"Furthermore, there are no experimental results demonstrating the effect of the permutation head and the design choices above — if we could get by with only using the Hungarian algorithm, why bother classifying an exponential number of permutations? Do they help when added as an auxiliary loss?",asp_substance,MET_EXP,More variations of experiments needs to be added
"First, In Theorem 2, which seems to be a main result of the paper, the authors were concerned with the condition when W_{ji} >0, but there is not conclusion if W_{ji} =0.",asp_substance,MET_RES,"Generalizability of results in terms of method parameters (only applies for some number of parameters, only for these type of matrices, etc)"
"Even with no data augmentation, and even with the original networks, membership can only be assessed with a 90% accuracy.",asp_substance,MET_RES,"Generalizability of results in terms of method parameters (only applies for some number of parameters, only for these type of matrices, etc)"
"2. As to the results of the Pose2Pose network, I wonder if there are some artifacts that will affect the performance of the Pose2Frame network.",asp_substance,MET_RES,"Generalizability of results in terms of method parameters (only applies for some number of parameters, only for these type of matrices, etc)"
"However, this only affects the method of drawing the samples from a fixed known distribution and should have no more effect on the results than say a choice of a pseudo-random number generator.",asp_substance,MET_RES,"Generalizability of results in terms of method parameters (only applies for some number of parameters, only for these type of matrices, etc)"
"On the other hand, the obtained results are very weak: only one layered version of the paper is analysed and the theorem applies only to networks with less than some threshold of parameters.",asp_substance,MET_RES,"Generalizability of results in terms of method parameters (only applies for some number of parameters, only for these type of matrices, etc)"
"Overall, while I find the proposed approach simple -- the paper needs to address some issues regarding the claims made and should provide more quantitative experimental results justifying the same.",asp_substance,MET_RES,"Generalizability of results in terms of method parameters (only applies for some number of parameters, only for these type of matrices, etc)"
"This shows itself in the results; i.e., the proposed algorithm is either negligibly performing better than GBDT or when  GBDT dependence removed, it performs worse. It seems to me",asp_substance,MET_RES,"Generalizability of results in terms of method parameters (only applies for some number of parameters, only for these type of matrices, etc)"
"Results and discussion about how the previous methods with full features perform compared to SASNet, and also how we can include those features into SASNet should complete the paper.",asp_substance,MET_RES,"Generalizability of results in terms of method parameters (only applies for some number of parameters, only for these type of matrices, etc)"
"2. Section 3.3 argues that K-matrices allow to obtain an improvement of inference speed, however, providing the results of convergence speed (e.g. training plots with a number of epochs) will allow a better understanding of the proposed approach and will improve the quality of the paper.",asp_substance,MET_RES,"Generalizability of results in terms of method parameters (only applies for some number of parameters, only for these type of matrices, etc)"
"Though promising results have been demonstrated, a drawback of the proposed method is that it introduces more memory overhead compared to GPipe.",asp_substance,MET_RES,"Generalizability of results in terms of method parameters (only applies for some number of parameters, only for these type of matrices, etc)"
"Even with the hybrid method, the accuracy still drops.",asp_substance,MET_RES,"Generalizability of results in terms of method parameters (only applies for some number of parameters, only for these type of matrices, etc)"
"Although the idea is interesting, I would like to see more experimental results showing the scalability of the proposed method and for evaluating defense strategies against different types of adversarial attacks.",asp_substance,MET_RES_EXP,"Although the idea is interesting, I would like to see more experimental results showing the scalability of the proposed method and for evaluating defense strategies against different types of adversarial attacks."
"Therefore, an interesting baseline for the evaluation of the ICL approach would be a predefined, fixed attention map consisting of concentric circles with the image center as their center, to show that the proposed approach does more than just deemphasizing the corners of the image)",asp_substance,MET_RWK,Improper comparison of related work in terms of implementation
"6. Similar as above question, on the object counting task, is there a way to compare with previous counting methods?",asp_substance,MET_RWK,Improper comparison of related work in terms of implementation
"However, in its current state the work lacks sufficiently strong baselines to support the paper’s claims; thus, the merits of this approach cannot yet be properly assessed.",asp_substance,MET_RWK,Improper comparison of related work in terms of implementation
"Third, it is rather surprising that the authors didn't mention anything about the traditional causal discovery methods based on conditional independence relations in the data, known as constraint-based methods, such as the PC algorithm (Spirtes et al., 1993), IC algorithm (Pearl, 2000), and FCI (Spirtes et al., 1993).",asp_substance,MET_RWK,Improper comparison of related work in terms of implementation
"All in all, the results show that the proposed method provides a significant speedup with respect to Shim et al., but it lacks comparison with other methods in the literature.",asp_substance,MET_RWK,Improper comparison of related work in terms of implementation
"This is a very good point, however the paper do not compare or contrast with existing methods.",asp_substance,MET_RWK,Improper comparison of related work in terms of implementation
"For the language translation results, were there any other state-of-the-art methods that the authors could compare against? It seems they are only comparing against their own implementations.",asp_substance,MET_RWK,Improper comparison of related work in terms of implementation
"Second, SST itself is only comparable with or even worse than the state-of-art methods.",asp_substance,MET_RWK,Improper comparison of related work in terms of implementation
- Did the authors inspect generated samples of the baseline and the proposed method? Is there a notable qualitative difference?,asp_substance,MET_RWK,Improper comparison of related work in terms of implementation
What instead is missing is an answer to the question: Is it worth using a neural graph? what are the advantages and disadvantages compared to previous approaches?,asp_substance,MET_RWK,Improper comparison of related work in terms of implementation
- It would be also better to show the coefficient of existing methods that have no theoretical justification.,asp_substance,MET_RWK,Improper comparison of related work in terms of implementation
"- It would be better to compare with a naive approach that uses domain discrepancy between each source and target as (fixed) coefficient since this approach such as Mansour (2009), Ben-David(2007, 2010) and Kuroki et al (2019) which explicitly consider the hypothesis class has theoretical justification in the form of generalization error bound in the target domain.",asp_substance,MET_RWK,Improper comparison of related work in terms of implementation
"- How general is the proposed approach? How likely is it to generalize to other approaches such as Jigsaw (Doersch et al., 2015) and Exemplar (Dosovitskiy et al., 2016)? It would be good to comment on this.",asp_substance,MET_RWK,Improper comparison of related work in terms of implementation
"As a final point; the hyper parameters that were tuned for the MNF, noisy K-FAC and KFLA baselines are not on common ground.",asp_substance,MET_RWK,Improper comparison of related work in terms of implementation
"In addition, only the transformer baselines were considered, and it would seem natural to consider LSTM-based baselines, or some other related techniques.",asp_substance,MET_RWK,Improper comparison of related work in terms of implementation
"In my view, they do not even show that the distribution of atom usage will be better with their algorithm after the learning has converged, as at least according to their learning curves, the baselines have not finished converging.",asp_substance,MET_RWK,Improper comparison of related work in terms of implementation
(a) a comparison to other methods (outside the current framework) for sound separation,asp_substance,MET_RWK,Improper comparison of related work in terms of implementation
"However, the method they propose offers very little that is new when compared to e.g. Vaswani (https://arxiv.org/pdf/1706.03762.pdf, section 3.5) (the authors acknowledge this work several times).",asp_substance,MET_RWK,Improper comparison of related work in terms of implementation
"Similarly, I'd have expected baselines that included those models in the evaluation section showing the differences in performance between the newly proposed Transformer model for trees and previously used methods.",asp_substance,MET_RWK,Improper comparison of related work in terms of implementation
"While most (or all) of the paper is devoted to illustrate the effectiveness of the approach against *non-protected* ML. My only and biggest concern with this paper is that no defense mechanism has been tested against, and there are many in the literature. (see e.g. Diakonikolas et al ICML 2019).",asp_substance,MET_RWK,Improper comparison of related work in terms of implementation
"1) There are several important works on model-parallelism and convergence guarantee of pipeline-based methods missing in this paper, for example [1][2].",asp_substance,MET_RWK,Improper comparison of related work in terms of implementation
"-- The overall error model in this paper, which is borrowed from Roy et al. is quite restrictive as at it assumes that the queries to the frequency estimation data structure are coming from the same distribution as that given by f_i’s themselves.",asp_substance,MET_RWK,Improper comparison of related work in terms of implementation
This latter baseline is a zero-cost baseline as it is not even dependent on the method.,asp_substance,MET_RWK,Improper comparison of related work in terms of implementation
The connections of the proposed approach with existing literature should be better explained.,asp_substance,MET_RWK,Improper comparison of related work in terms of implementation
"Unlike Arora's original work, the assumptions they make on their subject material are not supported enough, as in their lack of explanation of why linear addition of two word embeddings should be a bad idea for composition of the embedding vectors of two syntactically related words, and why the corrective term produced by their method makes this a good idea.",asp_substance,MET_RWK,Improper comparison of related work in terms of implementation
"- MAAC does not consistently outperform baselines, and it is not clear how the stated explanations about the difference in performance apply to other problems.",asp_substance,MET_RWK,Improper comparison of related work in terms of implementation
"Some baseline DA methods [A, B] and datasets [C, D] are not considered.",asp_substance,MET_RWK_DAT,"Some baseline DA methods [A, B] and datasets [C, D] are not considered."
- What is the difference between the result of Theorem 4.3 and the result from (Lacoste-Julien 2016)?,asp_substance,MET_RWK_RES,- What is the difference between the result of Theorem 4.3 and the result from (Lacoste-Julien 2016)?
Why is the random seed being used to compare the performance of different arms? Do you instead mean that s and s’ are two values of the arm in Figure 4?,asp_substance,MET_TNF,Incorrect claims about performance in tables and figures
"- Judging from Table 1, the proposed method does not seem to provide a large contribution.",asp_substance,MET_TNF,Incorrect claims about performance in tables and figures
"-  From the plots of learning curves in appendix, the proposed methods doesn’t seem to show a huge boost of performance comparing to the uniform bandit. Could you show aggregated comparison between the proposed method and uniform bandit similarly to what is done in Figure 4 ?",asp_substance,MET_TNF,Incorrect claims about performance in tables and figures
In Fig. 3 the author claim that the proposed method dominates the other methods in terms of privacy and utility but this is not correct.,asp_substance,MET_TNF,Incorrect claims about performance in tables and figures
"I enjoyed reading the submission, which is very clearly written, but due to the relatively limited value of the contributions, and excessive focus on the tunability metric which I do not feel is giustified, I slightly lean against acceptance here at ICLR. I do think, however, that it would make a great submission to a smaller venue or workshop.",asp_substance,OAL,Lacking clarity overall (needs better presentation)
"Overall, I find the paper a promising contribution. But until the authors provide a more thorough experimental evaluation, I hesitate to recommend acceptance.",asp_substance,OAL,Lacking clarity overall (needs better presentation)
"In its current state, I am not sure that it adds a lot to the manuscript.",asp_substance,OAL,Lacking clarity overall (needs better presentation)
"It reads very much like a STOC theory paper, and a lot of the key ML details that would be relevant to audience at this conference seem to have been shoved under the rug in a way.",asp_substance,OAL,Lacking clarity overall (needs better presentation)
"Overall, I think this paper is below the borderline of acceptance due to insufficient comparison with Sparsely-Gated MoE.",asp_substance,OAL,Lacking clarity overall (needs better presentation)
"While I like the premise of the paper, I feel that it needs more work.",asp_substance,OAL,Lacking clarity overall (needs better presentation)
"Overall, I quite liked the paper and think it is well-written, but I believe the authors need to highlight at least one practical advance introduced by the CW distance (in which case I will raise my score).",asp_substance,OAL,Lacking clarity overall (needs better presentation)
"Ultimately this paper is interesting but falls well below the standards of exposition that I expect from a theory paper and doesn’t go very far at connecting the analysis back to the claimed motivation of investigating practical invariances. If the authors significantly improve the quality of the draft, I’ll be happy to revisit it and re-evaluate my score.",asp_substance,OAL,Lacking clarity overall (needs better presentation)
"Overall, the paper requires significant improvement.",asp_substance,OAL,Lacking clarity overall (needs better presentation)
5. The paper is imprecise and unpolished and the presentation needs improvement.,asp_substance,OAL,Lacking clarity overall (needs better presentation)
"However, I found that the contribution of this paper is fairly small.",asp_substance,OAL,Lacking clarity overall (needs better presentation)
The paper would gain in clarity,asp_substance,OAL,Lacking clarity overall (needs better presentation)
"However, I think the relative novelty of the paper does not meet ICLR standards, and it’s better suited as a whitepaper attached to an open dataset release.",asp_substance,OAL,Lacking clarity overall (needs better presentation)
The paper has few really minor grammatical errors and typos. Please fix those before uploading the final draft.,asp_substance,OAL,Lacking clarity overall (needs better presentation)
I believe that this paper is thus not in its final form and could be largely improved.,asp_substance,OAL,Lacking clarity overall (needs better presentation)
"A good paper overall, but the experiments were relatively weak (common for most ICLR submissions) and the novelty was somewhat limited.",asp_substance,OAL,Lacking clarity overall (needs better presentation)
The paper has an interesting potential but seems a bit limited in its present form.,asp_substance,OAL,Lacking clarity overall (needs better presentation)
"In conclusion, I suggest a reject of this paper due to the lacks of comprehensive study and evaluation.",asp_substance,OAL,Lacking clarity overall (needs better presentation)
The paper is not very self contained.,asp_substance,OAL,Lacking clarity overall (needs better presentation)
The main problem is that directly predicting the context is intractable because of combinatorial explosion.,asp_substance,PDI,Need more interesting problem definition
"- the more interesting problem, RL + auxiliary loss isn’t evaluated in detail",asp_substance,PDI,Need more interesting problem definition
But the problem settings are not clear to me.,asp_substance,PDI,Need more interesting problem definition
"The paper not only claims 'large scale representation learning' but also utilizing the described idea to use neural networks to ""directly, approximately solve non-convex NP-hard optimization problems that arise naturally in unsupervised learning problems."" Both claims are not really shown in the paper: (i) The experiments are not large scale and (ii)  it becomes not clear how any substantiate insight with respect to NP-hard problems can be gained here apart from the fact that it tackles a ML problem, which many seem to be computationally hard problems.",asp_substance,PDI,Need more interesting problem definition
"The paper motivates the problem that we need to pick out an exploration sequence that optimizes learning progress, but then approximates it as simply measuring the return.",asp_substance,PDI,Need more interesting problem definition
It would be nice to position the ideas from the paper w.r.t. this line of research too.,asp_substance,PDI,Need more interesting problem definition
"The basic idea is very interesting, but I would have expect more interesting use cases as teased by the first sentence of the abstract ""find algorithms with strong worst-case guarantees for online combinatorial optimization problems"".",asp_substance,PDI,Need more interesting problem definition
"So at the end, I am a bit puzzled. I really like the idea, but I have the feeling that this technique should have been developed for more complicated setting. Or maybe it is actually not working on more difficult combinatorial problem (and this is hidden in the paper).",asp_substance,PDI,Need more interesting problem definition
The idea in this paper is novel but experiments do not seem to be enough.,asp_substance,PDI,Need more interesting problem definition
"The two ideas (use of grids, and intra-life curiosity vs inter-life curiosity) should be independently investigated and put in context of past work.",asp_substance,PDI_RWK,"The two ideas (use of grids, and intra-life curiosity vs inter-life curiosity) should be independently investigated and put in context of past work."
"The results and discussions in the main part of the paper are too light in my opinion; the average model accuracy across modules is not an interesting metric at all, although it does show that the Transformer performs better than recurrent networks.",asp_substance,RES,generalizability of results is questionable
"Further, it seems that the results in Table 4 might be a bit obscured by the size of the downstream task dataset.",asp_substance,RES,generalizability of results is questionable
"However, given that the achieved performance gains over the state-of-the-art are fairly small, it would be good to assess if the obtained improvements are statistically significant.",asp_substance,RES,generalizability of results is questionable
"[-] The results indicate that SAVP offers a trade-off between the properties of GANs and VAEs, but does not go beyond its individual parts.",asp_substance,RES,generalizability of results is questionable
"Most importantly, I would like to see a measure of variance/uncertainty like confidence intervals included in the results; otherwise it's impossible to assess whether the results are likely to be significant or not. Other questions:",asp_substance,RES,generalizability of results is questionable
"In addition, the results seem very weak.",asp_substance,RES,generalizability of results is questionable
Perhaps the results would be a little more convincing if additional common word embeddings were also tested.,asp_substance,RES,generalizability of results is questionable
"However, as the authors acknowledge the overall simplicity of the tasks being evaluated with mostly marginal improvements makes the overall evaluation fall short.",asp_substance,RES,generalizability of results is questionable
But the paper only provides empirical results on sentimental analysis and digit recognition.,asp_substance,RES,generalizability of results is questionable
* The biggest problem for me was the unconvincing results.,asp_substance,RES,generalizability of results is questionable
"The empirical evaluation is limited in considering only one task (clique finding), and the results seem to be quite sensitive to the chosen optimizer.",asp_substance,RES,generalizability of results is questionable
- The conclusions focus on the importance of section 3 and,asp_substance,RES,generalizability of results is questionable
"If this solution can be proven to improve a valuable metric (e.g. accuracy, interpretability, theoretical understanding, or computational efficiency) of a setup, it is then worthwhile being published.",asp_substance,RES,generalizability of results is questionable
"Results are much lower in less favorable cases, sometimes close to random (see last line of Table 3).",asp_substance,RES,generalizability of results is questionable
I believe the results all relate to inference but it would be good to get an overview of the impact of training time as well.,asp_substance,RES,generalizability of results is questionable
"However, I feel like the main result is a bit too predictable, and for acceptance I'd like to see a much more detailed exploration of the questions around systematic generalization.",asp_substance,RES,generalizability of results is questionable
Results on more scenes will make the performance more convincing.,asp_substance,RES,generalizability of results is questionable
- it's better to show time v.s. testing accuracy as well.,asp_substance,RES,generalizability of results is questionable
"However, the results are unconvincing: only the results for epsilon = 0.1 are shown, and even so the advantage is marginal.",asp_substance,RES,generalizability of results is questionable
"The authors discuss their results on k-medoids in the appendices, but it seems like these results aren't quite complete yet.",asp_substance,RES,generalizability of results is questionable
"From the perspective of a purely technical contribution, there are fewer exciting results.",asp_substance,RES,generalizability of results is questionable
"Moreover, the performance reported in this paper seems to be much inferior to the state-of-the-art results reported in the literature.",asp_substance,RES,generalizability of results is questionable
A. You demonstrate the results on CIFAR-10 for 10% error rates which corresponds to networks which are far from what is currently used in deep learning.,asp_substance,RES,generalizability of results is questionable
"Thus, it is hard to say whether the results are applicable in practice.",asp_substance,RES,generalizability of results is questionable
"However, this does not take into account the time already spent on pre-training. Perhaps the authors can include some results as to the total time taken as well as amortized total time over a number of different downstream tasks.",asp_substance,RES,generalizability of results is questionable
I think this work would have much greater impact if the authors can show that the power-law form holds for a larger variety of architectures and optimizers thus allowing researchers to more confidently incorporate the results of this work into the design and training deep neural networks.,asp_substance,RES,generalizability of results is questionable
"The current tasks and problem sizes are not very convincing, and the accuracy results are not very compelling.",asp_substance,RES,generalizability of results is questionable
The unsupervised results are more interesting but not very much explored (a single set of sampled faces).,asp_substance,RES,generalizability of results is questionable
It seems that the sampled reconstruction results (Fig. 8) are not as good as VAE on CIFAR10.,asp_substance,RES,generalizability of results is questionable
I would have expected to see some analysis and results on the translation quality over systematic noise applied to the input graph.,asp_substance,RES,generalizability of results is questionable
"iv) Finally, the reported results are mostly qualitative.",asp_substance,RES,generalizability of results is questionable
"In this regard, I encourage the authors to update the supplementary material in order to show extended qualitative results of the explanations produced by their method.",asp_substance,RES,generalizability of results is questionable
"Overall: this paper makes a convincing case that it can be used to generate higher quality images, but not that this improves the quality of the disentangled representations.",asp_substance,RES,generalizability of results is questionable
- No large corpus results.,asp_substance,RES,generalizability of results is questionable
It also shows itself in the results that final algorithm is almost indistinguishable from GBDT regarding results.,asp_substance,RES,generalizability of results is questionable
However such problems are entirely missing in the results section.,asp_substance,RES,generalizability of results is questionable
"Second, I notice that compared to the result in Table 1, PGD attack can yield better results [1].",asp_substance,RES,generalizability of results is questionable
"The advantage of this random encoding was to reduce the number of parameters for an embedding layer, but the results showed we gained much PPL from a 25% reduction in embedding size (Table 1).",asp_substance,RES,generalizability of results is questionable
1) Provide stronger empirical results (these are not too convincing).,asp_substance,RES,generalizability of results is questionable
"More importantly, the results presented are quite meager.",asp_substance,RES,generalizability of results is questionable
"If this is a method for image recognition, it would be better to present results for a more substantial image recognition problem than MNIST and CIFAR-10.",asp_substance,RES,generalizability of results is questionable
"something that is either deterministic, or a probabilistic result with a small",asp_substance,RES,generalizability of results is questionable
4. How sensitive are the results to the number of adaptive kernels in the layers.,asp_substance,RES,generalizability of results is questionable
"However, the results are not enough to be accepted to ICLR having a very high standard.",asp_substance,RES,generalizability of results is questionable
- The shown inception scores are far from state-of-the-art.,asp_substance,RES,generalizability of results is questionable
Unfortunately this paper offers only weak results.,asp_substance,RES,generalizability of results is questionable
"First of all, only a small number of qualitative results are reported and, therefore, it is very difficult to assess the proposed method and draw any conclusion.",asp_substance,RES,generalizability of results is questionable
- The authors report only negative results for the fully-unsupervised version of UD-GAN The paper lacks and in-depth discussion about why this negative result is interesting.,asp_substance,RES,generalizability of results is questionable
"In particular, the qualitative results are too limited and no quantitative evaluations is provided.",asp_substance,RES,generalizability of results is questionable
"While the motivation is nice, I find the results (especially in the unguided setup) underwhelming.",asp_substance,RES,generalizability of results is questionable
My major concern is whether the results are significant enough to deserve acceptance.,asp_substance,RES,generalizability of results is questionable
"If these networks can truly solve these problems, authors should report the success rate while varying the threshold, not individual accuracy of the items which can be arbitrarily high by violating constraints.",asp_substance,RES,generalizability of results is questionable
"Say for block size d=4, an index is required for each block, and the resulting compression ratio is at most 4 (correct me if I understand it wrong).",asp_substance,RES,generalizability of results is questionable
"Weaknesses: The main weakness of the paper is that the performance gains are extremely low compared to the next contender; perhaps they are statistically significant (this cannot be determined), but it's unclear why we wouldn't use GBDT.",asp_substance,RES,generalizability of results is questionable
"Unfortunately, the results are quite disappointing in terms of sound quality, and feature many artifacts.",asp_substance,RES,generalizability of results is questionable
"This seems like an unnecessary bottleneck in the model, and could partly explain the relatively poor quality of the results.",asp_substance,RES,generalizability of results is questionable
"I cannot understand from that part whether T(v_a, v_b,.) addition to v_a+v_b gives any improvement or not.",asp_substance,RES,generalizability of results is questionable
"- As shown in the ablation study, the main contribution on the obtained results seems to be the use of stronger transformations than in MixMatch.",asp_substance,RES,generalizability of results is questionable
"- While there seems to be a consistent improvement over TD3, this improvement is in some cases small (e,g. ants).",asp_substance,RES,generalizability of results is questionable
"I would suggest the authors to either introduce the two problems earlier or to simply say that near-optimal results are achieved, without giving detailed results, because it is very hard to understand them without any introduction of the task being achieved.",asp_substance,RES,generalizability of results is questionable
"However, the results are a bit misleading in their reporting of the std error.",asp_substance,RES,generalizability of results is questionable
"3) Furthermore, for the experiments only one neural network architecture was considered and it remains an open question, how the presented results translate to other architectures.",asp_substance,RES_EXP,Need more experimental results
2) The experimental results provided in this paper are weak.,asp_substance,RES_EXP,Need more experimental results
-	The experimental results of section 5.2 are somewhat disappointing.,asp_substance,RES_EXP,Need more experimental results
"Finally, the experimental results do not show any significant advantage over PGD, either in running time (they are slower) or norm perturbation.",asp_substance,RES_EXP,Need more experimental results
The experimental results are actually less impressive than what are claimed in contribution and conclusion.,asp_substance,RES_EXP,Need more experimental results
"- impact: the results achieved in the experiments are very small improvements compared to the baseline of RGCN (~ +0.01 in two experiments and ~ -0.04 in another) and often these small variations in results can be compensated with better baselines training (e.g. better hyper-params, ...)",asp_substance,RES_EXP,Need more experimental results
"- Although disc can be easily estimated in the regression task (differently from d_A distance which is a special case of disc), there are no experimental results of the regression task even in the synthetic data.",asp_substance,RES_EXP,Need more experimental results
So this work has to be supported with more detailed experimental results to express the potential of this approach fully.,asp_substance,RES_EXP,Need more experimental results
Both the results on the development set and on the test set should be reported for the validity of the experiments.,asp_substance,RES_EXP,Need more experimental results
There is in fact no experimental evidence that the practical advantages of BN are relevant to the results proven.,asp_substance,RES_EXP,Need more experimental results
- The experiments were entirely focused on uncertainty quality but we are always interested in both performance on the task at hand as as well as good uncertainty estimates. What was the performance based on e.g. classification accuracy on each of these tasks compared to the baselines? I believe that including these results will strengthen the paper and provide a more complete picture.,asp_substance,RES_EXP,Need more experimental results
(1) The experimental results cannot show the usefulness of the proposed GCN.,asp_substance,RES_EXP,Need more experimental results
"3) In its current form, the experimental results are extremely cherry-picked, with a very small number of tasks evaluated, and for each task a single selected baseline used.",asp_substance,RES_EXP,Need more experimental results
"- The use of RAM is a fairly serious limitation of your experimental setting in my view. You should include results also for the pixel space, even if negative.",asp_substance,RES_EXP,Need more experimental results
- Experimental results are provided only on MNIST and Fashion-MNIST.,asp_substance,RES_EXP,Need more experimental results
"2) The experiment results include standard errors for different replicates where such replicates correspond to different training random seeds (or different samples from the enriched set?), however, it doesn’t include any significance of the sampling.",asp_substance,RES_EXP,Need more experimental results
"- The experiments show good results compared to existing algorithms, but not impressively so.",asp_substance,RES_EXP,Need more experimental results
"ii) In table 2, I don’t really see any promising results compared to baselines. There are",asp_substance,RES_TNF,"The advantage of this random encoding was to reduce the number of parameters for an embedding layer, but the results showed we gained much PPL from a 25% reduction in embedding size (Table 1)."
"One thing that puts me off is that, in Table 2, AnyBurl (the single one baseline authors considered other than the original NeuralLP) yields better Hits@10 values than Neural-LP-N, but the corresponding bold in the results is conveniently omitted.",asp_substance,RES_TNF,"The advantage of this random encoding was to reduce the number of parameters for an embedding layer, but the results showed we gained much PPL from a 25% reduction in embedding size (Table 1)."
"2. Why were more baselines from the related work not included? I understand the experiments are a proof of concept, but it would be nice to get a feeling for what some of the other methods do.",asp_substance,RWK,Limited improvement over baselines
- For the automatic evaluation measures there should be multiple references per SPARQL query since this is how BLEU et al are supposed to be used.,asp_substance,RWK,Limited improvement over baselines
The selected baselines are not sufficient.,asp_substance,RWK,Limited improvement over baselines
The improvement from the baselines is also limited.,asp_substance,RWK,Limited improvement over baselines
"In addition, it could be worthwhile to compare and benchmark on existing evaluations: https://arxiv.org/pdf/1802.06806.pdf",asp_substance,RWK,Limited improvement over baselines
"Better literature review to reflect the relevant previous video action recognitions, especially those on video compositional models.",asp_substance,RWK,Limited improvement over baselines
"little improvements over the baselines or even significantly worse. More importantly,",asp_substance,RWK,Limited improvement over baselines
"However, as presented these ideas are poorly justified and careful comparisons against sensible baselines are missing.",asp_substance,RWK,Limited improvement over baselines
"Moreover, comparison with some of the DA baselines (ADDA[1], DSN[2]) is missing.",asp_substance,RWK,Limited improvement over baselines
"- Pioneering work is not necessarily equivalent to ""using all the GPUs""",asp_substance,RWK,Limited improvement over baselines
I feel the baseline in domain adaptation area is a bit limited.,asp_substance,RWK,Limited improvement over baselines
"The connection between mutual information and generalization has been studied in several contexts [see, e.g., the references in this paper and https://arxiv.org/abs/1511.05219 https://arxiv.org/abs/1705.07809 https://arxiv.org/abs/1712.07196 https://arxiv.org/pdf/1605.02277.pdf https://arxiv.org/abs/1710.05233 https://arxiv.org/pdf/1706.00820.pdf ] and further exploration is desirable.",asp_substance,RWK,Limited improvement over baselines
"A naive approach to estimate coefficient with single-source domain discrepancy measures such as [1]Mansour (2009), [2,3] Ben-David(2007, 2010), [4] Kuroki et al (2019), and W1-distance is not considered.",asp_substance,RWK,Limited improvement over baselines
"Questions: I would like to see more discussion about difference between this work and [Z Hu, arXiv:1905.13728].",asp_substance,RWK,Limited improvement over baselines
"In the absence of citation or comparison with any of the prior work on multivariate statistical dependence testing, the current submission is not suitable for publication.",asp_substance,RWK,Limited improvement over baselines
"For example, [1] could be used to reduce the variance of gradient w.r.t. \phi.",asp_substance,RWK,Limited improvement over baselines
A missing empirical analysis is on class-conditional noise (see for example Patrini et al. 17 for a definition).,asp_substance,RWK,Limited improvement over baselines
I highly encourage the authors to finetune the ImageNet pre-trained BagNet on PASCAL VOC and compare to the previous patch-based deep networks.,asp_substance,RWK,Limited improvement over baselines
"Since positional encoding with Fourier transforms is well known, this seems like the relevant benchmark but it receives only a brief treatment.",asp_substance,RWK,Limited improvement over baselines
"I would also expect more ablation studies about how to pick p_{\had d}, which seems to be the key of this approach, in MNIST and CIFAR10.",asp_substance,RWK,Limited improvement over baselines
The submission could maybe improved by segmenting the work into intro / related / background (with clear equations presenting the existing GP) / analysis / approach / experiments,asp_substance,RWK,Limited improvement over baselines
"I do apologize if my original comment wasn't clear regarding the contribution part of the paper. What I was trying to say is not that I didn't see the individual contributions of the paper, but instead that the paper does multiple things simultaneously, without comparing against the relevant baselines for any of the individual contributions.",asp_substance,RWK,Limited improvement over baselines
"I believe it will not be great, but I think for completeness, you should add such a baseline.",asp_substance,RWK,Limited improvement over baselines
6. On CIFAR10 the results seem to be worse that other methods.,asp_substance,RWK,Limited improvement over baselines
10. Another related paper seems to be Spatial Transformer Networks (Jaderberg et al.).,asp_substance,RWK,Limited improvement over baselines
I'd encourage the authors to extend the intro and position the ideas w.r.t. existing works and extend the evaluation.,asp_substance,RWK,Limited improvement over baselines
"For instance, I would like to see the comparison between DeepCS and SmartHash by Tang et al 2017.",asp_substance,RWK,Limited improvement over baselines
"- The WGAN-GP baseline is very weak, i.e. does not show any reasonable generated images (Fig. 9).",asp_substance,RWK,Limited improvement over baselines
"4. The scores achieved by the baseline are very far from state of the art, making the comparison mostly useless,",asp_substance,RWK,Limited improvement over baselines
"There is also a matter of ease of training compared to existing approximations or relaxations, e.g. Gumbel-Sinkhorn.",asp_substance,RWK,Limited improvement over baselines
"If one reframes the convergence rate in terms of FLOPs U=T^2 instead of iterations, then the convergence rate drops from 1/T to 1/sqrt(U), which undermines the claim in remark 3.4 that this analysis is superior to that of Yan et al. (2018).",asp_substance,RWK,Limited improvement over baselines
The biased introduced via this truncation has been studied in great depth in [3] and shown to be harmful.,asp_substance,RWK,Limited improvement over baselines
Minor comment: An interesting line of work is that of [3] which could be included in the discussion.,asp_substance,RWK,Limited improvement over baselines
* first paragraph page 2: some references to causality literature and definition of spuriousness as common cause,asp_substance,RWK,Limited improvement over baselines
This would be an effective baseline to compare. (Correct me if I am wrong here.),asp_substance,RWK,Limited improvement over baselines
- (W3) Baselines for transfer learning: I felt this was another notable oversight.,asp_substance,RWK,Limited improvement over baselines
"More details should be provided, and when comparisons to past works or inter-life curiosity are made, this should be controlled for.",asp_substance,RWK,Limited improvement over baselines
- About setup: the paper reports using ResNets for natural images as in Mescheder et al. (2018).,asp_substance,RWK,Limited improvement over baselines
"- The baseline scores of Reg-SGAN and Reg-WGAN seem to be worse than those reported in Mescheder et al. (2018), which have inception scores above 6 according to Figure 6 of their paper.",asp_substance,RWK,Limited improvement over baselines
"* CodeSLAM (best paper at CVPR'18) is referenced but there is no comparison with it, while a comparison on the EuRoC dataset should be possible.",asp_substance,RWK,Limited improvement over baselines
"- Authors do not visualize the attention (as is common in previous work involving attention in e.g., NLP).",asp_substance,RWK,Limited improvement over baselines
"4. Missing experiments to validate nature of bounds: Bartlett et al. [3] performed extensive experiments to exhibit the correct scaling of the generalization bounds with the ""model complexity"" introduced upto numerical constants.",asp_substance,RWK_EXP,Lack of experiments
"2. How is the performance of a simpler baseline such as combining a subset of new domain as training set to train MAML or PN (probably in 5-shot, 5-class case)?",asp_substance,RWK_EXP,Lack of experiments
"My main concern about the paper is that, currently, the experiments do not include any strong baseline (the ES currently is not a strong baseline, see comments below).",asp_substance,RWK_EXP,Lack of experiments
"Besides, the results on the sentimental analysis are comparable with the compared baselines.",asp_substance,RWK_RES,Results are more or less similar to baselines
"The authors stated that ""F-pooling remarkably increases accuracy and robustness w.r.t. shifts of moderns CNNs""; however, in Table 1-3, the winning margin of accuracy is actually quite small (<2%), and the consistency (<3.5% compared to the second best baseline except resnet-18 on CIFAR 100 has large improvement ~7-8%).",asp_substance,RWK_RES,Results are more or less similar to baselines
"The many-to-many results are clearly better than the pairwise results in this regard, but in the context of musical timbre transfer, I don't feel that this model successfully achieves its goal -- the results of Mor et al. (2018), although not perfect either, were better in this regard.",asp_substance,RWK_RES,Results are more or less similar to baselines
"In Figure 5, a legend indicating the relationship between color intensity and distance would be helpful.",asp_substance,TNF,"Incomprehensible tables and figures (what is the point of the plot, no decription of figure in main text, etc)"
"In Figure 6 there seem to be unnecessary discrepancies between the y-axis and colorbar of subplots (a) and (b), and keeping those more consistent would improve readability.",asp_substance,TNF,"Incomprehensible tables and figures (what is the point of the plot, no decription of figure in main text, etc)"
"Similarly, figure 3 as well as figures 5-7 and 8 in the appendix provide very good information about the tunability of the various optimizers without using the introduced metrics.",asp_substance,TNF,"Incomprehensible tables and figures (what is the point of the plot, no decription of figure in main text, etc)"
"Information similar (although not identical) to that summarized in table 5 could be captured by substituting the 3 metrics with the best performance after tuning for 4, 16 and 64 iterations respectively (just as examples).",asp_substance,TNF,"Incomprehensible tables and figures (what is the point of the plot, no decription of figure in main text, etc)"
"* minor: in figure 8 in the appendix, the results after 100 iterations is, as far as I understand, over a single replication, so is not particularly reliable (and will always be 100% of a single optimizer)",asp_substance,TNF,"Incomprehensible tables and figures (what is the point of the plot, no decription of figure in main text, etc)"
"Furthermore, it may be informative to show the saliency maps in Figure 5 not only for cases in which the learner classified the image correctly in both time steps, but also cases in which the learner classified the image correctly the first time and incorrectly the second time.",asp_substance,TNF,"Incomprehensible tables and figures (what is the point of the plot, no decription of figure in main text, etc)"
"From the plots in Figure 2 and 3, it is hard to find the convergence of the method within 100 iterations.",asp_substance,TNF,"Incomprehensible tables and figures (what is the point of the plot, no decription of figure in main text, etc)"
"Also, the claim in Fig. 1 that the transition from ‘’high capacity’’ to low capacity happens at the number of parameters in the network seems a bit loose and hard to substantiate from what I understand, and should be toned down. (*)",asp_substance,TNF,"Incomprehensible tables and figures (what is the point of the plot, no decription of figure in main text, etc)"
"Also, from the three Tables in the experimental part, the improvement of F-pooling over AA-pooling (developed by the main reference of this work) does not seem to be significant or consistent.",asp_substance,TNF,"Incomprehensible tables and figures (what is the point of the plot, no decription of figure in main text, etc)"
"For example, in Table 2, the F-pooling only wins at either accuracy (marginally) or consistency, but not both.",asp_substance,TNF,"Incomprehensible tables and figures (what is the point of the plot, no decription of figure in main text, etc)"
* Section 5.3 (Fig. 6) is the part most relevant to the generalisation problem.,asp_substance,TNF,"Incomprehensible tables and figures (what is the point of the plot, no decription of figure in main text, etc)"
"Yet, in Fig.1 some difference is observed between the methods, why is that so?",asp_substance,TNF,"Incomprehensible tables and figures (what is the point of the plot, no decription of figure in main text, etc)"
"To continue with the experimental evaluation, I found the plots with the predictive uncertainty in Figure 3 a bit confusing.",asp_substance,TNF,"Incomprehensible tables and figures (what is the point of the plot, no decription of figure in main text, etc)"
"There is no value in being very confident if you are wrong and vice-versa, so unless there is an accompanying plot/table reporting the accuracy I see not much value from this plot alone.",asp_substance,TNF,"Incomprehensible tables and figures (what is the point of the plot, no decription of figure in main text, etc)"
"And finally, the discussion of this figure makes claims about the behaviour of the model that seems to be too strong to be based on a single image experiment.",asp_substance,TNF,"Incomprehensible tables and figures (what is the point of the plot, no decription of figure in main text, etc)"
- Table 1: Not sure why there is only one model that employs beam search (with beam size = 2) among all the comparisons. It looks strange.,asp_substance,TNF,"Incomprehensible tables and figures (what is the point of the plot, no decription of figure in main text, etc)"
"- In Table 4, it is hard to compare DeepTwist with the other methods because activation quantization is not used.",asp_substance,TNF,"Incomprehensible tables and figures (what is the point of the plot, no decription of figure in main text, etc)"
An example is presented in Figure 3 but is not expanded upon in the main text.,asp_substance,TNF,"Incomprehensible tables and figures (what is the point of the plot, no decription of figure in main text, etc)"
"In my opinion, to support the above claim, shouldn’t the authors provide a similar table as Table 1, directly comparing the certified radii of the natural images and adversarial images?",asp_substance,TNF,"Incomprehensible tables and figures (what is the point of the plot, no decription of figure in main text, etc)"
"- From Figure 9, we see the certificate radii of the natural have at least two peaks. Though on average the certificate radii of the adversarial attacks is higher than that of the natural images, it is smaller than the right peak. Could the authors elaborate more of the results?",asp_substance,TNF,"Incomprehensible tables and figures (what is the point of the plot, no decription of figure in main text, etc)"
1. (Also AREA CHAIR NOTE): Another parallel submission to ICLR titled “Generative Ensembles for Robust Anomaly Detection” makes similar observations and seemed to suggest that ensembling can help counter the observed CIFAR/SVHN phenomena unlike what we see in Figure 10.,asp_substance,TNF,"Incomprehensible tables and figures (what is the point of the plot, no decription of figure in main text, etc)"
"2. Even though Figure 2b shows that SVHN test likelihoods are higher than CIFAR test likelihoods, the overlap in the histograms of CIFAR-train and CIFAR-test is much higher than the overlap in CIFAR-train and SVHN-test.",asp_substance,TNF,"Incomprehensible tables and figures (what is the point of the plot, no decription of figure in main text, etc)"
I don’t see a significant difference between RAN and DNN in Figure 5. Maybe more explanation or better visualization would help.,asp_substance,TNF,"Incomprehensible tables and figures (what is the point of the plot, no decription of figure in main text, etc)"
It is not clear how the compression ratio in table 1 is obtained.,asp_substance,TNF,"Incomprehensible tables and figures (what is the point of the plot, no decription of figure in main text, etc)"
