text,index,review_action,fine_review_action,target
"1. Why the color distribution of generated images is evaluated on a sampled subset of pixels, not full images?",,arg_request,arg-request_explanation,0
"2. On Figure 6, which classes are outlying on transformation limitation / data variability plots (bottom-right corner) and how it may be explained?",,arg_request,arg-request_explanation,0
"3. While StyleGAN can not preserve geometry of objects for shift in location-based attributes, when walks are learned in the W space, have you experimented on manipulating those attributes with z space? What are the results?",,arg_request,arg-request_explanation,0
1. Pictures in Fig. 2 are mixed up between G(z) and G(z + \alpha w),,arg_request,arg-request_edit,1
"2. In Fig. 2 edit(G(z, \alpha)) -> edit(G(z), \alpha))",,arg_request,arg-request_typo,3
3. In eq. (2) f^n(z) -> G(f^n(z)),,arg_request,arg-request_typo,3
4. In eq. (6) +\alpha^* -> -\alpha^*,,arg_request,arg-request_typo,3
"Overall, I quite liked the paper and think it is well-written, but I believe the authors need to highlight at least one practical advance introduced by the CW distance (in which case I will raise my score).",,arg_request,arg-request_explanation,0
The authors could run each WAE training process K times (with random initialization) to see if the closed-form distance enables more stable results.,,arg_request,arg-request_experiment,2
"Since the novelty of this work lies in the introduction of the CW distance, I would like to see an independent evaluation of this distance as a  general statistical distance measure (independently of its use in CWAE).",,arg_request,arg-request_experiment,2
Can you use this distance as a multivariate-Gaussian goodness of fit measure for high-dimensional data drawn from both Gaussian and non-Gaussian distributions and show that it actually outperforms other standard statistical distances (e.g. in two-sample testing power)?,,arg_request,arg-request_experiment,2
"Besides having closed form in the case of a Gaussian prior (which other statistical distances could potentially also achieve), it would be nice to see some discussion of why the authors believe their CW-distance is conceptually superior to such alternatives.",,arg_request,arg-request_explanation,0
- Silverman's rule of thumb is only asymptotically optimal when the underlying data-generating distribution itself is Gaussian. Perhaps you can argue here that due to CLT: the projected data (for high-dimensional latent spaces) should look approximately Gaussian?,,arg_request,arg-request_explanation,0
How this proxy incentives the agent to explore poorly-understood regions?,,arg_request,arg-request_explanation,0
Is it a baseline with the best hyperprameters in hindsight?,,arg_request,arg-request_explanation,0
The explanation in Fig 2 on why this is the case seem to me not so clear. Are you trying to show that the Wave-U-Net does not work since there is no 1/f^2 law for clean audio signals?,,arg_request,arg-request_explanation,0
"There is a little typo in Formula 1 for the STFT spectrogram, I would use the modulus |.| rather than || . ||.",,arg_request,arg-request_edit,1
Is Harmonic Convolution applicable to complex STFT coefficients as well?,,arg_request,arg-request_clarification,4
If so it would be better to define the operator in a more general notation.,,arg_request,arg-request_edit,1
"In Section 4.3 and 4.4, is the x_0 (defined in Section 2.1) complex-valued STFT coefficients or something else?",,arg_request,arg-request_explanation,0
"What is the L1 loss defined in Section 4.4? To obtain the final separated audio waveform, an inverse STFT is applied on what?",,arg_request,arg-request_explanation,0
These details can be written in supplementary material if more space is needed.,,arg_request,arg-request_explanation,0
"I would ideally like to see results on more optimizers, at the very least for Adam, even if for fixed hyper-parameters.",,arg_request,arg-request_experiment,2
"As far as I understand, this involves only minor changes in the code since reasonable hyperparameters required for the convergence of Adam have been extensively studied.",,arg_request,arg-request_experiment,2
"- Given the current form of the paper, the abstract and introduction should be modified to reflect the fact that only limited architectures and optimizers were experimented with, and the claims of the paper are not experimentally validated in general.",,arg_request,arg-request_edit,1
- It would be nice if more network architectures were analysed (such as VGG and DenseNets).,,arg_request,arg-request_experiment,2
- It would be nice if different stopping criteria were analysed.,,arg_request,arg-request_experiment,2
- It would greatly benefit the reader if eq. 5 were expanded.,,arg_request,arg-request_explanation,0
I think this work would have much greater impact if the authors can show that the power-law form holds for a larger variety of architectures and optimizers thus allowing researchers to more confidently incorporate the results of this work into the design and training deep neural networks.,,arg_request,arg-request_experiment,2
The above papers are not cited in this paper.,,arg_request,arg-request_edit,1
I highly encourage the authors to finetune the ImageNet pre-trained BagNet on PASCAL VOC and compare to the previous patch-based deep networks.,,arg_request,arg-request_experiment,2
"2. The model seems to be sensitive to the hyper-parameter \alpha, is this parameter always fixed at 0.5 or needed to fine-tune for different datasets?",,arg_request,arg-request_clarification,4
"- In the case of the search space II, how many GPU days does the proposed method require?",,arg_request,arg-request_explanation,0
"- About line 10 in Algorithm 1, how does the proposed method update the population P? Please elaborate on this procedure.",,arg_request,arg-request_explanation,0
"* Page 2 -- Paragraph 2: ""Therefore, methods than can exploit...""",,arg_request,arg-request_typo,3
* Page 3 -- 2nd line of math: Super-scripts are missing for some entries of the matrices W^A and W^{A+B},,arg_request,arg-request_typo,3
"* Page 5 -- Last paragraph: ""...merged logical units is more likly to get get stuck in a ...""",,arg_request,arg-request_typo,3
"* Page 5 -- Last paragraph: ""...and combining their distributions using the mulistart heuristic...""",,arg_request,arg-request_typo,3
- it is unclear and possibly could be better explained why one needs to concatenate the goals (what would change if we would not concatenate but estimate state densities rather than trajectories?),,arg_request,arg-request_clarification,4
"- 3.3 ""It is known that PER can become very expensive in computational time"" - please supply a reference",,arg_request,arg-request_edit,1
"- 3.3 ""After each update of the model, the agent needs to update the priorities of the transitions in the replay buffer with the new TD-errors"" - However the method only renews priorities of randomly selected transitions (why would there be a large overhead?).",,arg_request,arg-request_clarification,4
The authors should on the claimed contributions.,,arg_request,arg-request_edit,1
Is it a combination of DGR and HAT with some capacity expansion?,,arg_request,arg-request_explanation,0
"Clearly, replaying data accurately from all tasks will work well, but why is it harder to guard against the generative forgetting problem than the discriminative one?",,arg_request,arg-request_explanation,0
"- Relatedly, better baselines should be used; for example, if the memory used by the generative model is merely put to storing randomly chosen instances from the tasks, how will the results compare? Clearly storing instances bypasses the forgetting problem completely (as memory size approaches the dataset size it turns into the joint problem) and it's not clear how many instances are really needed per task, especially for these simpler problems.",,arg_request,arg-request_experiment,2
"- There are several methodological issues: Why are CIFAR results not shown in a table as is done for the other dataset? How many times were the experiments run and what were the variances? How many parameters are used (since capacity can increase?) It is for example not clear that the comparison to joint training is fair, when stating: ""Interestingly, DGM outperforms joint training on the MNIST dataset using the same architecture.",,arg_request,arg-request_edit,1
This suggests that the strictly incremental training methodology indeed forced the network to learn better generalizations,,arg_request,arg-request_explanation,0
"1) The introduction makes it seem the generative replay is new, without citing approaches such as DGR (which are cited in the related work).",,arg_request,arg-request_edit,1
"The initial narrative mixes prior works' contributions and this paper's contributions; the contributions of the paper itself should be made clear,",,arg_request,arg-request_edit,1
"2) Using the word ""task"" in describing ""joint training"" of the generative, discriminative, and classification networks is very confusing (since ""task"" is used for the continual learning description too,",,arg_request,arg-request_edit,1
3) There is no legend for CIFAR; what do the colors represent?,,arg_request,arg-request_clarification,4
"4) There are several typos/grammar issues e.g. ""believed to occurs"", ""important parameters sections"", ""capacity that if efficiently allocated"", etc.).",,arg_request,arg-request_typo,3
I would like to see how these curves vary with,,arg_request,arg-request_experiment,2
"But the way it is presented and explained lacks of clarity: for instance in Section 2, some notations are not well defined (e.g what is f?) .",,arg_request,arg-request_clarification,4
"So, my suggestion would be to improve the writing of this section to make the message stronger and relevant for a larger audience.",,arg_request,arg-request_clarification,4
"II) The Infoword model can be seen as a simple instance of word masking based models, and as an extension of deep infomax for sequences (it would be certainly nice to describe a little bit what Deep InfoMax is to facilitate the reading).",,arg_request,arg-request_clarification,4
Having a first loss function after paragraph 4 could help to understand the principle of this model (before restricting the model to n-grams),,arg_request,arg-request_edit,1
At last,,arg_request,arg-request_explanation,0
What was wrong with LSTMs/GRUs as they've been used extensively for recursive problems including operations on trees?,,arg_request,arg-request_explanation,0
"Even if it's not possible to prove something about this strategy, it would be useful to just state a desirable property that the sampling mechanism should have and then argue informally that this mechanism has that property.",,arg_request,arg-request_edit,1
- What implementation of the other algorithms did you use?,,arg_request,arg-request_clarification,4
"I would suggest saying something like s = ((x^g, x^c), g) where s is a state from the perspective of value iteration, (x^g, x^c) is a state of the system, which is a vector of values divided into two sub-vectors, x^g is the part of the system state that involves the state variables that are specified in the goal, x^c (for 'context')",,arg_request,arg-request_edit,1
"Why? If we are learning the distribution, would not it make sense to sample all architectures only after training the supernet at our best?",,arg_request,arg-request_explanation,0
"This is a lot of architectures, which makes me wonder: how would a “cost-aware” random sampling perform with the same number of sampled architectures?",,arg_request,arg-request_experiment,2
Do they here refer to the gradients with respect to the weights ONLY?,,arg_request,arg-request_clarification,4
Could we say that the advantage of the Gumbel Softmax technique is two-fold? i) make the loss differentiable with respect to the arch parameters; ii) reduce the variance of the estimate of the loss gradients with respect to the network weights.,,arg_request,arg-request_explanation,0
"2)	Can the author discuss why the soft sampling procedure in [1] is not enough? I have an intuitive understanding of this, but I think this should be clearly discussed in the manuscript as this is a central aspect of the paper.",,arg_request,arg-request_explanation,0
3)	The authors use a certain number of warmup steps to train the network weights without updating the architecture parameters to ensure that “the weights are sufficiently trained”. Can the authors discuss the choice on the number of warmup epochs?,,arg_request,arg-request_explanation,0
"As a generative model for unseen data, I would like to see the generated results, which is more convincing.",,arg_request,arg-request_experiment,2
"I am wondering for more complicated images, how is the performance?",,arg_request,arg-request_explanation,0
(a) a comparison to other methods (outside the current framework) for sound separation,,arg_request,arg-request_experiment,2
(b) a significant clarification of Figure 4.,,arg_request,arg-request_clarification,4
(c) The authors should present what they mean by a dilated convolution using the notation of the paper.,,arg_request,arg-request_explanation,0
"(d) In Figure 2, it is unclear to me how the 1/f^2 law is observed in (a) but not in (c) or (e).",,arg_request,arg-request_explanation,0
I would recommend that the authors perhaps shorten section 3 or remove figure 9 to fit it into 8 pages.,,arg_request,arg-request_edit,1
I would strongly recommend including the computational cost of each method in the evaluation section.,,arg_request,arg-request_experiment,2
"Can you explain somewhere exactly what you mean when you say ""learning dynamics of deep learning""? Given the specific nature of the results presented in the paper it would be nice to be precise also when it comes to the overall topic under study.",,arg_request,arg-request_clarification,4
"In Corollary 3.3. you characterize the convergence speed in a nice way, but I am missing the link to the behaviors observed empirically in e.g. Fig. 2. What am I missing?",,arg_request,arg-request_explanation,0
"Tiny detail: The axes of several of the plots given in the paper mis the lables which makes it hard to read. Straightforward to fix, but worth mentioning nevertheless.",,arg_request,arg-request_edit,1
"- Theorem 3.2: ""[...] converges at a speed proportional to [...]"". Isn't \bar{u}_t logarithmic (non-linear) in t?",,arg_request,arg-request_clarification,4
"- Theorem 3.2: Even if strong, I don't mind the assumption on a dataset merely consisting of two (weighted) data points. I would suggest to simulate this case without putting any condition on the initialization of the weights (ie, without assumptions H1-H2), and compare the empirical shape of the classification error with the one you obtain analytically in Figure 2 Right.",,arg_request,arg-request_experiment,2
"- Theorem 3.2 Interpretation: unfinished sentence ""We can characterize the convergence speeds more quantitatively with the""",,arg_request,arg-request_typo,3
"- Theorem 4.1: Can you give an intuition or lower/upper bounds for u(t) for the Hinge case, to make evident its difference from the binary entropy case (where u(t) ~ log(t))",,arg_request,arg-request_explanation,0
"If the paper could provide some guidance for model and data selection, that would be an interesting paper for the ICLR audience.",,arg_request,arg-request_explanation,0
"For instance, how deep should a model be for a classification or regression task?",,arg_request,arg-request_explanation,0
What is the minimum/maximum layers of a deep model? How much data is sufficient for a model to learn?,,arg_request,arg-request_explanation,0
What is the minimum/maximum size of the data set? Do we really need a large data set or just a subset that covers the data distribution?,,arg_request,arg-request_explanation,0
"What's the relation between the size of a model and that of a data set? By increasing the depth/width of a neural network, how much new data should be collected for achieving a reasonable performance?",,arg_request,arg-request_explanation,0
How about the gain of the task performance?,,arg_request,arg-request_explanation,0
"(4) based on the name ""CW distance"" I would expect the authors to actually prove that it is indeed a distance (i.e. all the main axioms).",,arg_request,arg-request_edit,1
(6) What is image(X) in Remark 4.1?,,arg_request,arg-request_clarification,4
It would be good to add further details on practical limitations.,,arg_request,arg-request_edit,1
Is there any hope this could be applied to problems like 3D imaging data or videos?,,arg_request,arg-request_explanation,0
How is the approach different in comparison to the StackGAN and it's variable which also use multiple levels of crude to fine image generation?,,arg_request,arg-request_explanation,0
So how does the paper try to make contributions to improve the disentangled features with the proposed method?,,arg_request,arg-request_explanation,0
"Apart from combining these to existing ideas, what can be considered as an added novelty to improve the quality of the disentangled features?",,arg_request,arg-request_clarification,4
"Moreover if x must be equal to 0 Ax \leq 0 and at that point Ax = 0, then that means there exists no x for which Ax < 0, so why not just say this outright?",,arg_request,arg-request_edit,1
Perhaps a cleaner definition would just be “A is full rank and there does not exist any X such that Ax < 0?,,arg_request,arg-request_edit,1
Also perhaps better to use the curly sign for vector inequality.,,arg_request,arg-request_edit,1
|_{y>0} x + b |_{y>0}  <— what is the purpose of the subscripts here? Why is this notation never introduced?,,arg_request,arg-request_explanation,0
"The paper offers some analysis, suggesting when each of the conditions occurs, upper bounds the smallest singular value of D A (where the example dependent diagonal matrix D incorporates the ReLU activation (shouldn’t this be more clearly introduced and notated?).",,arg_request,arg-request_edit,1
“From a high-level perspective both of these approaches” --> missing comma after “perspective”,,arg_request,arg-request_typo,3
"""as well as the gradient correspond to the highest",,arg_request,arg-request_typo,3
"""analyzing the lowest possible response"" what does ""response' mean here?",,arg_request,arg-request_explanation,0
"""We provide upper bounds on the smallest singular value"" -- the singular value of what? This hasn't been stated yet.",,arg_request,arg-request_explanation,0
"""reverse view on adversarial examples"" --- what this means isn't clear from the preceding text.",,arg_request,arg-request_explanation,0
"""we aim to theoretically derive means to uncover mechanisms of rectifier networks without assumptions on the weights"" -- what does ""mechanisms"" mean here?",,arg_request,arg-request_explanation,0
"Notation section -- need a sentence here at the beginning, can't just have a section heading followed by bullets.",,arg_request,arg-request_edit,1
"""realated""",,arg_request,arg-request_typo,3
More explanations are needed.,,arg_request,arg-request_explanation,0
One other limitation seems to be that Theorem 1 requires using a step size which seems to be much smaller than what one may hope to use in practice. Can you comment on this?,,arg_request,arg-request_explanation,0
"A minor comment is that the mutual information I(., .) being a function of two variables suddenly became a function of a single variable in Eq. (1) and in the text which precedes it.",,arg_request,arg-request_typo,3
There is a previous work that these authors should cite:,,arg_request,arg-request_edit,1
"Ullrich, K., Berg, R.V.D., Brubaker, M., Fleet, D. and Welling, M., 2019. Differentiable probabilistic models of scientific imaging with the Fourier slice theorem. arXiv preprint arXiv:1906.07582.",,arg_request,arg-request_result,5
How does your method compare to this paper?,,arg_request,arg-request_explanation,0
"I think these statistics would be useful to report in your work, as they are more familiar with folks in the cryoEM field.",,arg_request,arg-request_edit,1
"In Equation 3, how does one calculate Z, the normalization constant?",,arg_request,arg-request_clarification,4
"For the decoder, how large of the 3D space are you generating? What are the units? Are you using voxels to represent atomic density? What is the voxel size? Is it the same as on Page 11?",,arg_request,arg-request_explanation,0
I think more description of the neural network architecture would be useful (more than what is reported on page 12).,,arg_request,arg-request_edit,1
Further technical background and detail would drastically improve the paper.,,arg_request,arg-request_edit,1
"In addition, only the transformer baselines were considered, and it would seem natural to consider LSTM-based baselines, or some other related techniques.",,arg_request,arg-request_experiment,2
"For example, one question is how often a single partial tree has multiple possible completions in the data.",,arg_request,arg-request_clarification,4
Why is the random seed being used to compare the performance of different arms? Do you instead mean that s and s’ are two values of the arm in Figure 4?,,arg_request,arg-request_clarification,4
2. Can you elaborate more on the metric for measuring the learning progress LP? Why does the myopic metric make sense in spite of the there being plateaus in the training curves?,,arg_request,arg-request_explanation,0
"Even if done disjointly, how does the proposed joint learning is compared to those algorithms in these domains?",,arg_request,arg-request_clarification,4
"3) Top row of Figure 3 nicely explains how the learned sampling paradigm performs compared to other mechanisms (such as uniform, random, low-pass). But there is no comparision against other non-fixed techniques.",,arg_request,arg-request_edit,1
"It would be helpful to move some of the stuff in the appendix to the main text, and present in a neat way.",,arg_request,arg-request_edit,1
I also struggled a little to understand what is the difference between forward interpolate and filtering.,,arg_request,arg-request_clarification,4
The clarity of this paper needs to be strengthened.,,arg_request,arg-request_edit,1
"- abstract: uncover nonlinear observation? -> maybe change ""observation"" to ""latent dynamics""?",,arg_request,arg-request_typo,3
What's your opinion?,,arg_request,arg-request_explanation,0
"2) What's the relationship between reconstructed performance, heterogeneity of the sample and dimensions of latent space?",,arg_request,arg-request_explanation,0
"3) It would be interesting to show any relationship, reconstruction error with respect to the number of discrete multiclass.",,arg_request,arg-request_experiment,2
4) How is the proposed method generalizable?,,arg_request,arg-request_explanation,0
Perhaps the authors could give examples of situations where this would naturally arise.,,arg_request,arg-request_experiment,2
I was also curious as to why the learned Y's are blurry.,,arg_request,arg-request_clarification,4
"This sort of two-stage generation is also potentially interesting, I was wondering if the authors had ideas to generalize this idea.",,arg_request,arg-request_experiment,2
- It is a bit unclear to me how the authors propose to obtain independent posteriors over z and c. Is it purely empirical or is there a formal reason that guarantees it?,,arg_request,arg-request_explanation,0
"- In Figure 3, it would be good to label the upper trapezoid.",,arg_request,arg-request_edit,1
- Some paragraphs are very long and the manuscript may benefit from segmenting them into multiple paragraphs.,,arg_request,arg-request_edit,1
My only issue here is that very little information was given about the size of the training sets. Did they use all the samples?,,arg_request,arg-request_experiment,2
"Note: there is an error on page 9, in Figure 3.",,arg_request,arg-request_typo,3
"Thus, I suggest the authors could show the space and time comparisons with the baseline methods to show effectiveness of the proposed method.",,arg_request,arg-request_experiment,2
"In addition, I observe that in Table 1, the proposed method does not outperform the Joint Training in SVHN with A_10.",,arg_request,arg-request_explanation,0
I hope the author could explain this phenomenon.,,arg_request,arg-request_explanation,0
"Furthermore, I do not see legend in Figure 3 and thus I cannot figure out what the curves represent.",,arg_request,arg-request_edit,1
"Fourth, there are some grammar mistakes and typos.",,arg_request,arg-request_typo,3
"For example, there are two ""the"" in the end of the third paragraph in Related Work.",,arg_request,arg-request_typo,3
"In the last paragraph in Related Work, ""provide"" should be ""provides"".",,arg_request,arg-request_typo,3
"In page 8, the double quotation marks of ""short-term"" are not correct.",,arg_request,arg-request_typo,3
It also seems that curiosity in this context seems to be very related to surprise? There are neuroscience evidence indicating that humans turns to remember (putting more weights) on events that are more surprising.,,arg_request,arg-request_clarification,4
"In what way are the planes ""around their initialization""? If the initial depth map spans over multiple orders of magnitude, will the planes be uniformly sampled between the minimum and maximum disparity of the initial map?",,arg_request,arg-request_clarification,4
"If yes, it seems that the initial depth map is not really needed, just its minimum and maximum value is needed, but then how come the method can be applied iteratively with respect to depth?",,arg_request,arg-request_clarification,4
"Is there a mechanism to protect from interpolating across discontinuities? If no, were bleeding edge artifacts observed?",,arg_request,arg-request_explanation,0
"Lastly, if the authors are not planning to release the code, the implementation details section is a bit too high-level and does not contain enough details to reimplement the Author's technique.",,arg_request,arg-request_explanation,0
"- more details about this network are needed, as well as the others in the paper.",,arg_request,arg-request_clarification,4
1. How does varying the number of nearest neighbors change the network behavior?,,arg_request,arg-request_explanation,0
"2. At test time, a fixed number of images are used for denoising - how does the choice of these images change accuracy or adversarial robustness?",,arg_request,arg-request_explanation,0
"3. Does just simple filtering of the feature map, say, by local averaging, perform equally well?",,arg_request,arg-request_clarification,4
4. When do things start to break down? I imagine randomly replacing feature map values (i.e. with very poor nearest neighbors) will cause robustness and accuracy to go down - was this tested?,,arg_request,arg-request_explanation,0
Comparison with attention models is necessary to compare the important patches obtained from conventional networks.,,arg_request,arg-request_experiment,2
"Consider doing cross validation over those 42-49 data points, and report the mean of deviations computed on the test folds.",,arg_request,arg-request_experiment,2
Taking the standard deviation over the deviations measured on different folds of the data would be better measure of uncertainty.,,arg_request,arg-request_experiment,2
"- Page 8: ""differntiable methods for NAS."" differentiable is misspelled.",,arg_request,arg-request_typo,3
- It would be good to understand what benefit does the stochasticity of RBMs provide.,,arg_request,arg-request_clarification,4
How do deterministic neural networks perform on the addition and factoring tasks?,,arg_request,arg-request_explanation,0
"- The paper would be stronger if it includes more complex tasks, e.g., TSP, and show that the same ideas can be applied to improve the learning a solver for such tasks.",,arg_request,arg-request_experiment,2
"Overall, while I find the proposed approach simple -- the paper needs to address some issues regarding the claims made and should provide more quantitative experimental results justifying the same.",,arg_request,arg-request_edit,1
"In this light, experiments demonstrating comparisons between GANs and VAEs as the reference generative model for explanations would have made the paper stronger (as the proposed approach relies explicitly on how good the generative model is).",,arg_request,arg-request_edit,1
"- The paper proposes an interesting experiment to show that the proposed approach is somewhat capable of capturing slightly adversarial biases in the input domain (adding square to the top-left of images of class ‘8’). While I like this experiment, I feel this has not been explored to completion in the sense of experimenting with robustness with respect to structured as well as unstructured perturbations.",,arg_request,arg-request_experiment,2
- Typographical Errors: Section 3.1 repeats the use of D for a discriminator as well as the input distribution.,,arg_request,arg-request_typo,3
Procedure 1 and Procedure 2 share the same titles -- which is slightly misleading.,,arg_request,arg-request_edit,1
"In addition, Procedure 1 is not referenced in the text which makes is hard to understand the utility of the same.",,arg_request,arg-request_edit,1
"Citations used for Gradcam are wrong -- Sundarajan et al., 2016 should be changed to Selvaraju et al., 2017.",,arg_request,arg-request_edit,1
Additional experiments on at least ImageNet would have made the paper stronger.,,arg_request,arg-request_experiment,2
"Regarding contrastive explanations, experiments on datasets where distractor classes (y_probe) are present in addition to the class interest (y_true) seem important -- PASCAL VOC, COCO, etc.",,arg_request,arg-request_experiment,2
"Specifically, since the explanations provided are visual saliency maps the paper would have been stronger if there were experiments suggesting -- what needs to change in a region of an image classified as a ‘cat’ to be classified as a ‘dog’ while there is an instance of the class - ‘dog’ present in the image itself.",,arg_request,arg-request_experiment,2
Is there a particular reason the authors did not choose to adopt the above technique as a baseline?,,arg_request,arg-request_explanation,0
"Since, there is no clear metric to evaluate contrastive explanations -- human studies to judge the class-discriminativeness (or trust) of the proposed approach would have made the paper stronger.",,arg_request,arg-request_typo,3
"In that regard, I would like the authors to comment on the worst-case computational complexity of the numerical analysis for determining the volume of a preimage through multiple layers.",,arg_request,arg-request_experiment,2
The structure is obtained by the shared and sparse rows of matrix A. I would like the authors to comment on how the studies will be affected by this property of the common networks.,,arg_request,arg-request_experiment,2
"It would have been very interesting to study the quality of interpolations on more models and datasets, and compare their generalization capabilities as well as the bias present in the different datasets.",,arg_request,arg-request_experiment,2
- Does training the generator and interpolation jointly improve the quality of the generator in general ? It would have been nice to run this method on more complicated dataset like CIFAR10 and see if this method increase the overall FID score.,,arg_request,arg-request_experiment,2
However it's not clear how is this range used in practice ? Do you sample uniformly $\alpha$ in this range to train the linear interpolation ?,,arg_request,arg-request_clarification,4
Also how many steps are required to learn the linear interpolation ? How much the does it influence the quality of the interpolation ?,,arg_request,arg-request_clarification,4
- There is a typo in equation 6,,arg_request,arg-request_typo,3
- In figure 6: What does the right figure represent ? especially what are the different colours ?,,arg_request,arg-request_clarification,4
"Readability suggestion: the paper starts with a very nice motivating example, but when the setup is provided, i.e., that (x,c) pairs are the input to the learner, the intended content of c is not immediately clear- control variates could assume anything from general context information to privileged information.",,arg_request,arg-request_edit,1
A similarly informative example would be great!,,arg_request,arg-request_edit,1
"Clarification regarding lemma 1: it seems that if the true posterior cannot be expressed by q, a gap will necessarily remain, even in the “limit” of perfect learning. Is this correct?",,arg_request,arg-request_explanation,0
The paper misses the key baseline in Bayesian optimisation using tree structure [1] which can perform the prediction under the tree-structure dependencies.,,arg_request,arg-request_edit,1
Why doesn't the proposed bandit algorithm not pick out the best hyper-parameter?,,arg_request,arg-request_explanation,0
would a simpler hyper-parameter search procedure (picking the best hyper-parameter after the first 2000 episodes)?,,arg_request,arg-request_explanation,0
"Clarifications of these points, and more in general the philosophy behind the architectural choices made, would make this paper a much clearer accept.",,arg_request,arg-request_explanation,0
"- the general architecture, and specifically the logic behind the edge-to-edge convolution, and generally the different blocks in fig.1 ""graph translator"".",,arg_request,arg-request_explanation,0
"- how exactly do you do a L1 loss on graphs? I'd have to assume the topology of the graph is unchanged between Gy and T(Gx) ~ and then maybe take L1 of weight matrix? But then is this general enough ~ given your stated goal of modeling different topologies? Either ways, more explanation / and perhaps equations to clarify this loss would be very helpful.",,arg_request,arg-request_explanation,0
"- why do you need a conditional GAN discriminator, if you already model similarity by L1?",,arg_request,arg-request_explanation,0
Please explain the logic for this architectural choice.,,arg_request,arg-request_explanation,0
"-  could you please explain the setting for the “gold standard” experiment. I'd have to assume, for instance, you train a GNN in a supervised way by using both source (non-suspicious) and target (suspicious) behaviour, and label accordingly? That said I am not 100% sure of this problem setting.",,arg_request,arg-request_explanation,0
"Overall, if you want to claim theoretical guarantees you will have to significantly improve the manuscript.",,arg_request,arg-request_edit,1
What are the differences to your approach?,,arg_request,arg-request_explanation,0
"Also, please place the related work earlier on in the paper.",,arg_request,arg-request_edit,1
This needs to be changed: a) you should run all the baselines for each of the current tasks b) you should also expand the experiments evaluated to include tasks where it is not obvious that a hierarchy would help/is necessary c) you should include more baselines.,,arg_request,arg-request_experiment,2
- The paper is currently oriented towards discrete states. What can you say about continuous spaces?,,arg_request,arg-request_explanation,0
- The use of random exploration for the discoverer is underwhelming. Have you tried different approaches? Would more advanced exploration techniques work or improve the performance?,,arg_request,arg-request_explanation,0
Please run at least 10 experiments.,,arg_request,arg-request_experiment,2
"- The use of RAM is a fairly serious limitation of your experimental setting in my view. You should include results also for the pixel space, even if negative.",,arg_request,arg-request_experiment,2
"Even if they did show these convincingly, it is not obvious to me that it is valuable; the authors need to *show* that uniform usage is desirable.",,arg_request,arg-request_edit,1
"2:    The authors should compare against several costs/algorithms (e.g. l_0 with OMP, l_1 with LARS, etc.), and across various N_0/sparsity penalties, and across several datasets.",,arg_request,arg-request_experiment,2
- Some arguments that are presented could deserve a bit more precision.,,arg_request,arg-request_edit,1
"Similarly, using vector quantization does allow for on-chip low memory: we do not need to re-instantiate the compressed layer but we can compute the forward in the compressed domain (by splitting the activations into similar block sizes and computing dot products).",,arg_request,arg-request_edit,1
"For instance, authors could compress the widely used (and more challenging) ResNet-50 architecture, or try other tasks such as image detection (Mask R-CNN).",,arg_request,arg-request_experiment,2
"The table is missing results from: ""Hardware Automated Quantization"", Wang et al ; ""Trained Ternary Quantization"", Zhu et al ; ""Deep Compression"",  Han et al; ""Ternary weight networks"", Li et al (not an extensive list).",,arg_request,arg-request_result,5
"- Similarly, providing some code and numbers for inference time would greatly strengthen the paper and the possible usage of this method by the community.",,arg_request,arg-request_clarification,4
"Indeed, I wonder what the overhead of decrypting the weights on-the-fly is (although it only involves XOR operations and products)",,arg_request,arg-request_explanation,0
"- Small typos: for instance, two points at the very end of section 5.",,arg_request,arg-request_typo,3
"However, I think the paper would need either (1) more thorough experimental results (see comments above, points 2 and 3 of weaknesses) or (2) more justifications for its existence (see comments above, point 1 of weaknesses).",,arg_request,arg-request_experiment,2
- Using < > for latex brakets is not ideal.,,arg_request,arg-request_edit,1
"- ""derivable"" I guess you mean ""differentiable""",,arg_request,arg-request_typo,3
- Oliphant and Hunter are cited for Numpy/scipy and matplotlib but the,,arg_request,arg-request_edit,1
"While the generation of this type of explanations is somewhat novel, from the text it seems that the proposed method may not be able to indicate what part of the image content drove the model to predict class A. Is this indeed the case?",,arg_request,arg-request_clarification,4
The manuscript would benefit from positioning the proposed method w.r.t. these works.,,arg_request,arg-request_edit,1
"In this regard, I encourage the authors to update the supplementary material in order to show extended qualitative results of the explanations produced by their method.",,arg_request,arg-request_experiment,2
"In addition, I recommend complementing the presented qualitative comparisons with quantitative evaluations following protocols proposed in existing work, e.g. a) occlusion analysis (Zeiler et al., ECCV 2014, Samek et al.,2017), a pointing experiment (Zhang et al., ECCV 2016), or c) a measurement of explanation accuracy by feature coverage",,arg_request,arg-request_edit,1
It’s not clear to me that the proposed method can guarantee optimality any better.,,arg_request,arg-request_clarification,4
"It would be very interesting to see the test time behavior of the network when it is run with more iterations than it is trained with (say 10 or 20), especially since the depth error does not seem to have stopped decreasing at only 4 iterations.",,arg_request,arg-request_experiment,2
It’s not made entirely clear whether the training backpropagates through the update/construction of the pose and depth cost volumes.,,arg_request,arg-request_clarification,4
"In equation 5, “x” should be “i”.",,arg_request,arg-request_typo,3
"I would also expect more ablation studies about how to pick p_{\had d}, which seems to be the key of this approach, in MNIST and CIFAR10.",,arg_request,arg-request_experiment,2
A question I have is why the method doesn't perform well in certain cases.,,arg_request,arg-request_explanation,0
Edit: I also wonder if incorporating idf would be better if the values were computed by a larger corpus.,,arg_request,arg-request_explanation,0
"A citation to ""Beyond BLEU:Training Neural Machine Translation with Semantic Similarity"" from ACL 2019 should be incorporated into the related work.",,arg_request,arg-request_edit,1
Have you tried BERTScore on sentence similarity tasks? It's possible BERTScore could have strong performance and some readers may wonder this.,,arg_request,arg-request_result,5
"A citation to ""Deep Reinforcement Learning with Distributional Semantic Rewards for Abstractive Summarization"" from EMNLP 2019 should also be incorporated.",,arg_request,arg-request_edit,1
"The word ""language"" is misspelled twice in Appendix E.",,arg_request,arg-request_typo,3
> Consider explaining cryoSPARC in detail given that is the state-of-the-art technique and to which all the cryoDGRN results are compared.,,arg_request,arg-request_edit,1
> In Figure 4 and the related experiment,,arg_request,arg-request_explanation,0
"> What would runtime comparisons be for cryoSPARK and cryoDGRN, for an unsupervised heteregeneous reconstruction?",,arg_request,arg-request_explanation,0
It is also not clear how this theoretical result can shed insight on the empirical study of neural networks.,,arg_request,arg-request_explanation,0
Why is the second objective log(#params) instead of just #params when the introduction mentions explicitly that tuning the scales between different objectives is not needed in LEMONADE?,,arg_request,arg-request_clarification,4
How could scaling be handled?,,arg_request,arg-request_explanation,0
I would have expected to see some analysis and results on the translation quality over systematic noise applied to the input graph.,,arg_request,arg-request_experiment,2
Do we know how does the connectedness of the  input graph affect the translation quality in the case of strongly connected directed graphs? Or what happens if the target graph has a strong connectivity?,,arg_request,arg-request_explanation,0
"Towards this, how does the computational complexity scale wrt to the connectedness?",,arg_request,arg-request_explanation,0
A lot of clarity is required on the choice of evaluation metric; for example choice of distance measure ?,,arg_request,arg-request_explanation,0
What is the L1 norm applied on?,,arg_request,arg-request_explanation,0
Typo:. The “Inf” in Tabel 1,,arg_request,arg-request_typo,3
My main question is that: what is the main advantage of the Cramer-Wold distance to an MMD with a proper kernel?,,arg_request,arg-request_explanation,0
"My question here is that how is this method better than the standard VAE, where we also have an analytic form for the ELBO when the prior is Gaussian, an no sampling is required.",,arg_request,arg-request_explanation,0
"- the conditions presented in Theorem 4, seem hard to check in practice; what is the time complexity of this operation? I believe that checking if A is omnidirectional is equivalent to an LP but how do you solve the combinatorial size of doing that over all set of indices?",,arg_request,arg-request_explanation,0
Maybe more explanation and quantitative analysis (e.g. relating the volume of the preimage of an epsilon ball around z to the singular values) could be helpful.,,arg_request,arg-request_edit,1
- Is there actionable consequences one could draw from your papers? The way the results are presented seem like they are only useful inspection after training; are your results able to derive methods to enforce conditions on the pre-images for example?,,arg_request,arg-request_explanation,0
It is not clear how those observations can affect practical algorithms and this is something I hope the author can address.,,arg_request,arg-request_result,5
"1. The authors insist that their fous is on transfer and not competing on credit assignment. If accurate credit assignment leads to better transfer, shouldn't achieving the best credit assignment model (thus competing in credit assignment) lead to better transfer results?",,arg_request,arg-request_clarification,4
2. What effect does the window size for transforming states to observations have on the performance of SECRET?,,arg_request,arg-request_explanation,0
"3. On a high-level, how does SECRET compare to transfer through relational deep reinforcement learning: https://arxiv.org/abs/1806.01830? Relational models use self-attention mechanisms to extract and exploit relations between entities in the scenes for better generalization and transfer.",,arg_request,arg-request_experiment,2
I'm curious what happens if SECRET is allowed to exploit relations in the environment.,,arg_request,arg-request_experiment,2
4. What happens if the reward model uses very few trajectories and is not able to predict good rewards? Does transfer through credit assignment become detrimental?,,arg_request,arg-request_explanation,0
5. Are the samples generated in the target domain for collecting attention weights included in the number of episodes when evaluating SECRET?,,arg_request,arg-request_explanation,0
"6. On a lighter note, I don't believe using a coffe-brewing machine has a 'universally invariant structure' of coffee-making. That's a luxurious way of making coffee :) In the developing world, we still need to boil water, pour coffee powder in it, etc., all without a coffee-brewing machine.",,arg_request,arg-request_experiment,2
"In equations 1 and 2, should a, b be written in capital? Since they represent random variables.",,arg_request,arg-request_typo,3
"1. How did you get the instance boxes, union boxes, and binary masks in testing?",,arg_request,arg-request_explanation,0
2. What are the training and inference time?,,arg_request,arg-request_explanation,0
"1. In Section 2, I find the sentence ""We follow a similar intuition but instead of decomposing a 1D signal of time into its components, we transform the time itself and feed its transformation into the model that is to consume the time information"" really unclear. Could you rephrase it?",,arg_request,arg-request_edit,1
Did you try these encodings with non-recurrent architectures?,,arg_request,arg-request_experiment,2
"3. In Section 5.2, did you mean 'fixing t2v(\tau)[n] = sin(2\pi n \tau / 16)'? i.e. I think it's missing a 'tau'",,arg_request,arg-request_typo,3
"5. Could you clarify exactly how time is encoded for LSTM + T? Are you, in fact, just passing a float value? How is this encoded for each data set?",,arg_request,arg-request_clarification,4
"I would introduce the notation for t2v(tau) upfront and use that to define a(tau, k)[j] and f_j",,arg_request,arg-request_edit,1
"You show that this works for a rescaling from 2pi/7 to 2pi/14, but it would be nice if there was experimental confirmation of this property with frequency > 1.",,arg_request,arg-request_result,5
"Still, it would be good to show that trained performance doesn't depend on the initialization values more than a standard LSTM+T model.",,arg_request,arg-request_experiment,2
"* You have an interesting corner case where your neural network parameters are interpretable: you can interpret the omega values from your model as frequencies and investigate their values to see which kinds of periodicity your model uses. You do something like this on p.7, but it would be neat to see a histogram like the one you have for EventMNIST for one of the real-world datasets to see if it learns the domain-relevant time knowledge you claim that it should learn.",,arg_request,arg-request_experiment,2
"Overall, the paper requires significant improvement.",,arg_request,arg-request_edit,1
4. A theoretical analysis of the convergence of the optimization algorithm could be needed.,,arg_request,arg-request_explanation,0
1. What is the Gumbel-max trick?,,arg_request,arg-request_explanation,0
2. How to tune the parameters discussed in training details in the experiments?,,arg_request,arg-request_clarification,4
3. Why to use experience replay for the linear experiments?,,arg_request,arg-request_explanation,0
4. Are there evaluations on the utility of proposed compared to existing approaches?,,arg_request,arg-request_explanation,0
5. Does the proposed approach work in real-world problems?,,arg_request,arg-request_result,5
6. Was there any concrete theoretical guarantee to ensure the convergence of the algorithm.,,arg_request,arg-request_clarification,4
Some additional experiments can make the paper stronger:,,arg_request,arg-request_experiment,2
* Compare the result of the procedure to an exhaustive search in a setting where the latter is feasible (shallow architecture on an easy task with few possible bit widths),,arg_request,arg-request_experiment,2
"* Compare the procedure to other state of the art NAS procedures (DARTS and ENAS) with the same search space adapted to the quantization problem, to empirically show that the proposed procedure is a compromise between these two methods as claimed by the authors.",,arg_request,arg-request_experiment,2
1.	The paper is easy to follow but the authors are expected to clarify the rationality in integration of the loss function.,,arg_request,arg-request_edit,1
"How the parameter of \lambda_r, \lambda_t, and \lambda_r influence the performance.",,arg_request,arg-request_explanation,0
It would be better if the authors could present some analysis.,,arg_request,arg-request_explanation,0
"The authors are expected to make more comprehensive analysis with the state-of-the-art methods, and also analyze why some alternative methods outperforms the proposed methods in table I and table II.",,arg_request,arg-request_experiment,2
3.,,arg_request,arg-request_edit,1
