sentence,index,review_action,fine_review_action,aspect,target
"How are the lambda and threshold parameters tuned? The authors mention a validation set, are they just exhaustively explored on a 3D grid on the validation set?",,arg_request,arg-request_explanation,asp_replicability,0
"The results only compare with Shim et al. Why only this method? Why would it be expected to be faster than all the other alternatives? Wouldn't similar alternatives like the sparsely gated MoE, D-softmax and adaptive-softmax have chances of being faster?",,arg_request,arg-request_explanation,asp_soundness-correctness,0
"""Sparse Mixture of Sparse of Sparse Experts""",,arg_request,arg-request_edit,asp_clarity,1
"""if we only search right answer""",,arg_request,arg-request_edit,asp_clarity,1
"""it might also like appear""",,arg_request,arg-request_edit,asp_clarity,1
"""which is to design to choose the right""",,arg_request,arg-request_edit,asp_clarity,1
sparsly,,arg_request,arg-request_edit,asp_clarity,1
"""will only consists partial""",,arg_request,arg-request_edit,asp_clarity,1
"""with γ is a lasso threshold""",,arg_request,arg-request_edit,asp_clarity,1
"""an arbitrarily distance function""",,arg_request,arg-request_edit,asp_clarity,1
"""each 10 sub classes are belonged to one""",,arg_request,arg-request_edit,asp_clarity,1
"""is also needed to tune to achieve""",,arg_request,arg-request_edit,asp_clarity,1
I believe that the presentation of the proposed method can be significantly improved.,,arg_request,arg-request_edit,asp_clarity,1
"For example, in Sections 4 and 5 I was hoping to see comparisons to methods like MAML.",,arg_request,arg-request_experiment,asp_meaningful-comparison,2
"Also, I felt that the proposed approach in Section 5 is very similar to MAML intuitively. This makes a comparison with MAML even more desirable.",,arg_request,arg-request_experiment,asp_meaningful-comparison,2
"Also, this work would benefit significantly from a better experimental evaluation.",,arg_request,arg-request_experiment,asp_substance,2
It may be worth adding a short discussion/comparison to that work as it also considers zero-shot learning.,,arg_request,arg-request_experiment,asp_meaningful-comparison,2
"- Capitalize: “section” -> “Section”, “appendix” -> “Appendix”, “fig.” -> “Figure”.",,arg_request,arg-request_typo,asp_clarity,3
- “Hold-out” vs “held-out”,,arg_request,arg-request_typo,asp_clarity,3
Be consistent and use “held-out” throughout.,,arg_request,arg-request_edit,asp_clarity,1
"It would also be great if the paper could related to other existing work that uses Bayesian neural networks in an active learning setting such as Bayesian optimization [3, 4] or Bandits[5].",,arg_request,arg-request_experiment,asp_meaningful-comparison,2
"1) Besides an comparison to the work by Lakshminarayanan et. al, I would also like to have seen a comparison to other existing Bayesian neural network approaches such as stochastic gradient Markov-Chain Monte-Carlo methods.",,arg_request,arg-request_experiment,asp_meaningful-comparison,2
"2) To provide a better understanding of the paper, it would also be interesting to see how sensitive it is with respect to the ensemble size M.",,arg_request,arg-request_experiment,asp_soundness-correctness,2
"4) Figure 3: Are the results averaged over multiple independent runs? If so, how many runs did you perform and could you also report confidence intervals? Since all methods are close to each other, it is hard to estimate how significant the difference is.",,arg_request,arg-request_explanation,asp_clarity,0
"- Pg. 5, Section 3.4: ""...this is would achieve...""",,arg_request,arg-request_typo,asp_substance,3
"- Pg. 6: ...thedse value of 90...""",,arg_request,arg-request_typo,asp_substance,3
I would appreciate if the authors can survey and compare more baselines in their paper instead of listing some basic ones.,,arg_request,arg-request_experiment,asp_meaningful-comparison,2
"2.1 Baselines: For noisy labels, the authors should add MentorNet [1] as a baseline https://github.com/google/mentornet From my own experience, this baseline is very strong.",,arg_request,arg-request_experiment,asp_meaningful-comparison,2
"At the same time, they should compare with VAT [7].",,arg_request,arg-request_experiment,asp_meaningful-comparison,2
"For adversarial attacks, the author should compare with data type from [10], and list L-FBGS [11] as a basic baseline.",,arg_request,arg-request_experiment,asp_meaningful-comparison,2
"2.2 Datasets: For datasets, I think the author should first compare their methods on symmetric and aysmmetric noisy data.",,arg_request,arg-request_experiment,asp_substance,2
The authors are encouraged to conduct 1 NLP dataset.,,arg_request,arg-request_experiment,asp_substance,2
"The authors acknowledge this connection, but I think they should begin by introducing CE and then explain how Cakewalk generalizes it.",,arg_request,arg-request_edit,asp_meaningful-comparison,1
Is there any explanation for this?,,arg_request,arg-request_explanation,asp_soundness-correctness,0
"4. How were the hyperparameters (learning rate, AdaGrad $\delta$, Adam $\beta_1, \beta_2$) chosen?",,arg_request,arg-request_clarification,asp_replicability,4
I would suggest tuning these values for each method independently.,,arg_request,arg-request_experiment,asp_soundness-correctness,2
5. It would be nice to see experimental results on more than one problem.,,arg_request,arg-request_experiment,asp_substance,2
"The reason for this is only given in a single sentence at the end of Section 6, so it is a little confusing.",,arg_request,arg-request_explanation,asp_soundness-correctness,0
I would replace these values with N/A or something similar.,,arg_request,arg-request_edit,asp_soundness-correctness,1
"We should see the performance on other datasets  (e.g., some of the other datasets in Wu et al. (2018)).",,arg_request,arg-request_experiment,asp_meaningful-comparison,2
My introduction suggestion: do not talk about Convolutional neural networks (CNNs). There is a *lot* of work on graph convolutional networks (GCNs).,,arg_request,arg-request_edit,asp_meaningful-comparison,1
"- I don't understand why the authors say that their space ""interpolates smoothly"" just because the limit in the curvature is the same from the left and right side.",,arg_request,arg-request_explanation,asp_soundness-correctness,0
"For example, the absolute value function has the same limit from the left and the right at 0, but it's not differentiable there. Is it actually true that if we take the derivatives of the piecewise hyperbolic/spherical distance function that it's differentiable at c=0?",,arg_request,arg-request_explanation,asp_soundness-correctness,0
I do recommend that the authors compare against those results in a future update of this work.,,arg_request,arg-request_experiment,asp_substance,2
You probably have to limit the operation to a half-sphere (there's some ideas for this in Gu et al).,,arg_request,arg-request_edit,asp_soundness-correctness,1
"- For the synthetic tree, why is the number of edges 2(|V|-1) rather than |V|-1?",,arg_request,arg-request_explanation,asp_soundness-correctness,0
- Are the curvatures the same for each layer for the GCNs?,,arg_request,arg-request_clarification,asp_clarity,4
This is an interesting point to discuss (some of the NeurIPS papers I mentioned train the curvature for each layer).,,arg_request,arg-request_experiment,asp_substance,2
"Also, how do you select the number of factors of each type?",,arg_request,arg-request_clarification,asp_soundness-correctness,4
"The ""De Sa"" et al 2018 arxiv citation is really Sala et al and is an ICML '18 paper.",,arg_request,arg-request_typo,asp_soundness-correctness,3
"Similarly, Gulcehre et al is a 2019 ICLR paper, and so on. It's always good to get these right.",,arg_request,arg-request_typo,asp_soundness-correctness,3
- Is there any actual empirical importance from recovering the Euclidean case exactly for 0 curvature?,,arg_request,arg-request_explanation,asp_soundness-correctness,0
"- One useful thing to point out in B.3.3 is that in general, it need not be a diffeomorphism for all of M for any manifold, which leads to non-uniqueness.",,arg_request,arg-request_edit,asp_soundness-correctness,1
The authors need to describe in detail the algorithmic novelty of their work.,,arg_request,arg-request_edit,asp_originality,1
The definition of “recovering true factor exactly” need to be given.,,arg_request,arg-request_edit,asp_soundness-correctness,1
"Therefore, an appropriate choice of their values need to be given.",,arg_request,arg-request_edit,asp_replicability,1
"In the algorithm, the authors need to define the HT function in (3) and (4).",,arg_request,arg-request_edit,asp_clarity,1
"There are some typos that can be easily found, such as “of the out algorithm”.",,arg_request,arg-request_typo,asp_clarity,3
My main concern about this paper is why this algorithm has a better performance than CW attack?,,arg_request,arg-request_explanation,asp_meaningful-comparison,0
It would be nice to have some more intuitive explanations at least of Theorem 1.,,arg_request,arg-request_edit,asp_substance,1
"Also, it is clear in the experiments the superiority with respect to Arora in terms of iterations (and error), but what about computational time?",,arg_request,arg-request_clarification,asp_meaningful-comparison,4
"However, it is not clear to me that these are some novel results that can better help adversarial training.",,arg_request,arg-request_explanation,asp_substance,0
What would make it stronger imo is to address the issue of how much is gained from a discrete vs. continuous representation.,,arg_request,arg-request_edit,asp_substance,1
That makes sense -- but at what cost?,,arg_request,arg-request_clarification,asp_soundness-correctness,4
can  you elaborate?,,arg_request,arg-request_explanation,asp_soundness-correctness,0
"#1) I do not understand exactly what the ""general method"" means. Does it mean that you propose a method, where you can just change the F, such that to solve a different degradation problem? So you provide the general framework where somebody has to specify only the F?",,arg_request,arg-request_clarification,asp_substance,4
"For instance, Eq. 2,3 can be easily combined using the proportional symbol, Eq. 8,9,10,11 show actually the same thing.",,arg_request,arg-request_edit,asp_clarity,1
"#4) I think that the function F has to be differentiable, and this should be mentioned in the text.",,arg_request,arg-request_edit,asp_clarity,1
"Also, I believe that some actual (analytic) examples of F should be provided, at least  in the experiments.",,arg_request,arg-request_edit,asp_substance,1
The same holds for the p(Omega).,,arg_request,arg-request_edit,asp_substance,1
This parameter Omega is estimated individually for each degraded image?,,arg_request,arg-request_clarification,asp_substance,4
#5) Before Eq. 8 the matrix V is a function of z and should be presented as such in the equations.,,arg_request,arg-request_edit,asp_clarity,1
"#6) I believe that it would be nice to include a magnified image of Fig. 3, where the gradient steps are shown.",,arg_request,arg-request_edit,asp_clarity,1
"Also, my understanding is that the optimization goal is to find first a feasible solution, and then find the point that maximizes f. I think that this can be clarified in the text.",,arg_request,arg-request_clarification,asp_clarity,4
"I would suggest, at least, to include some empirical evidences in the experiments that show convergence.",,arg_request,arg-request_experiment,asp_clarity,2
#8) In the experiments I think that at least one example of F and p(Omega) should be presented.,,arg_request,arg-request_edit,asp_substance,1
"Also, what the numbers in Table 4 show? Which is the best value that can be achieved?",,arg_request,arg-request_explanation,asp_substance,0
"These numbers correspond to several images, or to a unique image?",,arg_request,arg-request_clarification,asp_substance,4
I would suggest to use the \simeq symbol instead.,,arg_request,arg-request_edit,asp_substance,1
"#2) After Eq. 6 the ""nonnegative"" should be ""nonzero"".",,arg_request,arg-request_edit,asp_substance,1
"#3) Additional density estimation models can be used e.g. VAEs, GMM.",,arg_request,arg-request_experiment,asp_substance,2
"#4) In Section 2 paragraph 2, the sentence ""However, they only ... and directly"" is not clear what means.",,arg_request,arg-request_clarification,asp_clarity,4
Most of my comments are improvements which can be easily included.,,arg_request,arg-request_edit,arg_other,1
"Also, I think that additional methods to compute the image prior should be included in the experiments.",,arg_request,arg-request_experiment,asp_substance,2
A good extension of this work would be to combine a text-derived embedding  or the synsets to interpolate the SPoSE dimensions for missing words in the original set.,,arg_request,arg-request_experiment,asp_substance,2
Or perhaps the object similarity ratings could be used in a semi-supervised setting to inform the learning of a co-occurence word embedding.,,arg_request,arg-request_experiment,asp_substance,2
This will allow the model to better describe a larger set of words.,,arg_request,arg-request_experiment,asp_substance,2
Another possible extension is to test this larger set of words on a non-behavioral NLP task to show possible improvements that the behavioral data and the interpretable space give.,,arg_request,arg-request_experiment,asp_substance,2
"Questions: I would like to see more discussion about difference between this work and [Z Hu, arXiv:1905.13728].",,arg_request,arg-request_explanation,asp_substance,0
"Comparing to the other work, what are strengths of this work? In addition, have the authors compared the performances of their work and [Z Hu, arXiv:1905.13728] using the same data?",,arg_request,arg-request_experiment,asp_substance,2
"Since the paper manages to use very few parameters (BTW, how many parameters in total do you have? Can you please add this number to the text?), it would be cool to see if second order methods like LBFGS can be applied here.",,arg_request,arg-request_clarification,asp_substance,4
"First, it doesn’t label the X axis.",,arg_request,arg-request_edit,asp_clarity,1
"Second, the caption mentions that early stopping is beneficial for the proposed method, but I can’t see it from the figure.",,arg_request,arg-request_edit,asp_clarity,1
"Third, I don’t get what is plotted on different subplots.",,arg_request,arg-request_edit,asp_clarity,1
"The text mentions that (a) is fitting the noisy image, (b) is fitting the noiseless image, and (c) is fitting noise. Is it all done independently with three different models?",,arg_request,arg-request_clarification,asp_clarity,4
"Then why does the figure says test and train loss? And why DIP loss goes up, it should be able to fit anything, right? If not and it’s a single model that gets fitted on the noisy image and tested on the noiseless image, then how can you estimate the level of noise fitting? ||G(C) - eta|| should be high if G(C) ~= x.",,arg_request,arg-request_edit,asp_clarity,1
"Also, in this quote “In Fig. 4(a) we plot the Mean Squared Error (MSE) over the number of iterations of the optimizer for fitting the noisy astronaut image x + η (i.e., FORMULA ...” the formula doesn’t correspond to the text.",,arg_request,arg-request_clarification,asp_clarity,4
I don’t get the details of the batch normalization used: with respect to which axis the mean and variance are computed?,,arg_request,arg-request_clarification,asp_clarity,4
"But first, it’s not obvious why this would be a good thing (or a bad thing for that matter)",,arg_request,arg-request_edit,asp_substance,1
"I think it should be vice versa, N >> n",,arg_request,arg-request_typo,arg_other,3
"It would be nice to have a version of Figure 6 with k = 6, so that one can see all feature maps (in contrast to a subset of them).",,arg_request,arg-request_edit,asp_clarity,1
"I’m also wondering, is it harder to optimize the proposed architecture compared to DIP?",,arg_request,arg-request_clarification,asp_substance,4
"- page 2: ""network's type to be class"" -> ""to be a class""",,arg_request,arg-request_typo,asp_clarity,3
- Evaluation Datasets: Did you take duplication in the crawled datasets into account? (Lopes et al. 2017 (DéjàVu: a map of code duplicates on GitHub) suggests that this is particularly problematic for JavaScript/TypeScript),,arg_request,arg-request_clarification,asp_substance,4
"When you say that full-matrix computation ""requires taking the inverse square root"", I assume you know that is not really correct?",,arg_request,arg-request_clarification,asp_soundness-correctness,4
"Why mention it here, if it's not being defined.",,arg_request,arg-request_explanation,asp_clarity,0
"It would help to clarify the connection between the Gram/correlation matrix of gradients and the Hessian and what is being done to ill-conditioning, since second order methods are basically designed for ill-conditioned problems..",,arg_request,arg-request_clarification,asp_clarity,4
"It is difficult to know what the theory says about the empirical results, given the tweaks discussed in Sec 2.2, and so it is difficult to know what is the benefit of the method versus the tweaks.",,arg_request,arg-request_explanation,asp_soundness-correctness,0
"If this method is to be useful, understanding how these spectral properties change during training for different types of networks is essential.",,arg_request,arg-request_explanation,asp_soundness-correctness,0
"You say that you ""informally state the main theorem.""  The level of formality/informality makes it hard to know what is really being said.  You should remove it if it is not worth stating precisely, or state it precisely.  (It's fair to modularize the proof, but as it is it's hard to know what it's saying, except that your method comes with some guarantee that isn't stated.)",,arg_request,arg-request_edit,asp_soundness-correctness,1
It is not clear to me why we need to add MC-Dropout to the ensemble.,,arg_request,arg-request_explanation,asp_soundness-correctness,0
What is the benefit of DEBAL over an ensemble method if both of them do not have Bayesian theoretic support?,,arg_request,arg-request_explanation,asp_soundness-correctness,0
"However, I do not understand how are the *discrete* output y is handled.",,arg_request,arg-request_explanation,asp_soundness-correctness,0
Is it indeed a discrete output (problem with lack of differentiability)? Softwax probability? Other modelling choice?,,arg_request,arg-request_clarification,asp_clarity,4
I’m not sure I understand what we are supposed to learn from the astrophysics experiments.,,arg_request,arg-request_clarification,asp_clarity,4
How do you guarantee that the representation learned by the neural network still obeys the property of Choquet integral? What is your loss or your algorithm?,,arg_request,arg-request_clarification,asp_replicability,4
These need to be further clarified.,,arg_request,arg-request_clarification,asp_replicability,4
"Thus, I would like to see more possible explanation on it.",,arg_request,arg-request_explanation,asp_soundness-correctness,0
"1. For the evaluation of DBA, I assume that there are 4 adversarial parties, controlling each of the 4 local triggers. When using centralized attacks, are there still 4 adversarial parties, although they share the same global trigger, or if there is only 1 adversarial party?",,arg_request,arg-request_clarification,asp_replicability,4
3. Can the authors show if the decomposition is also useful for trigger patterns that are not necessarily regular shapes?,,arg_request,arg-request_experiment,asp_substance,2
4. Can the authors show concrete examples on how the attacks are generated?,,arg_request,arg-request_explanation,asp_replicability,0
"Specifically, which features are perturbed, what are the values assigned as the trigger, and what is the corresponding target label?",,arg_request,arg-request_clarification,asp_replicability,4
"- Even though oracle results are interesting, to make it practical it may make sense to talk about a more realistic, weaker oracle where some of the queries may be incorrect.",,arg_request,arg-request_clarification,asp_substance,4
- It may even make sense to minimise the number of oracle calls which can be thought of as a resource and discuss the relationship between number of oracle calls and other resources such as space.,,arg_request,arg-request_edit,asp_substance,1
How do these compare? Does the proposed approach offer  insights on these datasets which are not captured by the comparison methods?,,arg_request,arg-request_clarification,asp_substance,4
"2. The human score of 91.4% is based on majority vote, which should be compared with an ensemble of deep learning prediction.",,arg_request,arg-request_experiment,asp_meaningful-comparison,2
To compare the author should use the average score of human.,,arg_request,arg-request_experiment,asp_meaningful-comparison,2
"It seems to me; the author assumed data from one modality is generated by all the latent factors (see Eq. (11)), then what is the point for assuming the prior of the latent factor is factorized (see Eq. (4) and (5))?",,arg_request,arg-request_explanation,asp_soundness-correctness,0
The author should comment on this.,,arg_request,arg-request_explanation,asp_soundness-correctness,0
"Also, the author should avoid using the word ""simply"" too often (see the last few paragraphs on page 5).",,arg_request,arg-request_edit,asp_clarity,1
"The author should also add more explanation here,",,arg_request,arg-request_edit,asp_clarity,1
d. What is the metric in Table 1 and 2?,,arg_request,arg-request_explanation,asp_clarity,0
e. What are the two modalities in Table 2? The author should explain.,,arg_request,arg-request_clarification,asp_clarity,4
The author should explain this.,,arg_request,arg-request_explanation,asp_soundness-correctness,0
"The author can perhaps consider the datasets used by Tsai et al. There are seven datasets, and they can all be modified to the setting of partially-observable multimodal data.",,arg_request,arg-request_experiment,asp_substance,2
I would like the method to be applied on other domains such as continuous non-convex optimization and reinforcement learning.,,arg_request,arg-request_experiment,asp_motivation-impact,2
"- The paper should refer to 1) the reward shaping literature, 2) the growing line of works concerned with control variates for REINFORCE (such as VIMCO, MuProp, REBAR) and 3) the growing line of works concerned about combinatorial optimization with reinforcement learning (Neural Combinatorial Optimization with Reinforcement Learning, etc.)",,arg_request,arg-request_edit,asp_meaningful-comparison,1
- I would also encourage the authors to come up with a more descriptive name for the approach.,,arg_request,arg-request_edit,asp_soundness-correctness,1
How to understand this phenomenon? Does this mean that the distribution of the layer outputs will not change too much if the layer is deep enough?,,arg_request,arg-request_explanation,asp_clarity,0
"2. Typos: the weight matrix in the end of page 2 should be N_l times N_{l-1}. Also, the x_i's in the first line of page 3 should be bold.",,arg_request,arg-request_edit,asp_soundness-correctness,1
"Section 2 contains mostly high level explanations of the work, which should be integrated in the introduction, and thus before the related work section, to improve readability.",,arg_request,arg-request_edit,asp_clarity,1
"Section 2 states that section 4.2 proves that a ""path method"" must be used in order to satisfy the axioms, but why such axioms are important is not stressed enough.",,arg_request,arg-request_explanation,asp_clarity,0
There is however many minor issues that should be fixed for the camera-ready version.,,arg_request,arg-request_edit,asp_clarity,1
"Although the recommended length is 8 pages, the strict limit is 10, so I would recommended to use a bit of the remaining extra space to conclude the paper properly with a discussion on the results and their consequences, as well as a conclusion to wrap up the paper.",,arg_request,arg-request_edit,asp_clarity,1
"- The term flow is never defined precisely, we need to infer it",,arg_request,arg-request_clarification,asp_clarity,4
- First paragraph would be more clear with simple word explanation rather than maths.,,arg_request,arg-request_clarification,asp_clarity,4
- Work on image indicators of importance could be compared better with current work.,,arg_request,arg-request_edit,asp_meaningful-comparison,1
"- This sentence is not clear: ""[...]; the nature of correlations in the two models may differ"".",,arg_request,arg-request_edit,asp_clarity,1
The statements should be put in perspective with others.,,arg_request,arg-request_edit,asp_clarity,1
- Section 3 should be introduced by explaining the goal of the section otherwise it breaks the flow of reading.,,arg_request,arg-request_edit,asp_clarity,1
- The role of the baseline x' should be better explained when it is presented (first paragraph of section 3).,,arg_request,arg-request_edit,asp_clarity,1
"Different terms should be use, even if the context makes it possible to infer which one is being referred to.",,arg_request,arg-request_edit,asp_clarity,1
Some explanations should be devoted to it.,,arg_request,arg-request_explanation,asp_clarity,0
"Why couldn't we take another layer's neuron as the neuron of interest, bounding the conductance measure on one layer as the input and the output of the model? If we make the input to be a neuron y rather than the true input x, we could take another neuron z in a subsequent layer to be the neuron of interest, resulting in conductance measure Cond^z_i(y).",,arg_request,arg-request_explanation,asp_clarity,0
- List of importance measure at beginning of Section 4 should probably have citations.,,arg_request,arg-request_edit,asp_meaningful-comparison,1
"- Backward reference to section 3 seems to be a mistake, should it be subsection 4.2?",,arg_request,arg-request_edit,asp_clarity,1
- Each of the justifications to get around the issue of distinguishing strange model behavior from bad feature importance technique should be explained briefly in paragraph before section 4.1.,,arg_request,arg-request_explanation,asp_clarity,0
"g(f(1 - epsilon)) = 0, why would it be 1- epsilon?",,arg_request,arg-request_typo,asp_clarity,3
- Problem explained in fifth Paragraph of section is not clear unless what the influence of the unit is clearly stated. Is it simply dg/df?,,arg_request,arg-request_clarification,asp_clarity,4
- A short explanation of what is tested in section 6 should be given at last paragraph of section 4.1.,,arg_request,arg-request_explanation,asp_clarity,0
"Although the results are favorable to the conductance metric, it is not clear how they precisely confirm the problem of incorrect signs presented in the caricature examples.",,arg_request,arg-request_explanation,asp_substance,0
- Footnote 2 on page 5 it difficult to read.,,arg_request,arg-request_edit,asp_clarity,1
- The importance of section 4.2 should be clarified.,,arg_request,arg-request_clarification,asp_clarity,4
More emphasis on the importance of the axioms (desirable properties) should be made.,,arg_request,arg-request_edit,asp_clarity,1
- Choices for experiments should be explained. Why choosing layers mixed** rather than others? Why choosing filters?,,arg_request,arg-request_explanation,asp_clarity,0
"Since this is qualitative, I suggest to change the saturation of the images to make them easier to interpret.",,arg_request,arg-request_edit,asp_clarity,1
"- Figure 4 could be more interesting if compared with other classes, like other animal faces.",,arg_request,arg-request_experiment,asp_substance,2
- Space should be added between figures to better divide the captions,,arg_request,arg-request_edit,asp_clarity,1
- The difference between experiments of Figure 5 and 6 should be made more clear.,,arg_request,arg-request_edit,asp_clarity,1
- Where are they? No discussion? No conclusion?,,arg_request,arg-request_explanation,arg_other,0
Typhimurium. Could you explain why your MUTAG is now a single graph and is cast as node,,arg_request,arg-request_explanation,asp_replicability,0
* The upsampling analysis is interesting but it is only done on synthetic data - will the result hold for natural images as well? should be easy to try and will allow a better understanding of this choice.,,arg_request,arg-request_result,asp_substance,5
"This is just channel-wise normalization with some extra parameters - no need to call it this way (even if it's implemented with the same function) as there is no ""batch"".",,arg_request,arg-request_edit,asp_clarity,1
* I would have loved to see actual analysis of the method's performance as a function of the noise standard deviation.,,arg_request,arg-request_edit,asp_substance,1
"Specifically, for a fixed k, how would performance increase or decrease, and vice versa - for a given noise level, how would k affect performance.",,arg_request,arg-request_explanation,asp_substance,0
* The actual standard deviation of the noise is not mentioned in any of the experiments (as far as I could tell),,arg_request,arg-request_clarification,asp_clarity,4
* What does the decoder produce when taking a trained C on a given image and changing the source noise tensor?,,arg_request,arg-request_result,asp_substance,5
"I think that would shed light on what structures are learned and how they propagated in the image, possibly more than Figure 6 (which should really have something to compare to because it's not very informative out of context).",,arg_request,arg-request_clarification,asp_clarity,4
Why should we use embedding to compare the similarity?,,arg_request,arg-request_explanation,asp_soundness-correctness,0
"For example, Page 6. “is pre-trained on a pure set of negative samples”— what is the objective function? How to train on only negative samples?",,arg_request,arg-request_clarification,asp_clarity,4
One way to improve to work would be to provide practical takeaways from the theory or to provide experiments in the main paper.,,arg_request,arg-request_experiment,asp_substance,2
- What are the practical implication of your work ? for instance does it say anything on how to tune $\gamma$ for CO ?,,arg_request,arg-request_explanation,asp_soundness-correctness,0
- (Minor) $\alpha$ not alway have the same unit: Thm 3.2 it is proportional to a strong convexity and in Lemma 4.7 it is proportional to a strong convexity squared (actually the PL of the squared norm of the gradient).,,arg_request,arg-request_edit,asp_soundness-correctness,1
For clarity it might be interesting to use the notation $\alpha^2$ in Lemma 4.7.,,arg_request,arg-request_edit,asp_clarity,1
The same way for unit consistency I would use $L_H^2$ instead of $L_H$,,arg_request,arg-request_edit,asp_clarity,1
The authors should address that in their revision.,,arg_request,arg-request_edit,asp_soundness-correctness,1
- How much does the image matter for the single-image data set?,,arg_request,arg-request_explanation,asp_substance,0
The selected images A and B are of very high entropy and show a lot of different objects (image A) and animals (image B). How do the results change if e.g. a landscape image or an abstract architecture photo is used?,,arg_request,arg-request_explanation,asp_substance,0
"- How general is the proposed approach? How likely is it to generalize to other approaches such as Jigsaw (Doersch et al., 2015) and Exemplar (Dosovitskiy et al., 2016)? It would be good to comment on this.",,arg_request,arg-request_explanation,asp_substance,0
It would therefore be instructive to add experiments for ResNet to see how well the results generalize to other network architectures.,,arg_request,arg-request_experiment,asp_substance,2
"- Does the MonoGAN exhibit stable training dynamics comparable to training WGAN on CIFAR-10, or do the training dynamics change on the single-image data set?",,arg_request,arg-request_explanation,asp_substance,0
"Overall, I’m leaning towards accepting the paper, but it would be important to see how well the experiments generalize to i) ResNet and ii) other (lower entropy) input images.",,arg_request,arg-request_experiment,asp_substance,2
"- the sentence ""Bacause the identity of the datapoint can never be learned by ..."" What is the identity of a dat point?",,arg_request,arg-request_clarification,asp_clarity,4
"I'm wondering whether other smoothness regularizations can achieve the same effect when applied to semi-supervised learning, e.g. spectral normalization[3].",,arg_request,arg-request_explanation,asp_soundness-correctness,0
It would be better to compare with them.,,arg_request,arg-request_experiment,asp_meaningful-comparison,2
Does the fMRI data exhibit time dependence?,,arg_request,arg-request_clarification,asp_substance,4
So what’s the key difference? Hierarchical learners can avoid this problem.,,arg_request,arg-request_clarification,asp_substance,4
"3, In Sec. 3, the first and section paragraph, I can not quite understand how the ME bias theory guide the following synthetic experiments.",,arg_request,arg-request_clarification,asp_substance,4
"4, in Sec. 3.1, considering the small number of training instances, whether it is large enough to train the NN/ not overfitting issues?",,arg_request,arg-request_explanation,asp_substance,0
5. Whether the ME bias mostly attributed to the one-hot representation? If one uses word2vec as the representation in NN,,arg_request,arg-request_explanation,asp_substance,0
It would be interesting to see how it can improve existing learning-based theorem provers.,,arg_request,arg-request_explanation,asp_substance,0
"My question is if we want to integrate the proposed method into theorem provers, after multiple steps of math reasoning, how would us know the goal has been proved? Is it possible that we can train a decoder that maps back from the latent space to the formula space?",,arg_request,arg-request_explanation,asp_soundness-correctness,0
Also can it work with theorems that decompose the current goal into several sub-goals?,,arg_request,arg-request_explanation,asp_soundness-correctness,0
"1. Typo: Third paragraph in section 1, ""...which is makes use of ..."".",,arg_request,arg-request_typo,asp_clarity,3
2. It's very confusing when the authors introduce \sigma and \omega in the beginning of section 4: why would you need two networks predict the same thing?,,arg_request,arg-request_explanation,asp_clarity,0
"7. Typo: Page 5 last paragraph, ""... negative instances for for each ..."".",,arg_request,arg-request_typo,asp_clarity,3
"For example, for baseline 1, it is very hard to understand why would we want to use such an unusual baseline, and why it is called a ""random baseline"".",,arg_request,arg-request_explanation,asp_replicability,0
More motivation for experimental section is needed.,,arg_request,arg-request_explanation,asp_motivation-impact,0
The authors should rethink the structure of the experimental section from the standpoint of convincing someone to use this method.,,arg_request,arg-request_edit,asp_clarity,1
In section 4.1 the authors have a good discussion on what is wrong with other methods in order to motivate their approach but then they don't deliver significant evidence in the later part of the section.,,arg_request,arg-request_explanation,asp_substance,0
The paper needs more discussion and experiments to explain how and why to use this approach.,,arg_request,arg-request_explanation,asp_substance,0
There are many typos and grammar errors,,arg_request,arg-request_typo,asp_clarity,3
I would have liked to see a bit more analysis as to why some pre-training strategies work over others.,,arg_request,arg-request_experiment,asp_substance,2
"However, this does not take into account the time already spent on pre-training. Perhaps the authors can include some results as to the total time taken as well as amortized total time over a number of different downstream tasks.",,arg_request,arg-request_result,asp_substance,5
"To verify this, the authors might need to compare the results with those methods (which use the generated soft probability of ground truth classes for training), and the ""random-noisy copies of soft principle label"" mentioned above.",,arg_request,arg-request_result,asp_substance,5
"The authors might want to compare to ""Mathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze.",,arg_request,arg-request_result,asp_meaningful-comparison,5
Some important equations in the paper should be numbered.,,arg_request,arg-request_typo,asp_substance,3
images. It would be great if the paper also made some attempt to,,arg_request,arg-request_experiment,asp_substance,2
- I'm a little unclear when data-augmentation is included in the,,arg_request,arg-request_explanation,asp_substance,0
"- Last paragraph page 4: ""when the accuracy gets over 60\% and again",,arg_request,arg-request_explanation,asp_substance,0
"* First paragraph page 5: ""more shallow"" --> ""shallower""",,arg_request,arg-request_typo,asp_substance,3
"* Page 7, first paragraph of section 5.: ""is ran"" --> ""is run""",,arg_request,arg-request_typo,asp_substance,3
"* Using ""scenarii"" for the plural of ""scenario"" I would say is pretty",,arg_request,arg-request_typo,asp_substance,3
"Specifically, what if the few-shot learning component is removed, and the network is trained with standard domain adaptation.",,arg_request,arg-request_explanation,asp_soundness-correctness,0
"Also it seems that the regular batch normalization could be very sensitive to domain shifts, and it would be good if the authors can test other normalization schemes such as layer/group normalization as baselines.",,arg_request,arg-request_experiment,asp_substance,2
"BSD 500 only contains 500 images, and it would be good if more diverse set of images are considered.",,arg_request,arg-request_experiment,asp_substance,2
Other domain transfer settings such as synthetic rendered vs. real (e.g. visDA challenge) could have been considered.,,arg_request,arg-request_experiment,asp_substance,2
"Did the authors also examine the relationship between mutual information and generalization error for CIFAR data sets? Does it not make sense to examine this for all (most of) the setups considered in Table 4, 8, and 9.",,arg_request,arg-request_explanation,asp_substance,0
"In these tables, how do the authors decide which hidden layer representations should be explored for their statistical characteristics?",,arg_request,arg-request_clarification,asp_soundness-correctness,4
"The reviewer feels that for CIFAR-10 and 100, some regularizers do consistently give best or close to best networks. Could the authors comment on this?",,arg_request,arg-request_clarification,asp_soundness-correctness,4
"It should be pointed out that the D in the hinge loss represents a neural network output without range restriction, while the D in the non-saturating loss represents sigmoid output, limiting to take in [0,1].",,arg_request,arg-request_edit,asp_substance,1
"In addition to report the median scores, standard deviations should be reported.",,arg_request,arg-request_experiment,asp_substance,2
It would be more convincing to see more experiments.,,arg_request,arg-request_experiment,asp_substance,2
"I would have been interested in ""false detection"" experiments: comparing estimator in a variety of problems where the mutual information is zero, but for different marginal distribution.",,arg_request,arg-request_experiment,asp_substance,2
Is it the data fit of the neural networks? With what specific measure?,,arg_request,arg-request_explanation,asp_substance,0
"In its current state, I am not sure that it adds a lot to the manuscript.",,arg_request,arg-request_experiment,asp_substance,2
It would be helpful to note in the description of Table 3 what is better (higher/lower).,,arg_request,arg-request_edit,asp_clarity,1
Also Table 3 seems to have standard deviations missing in Supervised DCGANs and Improved GAN for 4000 labels. And is there an explanation on why there isn’t an improvement in the FID score of SVHN for 1000 labels?,,arg_request,arg-request_explanation,asp_soundness-correctness,0
"What is the first line of Table 4? Is it supposed to be combined with the second? If not, then it is missing results. And is the Pi model missing results or can it not be run on too few labels? If it can’t be run, it would be helpful to state this.",,arg_request,arg-request_clarification,asp_clarity,4
"On page 11, “in Figure A2” the first word needs to be capitalized.",,arg_request,arg-request_typo,asp_clarity,3
"In Figure A1, why is there a dark point at one point in the inner circle? What makes the gradient super high there?",,arg_request,arg-request_clarification,asp_clarity,4
What are the differences of the 6 pictures in Figure A7? Iterations?,,arg_request,arg-request_clarification,asp_clarity,4
"Can the author be more specific about the x-axis in the illustration 1.a and 1.b? If I understand correctly, they are not the number of trails.",,arg_request,arg-request_explanation,asp_substance,0
I also suggest the paper discusses e-SNLI a bit more.,,arg_request,arg-request_edit,asp_meaningful-comparison,1
-	Should the title of the paper specify the paper is about “language-based” abductive reasoning.,,arg_request,arg-request_edit,asp_soundness-correctness,1
-,,arg_request,arg-request_typo,asp_soundness-correctness,3
"Do you agree that for this reason, all the experiments on the synthetic dataset is flawed?",,arg_request,arg-request_explanation,asp_soundness-correctness,0
"The main problems come from the experiments, which I would ask for more things.",,arg_request,arg-request_experiment,asp_substance,2
So how importance of each component to the whole framework? I would ask for the ablation study/additional experiments of using each component.,,arg_request,arg-request_experiment,asp_substance,2
How about combining only Pose2Pose/ Pose2Frame  with pix2pixHD? Whether the performance can get improved?,,arg_request,arg-request_experiment,asp_substance,2
"- Authors claim to identify a fundamental problem with the existing generative models for point clouds, yet Section 2 tries to show that a _specific version_ that uses DeepSet does not satisfy theoretical guarantees. What if we use e.g. a recurrent network instead?",,arg_request,arg-request_explanation,asp_soundness-correctness,0
I think that this is the crux of the paper that should be significantly expanded and experimentally validated.,,arg_request,arg-request_experiment,asp_substance,2
* The baselines in the experiments could be improved.,,arg_request,arg-request_experiment,asp_substance,2
"Secondly, it would be good to include a GAN-based baseline such as GAIN, as well as some more classic feature imputation method, e.g. MICE or MissForest.",,arg_request,arg-request_experiment,asp_substance,2
"1. Could you comment on the differences in your setup in Section 4.1 compared to the VAEAC paper? I’ve noticed that the results you report for this method significantly differ from the original paper, e.g. for VAEAC on Phishing dataset you report PFC of 0.24, whereas the original paper reports 0.394; for Mushroom it’s 0.403 vs. 0.244. I’ve compared the experimental details yet couldn’t find any differences, for example the missing rate is 0.5 in both papers.",,arg_request,arg-request_explanation,asp_soundness-correctness,0
"2. How do you explain that all methods have NRMSE > 1 on the Glass dataset (Table 1), meaning that they all most likely perform worse than a constant baseline?",,arg_request,arg-request_clarification,asp_soundness-correctness,4
* Contributions (1) and (2) should be merged together.,,arg_request,arg-request_edit,asp_clarity,1
*,,arg_request,arg-request_typo,asp_clarity,3
* Page 2: “This algorithm needs complete data during training cannot learn from partially-observed data only.”,,arg_request,arg-request_edit,asp_soundness-correctness,1
*,,arg_request,arg-request_edit,asp_clarity,1
"* Page 4, bottom: use Bernoulli distribution -> use factorized/independent Bernoulli distribution",,arg_request,arg-request_edit,asp_soundness-correctness,1
"* Page 5, bottom: the word “simply” is used twice",,arg_request,arg-request_typo,asp_clarity,3
* Page 9: learn to useful -> learn useful,,arg_request,arg-request_typo,asp_clarity,3
* Page 9: term is included -> term included,,arg_request,arg-request_typo,asp_clarity,3
* Page 9: variable follows Bernoulli -> variable following Bernoulli,,arg_request,arg-request_typo,asp_clarity,3
*,,arg_request,arg-request_typo,asp_clarity,3
"This also makes me wonder if this paper is more suitable for a journal - both in terms of the extensive supplementary material (e.g., curvature sampling algorithm and other details can be found only in supplementary), as well as the more rigorous review process that a journal paper goes through.",,arg_request,arg-request_edit,asp_clarity,1
"Since in the supplementary experiments also, it seems that curvature does have small variance in the results, how would the authors assess the robustness of the curvature sampling method with respect to the results?",,arg_request,arg-request_explanation,asp_soundness-correctness,0
-- Consider using a standard fonts for the equations.,,arg_request,arg-request_edit,arg_other,1
"-- In Equation (13), is there an activation function between W1 and W2?",,arg_request,arg-request_explanation,asp_substance,0
"-- Based on Table 1, why did not evaluate the proposed model with beam-search?",,arg_request,arg-request_experiment,asp_substance,2
1) I think definition 2.5 of Higher order Lipschitz is very strong assumption to have. What exactly means? Essentially the authors upper bounded any difficult term appear in the theorems. Is it possible to avoid having something so strong? Please elaborate.,,arg_request,arg-request_explanation,asp_soundness-correctness,0
2) In assumption 3.1 is not clear what $L_H$ is.,,arg_request,arg-request_clarification,asp_clarity,4
"3) What is the main difference on the combination of assumptions on Theorems 3.2, 3.2 and 3.4. Which one is stronger. Is there a reason for the existence of Theorem 3.3?",,arg_request,arg-request_explanation,asp_clarity,0
Can we avoid having the PL condition?,,arg_request,arg-request_explanation,asp_soundness-correctness,0
The authors need to elaborate more on this.,,arg_request,arg-request_explanation,asp_soundness-correctness,0
"In first paragraph of page 5 where the authors divide the existing literature into the three particular cases, I am suggesting to add the refereed papers inside each one of this cases (which papers assumed function g bilinear , which papers strongly convex-concave etc.)",,arg_request,arg-request_edit,asp_meaningful-comparison,1
I understand that the main contribution of the work is the theoretical analysis of the proposed method but would like to see some numerical evaluation in the main paper.,,arg_request,arg-request_result,asp_substance,5
There are some preliminary results in the appendix but it will be useful for the reader if there are are some plots showing the benefit of the method in comparison with existing methods that guarantee convergence (which method is faster?).,,arg_request,arg-request_result,asp_substance,5
I will keep my score the same but I highly encourage the authors to add some clarification related to my last comment on the globally bounded gradient.,,arg_request,arg-request_edit,asp_clarity,1
This needs to be clear in the paper (add clear arguments and related references).,,arg_request,arg-request_edit,asp_clarity,1
The authors either need to remove these results or restate them in a different way in order to satisfy the assumed conditions.,,arg_request,arg-request_edit,asp_soundness-correctness,1
Have authors ever tried Sinkhorn iteration?,,arg_request,arg-request_clarification,asp_substance,4
It would be nice if authors can provide some reasons and comparisons for their choice on the optimizer of W_U.,,arg_request,arg-request_explanation,asp_replicability,0
Does it mean that the W_s is not as stable as W_L or W_U during training?,,arg_request,arg-request_clarification,asp_soundness-correctness,4
"In such a situation, both the value and the dynamics of W_s will be very close to that of W_U. Does it mean that W_L is not so important as W_U?",,arg_request,arg-request_clarification,asp_soundness-correctness,4
"- The end of the 2nd line of lemma 1: P, G should be \mathbb{P}, \mathbb{G}",,arg_request,arg-request_typo,asp_clarity,3
- The 3rd line of lemma 1: epsilon1 -> epsilon_1,,arg_request,arg-request_typo,asp_clarity,3
"- Page 14, Eq(14), \lambda should be s",,arg_request,arg-request_typo,asp_clarity,3
"- Page 14, Eqs(13, 14), w(\mathbb{P}, \mathbb{G}) should appear on the right.",,arg_request,arg-request_typo,asp_clarity,3
Results on more scenes will make the performance more convincing.,,arg_request,arg-request_experiment,asp_substance,2
"I also wonder if the video data will be released, which could be important for the following comparisons.",,arg_request,arg-request_result,asp_replicability,5
"2. As to the results of the Pose2Pose network, I wonder if there are some artifacts that will affect the performance of the Pose2Frame network.",,arg_request,arg-request_explanation,asp_substance,0
"Then, there will be another question: how the two networks are trained? Are they trained separately or jointly?",,arg_request,arg-request_explanation,asp_replicability,0
I wonder how the straightforward regression term plus the smooth term will perform for the mask.,,arg_request,arg-request_explanation,asp_substance,0
"Here, the straightforward regression term means directly regress the output mask to the target densepose mask. Will the proposed mask term perform better?",,arg_request,arg-request_explanation,asp_substance,0
It would have been useful to compare the general models here with some specific math problem-focused ones as well.,,arg_request,arg-request_experiment,asp_meaningful-comparison,2
- One area that could stand to be improved is prior work.,,arg_request,arg-request_experiment,asp_meaningful-comparison,2
I'd like to see more of a discussion of *prior data sets* rather than papers proposing models for problems.,,arg_request,arg-request_experiment,asp_meaningful-comparison,2
"- The authors note that previous datasets are often specific to one type of problem (i.e., single variable equation solving). Why not then combine multiple types of extant problem sets?",,arg_request,arg-request_explanation,asp_substance,0
"Presumably the cost here would be to physically scan some of these workbooks, but this seems like a very limited investment. Why not build datasets based on workbooks, problem solving books, etc?",,arg_request,arg-request_explanation,asp_substance,0
- How do are the difficulty levels synthetically determined?,,arg_request,arg-request_explanation,asp_soundness-correctness,0
"- When generating the questions, the authors ""first sample the answer"". What's the distribution you use on the answer? This seems like it dramatically affects the resulting questions, so I'm curious how it's selected.",,arg_request,arg-request_explanation,asp_soundness-correctness,0
- The general methodology of generating questions and ensuring that no question is too rare or too frequent and the test set is sufficiently different---these are important questions and I commend the authors for providing a strong methodology.,,arg_request,arg-request_edit,asp_soundness-correctness,1
But it's not clear to me why testing more sophisticated models that are tailored for math questions would *not* be useful.,,arg_request,arg-request_clarification,asp_soundness-correctness,4
- Really would be good to do real-world tests in a more extensive way.,,arg_request,arg-request_experiment,asp_substance,2
"A 40-question exam for 16 year olds is probably far too challenging for the current state of general recurrent models. Can you add some additional grades here, and more questions?",,arg_request,arg-request_edit,asp_substance,1
"- For the number of thinkings steps, how does it scale up as you increase it from 0 to 16? Is there a clear relationships here?",,arg_request,arg-request_explanation,asp_clarity,0
"- Minor typo: in the abstract: ""test spits"" should be ""test splits""",,arg_request,arg-request_typo,asp_clarity,3
"While these may indeed be working as advertised, it would be good to see some evaluation that verifies that after learning, what is actually happening is coreference.",,arg_request,arg-request_result,asp_substance,5
Why does the graph update require coreference pooling again?,,arg_request,arg-request_explanation,asp_soundness-correctness,0
"Don't the updates in Eq 1 and 2 take care of this? The ablation does not test this, right?",,arg_request,arg-request_explanation,asp_soundness-correctness,0
"Another modeling choice that is not clear is regarding how the model processes the text -- reading prefixes of the paragraph, rather than one sentence at a time. What happens if the model is changed to be read one sentence at a time?",,arg_request,arg-request_explanation,asp_soundness-correctness,0
"On page 4, State update function, what is the meaning of variable ""Epsilon"" in the equation?",,arg_request,arg-request_clarification,asp_clarity,4
"From the supplementary, it seems Epsilon means the environment?",,arg_request,arg-request_clarification,asp_clarity,4
"Since q_rel require one hot vector as input, how to sample the q_rel given the importance score and how backprob the gradient in this case?",,arg_request,arg-request_clarification,asp_replicability,4
My assumption is the visual feature already contains the label information for image captioning.,,arg_request,arg-request_explanation,asp_soundness-correctness,0
"5. On relational detection task, is there a way to compare with the STOA method on some specific data split? This will leads to much more convincing results.",,arg_request,arg-request_experiment,asp_substance,2
"6. Similar as above question, on the object counting task, is there a way to compare with previous counting methods?",,arg_request,arg-request_experiment,asp_substance,2
Is the number right?,,arg_request,arg-request_clarification,asp_soundness-correctness,4
Is \eta_{\mathcal{H}} is a better estimation that previous adaptability term in multi-source DA? What is the role of \eta_{\mathcal{H}} in the algorithm design? How to control it in empirical algorithm?,,arg_request,arg-request_explanation,asp_soundness-correctness,0
Minor: The abstract could be improved by providing more clear pointers to the presented novelty.,,arg_request,arg-request_edit,asp_substance,1
"-4 Talking about ""agents"" and ""Multi-Agent"" is a somewhat confusing given the slightly different use of the same term in the reinforcement literature. Why not just ""mapping"" or ""network""?",,arg_request,arg-request_explanation,asp_soundness-correctness,0
So is the REINFORCE estimator used or something? Not that the importance sampling matter is orthogonal.,,arg_request,arg-request_explanation,asp_soundness-correctness,0
"In the case of continuous data x, is the reparameterization trick used?",,arg_request,arg-request_clarification,asp_soundness-correctness,4
"The paper should quantify that this leads to diverse ""agents"".",,arg_request,arg-request_edit,asp_soundness-correctness,1
"Have you considered training different agents on different subsets of the data, or trying different learning algorithms/architectures to learn them?",,arg_request,arg-request_explanation,asp_substance,0
More experiments on the diversity would help make the paper more convincing.,,arg_request,arg-request_experiment,asp_substance,2
"Instead of reporting a single time-vs-distortion data point, the authors could show the full trade-off curve.",,arg_request,arg-request_experiment,asp_substance,2
"- The authors only provide running times, not the number of iterations.",,arg_request,arg-request_experiment,asp_substance,2
"In principle all the algorithms should have a similar bottleneck in each iteration (computing a gradient for the input image), but it would be good to verify this with an iteration count vs success rate (or distortion) plot.",,arg_request,arg-request_experiment,asp_substance,2
This would also allow the authors to compare their theoretical iteration bound with experimental data.,,arg_request,arg-request_experiment,asp_substance,2
"In addition to these three main points, the authors could strengthen their results by providing experiments on another dataset (e.g., CIFAR-10) or model architecture (e.g., a ResNet), and by averaging over a larger number of test data points (currently 200).",,arg_request,arg-request_experiment,asp_substance,2
I encourage the authors to clarify these points in an updated version of their paper.,,arg_request,arg-request_clarification,asp_substance,4
"- How multimodal are the datasets provided by UCI? It seems like they consist of different tabular datasets with numerical or categorical variables, but it was not clear what the modalities are (each variable is a modality?) and how correlated the modalities are. If they are not correlated at all and share no joint information I'm not sure how these experiments can represent multimodal data.",,arg_request,arg-request_clarification,asp_soundness-correctness,4
"They should consider larger-scale datasets including image and text-based like VQA/VCR, or video-based like the datasets in (Tsai et al., ICLR 2019).",,arg_request,arg-request_experiment,asp_substance,2
"- In terms of prediction performance, the authors should also compare to [1] and [2] which either predict the other modalities completely during training or use tensor-based methods to learn from noisy or missing time-series data.",,arg_request,arg-request_edit,asp_meaningful-comparison,1
Is this how you implemented minibatching in practice? How easily is this applied to convolutional architectures?,,arg_request,arg-request_explanation,asp_soundness-correctness,0
- How many samples did you use from p(theta|x) during training?,,arg_request,arg-request_explanation,asp_soundness-correctness,0
- The experiments were entirely focused on uncertainty quality but we are always interested in both performance on the task at hand as as well as good uncertainty estimates. What was the performance based on e.g. classification accuracy on each of these tasks compared to the baselines? I believe that including these results will strengthen the paper and provide a more complete picture.,,arg_request,arg-request_experiment,asp_substance,2
- Have you checked / visualised what type of weight distributions do CDNs capture? It would be interesting to see if e.g. the marginal (across the dataset) weight distribution at each layer has any multimodality as that could hint that the network learns to properly specialise to individual data points.,,arg_request,arg-request_experiment,asp_substance,2
How influential is that extra term to the uncertainty quality that you obtain in the end?,,arg_request,arg-request_explanation,asp_soundness-correctness,0
How does this term affect the learned distributions in case of CDNs?,,arg_request,arg-request_explanation,asp_soundness-correctness,0
More discussion on both of these aspects can help in improving this paper.,,arg_request,arg-request_experiment,asp_substance,2
"Thus I believe that it would be better if you consider the same hyper parameter on all of these methods, e.g. the precision of the Gaussian prior.",,arg_request,arg-request_experiment,asp_substance,2
* Fig. 3 needs more explanation.,,arg_request,arg-request_explanation,asp_clarity,0
"The horizontal axes are unlabelled, and ""margin normalization"" is confusing when shown together with SN without an explanation.",,arg_request,arg-request_edit,asp_clarity,1
Perhaps it's helpful to briefly introduce it in addition to citing Bartlett et al. 2017.,,arg_request,arg-request_edit,asp_meaningful-comparison,1
* The epsilons in Fig. 5 have very different scales (0 - 0.5 vs. 0 - 5). Are these relevant to the specific algorithms and why?,,arg_request,arg-request_explanation,asp_clarity,0
"A typo in page 6, last line: wth -> with",,arg_request,arg-request_typo,arg_other,3
However it is valuable to have a similar analysis closer to the batch normalization setting used by most practitioners.,,arg_request,arg-request_experiment,asp_meaningful-comparison,2
"- Currently there is no experimental evaluation of the claims, which would be valuable given that the setting doesn't immediately apply in the normal batch normalization setting.",,arg_request,arg-request_experiment,asp_substance,2
I would like to see evidence that the main benefit from batch normalization indeed comes from picking a good adaptive learning rate.,,arg_request,arg-request_result,asp_soundness-correctness,5
"However, I found the contribution of the actor-critic model is insufficient and requires additional experimental validation (see below).",,arg_request,arg-request_experiment,asp_substance,2
"2. Since the actor-critic model is the novel component in this model (propagating gradients through the learned dynamics), I would like to see additional analysis and baseline comparisons of this method to previous actor-critic policy learning methods, such as DDPG and SAC training on the (fixed) latent state representations, and recent work of MVE or STEVE that use the learned dynamics to accelerate policy learning with multi-step updates.",,arg_request,arg-request_experiment,asp_substance,2
It'd be interesting to see how optimization of the actions would lead to better state representations by propagating gradients from the actor-critic model to the world model.,,arg_request,arg-request_experiment,asp_substance,2
Reward prediction along --> Reward prediction alone,,arg_request,arg-request_typo,asp_clarity,3
this limitation in latenby?,,arg_request,arg-request_typo,asp_clarity,3
"The large number of experiment although welcoming needs to be properly discussed and related to the state of the art numbers, including any work that the authors are referring themselves in this submission.",,arg_request,arg-request_edit,asp_soundness-correctness,1
It would be better if these heuristic arguments can be formed as theorems as well.,,arg_request,arg-request_result,asp_soundness-correctness,5
Is this a coincidence or a general phenomenon? Is there a theoretical explanation?,,arg_request,arg-request_explanation,asp_soundness-correctness,0
"Therefore, an interesting baseline for the evaluation of the ICL approach would be a predefined, fixed attention map consisting of concentric circles with the image center as their center, to show that the proposed approach does more than just deemphasizing the corners of the image)",,arg_request,arg-request_experiment,asp_substance,2
"Therefore, it may be a good idea for the authors to analyze the correlation between FSM changes and accuracy changes.",,arg_request,arg-request_experiment,asp_substance,2
"However, given that the achieved performance gains over the state-of-the-art are fairly small, it would be good to assess if the obtained improvements are statistically significant.",,arg_request,arg-request_experiment,asp_substance,2
"Furthermore, it may be informative to show the saliency maps in Figure 5 not only for cases in which the learner classified the image correctly in both time steps, but also cases in which the learner classified the image correctly the first time and incorrectly the second time.",,arg_request,arg-request_experiment,asp_substance,2
"Additionally, the previously mentioned evaluation steps, i.e., using a fixed attention map as baseline for the evaluation and evaluating the correlation between FSM and accuracy may be informative to illustrate the advantages of the proposed approach.",,arg_request,arg-request_experiment,asp_substance,2
The efficacy of the separate modifications should be tested.,,arg_request,arg-request_experiment,asp_substance,2
"Therefore I would like to see experiments with the ES cost function, but with",,arg_request,arg-request_experiment,asp_substance,2
"- I might have missed that, but are the authors offering an interpretation of their observation that the performance of the self-modulation model performs worse in the combination of spectral normalization and the SNDC architecture?",,arg_request,arg-request_experiment,asp_substance,2
I am curious about why that might be.,,arg_request,arg-request_explanation,asp_substance,0
- I would like to see some more interpretation on why this method works.,,arg_request,arg-request_clarification,asp_substance,4
- Did the authors inspect generated samples of the baseline and the proposed method? Is there a notable qualitative difference?,,arg_request,arg-request_experiment,asp_substance,2
"Overall, the idea is simple, the explanation is clear and experimentation is extensive. I would like to see more commentary on why this method might have long-term impact (or not).",,arg_request,arg-request_clarification,asp_motivation-impact,4
The authors need to be clear about their contribution. Is the paper only about replacing the traditional parametric functions of influence and probability with  deep neural networks?,,arg_request,arg-request_clarification,asp_motivation-impact,4
The experimental sections look rather mechanical. I would have put some results on the learned embedding. Or some demonstration of the embedded history or probability to intuitively convey the idea and how it works.,,arg_request,arg-request_experiment,asp_meaningful-comparison,2
"This is discussed on p4, but it's unclear to me how keeping $\sigma$ separate is benefitial for the analysis, and this is not picked up again explicitly again?",,arg_request,arg-request_explanation,asp_clarity,0
"- page 4, Sect. 4.4: Architecture of $\alpha$ would be nice (more than a linear layer?)",,arg_request,arg-request_explanation,asp_replicability,0
"- page 5, paragraph 3: ""we from some"" -> ""we start from some""",,arg_request,arg-request_typo,asp_clarity,3
"- p6par1: ""much cheaper then computing"" -> than",,arg_request,arg-request_typo,asp_clarity,3
"- p6par6: ""on formulas that with"" -> no that",,arg_request,arg-request_typo,asp_clarity,3
"- p6par7: ""measure how rate"" -> ""measure the rate""",,arg_request,arg-request_typo,asp_clarity,3
"- p8par1: ""approximate embedding $\alpha(e(\gamma'(...)))$ - $e$ is undefined and should probably be $e'$ (this is also the case in the caption of Fig. 5), and $c'$ should probably be included as well.",,arg_request,arg-request_edit,asp_soundness-correctness,1
"I believe that's what ""Pred (One Step)"" expresses, but it would maybe be generally helpful to be more precise about the notation",,arg_request,arg-request_edit,asp_clarity,1
The paper can be made more mathematically precise.,,arg_request,arg-request_edit,asp_clarity,1
The input and output types of each block in Figure 1. should be clearly stated.,,arg_request,arg-request_edit,asp_clarity,1
"For example, in Section 3.2, it can be made clear that the domain of the quantization function is the real and the codomain is a sequence k bits.",,arg_request,arg-request_edit,asp_clarity,1
"Since the paper relies so heavily on Lee et al., the authors should make an effort to summarize the approach in a mathematically precise way.",,arg_request,arg-request_edit,asp_clarity,1
Both the results on the development set and on the test set should be reported for the validity of the experiments.,,arg_request,arg-request_experiment,asp_substance,2
"In other words: at test time, do you present the original images only or transformed images too?",,arg_request,arg-request_explanation,asp_replicability,0
"-	In section 4, it is unclear why only the maximal activation of the softmax layer is used to characterize a sample? Why not considering the full distribution that should contain richer information? Why just focusing on the output layer and why not using the info available at intermediate layers?",,arg_request,arg-request_explanation,asp_soundness-correctness,0
"The authors should more clearly define what the private, public and evaluation sets are, right from the beginning.",,arg_request,arg-request_edit,none,1
"Again, why not using the distribution of the outputs of all layers? Why focusing only on the output of the last layer?",,arg_request,arg-request_explanation,asp_soundness-correctness,0
- How is the linearization of the inout done? It  typically matters,,arg_request,arg-request_explanation,asp_substance,0
"- Given the small size of the dataset, I would propose experimenting with non-neural approaches as well, which are also quite common in NLG.",,arg_request,arg-request_experiment,asp_replicability,2
What were the guidelines used?,,arg_request,arg-request_explanation,asp_clarity,0
-- What can be said about how computationally demanding the procedure is? running label propagation within meta learning might be too costly.,,arg_request,arg-request_explanation,asp_soundness-correctness,0
-- solving Eq 3 by matrix inversion does not scale. Would be best to also show results using iterative optimization,,arg_request,arg-request_edit,asp_soundness-correctness,1
It would be interesting to see how deep networks do for the hard cases.,,arg_request,arg-request_experiment,asp_substance,2
"It would also be interesting to see if additional assumptions, such as the existence of clusters or separation between clusters, make ordinal embedding simpler and thus tractable.",,arg_request,arg-request_result,asp_substance,5
Thinking along these directions would bring more insight and impact to both the ordinal embedding problem and optimization in deep networks.,,arg_request,arg-request_result,asp_substance,5
--> isn't this the same as using the hinge loss to bound the zero-one loss?,,arg_request,arg-request_clarification,asp_substance,4
It would be good to see more analysis on the axioms 1 through to 4 for the sake of completeness in the light of partial axiomatization of conductance.,,arg_request,arg-request_explanation,asp_substance,0
"It would be good to see some more insight in order to relate to interpretability of the importance of neurons, although there has been no claims made on it as its hard to measure importance without interpretability.",,arg_request,arg-request_explanation,asp_substance,0
"One issue is perhaps, very little in terms of related work.",,arg_request,arg-request_edit,asp_meaningful-comparison,1
I am curious to know if authors observe similar convergence gains in bAbI tasks as well. Can you please provide the mean learning curve for bAbI task for DNC vs proposed modifications?,,arg_request,arg-request_explanation,asp_substance,0
Can the authors elaborate on why this choice should intuitively be better than the proposed method alone?,,arg_request,arg-request_edit,asp_meaningful-comparison,1
"If that is true, I would suggest the authors to make this hidden assumption clearer in the paper",,arg_request,arg-request_edit,asp_clarity,1
1. The observation that gradients explode in spite of BN is quite counter-intuitive. Can you give an intuitive explanation of why this occurs?,,arg_request,arg-request_explanation,asp_soundness-correctness,0
"2. In a similar vein, there a number of highly technical results in the paper and it would be great if the authors provide an intuitive explanation of their theorems.",,arg_request,arg-request_clarification,asp_clarity,4
3. Can the statistics of activations be controlled using activation functions or operations which break the symmetry?,,arg_request,arg-request_explanation,asp_clarity,0
"For instance, are BSB1 fixed points good for training neural networks?",,arg_request,arg-request_explanation,asp_clarity,0
"4. Mean field analysis, although it lends an insight into the statistics of the activations, needs to connected with empirical observations.",,arg_request,arg-request_experiment,asp_substance,2
It would be good to mention this in the introduction or the conclusions.,,arg_request,arg-request_edit,asp_soundness-correctness,1
-	Can you elaborate more on why BatchNorm statistics are computed across all devices as opposed to per-device? Was this crucial for best performance?,,arg_request,arg-request_explanation,asp_soundness-correctness,0
-	It is not clear if provided analysis for large-scale GANs apply for small-medium sized GANs.,,arg_request,arg-request_clarification,asp_clarity,4
Providing such analysis would be also helpful for the community.,,arg_request,arg-request_result,asp_clarity,5
"-	How do you see the impact of the suggested techniques on tackling harder data-modalities for GANs, e.g. text or sequential data in general?",,arg_request,arg-request_explanation,asp_soundness-correctness,0
"For the language translation results, were there any other state-of-the-art methods that the authors could compare against? It seems they are only comparing against their own implementations.",,arg_request,arg-request_clarification,asp_substance,4
Objectively saying that the author's method is better than CycleGAN is difficult. How does their ensemble method compare to just their single-agent dual method? Is there a noticeable difference there?,,arg_request,arg-request_clarification,asp_soundness-correctness,4
"Perhaps I missed it, but I believe Dan Ciresan's paper ""Multi-Column Deep Neural Networks for Image Classification"" should be cited.",,arg_request,arg-request_edit,asp_meaningful-comparison,1
"Minor:  Since the action is denoted by ""a"",  it will be more clear if the authors use another symbol to denote the parameter of q(z) instead of ""\alpha"" at Eq 10 and 15.",,arg_request,arg-request_typo,asp_clarity,3
Does such approximation guarantee the policy improvement?,,arg_request,arg-request_explanation,asp_soundness-correctness,0
Any justification?,,arg_request,arg-request_explanation,asp_soundness-correctness,0
"(3) Instead of using the mean policy approximation in Eq 12, the authors should consider existing Monte Carlo techniques to reduce the variance of the gradient estimation.",,arg_request,arg-request_experiment,asp_substance,2
"For example, [1] could be used to reduce the variance of gradient w.r.t. \phi.",,arg_request,arg-request_experiment,asp_substance,2
(4) Are \theta and \phi jointly and simultaneously optimized at Eq 12?,,arg_request,arg-request_explanation,asp_replicability,0
The authors should clarify this point.,,arg_request,arg-request_explanation,asp_replicability,0
"(5) Due to the mean policy approximation, does the mean policy depend on \phi?",,arg_request,arg-request_explanation,asp_replicability,0
The authors should clearly explain how to update \phi when optimizing Eq 12.,,arg_request,arg-request_explanation,asp_replicability,0
"(6) If the authors jointly and simultaneously optimize \theta and \phi, why a regularization term about q_{\phi}(z)  is missing in Eq 12 while a regularization term about \pi_{\theta|z} does appear in Eq 12?",,arg_request,arg-request_explanation,asp_soundness-correctness,0
"For example, how to compute the gradient w.r.t. \phi? Since the mean policy is used, it is not apparent that how to compute the gradient w.r.t. \phi.",,arg_request,arg-request_experiment,asp_replicability,2
"Minor, 1/2 is missing in the last line of Eq 19.",,arg_request,arg-request_typo,asp_clarity,3
"That said, I would like to see more analysis on the behavior of the proposed method under various interesting cases not tested yet.",,arg_request,arg-request_experiment,asp_substance,2
How does the proposed method perform in more complicated tasks such as,,arg_request,arg-request_experiment,asp_replicability,2
Will it still be more favorable than other concerning baselines?,,arg_request,arg-request_explanation,asp_replicability,0
"- The word in the title should be “Convolutional”, right?",,arg_request,arg-request_typo,asp_clarity,3
"Also, the writing can be improved by making the writing more concise and formal (examples of informal: ""spoil the network"", ""model is spoiled"", ""problem of increased classes"", ""many recent researches have been conducted"", ""lots of things to consider for training"", ""supervised learning was trained"" etc.).",,arg_request,arg-request_edit,asp_clarity,1
The contributions of the method could also be underlined more clearly in the abstract and introduction.,,arg_request,arg-request_edit,asp_clarity,1
How can you interpret such a thought experiment?,,arg_request,arg-request_explanation,asp_substance,0
"Could you elaborate more on why the selection network is needed? How would it compare to a simple strategy of only including the datapoints whose top-1 prediction of ""classification network"" is greater than some threshold?",,arg_request,arg-request_explanation,asp_substance,0
"Finally, could you show a plot of top-1 prediction of ""classification network"" vs score of ""selection network"" and elaborate on that?",,arg_request,arg-request_experiment,asp_substance,2
"It would be interesting to see how these heuristics would do without ""selection network"", for example, either by doing simple self-training with thresholding on the score of the classifier or by applying only these heuristics in combination with TempEns+SNTG.",,arg_request,arg-request_experiment,asp_substance,2
How did you chose the current values? How sensitive is it? Why various datasets need different settings? How the threshold value can be set in practice?,,arg_request,arg-request_explanation,asp_substance,0
Another important parameters is the number of iterations of the algorithm.,,arg_request,arg-request_clarification,asp_substance,4
How was it chosen?,,arg_request,arg-request_clarification,asp_substance,4
"Concerning the experiments of section 4.2, how would the baseline methods of section 4.1 do in this case? Why did you select to study animal vs non-animals sets of classes?",,arg_request,arg-request_explanation,asp_substance,0
What would happen if you use random class splits or split animal classes (like in a more realistic scenario)?,,arg_request,arg-request_explanation,asp_substance,0
- The setting of including unrelated classes in the unlabeled data resembles transfer learning setting. Could you explain why the ideas from transfer learning are not applicable in your case?,,arg_request,arg-request_explanation,asp_substance,0
"- In the training procedure of ""selection network"" of Sections 3.1, do you use the same datapoints to train a ""classification network"" and ""selection network""? If it is the case, how do you insure that the ""classification network"" does not learn to fit the data perfectly and thus all labels s_i are 1?",,arg_request,arg-request_explanation,asp_substance,0
"- In the last sentences of the first paragraph on p.2 you make a contrast between using softmax and sigmoid functions, however, normally the difference between them is their use in binary or multiclass classification. Is there anything special that you want to show in you case?",,arg_request,arg-request_explanation,asp_substance,0
"- What do you mean in section 3.3 by ""if one class dominates the dataset, the model tends to overfit""?",,arg_request,arg-request_explanation,asp_substance,0
- I think parameters of training the networks from the beginning of section 4 could be moved to the supplementary materials.,,arg_request,arg-request_edit,arg_other,1
- Figure 3: wouldn't the plot of accuracy vs amount of data be more suitable here?,,arg_request,arg-request_explanation,asp_clarity,0
- Synthetic experiments of supplementary materials: the gains of the methods seem to be small. What are the numerical results? What would happen if you allow to select starting point at random (a more realistic case)?,,arg_request,arg-request_result,asp_substance,5
"- Can you explain the sentence ""To prevent data being added suddenly, no data was added until 5 iterations""?",,arg_request,arg-request_explanation,asp_substance,0
- How was it possible to improve the performance in experiment of section 4.2 with 100% of irrelevant classes?,,arg_request,arg-request_explanation,asp_substance,0
"Why, for example, did the authors only perform variational inference with the current and previous frames? Did conditioning on additional frames offer limited further improvement? Can the blurriness instead be attributable to the weak inference model?",,arg_request,arg-request_experiment,asp_substance,2
"If the authors have any ablation studies to back up their design choices, that would also be much appreciated, and will make this a more valuable paper for readers.",,arg_request,arg-request_clarification,asp_substance,4
"- While the results on SQOOP dataset are interesting, it would have been very exciting to see results on other synthetic datasets.",,arg_request,arg-request_experiment,asp_meaningful-comparison,2
"However, in the most general setting NMN will form a DAG and it would have been interesting to see what form of DAGs generalize better than other.",,arg_request,arg-request_experiment,asp_substance,2
"- It is not clear to me how the analysis done in this paper will generalize to other more complex datasets where the network layout NMN might be more complex, the number of modules and type of modules might also be more.",,arg_request,arg-request_experiment,asp_substance,2
"- Given that the accuracy drop is very significant moving from NMN-Tree to NMN-Chain, is there an explanation for this drop?",,arg_request,arg-request_explanation,asp_soundness-correctness,0
"- While the authors mention multiple times that #rhs/#lhs = 1 and 2 are more challenging than #rhs/#lhs=18, they do not sufficiently explain why this is the case anywhere in the paper.",,arg_request,arg-request_explanation,asp_soundness-correctness,0
- Small typo in the last line of section 4.3 on page 7. It should say: This is in stark contrast with “NMN-Tree” …,,arg_request,arg-request_typo,asp_clarity,3
"- Small typo in the “Layout induction” paragraph, line 6 on Page 7:",,arg_request,arg-request_typo,asp_clarity,3
…,,arg_request,arg-request_typo,asp_clarity,3
"I wonder except for the difference of memory consumption, how different it is compared to parameter space exploration.",,arg_request,arg-request_experiment,asp_substance,2
I am curious whether the paper has other good side effects.,,arg_request,arg-request_experiment,asp_substance,2
"For example, will the dropout cause the policy to be more robust?",,arg_request,arg-request_explanation,asp_soundness-correctness,0
"Furthermore, If I deploy the learning algorithm on a physical robot, will the temporally consistent exploration cause less wear and tear to the actuators when the robot explores.",,arg_request,arg-request_explanation,asp_soundness-correctness,0
"In addition, I would like to see some discussions whether this technique could be applied to off-policy learning as well.",,arg_request,arg-request_experiment,asp_substance,2
"In addition, the applications are very similar: image inpainting, denoising, super-resolution etc. Yeh et al.’s method should be the right baseline instead of the nearest neighbor algorithm.",,arg_request,arg-request_result,asp_meaningful-comparison,5
"Some other comments: what are the parameters of the degradation in the applications? For example, in image inpainting, does the proposed method learn the mask as well? So it is blind inpainting?",,arg_request,arg-request_explanation,asp_substance,0
On a more detailed level it is not clear why the log n representation for items is choosen -- why not just map to embeddings directly?,,arg_request,arg-request_explanation,none,0
"Do you mean that each latent variable is fixed to a value, such that this factor of variation is disabled? Are the canonicalizers pre-specified using meta-labels? Are they updated/learned during model training?",,arg_request,arg-request_explanation,asp_soundness-correctness,0
More explanation of canonicalization is needed.,,arg_request,arg-request_explanation,asp_soundness-correctness,0
Perhaps an example in linear algebra is needed.,,arg_request,arg-request_explanation,asp_soundness-correctness,0
2. The learning of the proposed model relies on meta-data description such that the learning is supervised. Can the method be applicable to situations where no meta-data and no class labels are available?,,arg_request,arg-request_explanation,asp_soundness-correctness,0
3. How can the proposed method be generalized to non-image data?,,arg_request,arg-request_explanation,asp_soundness-correctness,0
(1) than -> that,,arg_request,arg-request_typo,asp_clarity,3
"(2) Eq. (3): is there a superscription ""(j)"" on z_canon in decoder?",,arg_request,arg-request_typo,asp_clarity,3
"For the purpose of introduction, it might be better to give examples of expectation objectives such as:",,arg_request,arg-request_explanation,asp_motivation-impact,0
"- dropout: q is the distribution of NN outputs given the input image and integrating out latent dropout noises, gamma are parameters of this NN.",,arg_request,arg-request_explanation,asp_motivation-impact,0
"- VAE, GAN: q is the generative model defined as a mapping of a standard multivariate normal distribution by a NN.",,arg_request,arg-request_explanation,asp_motivation-impact,0
- sigmoid belief networks: q is a Bayesian network where each conditional distribution is a logistic regression model.,,arg_request,arg-request_explanation,asp_motivation-impact,0
"Then to state to which of these cases the results of the paper are applicable, allow for an improvement of the variance and at what additional computational cost (considering the cost of evaluating the discrete derivatives).",,arg_request,arg-request_explanation,asp_motivation-impact,0
I believe it should be discussed in related work.,,arg_request,arg-request_edit,asp_substance,1
Limitations / where the proposed method brings an improvement should be highlighted.,,arg_request,arg-request_result,asp_motivation-impact,5
"Equations (5) and (6) require a theorem of differentiating under integral (expectation), such as Leibnitz rule, which in case of (6) requires q_gamma(y)f(y) to be continuous in y and q_gamma(y) continuously differentiable in gamma.",,arg_request,arg-request_edit,asp_soundness-correctness,1
1. This work can make it clearer in principle how anti-aliasing contributes to improving the classification performance and robustness.,,arg_request,arg-request_edit,asp_clarity,1
"2. When showing the optimality of F-pooling in Section 2.3, the criterion is to reconstruct the original signal x.",,arg_request,arg-request_explanation,asp_substance,0
Experiments could be conducted on more benchmark datasets with more CNN architectures to convincingly show the effectiveness of the proposed F-pooling.,,arg_request,arg-request_experiment,asp_substance,2
"Although I do understand the problem of evaluation in unsupervised DA, this should have at least been done in the semi-supervised case, and some analysis/discussion should be included for the unsupervised one.",,arg_request,arg-request_edit,asp_meaningful-comparison,1
What if the proposed method performs that much better than baselines but they hyperparameters are not set correctly?,,arg_request,arg-request_explanation,asp_substance,0
"On the negative side, I think the relevance and novelty of the results should be explained better.",,arg_request,arg-request_edit,asp_clarity,1
Maybe the authors can elaborate more on the significance/relevance of this contribution.,,arg_request,arg-request_explanation,asp_soundness-correctness,0
- Eq(2),,arg_request,arg-request_typo,asp_soundness-correctness,3
- Theorem 2 should be Theorem 1,,arg_request,arg-request_edit,asp_soundness-correctness,1
"- ""there are constraints per which state can transition""",,arg_request,arg-request_typo,asp_clarity,3
"- ""P1 is agent"" -> ""P1 is the agent""",,arg_request,arg-request_typo,asp_clarity,3
"- ""Pinker"" -> ""Pinsker""",,arg_request,arg-request_typo,asp_clarity,3
- C_R in Eq(5) is not introduced.,,arg_request,arg-request_edit,asp_clarity,1
"A more interesting question to study is how this gap affect the iterative algorithm, and whether/how the error in the imperfect target policy accumulates/diminishes so that iteratively minimizing KL-divergence with imperfect \pi* (by MCTS) could still lead to optimal policy (Nash equilibrium).",,arg_request,arg-request_experiment,asp_soundness-correctness,2
Did you try applying the classification loss to both the encoded representation and the canonicalized representation at the same time?,,arg_request,arg-request_explanation,asp_soundness-correctness,0
"For Figure 2, Did you try applying canonicalizations in different orders? Do they give the same results?",,arg_request,arg-request_explanation,asp_soundness-correctness,0
I'm not sure I understand the PCA figures. Can you please explain how the first principal component was used to generate them?,,arg_request,arg-request_explanation,asp_replicability,0
"* ""data  tripets"" on page 2",,arg_request,arg-request_typo,asp_clarity,3
* Figure 5 should appear after Figure 4.,,arg_request,arg-request_edit,asp_clarity,1
"1) as an experimental study, it would be valuable to compare the performance of curiosity-based learning versus learning based on well-defined extrinsic rewards.",,arg_request,arg-request_experiment,asp_meaningful-comparison,2
"But for problems with well-defined extrinsic rewards, such a comparison could help readers understand the relative performance of curiosity-based learning and/or how much headroom there exists to improve the current methods.",,arg_request,arg-request_experiment,asp_meaningful-comparison,2
"The authors did provide literature in classification that had similar findings, but it would be beneficial for the authors to explore reasons that random features perform well in reinforcement learning.",,arg_request,arg-request_edit,asp_soundness-correctness,1
It would be nice to see a better case made for spherical convolutions within the experimental section.,,arg_request,arg-request_experiment,asp_substance,2
"While most related work was covered well, I believe the authors could have a more up-to-date list of recent work that reconstructs triangle-mesh representations from images [A-C] (especially since several of these methods has an architecture that involves encoding and subsequent compositional refinement).",,arg_request,arg-request_edit,asp_meaningful-comparison,1
I found that the core technical description was quite brief and would have benefited from simply more detail and space.,,arg_request,arg-request_edit,asp_substance,1
"You have argued that your method is sensible to try (cog. sci motivations), and shown that one instance works, but what can we expect in a more mathematical or general sense? Can any sizes of encoder and mapping network fit together? How does the number of mapping layers effect performance? Won't we eventually expect vanishing/exploding gradients with particular activation and can one address this in some way?",,arg_request,arg-request_explanation,asp_clarity,0
"Weak reject because the idea is quite interesting, but I believe a more thorough explanation and expanded experimental comparison would be of great help to ensure the community can appreciate this work.",,arg_request,arg-request_experiment,asp_substance,2
"[A] Pixel2Mesh: Generating 3D Mesh Models from Single RGB Images. Wang, Zhang, Li, Fu, Liu and Jiang. ECCV 2018.",,arg_request,arg-request_edit,asp_meaningful-comparison,1
"[B] MeshCNN: A Network with an Edge. Hanocka, Hertz, Fish, Giryes, Fleishman and Cohen-Or. SIGGRAPH 2019.",,arg_request,arg-request_edit,asp_meaningful-comparison,1
"[C] GEOMetrics: Exploiting Structure for Graph-Encoded Objects. Smith, Fujimoto, Romero and Meger. ICML 2019.",,arg_request,arg-request_edit,asp_meaningful-comparison,1
It will be also interesting to see how does the proposed method perform on large-scale datasets such as DomainNet and Office-Home dataset:,,arg_request,arg-request_experiment,asp_substance,2
"To improve the rating, the author should explain the following questions:",,arg_request,arg-request_explanation,arg_other,0
"1). Why minimizing the upper bound in this scenario would help to minimize the loss on the target domain, when the upper bound can be substantially huge? How about just evaluate \alpha by the closeness of the source domain with the target domain?",,arg_request,arg-request_explanation,asp_soundness-correctness,0
While the experiments are only performed on sentimental analysis and digit recognition. How about evaluating the proposed methods on real image recognition such as DomainNet or Office-Home?,,arg_request,arg-request_experiment,asp_substance,2
It asks an interesting question: can we learn a model of the training dynamics to avoid actually having to do the training?,,arg_request,arg-request_explanation,asp_substance,0
"Most importantly, I would like to see a measure of variance/uncertainty like confidence intervals included in the results; otherwise it's impossible to assess whether the results are likely to be significant or not. Other questions:",,arg_request,arg-request_result,asp_substance,5
Why was this policy used as the baseline? It seems extremely basic and unlikely to truly lead to optimal performance.,,arg_request,arg-request_clarification,asp_substance,4
"2. Why were more baselines from the related work not included? I understand the experiments are a proof of concept, but it would be nice to get a feeling for what some of the other methods do.",,arg_request,arg-request_result,asp_substance,5
3. How do PPO and SimPLe handle partial observability? Is it principled to apply them to partially-observable environments?,,arg_request,arg-request_explanation,asp_substance,0
4. Why not use continuous actions with a parameterized policy (e.g. Gaussian)?,,arg_request,arg-request_explanation,asp_substance,0
5. Is it reasonable to assume that the learning dynamics of all deep learning architectures are similar enough that a model trained on one set of deep learning architectures and problems will generalize to new architectures and problems?,,arg_request,arg-request_explanation,asp_substance,0
These studies should be mentioned.,,arg_request,arg-request_edit,asp_meaningful-comparison,1
"I think the results as well as the new combination of existing approaches in the paper warrants publication. But it should be amended significantly to situate itself within the existing literature. I therefore award a ""weak accept"".",,arg_request,arg-request_edit,asp_meaningful-comparison,1
"Rather than motivating the work in this way, it might be helpful to focus the contribution as a combination of future time step prediction and discretization (both of which have been considered in previous work, but not in combination).",,arg_request,arg-request_edit,asp_soundness-correctness,1
"- Section 4: Would it be possible to train the vq-wav2vec model jointly with BERT, i.e. as one model? I suspect it would be difficult since, for the masking objective, the discrete units are already required, but maybe there is a scheme where this could work.",,arg_request,arg-request_explanation,asp_clarity,0
"- Section 2.2: Similarly to the above question, would there be a way to incorporate the BERT principles directly into an end-to-end model, e.g. by randomly masking some of the continuous input speech?",,arg_request,arg-request_explanation,asp_clarity,0
"- Section 3.3: What exactly does ""mode collapse"" refer to in this context? Would this be using only one codebook entry, for instance?",,arg_request,arg-request_clarification,asp_clarity,4
Is it possible that the output acoustic model is simply better-matched to continuous rather than discrete input (direct vq-wav2vec gives discrete while BERT gives continuous)? Would it make sense to train the wav2vec acoustic model on top of the vqvae codebook entries (e) instead of directly on the symbols?,,arg_request,arg-request_explanation,asp_clarity,0
"- ""gumbel"" -> ""Gumbel"" (throughout; or just be consistent in capitalization)",,arg_request,arg-request_typo,asp_clarity,3
"- ""which can be mitigated my workarounds"" -> ""which can be mitigated *by*",,arg_request,arg-request_typo,asp_clarity,3
"- ""work around"" -> ""workaround""",,arg_request,arg-request_typo,asp_clarity,3
"- I am in general curious to see if it will be beneficial to fine-tune the modules themselves can further improve performance. It maybe hard to do it entirely end-to-end, but maybe it is fine to fine-tune just a few top layers (like what Jiang et al did)?",,arg_request,arg-request_experiment,asp_substance,2
"For example, what benefit we can get if we have perfect object detection? Where can we get if we have perfect relationships?",,arg_request,arg-request_explanation,asp_substance,0
"One suggestion is that it might be useful to also release the structured (parsed) form besides the freeform inputs and outputs, for analysis and for evaluating structured neural network models like the graph networks.",,arg_request,arg-request_clarification,asp_replicability,4
page 3: “freefrom inputs and outputs” -> “freeform inputs and outputs”,,arg_request,arg-request_typo,asp_clarity,3
"- The authors state that x_j is sampled from the ""prior network"" to calculate E_x_j in Equation 10, but I didn’t understand how this network is set up. Could you explain it in detail?",,arg_request,arg-request_explanation,asp_soundness-correctness,0
"Specifically, how performance differs compared to when p (m | z) is not used and the decoder p (x | z, m) is conditioned by the mask included in the training set instead of the generated mask?",,arg_request,arg-request_clarification,asp_soundness-correctness,4
"- Why did you not do image inpainting in higher-dimensional experiments like Ivanov et al. (2019), i.e., considering each pixel as a different modality? Of course, I know that Ivanov et al. require the full data as input during training, but I’m interested in whether VSAE can perform inpainting properly even if trained given imperfect images.",,arg_request,arg-request_experiment,asp_substance,2
"1) Compare SAVP to the SVG-LP/FP model on a controlled synthetic dataset such as Stochastic Moving MNIST (Denton & Fergus, 2018)?",,arg_request,arg-request_experiment,asp_substance,2
"2) Comment on the plausibility of the samples generated by SAVP? Do some samples show imagined objects – implausible interactions for the robotic arm dataset? If so, what would be the advantage over blurry but plausible generations of a VAE?",,arg_request,arg-request_clarification,asp_substance,4
Please clarify.,,arg_request,arg-request_clarification,asp_substance,4
I wonder why the authors didn’t compare or mention optimizers such as ADAM and ADAGRAD which adapt their parameters on-the-fly as well.,,arg_request,arg-request_explanation,asp_substance,0
Also how does this compare to adaptive hyperparameter training techniques such as population based training?,,arg_request,arg-request_explanation,asp_substance,0
How is this a reasonable assumption?,,arg_request,arg-request_explanation,asp_soundness-correctness,0
"Question: In your experiments, you set the learning rate to be really low. What happens if you set it to be arbitrarily high? Can you algorithm recover good learning rates?",,arg_request,arg-request_experiment,asp_substance,2
"- in page 3, 2line above eq(2): what is delta in the variance of the multivariate normal distribution?",,arg_request,arg-request_clarification,asp_clarity,4
"- The randomized weight is not very practical. Though it may be the standard approach of mean field,",,arg_request,arg-request_edit,asp_soundness-correctness,1
"You have the phrase ""allowing to imagine thousands of trajectories in parallel"". I would like some elaboration on this. I think you have ideas of what is happening in the latent space that I am not following.",,arg_request,arg-request_explanation,asp_clarity,0
You are heavy on the machinery and math. I find the learning in the latent space the important part and there are things like how much simulation is done in the latent learning not clearly spelled out. How does the effort compare to the 1E9 steps of the base line your refer to?,,arg_request,arg-request_explanation,asp_soundness-correctness,0
"Now may be the time to move you to understanding what structures get learned in latent space, are the in fact compact, diverse?",,arg_request,arg-request_explanation,asp_soundness-correctness,0
Perhaps there is room for memory/memories in the latent space?,,arg_request,arg-request_explanation,asp_soundness-correctness,0
"The reviewer encourages the authors to make further new developments and have a more comprehensive literature review. But in the current form, the paper has less value to be published in ICLR.",,arg_request,arg-request_edit,asp_meaningful-comparison,1
- It would be also better to show the coefficient of existing methods that have no theoretical justification.,,arg_request,arg-request_experiment,asp_substance,2
"- It would be better to compare with a naive approach that uses domain discrepancy between each source and target as (fixed) coefficient since this approach such as Mansour (2009), Ben-David(2007, 2010) and Kuroki et al (2019) which explicitly consider the hypothesis class has theoretical justification in the form of generalization error bound in the target domain.",,arg_request,arg-request_experiment,asp_substance,2
The main contribution (backpropagating analytic gradients through imagined trajectories?) could potentially be highlighted more but otherwise the paper was clear.,,arg_request,arg-request_edit,asp_clarity,1
I wonder if the authors ever looked at how much the size of the latent vector determines the performance of the system?,,arg_request,arg-request_explanation,asp_clarity,0
Is there an optional latent vector size across domains or is that optimal size task dependent?,,arg_request,arg-request_explanation,asp_clarity,0
"Additionally, how much variance is there in the imagined trajectories from a certain starting state? In other words, are the endpoints of most imagined trajectories similar or very different?",,arg_request,arg-request_explanation,asp_clarity,0
"- On page 2 it says “We approach this limitation in latenby”, which I assume is a typo?",,arg_request,arg-request_typo,asp_clarity,3
"Further, there a number of experimental details that need to be further elaborated upon.",,arg_request,arg-request_explanation,asp_replicability,0
"e.g., architectures and hyper-parameters used, and training procedures (I encourage the authors to utilize the appendices for this).",,arg_request,arg-request_edit,asp_replicability,1
It is unclear to me how difficult/easy these results would be to reproduce. Do the authors intend to release code for their implementations and experiments?,,arg_request,arg-request_clarification,asp_replicability,4
"In particular, it is unclear what the assumption on the size of the unlabelled test set is.",,arg_request,arg-request_clarification,asp_clarity,4
"- While the method is presented as jointly learning all the components, in the experimental section it is stated that the embedding network (the meta-learner) and the GAN-based domain adaptation are done separately. Can the authors comment on this further?",,arg_request,arg-request_explanation,asp_soundness-correctness,0
"Is this different from first learning a image translation mapping (using the unlabelled data in the target domain), and then applying existing meta-learning models/algorithms to the labelled data in the target domain?",,arg_request,arg-request_explanation,asp_soundness-correctness,0
"It is my impression that, if the authors elaborate on the experimental protocol and implementation details, this paper would be a good fit for the venue.",,arg_request,arg-request_edit,asp_substance,1
1. The proposed model? Is using a conditional weight prior p(\theta | x) (Eq 3) instead of p(\theta) (as in BNNs)  necessary for the inflated uncertainties on OOD data?,,arg_request,arg-request_explanation,asp_soundness-correctness,0
"2. The  objective? The proposed objective,  Eq 5, trades off stochastically approximating the (conditional) marginal likelihood against not deviating too much from p(\theta) =  MN(0, I, I) in the KL sense.",,arg_request,arg-request_explanation,asp_soundness-correctness,0
"-  Instead of relying on the KL regularizer, a standard approach to learning the model in Eq 3 would be to use well understood MCMC or variational methods that explicitly retain uncertainty in \theta and induce predictive uncertainties.  Were they explored and found to be not effective?",,arg_request,arg-request_explanation,asp_soundness-correctness,0
It would be nice to see how a “gold standard” HMC based inference does on at least the small toy problem of Sec 5.1?,,arg_request,arg-request_experiment,asp_substance,2
Comparisons against these more standard inference algorithms is essential for understanding what advantages are afforded by the objective proposed in the paper.,,arg_request,arg-request_experiment,asp_substance,2
"In the real world, one would not have access to OOD data during training, how is one to pick \lambda in such cases?",,arg_request,arg-request_explanation,asp_soundness-correctness,0
"Why does it go to nearly zero around x = 0, while being higher in surrounding regions with more data?",,arg_request,arg-request_explanation,asp_soundness-correctness,0
It would be good to include comparisons against VI with \lambda = 1.,,arg_request,arg-request_experiment,asp_substance,2
I'd be interested to hear if the authors see a connection between their formalism and the one of Reference prior in Bayesian inference (Bernardo et al https://arxiv.org/pdf/0904.0156),,arg_request,arg-request_explanation,asp_meaningful-comparison,0
Another question which is not answered in the paper is the number $k$ of images to select for each pair of classifiers. Is this number task-dependent? Is it related to the number of classes?,,arg_request,arg-request_explanation,asp_clarity,0
What is a general guideline for one to choose this number $k$ given a new application scenario?,,arg_request,arg-request_clarification,asp_clarity,4
"I would suggest rewriting this paragraph to make it clearer and less speculative, and acknowledge that although rotating filters might increase computational complexity, it has often been shown very effective.",,arg_request,arg-request_edit,asp_clarity,1
Some more explanation / discussion would be good.,,arg_request,arg-request_explanation,asp_clarity,0
It would be nice to explain the spherical parameterization in more detail. Is this operation itself rotation equivariant?,,arg_request,arg-request_explanation,asp_substance,0
"- Abstract: ""to extract non-trivial features"".",,arg_request,arg-request_edit,arg_other,1
The word non-trivial really doesn't add anything here.,,arg_request,arg-request_edit,arg_other,1
"Similarly ""offers multi-level feature extraction capabilities"" is almost meaningless since all DL methods can be said to do so.",,arg_request,arg-request_edit,arg_other,1
"- Below eq. 5, D_R^{-1} should equal D_R(-omega, -nu, -phi).",,arg_request,arg-request_edit,arg_other,1
The order is reversed when inverting.,,arg_request,arg-request_edit,arg_other,1
"- ""Different notations of convolutions"" -> notions",,arg_request,arg-request_typo,arg_other,3
"- ""For spherical functions there is no consistent and well defined convolution operators."" As discussed above, the issue is quite a bit more subtle. There are exactly two well-defined convolution operators, but they have some characteristics deemed undesirable by the authors.",,arg_request,arg-request_edit,asp_clarity,1
"- ""rationally symmetric"" -> rotationally",,arg_request,arg-request_typo,arg_other,3
"- ""exact hierarchical spherical patterns"" -> extract",,arg_request,arg-request_typo,arg_other,3
- It seems quite likely that the unpacking of the icosahedral/hexagonal grid as done in this paper has been studied before in other fields.,,arg_request,arg-request_edit,asp_originality,1
"References would be in order. Similarly, hexagonal convolution has a history in DL and outside.",,arg_request,arg-request_edit,arg_other,1
"- Bottom of page 7, capitalize ""for"".",,arg_request,arg-request_typo,arg_other,3
"- ""principle curvatures"" -> principal.",,arg_request,arg-request_typo,arg_other,3
"- ""deferent augmentation modes"" -> different",,arg_request,arg-request_typo,arg_other,3
"- ""inspite"" -> in spite",,arg_request,arg-request_typo,arg_other,3
"- ""reprort"" -> report",,arg_request,arg-request_typo,arg_other,3
"- ""utlize"" -> utilize",,arg_request,arg-request_typo,arg_other,3
"- ""computer the convolution"" -> compute",,arg_request,arg-request_typo,arg_other,3
How do we take a limit of M -> ∞ ? Does k also go ∞?,,arg_request,arg-request_explanation,asp_replicability,0
"- The definition of g should depend on only \theta_k^I and \hat{\delta}_k^M, not \theta_k^k.",,arg_request,arg-request_typo,asp_clarity,3
"- The equation (1) should hold for any \theta’, not \theta.",,arg_request,arg-request_typo,asp_clarity,3
"- The equation (1) should contain \rho, not p.",,arg_request,arg-request_typo,asp_clarity,3
"Need to compare in more challenging settings where the success rate is meaningful, e.g. smaller epsilon or a more robust NN using some defence.",,arg_request,arg-request_experiment,asp_substance,2
Some intuitive explanation on why this should help and/or empirical comparison would be a great addition.,,arg_request,arg-request_explanation,asp_substance,0
It would be very useful to write the update more explicitly and compare and contrast this 2 very similar updates.,,arg_request,arg-request_experiment,asp_substance,2
"- I am not sure what the authors mean by “the Frank-Wolfe gap is affine invariant”. If we scale the input space by a, the gap should be scaled by a^2 - how/why is it invariant?",,arg_request,arg-request_explanation,asp_soundness-correctness,0
- I am not sure what you mean in 5.4 “we omit all grid search/ binary search steps,,arg_request,arg-request_clarification,asp_clarity,4
- In remark 4.8 in the end option I and II are inverted by mistake,,arg_request,arg-request_edit,asp_soundness-correctness,1
"- In 5.1, imagenet results are normally top-5 error rate not top-1 acc, would be better to report that more familiar number.",,arg_request,arg-request_edit,asp_soundness-correctness,1
1 - I would have liked to see more discussion about why the two results differ to give readers intuition about where the extra flexibility comes from.,,arg_request,arg-request_explanation,asp_substance,0
"Because these differences aren't explained, the synthetic tasks in the experimental section make this approach look artificially good in comparison to Hartford et al.",,arg_request,arg-request_explanation,asp_meaningful-comparison,0
I would have also liked to see a comparison to these methods in the the classification results.,,arg_request,arg-request_result,asp_meaningful-comparison,5
So the two results coincide for the exchangeable case. Might be worth pointing this out.,,arg_request,arg-request_edit,asp_meaningful-comparison,1
"In Appendix B.1, they report mixed results in terms of wall-clock time, and I strongly feel that these results should be in the main body of the paper.",,arg_request,arg-request_edit,asp_clarity,1
It would be interesting to see how these two algorithms stack up.,,arg_request,arg-request_experiment,asp_meaningful-comparison,2
"However, it trades-off computational cost for progress-per-iteration, so I think that an explicit analysis of this trade-off (beyond what’s in Appendix B.1) must be in the main body of the paper.",,arg_request,arg-request_edit,asp_substance,1
"- In active learning, the proposed method should be compared with other methods such as Bayesian DNN using dropout, etc.",,arg_request,arg-request_experiment,asp_meaningful-comparison,2
- How does the estimation accuracy of GAN relate to the estimation accuracy of the proposed method? Showing a quantitative description would be nice.,,arg_request,arg-request_explanation,asp_clarity,0
"As a concrete experiment to determine the importance, what would be the accuracy and computational comparison of ensembling 4+ models without MC-dropout vs. 3 ensembled models with MC-dropout?",,arg_request,arg-request_explanation,asp_clarity,0
"At the point (number of extra ensembles) where the computational time is equivalent, is the learning curve still better?",,arg_request,arg-request_explanation,asp_soundness-correctness,0
"-In the introduction, ""it is in general impossible to find an embedding in R^d such that ..."", why do we have to make v and v'(and u, and u') far from each other?",,arg_request,arg-request_explanation,asp_replicability,0
"- In Equation (2), How is P_ij defined exactly, are they parameters? I am quite confused about this part",,arg_request,arg-request_clarification,asp_clarity,4
"- In Equation (6), the posterior distribution should be P(X|G) since X is the latent variable to be inferred, right",,arg_request,arg-request_edit,asp_replicability,1
"- In Table 2 and 3, how are the degree and block information leveraged into the model?",,arg_request,arg-request_clarification,asp_replicability,4
This should be made crystal-clear in the paper.,,arg_request,arg-request_edit,asp_originality,1
"In this sense, claims about the low-variance of GO-gradient wrt to other reparametrization baed estimators should be removed, as they are the same.",,arg_request,arg-request_edit,asp_clarity,1
3. Authors should be much more clear about which is their original contribution to the problems stated in Section 4 and Section 5.,,arg_request,arg-request_edit,asp_originality,1
Did you try to have a single network?,,arg_request,arg-request_explanation,asp_substance,0
"2. If you consider \sigma, why do you also predict the rewrite success with \omega? Couldn't it be simply a function from S x S -> L ?",,arg_request,arg-request_explanation,asp_soundness-correctness,0
"It would be helpful to have a brief paragraph describing this architecture, for readers not familiar with the referenced paper.",,arg_request,arg-request_edit,asp_replicability,1
"4. How large is the training set of (T, P) pairs? I don't think this is mentioned in the paper.",,arg_request,arg-request_explanation,asp_replicability,0
You mention that negative mining should improve over this strategy. What does negative mining correspond to in this context? Are there bad rewrites better than others?,,arg_request,arg-request_explanation,asp_replicability,0
"6. Did you consider using an inverse function (say G), that maps an embedding in L / L' back to S (i.e. the inverse function of gamma / gamma').",,arg_request,arg-request_experiment,asp_substance,2
"I would imagine that even if an embedding X is a bit noisy, because not exactly equal to gamma(P) where P is the expression it represents, you could consider doing the propagation with gamma(G(X)).",,arg_request,arg-request_experiment,asp_substance,2
"Also, G could be used to check whether you obtain the expected formula after 4 steps, which would be a more informative information than the L2 distance between the resulting embedding and the embedding of the final formula.",,arg_request,arg-request_experiment,asp_substance,2
However in Figure 1c SGD with momentum at batch size 8k uses an effective LR of 4.,,arg_request,arg-request_clarification,asp_clarity,4
Can you explain this inconsistency i.e. why is there such a huge jump from 4 to 32 (in reality we expect the effective LR to stay constant in the curvature regime).,,arg_request,arg-request_explanation,asp_clarity,0
However 4 to 32 seems a bit inconsistent.,,arg_request,arg-request_clarification,asp_clarity,4
2) Does momentum help in constant step budget (with sufficiently large steps so that training loss is small)?,,arg_request,arg-request_clarification,asp_clarity,4
"3) Readability: Consider explaining what is meant by ""warm-up"", ""epoch budget"", ""step budget"" clearly and upfront.",,arg_request,arg-request_clarification,asp_clarity,4
"Edit (leaving everything else unchanged for now): After reading R3's assessment, I agree with them that it's worrying that the Deterministic Meta-Dropout performs better than baseline MAML - maybe it's an effect of a larger number of parameters in the model?",,arg_request,arg-request_clarification,asp_substance,4
"I would strongly encourage the authors to incorporate the baseline ""(1)"" as proposed by R3 in a future version of the paper as I agree with them that this is a relevant baseline.",,arg_request,arg-request_experiment,asp_substance,2
I think some more motivations or explorations (what kind of information did BERT learn) are needed to understand why that is the case.,,arg_request,arg-request_experiment,asp_substance,2
"A more economical approach is to use BERT-trained model as initialization for acoustic model training, which is the classical way how RBMs pre-training were used in ASR.",,arg_request,arg-request_edit,asp_soundness-correctness,1
"4. Another curious question is whether the features would still provide as much improvement when a stronger ASR system than AutoSeg (e.g., Lattice-free MMI) is used.",,arg_request,arg-request_explanation,asp_clarity,0
"The paper is easy to understand but several sections (starting from section 2) could use an english language review (e.g. ""search right"" -> ""search for the right"", ""predict next word"" -> ""predict the next word"", ...) In section 3, can you be more specific about the gains in training versus inference time?",,arg_request,arg-request_edit,asp_clarity,1
I believe the results all relate to inference but it would be good to get an overview of the impact of training time as well.,,arg_request,arg-request_explanation,asp_substance,0
Maybe in section 3.7 you can address how often that occurs as well?,,arg_request,arg-request_edit,asp_substance,1
- it wasn't clear how the sparsity percentage on page 3 was defined?,,arg_request,arg-request_explanation,asp_replicability,0
- can you motivate why you are not using perplexity in section 3.2?,,arg_request,arg-request_explanation,asp_soundness-correctness,0
"For example, the original GNN, ChebNet, etc. that leads to GCN can be mentioned.",,arg_request,arg-request_edit,arg_other,1
"The reviewer recommends the authors to read some literature review of the topic of GCN and re-organize the references, and use search engines to have a better view on the state of the art of (hyperbolic) geometric theories and graph convolutional networks.",,arg_request,arg-request_edit,arg_other,1
This is a thriving area that requires a careful literature review.,,arg_request,arg-request_edit,arg_other,1
The authors are suggested to use unified notations to denote vectors/matrices (e.g. all bold).,,arg_request,arg-request_edit,asp_clarity,1
The introduction can start at a lower level (such as flat/hyperbolic neural networks).,,arg_request,arg-request_edit,asp_clarity,1
"Section 3.4, as the main technical innovation, can be extended and includes some demonstrations.",,arg_request,arg-request_edit,none,1
"1.	From the generative model point of view, could the authors elaborate on the comparison against cGAN (aside from the descriptions in Appendix 2)? It is quoted “cGAN…often lack satisfactory diversity in practice”.",,arg_request,arg-request_explanation,asp_meaningful-comparison,0
"Also, can cGAN be used estimate the density of X (posterior or not)?",,arg_request,arg-request_explanation,asp_clarity,0
2.,,arg_request,arg-request_clarification,asp_clarity,4
3.	“we find it advantageous to pad both the in- and output of the network with equal number of zeros”: Is this to effectively increase the intermediate network dimensions?,,arg_request,arg-request_explanation,asp_clarity,0
"Also, does this imply that for both forward and inverse process those zero-padded entries always come out to be zero?",,arg_request,arg-request_explanation,asp_soundness-correctness,0
"This is not necessarily a drawback, I am curious if this model could succeed on higher dimensional data (e.g., image), especially with the observation y.",,arg_request,arg-request_experiment,asp_substance,2
=> Environment: The experimental section of the paper can be further improved.,,arg_request,arg-request_experiment,asp_substance,2
"The approach is evaluated only in three cases: fish, walker, cheetah. Can it be applied to more complex morphologies? Humanoid etc. maybe?",,arg_request,arg-request_experiment,asp_clarity,2
How would the given graph network compare to this?,,arg_request,arg-request_explanation,asp_soundness-correctness,0
"Does the controller also start from scratch? If so, why? Also, it is not clear what is the meaning of generations if the graph is fixed, can't it be learned altogether at once?",,arg_request,arg-request_explanation,asp_soundness-correctness,0
A comparison with Graph Convolutional Network based techniques seems appropriate (e.g. Kipf and Welling 2017).,,arg_request,arg-request_experiment,asp_meaningful-comparison,2
How do the FID/Inception improvements compare to (Mescheder et al 2018)?,,arg_request,arg-request_explanation,asp_soundness-correctness,0
It would be interesting to discuss why the FID score for SVHN gets worse in presence of 1000 labels.,,arg_request,arg-request_edit,asp_clarity,1
It would be informative to show the generated images w/ and w/o manifold regularization.,,arg_request,arg-request_edit,asp_clarity,1
More analysis should be provided on why (Kumar et al 2017) perform so well on SVHN.,,arg_request,arg-request_experiment,asp_substance,2
It should be stated that bold values in tables do not represent best results (as it is usually the case) but rather results for the proposed approach.,,arg_request,arg-request_edit,asp_clarity,1
What does “keeping notes” mean?,,arg_request,arg-request_explanation,asp_clarity,0
\alpha^i_t and u^i are also pretty complex and it would be good to conduct some ablation analysis.,,arg_request,arg-request_result,asp_substance,5
"- tau Yih et al, 2016 --> Yih et al, 2016",,arg_request,arg-request_typo,asp_clarity,3
"Also, I am left wondering what are considered the parameters of the models -- are only the neuromodulatory terms considered as the additional trainable parameters compared to baseline LSTMs? How are the Hebbian synapses themselves considered in this calculation?",,arg_request,arg-request_explanation,asp_soundness-correctness,0
"If the Hebbian synapses are not considered, then the authors need a control with matched memory-capacities to account for the extra capacity afforded by the Hebbian synapses.",,arg_request,arg-request_edit,asp_soundness-correctness,1
"Given the ties between Hebbian synapses and attention (see Ba et al), an important control here could be an LSTM with Bahdanau (2014) style attention.",,arg_request,arg-request_experiment,asp_substance,2
"Finally, the style (font) of the paper does not adhere to the ICLR style template, and must be changed.",,arg_request,arg-request_edit,asp_clarity,1
You may be able to show more plots which help display the quality of the embedding space varying with the number of triplets used.,,arg_request,arg-request_edit,asp_clarity,1
Error bar may also be needed for comparison.,,arg_request,arg-request_result,asp_meaningful-comparison,5
"- for the experiment with Imagenet images, it is not very clear how many pictures are used. Is this number 2500?",,arg_request,arg-request_clarification,asp_replicability,4
"- the authors fix the number of layers of the used network based on ""our experience"". For the sake of completeness, more experiments in this area would be nice.",,arg_request,arg-request_experiment,asp_substance,2
"- I don't see a discussion about the downsides of the method (for example, the large number of triplet comparison examples needed for training; and possible methods to overcome this problem).",,arg_request,arg-request_clarification,asp_soundness-correctness,4
- in section 4.4,,arg_request,arg-request_explanation,asp_substance,0
- in section 4.3 how is the reconstruction built (Figure 3b)?,,arg_request,arg-request_clarification,asp_replicability,4
"- In figure 3 (c) ""number |T of input"" should be  ""number |T| of input""",,arg_request,arg-request_typo,asp_clarity,3
"- In figure 5 (a) ""cencept"" should be ""concept""",,arg_request,arg-request_typo,asp_clarity,3
"- In figure 8 ""Each column corresponds to ..."" should be ""Each row corresponds to ..."".",,arg_request,arg-request_typo,asp_clarity,3
"- In the last paragraph of A1 ""growth of the layer width respect"" should be ""growth of the layer width with respect""",,arg_request,arg-request_typo,asp_clarity,3
"- In the second paragraph of A2 ""hypothesize the that relation"" should be ""hypothesize that the relation"".",,arg_request,arg-request_typo,asp_clarity,3
"- In section 4.3 last paragraph, first sentence: ""with the maximunm number"" should be ""with the maximum number""",,arg_request,arg-request_typo,asp_clarity,3
"1. Are e_{i,t} and lambda_{i,t} vectors?",,arg_request,arg-request_clarification,asp_clarity,4
What happens if there are multiple mentions in the text? Which one does it look at?,,arg_request,arg-request_explanation,asp_soundness-correctness,0
"3. For the LSTM in the graph update, why does it have only one input? Shouldn’t it have two inputs, one for previous hidden state and the other for input?",,arg_request,arg-request_clarification,asp_soundness-correctness,4
"This is great, but could you also report the number when the full dataset is used?",,arg_request,arg-request_edit,asp_replicability,1
5. What does it mean that in training time the model “updates” the location node representation with the encoding of correct span. Do you mean you use the encoding instead?,,arg_request,arg-request_explanation,asp_soundness-correctness,0
"6. For ProPara task 2, what threshold did you choose to obtain the P/R/F1 score?",,arg_request,arg-request_explanation,asp_replicability,0
Is it the threshold that maximizes F1?,,arg_request,arg-request_clarification,asp_clarity,4
Could you comment on the importance of this component? Did you test the model on other types of videos where this hypothesis is less relevant?,,arg_request,arg-request_experiment,asp_substance,2
The cited paper 'Learning an adaptive learning rate schedule' does not appear online.,,arg_request,arg-request_edit,arg_other,1
Other Comments - The citation to Zaremba et al. in Table 1 made it seem like the perplexity result on that line of the table was directly from Zaremba et al's paper. I'd recommend removing the citation from that line to avoid confusion.,,arg_request,arg-request_edit,asp_clarity,1
"One thing I would have loved to see from this paper is a comparison of modulated-plasticity LSTMs with the sota from Melis et al., 2017.",,arg_request,arg-request_experiment,asp_meaningful-comparison,2
I think the paper is solid as-is; positive results in this comparison would take it to the next level.,,arg_request,arg-request_experiment,asp_substance,2
Why were zero-sequences necessary in Experiment 1?,,arg_request,arg-request_clarification,asp_clarity,4
"Perhaps the authors could clarify on what a confounding ""time-locked scheduling strategy"" would look like in this task?",,arg_request,arg-request_clarification,asp_clarity,4
"Why does Experiment 1 present pairs of stimuli, rather than high-dimensional individual stimuli?",,arg_request,arg-request_explanation,asp_clarity,0
Why is non-plastic rnn left out of Figure 2b?,,arg_request,arg-request_clarification,asp_clarity,4
"""However, in Nature,"" -- no caps",,arg_request,arg-request_typo,asp_clarity,3
"in appendix: ""(see Figure A.4)"" -- the figure is labeled ""Figure 3""",,arg_request,arg-request_typo,asp_clarity,3
A general overview of related work in these directions are needed.,,arg_request,arg-request_edit,asp_clarity,1
1. The paper should also consider more recently proposed evaluation metrics such as discussed in https://arxiv.org/pdf/1805.09733.pdf,,arg_request,arg-request_experiment,asp_replicability,2
2. The author should try to avoid using yellow color in plots.,,arg_request,arg-request_edit,asp_clarity,1
"For the bAbi task, although there is a significant improvement (43%) in the mean error rate compared to the original DNC, it's important to note that performance in this task has improved a lot since the DNC paper was release. Since this is the only non-toy task in the paper, in my opinion, the authors have to discuss current SOTA on it, and have to cite, for example the universal transformer[1], entnet[2], relational nets [3], among others architectures that shown recent advances on this benchmark.",,arg_request,arg-request_experiment,asp_meaningful-comparison,2
"It would also be good if besides the mean error rates, they reported best runs chosen by performance on the validation task, and number of the tasks solve (with < 5% error) as it is standard in this dataset.",,arg_request,arg-request_result,asp_substance,5
"4) Table 1 difference between DNC and DNC (DM) is not clear. I am assuming it's the numbers reported in the paper, vs the author's implementation?",,arg_request,arg-request_clarification,asp_clarity,4
"5)In session 3.1-3.3, for completeness. I think it would be helpful to explicitly compare the equations from the original DNC paper with the new proposed ones.",,arg_request,arg-request_experiment,asp_substance,2
"In sec. 4, there's a reference to an initial set of experiments with count-based methods without much details.",,arg_request,arg-request_edit,asp_substance,1
Another area of improvement is the experiments around VAE.,,arg_request,arg-request_experiment,asp_substance,2
Also it's not clear from the details in the paper what are the architectures for the VAE and RFs (there's a reference to the code but would've been better to have sufficient details in the paper).,,arg_request,arg-request_edit,asp_clarity,1
An interesting area for future work could be on early stopping techniques for embedding training - it seems that RFs perform well without any training while in some scenarios the IDFs work overall the best.,,arg_request,arg-request_experiment,asp_soundness-correctness,2
So it would be interesting to explore how much training is needed for the embedding model.,,arg_request,arg-request_experiment,asp_soundness-correctness,2
- Was the auxiliary tower used during the training of the shared weights W?,,arg_request,arg-request_explanation,none,0
"- Figure 4 does not illustrate M=4 and N=4, e.g. which operation belongs to which layer?",,arg_request,arg-request_explanation,none,0
"- Did the experiments on CIFAR-10 and ImageNet use the cosine learning rate schedule [1]? If or if not, either way, you should specify it in a revised version of this paper, e.g. did you use the cosine schedule in the first 120 steps to train the shared parameters W, did you use it in the retraining from scratch?",,arg_request,arg-request_explanation,asp_replicability,0
"This triggers my curiosity of which difficulty is it? It would be nice to see this point more elaborated, and to see ablation study experiments.",,arg_request,arg-request_experiment,asp_substance,2
2. Missed citation: MnasNet [2] also incorporates the cost of architectures in their search process.,,arg_request,arg-request_edit,arg_other,1
I think this will be a good comparison.,,arg_request,arg-request_experiment,asp_substance,2
"- Section 3.3: “Different from pruning, which the search space is usually quite limited”. “which” should be “whose”?",,arg_request,arg-request_typo,asp_clarity,3
- Section 4.4.1: “DSO-NAS can also search architecture [...]”  -> “DSO-NAS can also search for architectures [...]”,,arg_request,arg-request_typo,asp_clarity,3
"e.g. ""In specific"" --> ""Specifically"" in the abstract, ""computational budge"" -> ""budget"" (page 6) etc.",,arg_request,arg-request_typo,asp_substance,3
- From figure 2 it's not clear why all non-zero connections in (b) are then equally weighted in (c). Would keeping the non-zero weightings be at all helpful?,,arg_request,arg-request_explanation,asp_substance,0
-  Why have you chosen the 4 operations at the bottom of page 4? It appears to be a subset of those used in DARTS.,,arg_request,arg-request_explanation,asp_substance,0
- How do you specifically encode the number of surviving connections? Is it entirely dependent on budget?,,arg_request,arg-request_clarification,asp_substance,4
- You should add DARTS 1st order to table 1.,,arg_request,arg-request_experiment,asp_substance,2
- Measuring in GPU days is only meaningful if you use the same GPU make for every experiment. Which did you use?,,arg_request,arg-request_clarification,asp_substance,4
What I really want this paper to explore is when and why this happens.,,arg_request,arg-request_experiment,asp_substance,2
"Even on synthetic data, when do or don't we see generalization (systematic or otherwise) from NMNs/MAC/FiLM?",,arg_request,arg-request_explanation,asp_soundness-correctness,0
"That suggests that from the point of view of training, there are many equally good solutions, which suggests a number of interesting questions. If you did large numbers of training runs, would the models occasionally find the right solution? Could you somehow test for if a given trained model will show systematic generalization?",,arg_request,arg-request_explanation,asp_soundness-correctness,0
"Is there any way to help the models find the ""right"" (or better) solutions - e.g. adding regularization, or changing the model size?",,arg_request,arg-request_explanation,asp_soundness-correctness,0
"However, I feel like the main result is a bit too predictable, and for acceptance I'd like to see a much more detailed exploration of the questions around systematic generalization.",,arg_request,arg-request_experiment,asp_substance,2
Can you clarify how you view the relationship between the approaches mentioned above?,,arg_request,arg-request_clarification,asp_soundness-correctness,4
I’m curious to hear the authors’ thoughts in this.,,arg_request,arg-request_clarification,asp_substance,4
"- Reading Figure 2b, one could be tempted to draw a correlation between the complexity of the dataset and the gains achieved by self-modulation over the baseline (e.g., Bedroom shows less difference between the two approaches than ImageNet). Do the authors agree with that?",,arg_request,arg-request_experiment,asp_substance,2
Am I missing something?,,arg_request,arg-request_clarification,asp_soundness-correctness,4
What is G_t in Theorem 2.5. It should be defined in the theorem itself.,,arg_request,arg-request_edit,asp_substance,1
I would like to suggest to use the state-of-the-arts classifier for the principal task and to evaluate how much gain your method can get with the help of auxiliary tasks.,,arg_request,arg-request_experiment,asp_substance,2
I would appreciate it if the author could provide more explanation on the introduction of the super-graph in training.,,arg_request,arg-request_edit,asp_clarity,1
"I think it is ok to say that meta dropout tries to optimize a lower bound of log p(Y|X;\theta,\phi^*), but meta dropout does not regularize the variational framework because there is no variational inference framework.",,arg_request,arg-request_edit,asp_clarity,1
Experiments on adversarial robustness can be further improved.,,arg_request,arg-request_experiment,asp_substance,2
(1) the settings and the analysis of adversarial robustness experiment can be discussed in details.,,arg_request,arg-request_edit,asp_substance,1
"For example, how to build ''adversarial learning baseline'' in meta learning settings and why the result implies the perturbation directions for generalization and robustness relates to each other; (2) how other regularization methods (e.g., Mixup, VIB and Information dropout) perform on adversarial robustness? Does Meta dropout performs better than them?",,arg_request,arg-request_explanation,asp_substance,0
"I suggest trying some other STOA attack methods (e.g., iterative methods).",,arg_request,arg-request_experiment,asp_substance,2
"Page 3, Regularization methods, 3rd line, ````wwwdiscuss",,arg_request,arg-request_typo,asp_clarity,3
"Page 7, 2nd line from the bottom, FSGM->FGSM",,arg_request,arg-request_typo,asp_clarity,3
- What is dt in Algorithm 1 description?,,arg_request,arg-request_clarification,asp_clarity,4
-typo “implmented”,,arg_request,arg-request_typo,asp_clarity,3
-What’s the 3d plot supposed to represent?,,arg_request,arg-request_clarification,asp_clarity,4
Doesn't the classification loss have a dependency on the input condition?,,arg_request,arg-request_clarification,asp_replicability,4
"--What does a ""heavy classifier"" imply concretely?",,arg_request,arg-request_clarification,asp_replicability,4
→ what do the authors mean “,,arg_request,arg-request_clarification,asp_clarity,4
"Besides visible artifacts, what does it mean? Why does a smoother G reduces those artifacts?",,arg_request,arg-request_explanation,asp_substance,0
Can this be useful for someone having less compute power and working on something similar to CelebA?,,arg_request,arg-request_experiment,asp_substance,2
"Moreover, authors should conduct experiments on state-of-the-art benchmarks, including natural images.",,arg_request,arg-request_experiment,asp_substance,2
"This is because the digitals images in MNIST do not have rich texture and detail structures, thus are not very challenging for standard image restoration methods.",,arg_request,arg-request_experiment,asp_substance,2
"Considering theoretically, what advantages truly follow from the paper for optimizing a given function? Let’s consider the following cases.",,arg_request,arg-request_result,asp_substance,5
Is it known / not known in optimization?,,arg_request,arg-request_clarification,asp_substance,4
"As the submission is a theoretical result with no immediate applicability, it would be very helpful if the authors could detail the technical improvements over this related work.",,arg_request,arg-request_edit,asp_substance,1
"Note, ICLR policy says that arxiv preprints earlier than one month before submission are considered a prior art. Could the authors elaborate more on possible practical/theoretical applications?",,arg_request,arg-request_result,asp_substance,5
Training loss plots would be more clear in the log scale.,,arg_request,arg-request_experiment,asp_substance,2
Comparison of batch normalization and weight normalization algorithms for the large-scale image classification.,,arg_request,arg-request_experiment,asp_meaningful-comparison,2
"Is there a rationale in this setting or it is just a more tricky method to fix the weight norm to some constant, e.g. ||w||=1?",,arg_request,arg-request_explanation,asp_substance,0
One thing that I missed while reading the paper is more comment on negative results.,,arg_request,arg-request_explanation,asp_substance,0
"Did the authors tried any version of their model with convolutions or pooling and found it not to perform as well? Measuring the number of parameters when including pooling or convolutions can become tricky, was that part of the reason?",,arg_request,arg-request_experiment,asp_clarity,2
"""Regularizing by stopping early for regularization,""",,arg_request,arg-request_typo,asp_substance,3
"In this paper ""large compression ratios"" means little compression, which I found confusing.",,arg_request,arg-request_clarification,asp_clarity,4
It would be nice to see more detailed experiments as done in Schott.,,arg_request,arg-request_experiment,asp_substance,2
1- Why not use a single Boltzmann machine with 128 fully connected latent variables? Could you add this experiment please.,,arg_request,arg-request_experiment,asp_substance,2
2- Why is a two-stage pre-training (Figure 2) process needed? Why not just a single stage?,,arg_request,arg-request_explanation,asp_replicability,0
3- Is the key that you used only 679 patches containing 98% of occurrences in the dataset as the first stage? What if we vary this percentage? How sensitive are the results? Such experiments could be useful to understand better why your method appears to work well.,,arg_request,arg-request_experiment,asp_substance,2
4- Could you please add the found J_h's to the appendix.,,arg_request,arg-request_edit,arg_other,1
Could it be that what we are seeing is the attack being denoised?,,arg_request,arg-request_explanation,asp_soundness-correctness,0
"I am puzzled and looking forward to answers to the above questions. I don't yet understand what is the thing that makes this approach appear to work, or why you were able to drop the Bayes inference inversion altogether as done by Schott.",,arg_request,arg-request_explanation,asp_soundness-correctness,0
Even though the authors answer positively to each of their four questions in the experiments section,,arg_request,arg-request_edit,asp_soundness-correctness,1
"Furthermore, the usage of the evaluation method unclear as well, it seems to be designed for evaluating the effectiveness of different adversarial attacks in Figure 2.",,arg_request,arg-request_clarification,asp_clarity,4
"Indeed, why the commonly used attack success ratio or other similar measures cannot be used in the case?",,arg_request,arg-request_explanation,asp_substance,0
"The paper also lacks experimental results, and the main conclusion from these results seems to be ""MNIST is not suitable for benchmarking of adversarial attacks"".",,arg_request,arg-request_result,asp_soundness-correctness,5
"If the authors claim that the proposed MaxConfidence attack method is more powerful than the MaxLoss based attacks, they should provide more comparisons between these methods.",,arg_request,arg-request_edit,asp_meaningful-comparison,1
Constructing the full weight matrix requires the whole dataset as input and selecting k-nearest neighbor is a non-differentiable operation. Can you give more explanations?,,arg_request,arg-request_explanation,asp_soundness-correctness,0
-Does episode training help label propagation? How about the results of label propagation without the episode training?,,arg_request,arg-request_explanation,asp_clarity,0
The authors might also want to compare,,arg_request,arg-request_experiment,asp_substance,2
1. In figure-4 you mention that entropy regularizer helps to keep the initial ME score. Can you elaborate more about the way in which entropy regularizer is used with regular MLE training?,,arg_request,arg-request_explanation,asp_substance,0
2. It is not very clear how is the base rate computed in Figure 5. I have a guess. But it is better to explain it in detail.,,arg_request,arg-request_explanation,asp_substance,0
4. Are the authors willing to release the code and data to reproduce the results?,,arg_request,arg-request_explanation,asp_substance,0
"1. Page 3: second para, line 4: “our aim is to study”",,arg_request,arg-request_edit,asp_substance,1
2. Page 5: last line: estimate for -> estimated for,,arg_request,arg-request_edit,asp_substance,1
3. Section 4.2: 3rd line: “the class for the from”,,arg_request,arg-request_edit,asp_substance,1
"What would have been helpful is to show the accuracy of their margin for simple binary toy 2D problems, where the true margin and their approximation can be visualized.",,arg_request,arg-request_experiment,asp_substance,2
So clarifying the above question will help to judge the paper's novelty.,,arg_request,arg-request_clarification,asp_originality,4
1. Can you clarify why the proposed approach is better than the Meta-RevGrad baseline?,,arg_request,arg-request_clarification,asp_meaningful-comparison,4
What happen for Meta-RevGrad + idt or Meta-RevGrad + revMap?,,arg_request,arg-request_experiment,asp_substance,2
"2. How is the performance of a simpler baseline such as combining a subset of new domain as training set to train MAML or PN (probably in 5-shot, 5-class case)?",,arg_request,arg-request_experiment,asp_substance,2
"i.e., omniglot <-> omniglot-M. I wonder whether the proposed approach can still work in large domain shift such as omniglot to fashion-mnist etc.",,arg_request,arg-request_experiment,asp_substance,2
"1. Where is L_da in Figure 2? In Figure 2, what’s the unlabelled data from which testing tasks are drawn? Is it from meta-test data training set?",,arg_request,arg-request_clarification,asp_replicability,4
"2. In the caption of figure 2, there should be a space after `"":"".",,arg_request,arg-request_typo,asp_clarity,3
"1. The paper should also talk about the details of ARNet and discuss the difference, as I assume they are the most related work",,arg_request,arg-request_experiment,asp_meaningful-comparison,2
"2. during sampling, either training or testing, how do authors handle temporal overlap or make it overlap?",,arg_request,arg-request_clarification,asp_replicability,4
"3. can you provide the training memory, inference speed, and total training time?",,arg_request,arg-request_result,asp_replicability,5
"5. lack of related work:  4D Spatio-Temporal ConvNets: Minkowski Convolutional Neural Networks, CVPR 2019.",,arg_request,arg-request_edit,arg_other,1
"Since this paper is about distributed robustness and distributed attacks, it would be very informative to the community to illustrate DBA attack on these methods to have a more compelling message.",,arg_request,arg-request_experiment,asp_substance,2
"I think the authors should move a portion of the big bar plot (too low resolution, btw) into the main text and discuss it thoroughly.",,arg_request,arg-request_edit,asp_substance,1
"Details on how to generate the dataset, however, can be moved into the appendix.",,arg_request,arg-request_edit,asp_clarity,1
"I am also not entirely satisfied by using accuracy as the only metric; how about using something like beam search to build a ""soft"", secondary metric?",,arg_request,arg-request_experiment,asp_substance,2
One other thing I want to see is a test set with multiple different difficulty levels.,,arg_request,arg-request_experiment,asp_substance,2
"The authors try to do this with composition, which is good, but I am not sure whether that captures the real important thing - the ability to generalize, say learning to factorise single-variable polynomials and test it on factorising polynomials with multiple variables? And what about the transfer between these tasks (e.g., if a network learns to solve equations with both x and y and also factorise a polynomial with x, can it generalize to the unseen case of factorising a polynomial with both x and y)?",,arg_request,arg-request_explanation,asp_soundness-correctness,0
"Also, is there an option for ""unsolvable""?",,arg_request,arg-request_clarification,asp_soundness-correctness,4
"First of all, I don’t understand how the main equation of the compound density network in Equation (3) is different from the general case of a Bayesian neural network? Can the authors please comment on that?",,arg_request,arg-request_explanation,asp_replicability,0
Is there a reason why the authors do not introduce their objective by following the variational framework?,,arg_request,arg-request_explanation,asp_replicability,0
What is the purpose then for introducing the matrix variate Gaussian?,,arg_request,arg-request_explanation,asp_replicability,0
I expect the authors to comment on that.,,arg_request,arg-request_explanation,asp_replicability,0
"Finally, it is unclear how the authors have picked the best \lambda parameter for their approach? On page 5 they state that they “pick the value that results in a good trade-off between high uncertainty estimates and high prediction accuracy.” Does this mean that you get to observe the performance in the test in order to select the appropriate value for \lambda? If this is the case this is completely undesirable and is considered a bad practice.",,arg_request,arg-request_explanation,asp_replicability,0
I think this point should be discussed in the paper.,,arg_request,arg-request_edit,asp_soundness-correctness,1
This restriction should be noted in the paper.,,arg_request,arg-request_edit,asp_soundness-correctness,1
"Also, I suggest moving Section 4 to be right after Section 2, since Section 4 presents existing techniques similarly to Section 2, while Section 3 presents the main contribution.",,arg_request,arg-request_edit,asp_clarity,1
and it also includes some weak claims that should be removed.,,arg_request,arg-request_edit,asp_substance,1
"For example, the number of labeled examples in Table 1 is fairly large and inconsistent (4K, 1K, 10K for the 3 organic datasets).",,arg_request,arg-request_edit,asp_substance,1
"In this reviewer's opinion, it would be a lot more reasonable to have instead a learning curve showing the results for, say, 100, 500, 1K, 5K, and 10K labeled examples for all three domains.",,arg_request,arg-request_result,asp_substance,5
"In 4.1, you are using different epsilon policies for synthetic vs organic datasets; why?",,arg_request,arg-request_explanation,asp_substance,0
"The explanation for underperforming on SVHM (page 7) may be valid, but you could easily prove it right or wrong by adding an option to SST for ""stratified SSL."" Without this extra work, your claim is just a conjecture.",,arg_request,arg-request_explanation,asp_substance,0
You should also show the performance of regular SSL methods in the setup on Table 4.,,arg_request,arg-request_experiment,asp_substance,2
"however, you do not provide any evidence for it, so you should avoid making such claims.",,arg_request,arg-request_edit,asp_substance,1
"- on page 2, the two terms classification & selection network appear ""out of the blue;"" it would be quite helpful to make it clear from the abstract that the proposed implementation is for neural networks.",,arg_request,arg-request_clarification,asp_substance,4
- figures 2 & 3 should be a lot larger in order to be readable,,arg_request,arg-request_edit,asp_clarity,1
"- 4.1.2 top of page 7: claims such as ""SST could have obtained better performance"" have no place in such a paper; you could instead make a note about the method being ""prohibitively CPU intensive for the time being""",,arg_request,arg-request_edit,asp_substance,1
"- lower on the same page you say: ""SST may get better performance"" - see above",,arg_request,arg-request_edit,asp_substance,1
"- Minor grammatical mistakes (missing ""a"" or ""the"" in front of some terms, suggest proofread.)",,arg_request,arg-request_typo,asp_clarity,3
The bounds obtained are polynomial in the sequence length T (often sublinear or linear) and should be compared with the existing bounds for a thorough comparison.,,arg_request,arg-request_experiment,asp_substance,2
It would be good to have some experiments in the sequence to sequence setting to understand if the obtained complexities are in fact what one would expect in practice.,,arg_request,arg-request_experiment,asp_substance,2
General comments: THe appendix has some good discussion and it would be great if some of that discussion was moved to the main paper.,,arg_request,arg-request_edit,asp_clarity,1
Have the authors tried out such techniques?,,arg_request,arg-request_explanation,asp_clarity,0
Is there any theoretical motivation or guarantee for this assumption as well?,,arg_request,arg-request_explanation,asp_substance,0
"- The experimental results show that the proposed strategy performs well in problems that are low-rank, while the performance may degrade in problems where the low-rank assumption is not met. Would it be possible to detect the rank of the problem in a dynamical manner (i.e., during the learning), so that the number of incomplete observations of Q can be increased to improve the performance, or the solution strategy (e.g. whether to use the low-rank assumption or not) can be adapted to the nature of the problem?",,arg_request,arg-request_explanation,asp_substance,0
- The Q-value matrices and functions considered in the problem have a special structure as they result from Markov Decision Processes. Would it be possible to go beyond the low-rank assumption and propose and use a more elaborate type of prior that employs the special structure of MDPs?,,arg_request,arg-request_explanation,asp_substance,0
- Please clearly define the notation used in Section 4.2.,,arg_request,arg-request_clarification,asp_clarity,4
- it would be nice to also propose unconditioned experiments.,,arg_request,arg-request_experiment,asp_substance,2
It would be good to give an idea in the text of TPU-GPU equivalence in terms of feasibility of a standard GPU implementation - computation time it would involve.,,arg_request,arg-request_edit,asp_replicability,1
- I understand that no data augmentation was used during training?,,arg_request,arg-request_clarification,asp_clarity,4
"- clarification of the truncation trick: if the elements of z are re-sampled and are still above the threshold, are they re-sampled again and again until they are all below the given threshold?",,arg_request,arg-request_explanation,asp_clarity,0
- A sentence could be added to explain the truncation trick in the abstract directly since it is simple to understand and is key to the quality of the results.,,arg_request,arg-request_edit,asp_soundness-correctness,1
- A reference to Appendix C could be given at the beginning of the Experiments section to help the reader find these details more easily.,,arg_request,arg-request_edit,asp_clarity,1
- It would be nice to display more Nearest neighbors for the dog image.,,arg_request,arg-request_edit,asp_clarity,1
- It would be nice to add a figure of random generations.,,arg_request,arg-request_result,asp_substance,5
- make the bib uniform: remove unnecessary doi - url - cvpr page numbers,,arg_request,arg-request_edit,asp_clarity,1
It would be interesting to see if the proposed method is competitive for training contemporary networks and w.r.t. simple schedule schemes.,,arg_request,arg-request_experiment,asp_substance,2
"In particular, the example in section 3.1 says that a noisy information bottleneck is introduced, but then says that the modified and unmodified models have ""training algorithms that are exactly equivalent."" I think this example needs to be clarified.",,arg_request,arg-request_clarification,asp_clarity,4
What is the relationship between $\theta$ and $\tilde\theta$ exactly?,,arg_request,arg-request_clarification,asp_clarity,4
"In this simple model, can we not calculate the mutual information directly (i.e., without the bottleneck)?",,arg_request,arg-request_explanation,asp_replicability,0
"The connection between mutual information and generalization has been studied in several contexts [see, e.g., the references in this paper and https://arxiv.org/abs/1511.05219 https://arxiv.org/abs/1705.07809 https://arxiv.org/abs/1712.07196 https://arxiv.org/pdf/1605.02277.pdf https://arxiv.org/abs/1710.05233 https://arxiv.org/pdf/1706.00820.pdf ] and further exploration is desirable.",,arg_request,arg-request_experiment,asp_substance,2
"Such a perspective is interesting, but needs to be further developed and explained.",,arg_request,arg-request_experiment,asp_substance,2
"Specifically, how can mutual information in this context be formally linked to generalization/overfitting?",,arg_request,arg-request_explanation,asp_replicability,0
"As a result, constraining the model will alter the mutual information and I think the effect of this should be remarked on.",,arg_request,arg-request_explanation,arg_other,0
"Overall, I think this paper has some interesting ideas, but those need to be fleshed out and clearly explained in a future revision.",,arg_request,arg-request_edit,asp_clarity,1
I think it would be worth discussing this more.,,arg_request,arg-request_explanation,asp_soundness-correctness,0
1. I don't see why MINE cannot be applied to the fMRI dataset and results be reported.,,arg_request,arg-request_explanation,asp_substance,0
"2. There are several errors in the writing - hyperparamters in Abstract, repetition of the word ""Section"" in fMRI experiment section etc. - which needs to be fixed.",,arg_request,arg-request_edit,asp_clarity,1
"- In the paper, the states are represented by only object positions (x, y, z). Is this sufficient? (e.g. velocity is unnecessary?)",,arg_request,arg-request_explanation,asp_substance,0
Is this additional assumption realistic enough and has it been adopted in other previous works? Is there any way to discriminate robot states and object states automatically?,,arg_request,arg-request_experiment,asp_substance,2
"- Can MISC deal with the problems where the number of objects of interest is more than two? In this case, how can we define mutual information?",,arg_request,arg-request_explanation,asp_substance,0
"- In Eq. (4), T(x_1:N, y_1:N) is assumed to be decomposable into the sum of T(x_t, y_t) / N. Can this make the lower bound (Eq. (3)) arbitrarily loose since the class of functions becomes very limited?",,arg_request,arg-request_explanation,asp_substance,0
"- Similarly to the problem of sparse reward, if the robot and the object are far apart and it is difficult to reach the object with random exploration, it would also be difficult to train the mutual information discriminator. How was the discriminator trained? How many time steps were used to train MI discriminator?",,arg_request,arg-request_experiment,asp_substance,2
"Compared to using just a very simple dense reward (e.g. negative L2 distance between the robot and the object), what would be the advantage of using MI discriminator? It would be great to show the comparison between using simple dense reward and MI discriminator for each Push, Pick&Place, and Slide task.",,arg_request,arg-request_clarification,asp_substance,4
"- For the MISC+DIAYN, what if we train the agent using MISC and DIAYN at the same time, instead of pre-training MISC first and fine-tuning DIAYN later?",,arg_request,arg-request_experiment,asp_replicability,2
"Then, how about the opposite situation? What if the task requires that the robot should 'avoid' the object of interest? Does MISC still work? Is it helpful for the improvement of sample efficiency?",,arg_request,arg-request_clarification,asp_substance,4
"- In section 4.3, what happens if we transfer the learned discriminator to Pick&Place from Push that has a gripper fixed to be closed, rather than the opposite direction (i.e. from Pick&Place to Push)? Does the MISC-t still well work? Can the learned MI discriminator be transferred to different tasks even when the state space is different?",,arg_request,arg-request_experiment,asp_meaningful-comparison,2
"In the future, I am quite curious about how these mono-image learned features would fare on more complex downstream tasks (e.g., segmentation, keypoint detection) which necessarily rely less on texture.",,arg_request,arg-request_result,asp_replicability,5
"It seems to me that for completeness, Table 4 should include the result of training a supervised network on top of random conv1/2 and Scattering network features, because this experiment is actually testing what we want - performance of the features when fine-tuned for a downstream task.",,arg_request,arg-request_result,asp_substance,5
"So for example, even if a linear classifier on top of Scattering features does poorly, if downstream fine-tuning results in the same performance as another pre-training method, then Scattering is a perfectly fine approach for initial features.",,arg_request,arg-request_result,asp_substance,5
I wonder if the learned features require fewer fully supervised images to obtain the same performance on the downstream task?,,arg_request,arg-request_explanation,asp_substance,0
Can the authors clarify how the neural style transfer experiment is performed?,,arg_request,arg-request_clarification,asp_substance,4
Are all these features taken directly from the self-supervised network or is it fine-tuned in some way?,,arg_request,arg-request_explanation,asp_substance,0
I wonder how feasible it is to find a proxy metric that corresponds to the performance on downstream tasks which is expensive to compute.,,arg_request,arg-request_experiment,asp_substance,2
It might be interesting to try to generate synthetic images (or modify real ones) that are good for this purpose and observe their properties.,,arg_request,arg-request_experiment,asp_substance,2
"* For the synthetic dataset one-to-one mapping of one-hot vectors it is clear that an ME bias would incur an advantage for classification on a zero-shot basis, i.e. an increased an accuracy for the first time a new input is observed. Is ME bias considered to be useful here because (i) we care directly about improving this type of zero-shot classification, or is it (ii) because it's implicitly assumed that a better initial prediction will lead to faster learning on these examples?",,arg_request,arg-request_explanation,asp_substance,0
"So at the beginning of training, while there are lots of new classes to benefit from the ME bias, the maximum advantage per example is small, and vice versa at the end of training - how do these conflicting effects balance each other out over the course of training and how do the benefits of ME bias scale with the number of classes?",,arg_request,arg-request_explanation,asp_substance,0
* It would be interesting to see a more detailed analysis of the predictions of the image classifiers in section 4.2.,,arg_request,arg-request_explanation,asp_substance,0
"* Is the acronym ME pronounced like the word “me” or is it spelled out “M-E”? If the latter, then all cases of “a ME bias” should be corrected to “an ME bias”.",,arg_request,arg-request_typo,asp_substance,3
"* Section 4.2 line 3: “sample the class [for the] from a power law distribution""",,arg_request,arg-request_edit,asp_substance,1
"But there are some errors of expression, so it should be checked.",,arg_request,arg-request_edit,asp_clarity,1
I wonder why the numbers are so different.,,arg_request,arg-request_explanation,asp_meaningful-comparison,0
"Maybe it's because of different scales? Previous works usually scale each pixel to [0,1] or [-1,1], maybe the authors use the [0, 255] scale? But 0.1/255 will be much smaller than 0.031.",,arg_request,arg-request_explanation,asp_substance,0
2. What's the training time of the proposed method compared with vanilla adversarial training?,,arg_request,arg-request_clarification,asp_substance,4
"Regardless, given the prevalence of these types of testbed environments, either is a useful discussion to have. Maybe the end result could minimally be a new baseline that can help quantify the ‘difficulty’ of a particular environment.",,arg_request,arg-request_edit,asp_soundness-correctness,1
"Even the ‘focused experiments’ can be explained with the intuitive narrative that in the state/action space, there is always more uncertainty the farther one goes from the starting point and this is more of a result of massive computation being applied primarily to problems that are designed to provide some level of novelly (the Roboschool examples are a bit more interesting, but also less conclusive).",,arg_request,arg-request_edit,asp_soundness-correctness,1
"With respect to this specific setting, the authors may want to consider [Mirowski, et al., Learning to Navigate in Complex Environments, ICLR17] with respect to auxiliary loss + RL extrinsic rewards to improve performance (in this case, also in maze environments).",,arg_request,arg-request_experiment,asp_meaningful-comparison,2
"In considering combining with extrinsic rewards, I would also consider [Mirowski, et al., ICLR17], which is actually more involved in this regard.",,arg_request,arg-request_edit,asp_meaningful-comparison,1
"If the goal is to learn representation for low-shot setting, the method needs to be compared with other representation learning methods such as jigsaw[1], colorization[2] and rotation[3].",,arg_request,arg-request_experiment,asp_substance,2
"First, many details are missing, especially in the experiments, which makes the proposed method suspicious and non-convincing.",,arg_request,arg-request_explanation,asp_clarity,0
"For example, what is the number_iterations in the experiments? How are they chosen or what's the specific stopping criteria?",,arg_request,arg-request_experiment,asp_substance,2
"The descriptions of the datasets used are not clear, e.g., the number of classes for each data.",,arg_request,arg-request_clarification,asp_substance,4
"Second, many typos and grammar errors need to fix, e.g., ""the proposed SST is suitable for lifelong learning which make use..."", ""the error 21.44% was lower than"" 18.97?",,arg_request,arg-request_typo,asp_substance,3
Combining SST with other existing techniques can help.,,arg_request,arg-request_edit,asp_substance,1
Further demonstrations are necessary for the proposed SST method.,,arg_request,arg-request_result,asp_substance,5
Do the same model is trained for multiple tasks? Is each of the tasks trained sequentially or simultaneously?,,arg_request,arg-request_clarification,asp_replicability,4
"However, it looks to me that the authors need to better explain the motivation of DiVA, the differences of DiVA from existing supervised VAE, and the experimental settings, before the acceptance of this paper.",,arg_request,arg-request_explanation,asp_motivation-impact,0
"More precisely, for the digit datasets, the reviewer was interested to see how the proposed MDL performs on jointly adapting SVHN, MNIST, MNIST-M, and USPS or jointly adapting DSLR, Amazon, and Webcam for OFFICE dataset.",,arg_request,arg-request_experiment,asp_substance,2
"Moreover, comparison with some of the DA baselines (ADDA[1], DSN[2]) is missing.",,arg_request,arg-request_experiment,asp_substance,2
"The reviewer is also interested to see how the the generalization bound introduced in this paper is related to the recent theoretical works [3],[4] on MDL.",,arg_request,arg-request_experiment,asp_meaningful-comparison,2
It would make the paper much stronger if the authors perform quantitative + qualitative evaluation on the MF training of RBMs first.,,arg_request,arg-request_explanation,asp_substance,0
"It would be very interesting to contrast the proposed method with other previously proposed MF based method, in particular using Free energy to approximate the expectation of the model without constraints.",,arg_request,arg-request_experiment,asp_substance,2
"For example, it is curious to see how denoising Auto encoders would perform.",,arg_request,arg-request_experiment,asp_substance,2
"In addition, it could be worthwhile to compare and benchmark on existing evaluations: https://arxiv.org/pdf/1802.06806.pdf",,arg_request,arg-request_experiment,asp_substance,2
"- The authors should make a distinction on what kinds of attack is considered: white box, black box or grey box.",,arg_request,arg-request_explanation,asp_substance,0
I think the authors should just acknowledge that you can't soundly *sample* from their generative model the way e.g. VAE or GAN allows (their function composition is not a sampling method).,,arg_request,arg-request_edit,asp_soundness-correctness,1
"I think this function composition angle should be deemphasized in the title/abstract, but I think the paper stands",,arg_request,arg-request_edit,asp_soundness-correctness,1
"- In Figure 2 it's pretty hard to see the differences between the methods. What exactly is being visualized here? DeepSDF shold be visualizing surface normals vs. HOF which is point clouds, right?",,arg_request,arg-request_clarification,asp_clarity,4
"- For predicting a deformation R^3 -> R^3 function composition sort of makes sense, but how generalizable",,arg_request,arg-request_explanation,asp_clarity,0
- it's better to show time v.s. testing accuracy as well.,,arg_request,arg-request_result,asp_substance,5
the per-epoch time for each method is different.,,arg_request,arg-request_result,asp_substance,5
"1. The focus of this paper is the hyper-parameter, please focus and explain more on the usage with hyper-parameters.",,arg_request,arg-request_edit,asp_substance,1
"- in section 2.2, please explain more how gradients w.r.t hyper-parameters are computed.",,arg_request,arg-request_explanation,asp_replicability,0
2. No need to write so much decorated bounds in section 3.,,arg_request,arg-request_edit,arg_other,1
3. Could authors explain the time complexity of inner loop in Algorithm 1? Does it take more time than that of updating model parameters?,,arg_request,arg-request_clarification,asp_substance,4
"However,",,arg_request,arg-request_experiment,asp_substance,2
Is it better to decay learning rates for toy data sets?,,arg_request,arg-request_experiment,asp_substance,2
"It is known that SGD with fixed step-size can not find the optimal for convex (perhaps, also simple) problems.",,arg_request,arg-request_experiment,asp_substance,2
"- how to tune lambda? it is an important hyper-parameter, but it is set without a good principle, e.g., ""For SGD-APO, we used lambda = 0.001, while for SGDm-APO, we used lambda = 0.01"", ""while for RMSprop-APO, the best lambda was 0.0001",,arg_request,arg-request_explanation,asp_replicability,0
What are reasons for these?,,arg_request,arg-request_explanation,asp_replicability,0
"Tuning a good lambda v.s. tuning a good step-size, which one costs more?",,arg_request,arg-request_explanation,asp_substance,0
"1. the NPN model seems a good alternative, will be good to have a discussion about why your model is better than NPN.",,arg_request,arg-request_experiment,asp_meaningful-comparison,2
I will be curious to know how it performs.,,arg_request,arg-request_explanation,asp_meaningful-comparison,0
2. A more detailed illustration of the system / network is needed. Would have made it much easier to understand the paper.,,arg_request,arg-request_edit,asp_substance,1
3. What are the results when using the whole training set of Recipes ?,,arg_request,arg-request_explanation,asp_soundness-correctness,0
Why not compare with Sparsely-Gated MoE?,,arg_request,arg-request_experiment,asp_substance,2
I wonder the motivation of analyzing generalization of RNNs by the techniques established by Bartlett.,,arg_request,arg-request_explanation,asp_soundness-correctness,0
"1. For the experiment of 1D signal on sine wave, the AA-pooling and F-pooling give the same result?",,arg_request,arg-request_clarification,none,4
Are there any other advantages for F-pooling s.t. people might want to use it as opposed to AA-pooling?,,arg_request,arg-request_clarification,asp_substance,4
3. What are the limitations of the F-pooling? It is good to me that the authors discuss one limitation on the imaginary part of output and I would like to hear more on other potential limitations for this method.,,arg_request,arg-request_clarification,asp_soundness-correctness,4
"- also, if the authors can explain more on sec 2.5 it will be helpful. If we simply ignore the imaginary part, although the theory is not applicable, but what would the empirical performance be?",,arg_request,arg-request_explanation,asp_substance,0
"Rather than ""skill discovery"", I suggest the authors position MISC relative to earlier work on empowerment, wherein a single policy was used to maximize mutual information of the form I(a; s_t | s_{t-1}).",,arg_request,arg-request_edit,asp_clarity,1
"I believe this would be a much more appropriate baseline, and I'd be curious to hear the intuition for why I(s_c ; s_i) should be superior.",,arg_request,arg-request_result,asp_clarity,5
"However, it is unclear whether or not VIME and PER were modified to incorporate domain knowledge (i.e. s_i/s_c distinction).",,arg_request,arg-request_clarification,asp_substance,4
"Indeed, an appendix would be greatly appreciated, as many experimental details were omitted.",,arg_request,arg-request_edit,asp_meaningful-comparison,1
"Ideally, an experimental setup with previously published results (e.g. control suite for DIAYN, Seaquest for DISCERN) would be considered, but I can understand why this wasn't done as incorporating domain knowledge is the main contribution of the paper.",,arg_request,arg-request_edit,asp_clarity,1
"That said, the claims should be weakened to reflect this gap, and domain knowledge should be mentioned more prominently (e.g. states of interest vs context are given, not learned).",,arg_request,arg-request_edit,asp_meaningful-comparison,1
Can the authors elaborate on this?,,arg_request,arg-request_explanation,asp_soundness-correctness,0
What is the benefit of using DL algorithms within the oracle-augmented datastream model?,,arg_request,arg-request_explanation,asp_substance,0
Is a simple algorithm enough? What algorithms should we ideally use in practice?,,arg_request,arg-request_explanation,asp_substance,0
What if you used simpler online learning algorithms with formal accuracy guarantees?,,arg_request,arg-request_experiment,asp_substance,2
"Overall, I am in favour of accepting this paper given some clarifications and improving the evaluations.",,arg_request,arg-request_edit,asp_soundness-correctness,1
"There are a few more references that had similar ideas and might be worth adding: Brabandere et al. ""Dynamic Filter Networks"", Klein et al. ""A dynamic convolutional layer for short range weather prediction"", Riegler et al. ""Conditioned regression models for non-blind single image super-resolution"", and maybe newer works along the line of Su et al. ""Pixel-Adaptive Convolutional Neural Networks"".",,arg_request,arg-request_edit,asp_meaningful-comparison,1
Does this imply any topological constraints?,,arg_request,arg-request_clarification,asp_clarity,4
"Is this the most suitable shape to sample from? How do you draw samples from the sphere (Similarly, how are the points sampled for the training objects)?",,arg_request,arg-request_explanation,asp_clarity,0
What happens if you instead densely sample from a 3D box (similar to the implicit shape models)?,,arg_request,arg-request_explanation,asp_soundness-correctness,0
"What is c? Isn't it always 3, or how else is it possible to composite the mapping network?",,arg_request,arg-request_explanation,asp_clarity,0
It would be great if this paper could follow those recommendations to get better insights in the results.,,arg_request,arg-request_edit,asp_soundness-correctness,1
"Further, I could not find what k was set to in the evaluation of Tab. 1.",,arg_request,arg-request_clarification,asp_clarity,4
"Tab. 4 shows to some extend the influence of k, but I would like to see a more extensive evaluation.",,arg_request,arg-request_experiment,asp_substance,2
"How does performance change for larger k, and what happens if k is larger at testing then on at training, etc.?",,arg_request,arg-request_explanation,asp_clarity,0
- The tables will look a lot nicer if booktab is used in LaTeX,,arg_request,arg-request_edit,asp_clarity,1
"On the writing part, it would be nice to provide more context for both transformer, Proximal Policy Optimization and Simulated Policy Learning to make the paper more self-complete.",,arg_request,arg-request_edit,arg_other,1
"Why these methods are not considered in the beginning? In my opinion, transformer is good for modeling long term dependency and concurrent predictions which is not necessarily the case for learning curves.",,arg_request,arg-request_explanation,asp_substance,0
How does the transformer based method comparing to others?,,arg_request,arg-request_experiment,asp_substance,2
- what prior distributions p(z) and p(u) are used? What is the choice based on?,,arg_request,arg-request_explanation,asp_substance,0
"- abbreviation IPM is referred several times in the paper, but remains undefined in the paper until end of page 4, please define earlier.",,arg_request,arg-request_explanation,asp_substance,0
"- The model G_theta does not appear in the training objective function (4), how is this module trained precisely?",,arg_request,arg-request_clarification,asp_substance,4
"“vedio” op page 4, “circile” on page 5, “condct” on page 8, etc.",,arg_request,arg-request_typo,arg_other,3
The refenence to  Bengio 2018 is incomplete: what do you refer to precisely?,,arg_request,arg-request_clarification,asp_substance,4
"Please comment on the choice, and its impact on the behavior of the model.",,arg_request,arg-request_explanation,asp_substance,0
- The toy data set experiments could be dropped  to make room for experiments suggested below.,,arg_request,arg-request_edit,asp_substance,1
- An experimental study of the effect of the mixing parameter “s” would be useful to include.,,arg_request,arg-request_experiment,asp_substance,2
- Experimental evaluation of auto-encoding using a variable number of input points is interesting to add: ie how do the two evaluation measures evolve as a function of the number of points in the input point cloud?,,arg_request,arg-request_experiment,asp_substance,2
"- Similar, it is interesting to evaluate how auto encoding performs when non-uniform decimation of the input cloud is performed, eg what happens if we “chop off” part of the input point cloud (eg the legs of the chair), does the model recover and add the removed parts?",,arg_request,arg-request_experiment,asp_substance,2
- Analysis of shapes with different genus and dimensions would be interesting.,,arg_request,arg-request_result,asp_substance,5
"Does the model manage to capture that some shapes have holes, or consists of a closed 2D surface (ball) vs an open surface (disk),",,arg_request,arg-request_explanation,asp_substance,0
I would like to see additional experiments to answer this questions.,,arg_request,arg-request_experiment,asp_substance,2
"You should use an existing ES implementation (e.g., from some well-known package) instead of a naive implementation, and as additional baseline also CMA-ES.",,arg_request,arg-request_edit,asp_soundness-correctness,1
If you can also compare against one or two algorithms of your choice from the recent literature it would also give more value to the comparison.,,arg_request,arg-request_experiment,asp_meaningful-comparison,2
This statement is ambiguous and potentially unsupported by evidence. how do you define complex? that can or that did discover?,,arg_request,arg-request_explanation,asp_soundness-correctness,0
"What is the point that you are trying to make? Also, note that some of the algorithms that you are citing there have indeed applied beyond architecture search, eg. Bayesian optimization is used for gait optimization in robotics, and Genetic algorithms have been used for automatic robot design.",,arg_request,arg-request_explanation,asp_soundness-correctness,0
"- Sec 2.2: ""(GNNs) are very effective"" effective at what? what is the metric that you consider?",,arg_request,arg-request_explanation,asp_soundness-correctness,0
"- Sec 3 ""(PS), where weights are reused"" can you already go into more details or refer to later sections?",,arg_request,arg-request_explanation,asp_soundness-correctness,0
"- ""to speed up and trade off between evaluating fitness and evolving new species"" Unclear sentence. speed up what",,arg_request,arg-request_clarification,asp_clarity,4
- Sec 3.4 can you recap all the parameters after eq.11? going through Sec 3.2 and 2.2 to find them is quite annoying.,,arg_request,arg-request_clarification,asp_clarity,4
"- Providing the same computational budget seem rather arbitrary at the moment, and it heavily depends from implementation. How many evaluations do you perform for each method? why not having the same budget of experiments?",,arg_request,arg-request_explanation,asp_soundness-correctness,0
Attacking CRBMs is highly relevant and should be included as a baseline.,,arg_request,arg-request_result,asp_meaningful-comparison,5
Authors should clarify the justification behind experimenting only on 'first 500 test images'.,,arg_request,arg-request_clarification,asp_clarity,4
Including at least one set of black box attacks is necessary to verify to what degree the vanishing gradient is the case here.,,arg_request,arg-request_experiment,asp_substance,2
- What are the motivations to use Frank-Wolfe ? Usually this algorithm is used when the constraints are to complicated to have a tractable projection (which is not the case for the L_2 and L_\infty balls) or when one wants to have sparse iterates which do not seem to be the case here.,,arg_request,arg-request_explanation,asp_soundness-correctness,0
- Consequently why did not you compare simple projected gradient method ?,,arg_request,arg-request_experiment,asp_substance,2
- What is the difference between the result of Theorem 4.3 and the result from (Lacoste-Julien 2016)?,,arg_request,arg-request_explanation,asp_substance,0
",  case assumption 4.5 do not make any sense since $y = y_{tar}$ (we just need to note $\|\nabla f(O,y_{tar})\| = C_g$) and some notation could be simplified setting for instance $f(x,y_{tar})  = f(x)$.",,arg_request,arg-request_edit,asp_clarity,1
- Sec 3.1 theta_i -> x_i,,arg_request,arg-request_edit,asp_soundness-correctness,1
"Beyond this simplification, I am not clear if that is actually intended by the authors.",,arg_request,arg-request_clarification,asp_substance,4
The authors explain how they trained their own model but there is no mention on how they trained benchmark models.,,arg_request,arg-request_clarification,asp_substance,4
"However, given that the datasets used in the experiments were not used in the associated benchmark papers, it is necessary for authors to explain how they trained competing models.",,arg_request,arg-request_explanation,asp_substance,0
"Without a proper comparison (formal and experimental) with these lines of work, the paper is incomplete.",,arg_request,arg-request_experiment,asp_meaningful-comparison,2
"But I assume it is very cheap compared to training e.g. the ResNet, correct?",,arg_request,arg-request_clarification,asp_clarity,4
"I also did not find an explanation of which version backward/forward losses [Patrini et al. 17] is used in the experiments: are the noise transition matrices estimated on the data or assumed to be known (for fair comparison, I would do the former).",,arg_request,arg-request_clarification,asp_clarity,4
A missing empirical analysis is on class-conditional noise (see for example Patrini et al. 17 for a definition).,,arg_request,arg-request_experiment,asp_substance,2
An additional column on the table showing that the algorithm can also work in this case would improve the confidence that the proposed method is useful in practice.,,arg_request,arg-request_result,asp_clarity,5
A proper baseline should have been compared.,,arg_request,arg-request_experiment,asp_meaningful-comparison,2
"One more unclear but important point: is Table 3 obtained by white-box attacks on the Resnet/Denset but oblivious of the MCD? Is so, I don’t think such an experiment tells the whole story: as the the MCD would arguably also be deployed for classification, the attacker would also target it.",,arg_request,arg-request_explanation,asp_soundness-correctness,0
"I don’t follow this argument: this is just part of the classifier. White box attacks are by definition performed with the knowledge of the model, what",,arg_request,arg-request_explanation,asp_soundness-correctness,0
a) Why not try to do this with Variational inference? It should conceptually still work and be fast and potentially more robust.,,arg_request,arg-request_explanation,asp_substance,0
Do the authors think that ultimately learning priors with models like GraphRNN might be more promising for certain applications?,,arg_request,arg-request_explanation,asp_substance,0
A more predictive generative model that makes less hard assumptions on graph data would be interesting.,,arg_request,arg-request_experiment,asp_substance,2
"However, the experiments are not enough to show that the proposed method is really needed to achieve the results. If these are answered well, I'd be happy to change my evaluation.",,arg_request,arg-request_experiment,asp_soundness-correctness,2
1. The method should be compared with other combinations of components.,,arg_request,arg-request_edit,asp_meaningful-comparison,1
"At least, it should be compared with ""Multi-bit quantization only (Xu et al., 2018)"" and ""Multi-bit-quantization + Viterbi-based binary code encoding"".",,arg_request,arg-request_edit,asp_meaningful-comparison,1
"2. The experiments with ""Don't Care"" should go to the experiment section, and the end-to-end results should be present but not the ratio of incorrect bits.",,arg_request,arg-request_edit,asp_substance,1
"3. Similarly, the paper will become stronger if it has some experimental results that compare quantization methods.",,arg_request,arg-request_edit,asp_meaningful-comparison,1
* It was a bit hard to understand how a matrix is processed through the flowchart in Fig. 1 at first glance.,,arg_request,arg-request_clarification,asp_clarity,4
It would help readers to understand it better if it has a corresponding figure which shows how a matrix is processed through the flowchart.,,arg_request,arg-request_clarification,asp_clarity,4
"4.	For the final purpose, comparing problem similarity, I am wondering what the result will be if we train a supervised model based problem-problem similarity labels?",,arg_request,arg-request_explanation,asp_soundness-correctness,0
