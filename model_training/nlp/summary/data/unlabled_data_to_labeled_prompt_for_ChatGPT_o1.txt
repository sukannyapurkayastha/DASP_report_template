i have the following inputs. please give me the respective outputs but streamline them. dont leave out any of them!

idea is to have a jsonl file that contains input and output data, like {"input": "{text to summarize}", "output": "Summary: {summarized text}"}


{"input": "1. It seems that part of the techniques is from previous results, such as the bonus function oracle. It will be helpful if there is a section discussing technical novelty. 2. It seems that the setting is closely related to low switching RL and RL with delayed feedback. It will be interesting if the authors could briefly discuss about the connections. What if there are more than one agents are to be activated instead of just having one agent at each time t? How is an agent chosen to be activated in Line 5 of both Algorithm 1 and 2? As far as I know, MARL often adopts the Centralized Training Decentralized Execution(CTDE) framework to avoid the action space growing exponentially with the number of users. However, it is unclear whether the proposed scenario follows "decentralized execution". Agents are supposed to execute based on partial observations in a decentralized manner, but the proposed approach appears to involve a central server consistently during execution. If the proposed scenario is inconsistent with CTDE, I would be interested to hear from the authors what the distinct advantages or necessity of this scenario is. I think the major concern is non-optimal complexity bounds. Although it seems unreasonable to ask for a matching upper\&lower regret bound for the contextual bandit problem, the part about RL could be possibly improved (at least, the dependence on $H$ is not tight). Also I am curious that what is the current best lower bound for the communication cost to reach an $\sqrt{T}$ regret bound. It would be an interesting problem to study the exact trade-off between the communication cost and regret."}
{"input": "3. For the communication complexity bound in theorem 5.1, should it be $/\alpha$ instead of $\alpha$? In addition, why not choose $\alpha=1/M$ in both theorems? In this way, the communication cost can be improved. (Please correct me if I misunderstood anything) 4. Line ?? in line 214 of page 6. Please correct it. The clarity of some of the important quantities are not well defined or explained. For example, 1) In the sample complexity result of Theorem 4.3, $\tilde{\beta}_1$ is used. However, it was not defined. It’s unclear what this notation is referring to. Similarly, in Theorem 5.1, $\tilde{\beta}_2$ is used. 2) The oracle for to compute bonus term bk+1,h is crucial in understanding the algorithms. However, it was not very well-explained or shown anywhere in the main paper. 2) Line 214, ?? -> 12 3) In Line 1 of algorithm 3, $k=[K]$ -> $k\in [K]$. 4) In Line 154, the trajectory should be $(s_h, a_h, \cdots, s_H, a_H)$. What is the oracle to compute the bonus term? What are the terms $\tilde{\beta}_1$ and $\tilde{\beta}_2$? Please clarify the two sentences from Line 300 to Line 302. Theorems 4.3 and 5.1: $\tilde{\beta}$ is not properly defined in the statement, and $\beta_t$ should be fixed to $\tilde{\beta}$."}
{"input": "3) The two sentences from Line 300 to Line 302 are confusing. Please clarify them. I’m open to revising my score based on the authors' responses."}
{"input": "While I have studied general value function approximation, I do not have research background in this field for multi-agent scenarios. Therefore, my critique may not have captured the weaknesses of this paper. A minor concern might be about the technical novelty given previous methods on measuring the uncertainty."}
{"input": "The clarity of some of the important quantities are not well defined or explained. For example, 1) In the sample complexity result of Theorem 4.3, $\tilde{\beta}_1$ is used. However, it was not defined. It’s unclear what this notation is referring to. Similarly, in Theorem 5.1, $\tilde{\beta}_2$ is used."}
{"input": "The theoretical foundations and proofs regarding the communication criterion are important and interesting. Also, the trade-off between regret and communication complexity via parameter $\alpha$ offers valuable insights."}
{"input": "What is the meaning of "fully asynchronous" in table 1?"}
{"input": "1. The problem of asynchronous MARL with general function approximation is interesting and important."}
{"input": "This paper is well written and the contribution is solid."}
{"input": "A minor concern might be about the technical novelty given previous methods on measuring the uncertainty."}
{"input": "A simple question: Why is the order of the Eluder dimension of regret reported as $O(\sqrt{\text{dim}_E})$ instead of $O(\text{dim}_E)$? The regret in Line 677 shows similar results to numerous other papers, but is written in $\sqrt{O(\text{dim}_E)}$ despite the fact that $O(\text{dim}_E)$ dominates. For the linear MDP case, [1] presents a lower bound of $\Omega(dH\sqrt{T})$, and considering $\text{dim}_E = \tilde{O}(d)$, the reported results seem lower than the known lower bound. Many papers report similar results as the authors, but am I missing something?"}
{"input": "A simple question: Why is the order of the Eluder dimension of regret reported as $O(\sqrt{\text{dim}_E})$ instead of $O(\text{dim}_E)$? The regret in Line 677 shows similar results to numerous other papers, but is written in $\sqrt{O(\text{dim}_E)}$ despite the fact that $O(\text{dim}_E)$ dominates. For the linear MDP case, [1] presents a lower bound of $\Omega(dH\sqrt{T})$, and considering $\text{dim}_E = \tilde{O}(d)$, the reported results seem lower than the known lower bound. Many papers report similar results as the authors, but am I missing something?"}
{"input": "2. This paper is the first to consider the setting with general function approximation. The results are solid and the proof looks good to me. 1. It seems that part of the techniques is from previous results, such as the bonus function oracle. It will be helpful if there is a section discussing technical novelty."}
{"input": "2. It seems that the setting is closely related to low switching RL and RL with delayed feedback. It will be interesting if the authors could briefly discuss about the connections. How is an agent chosen to be activated in Line 5 of both Algorithm 1 and 2? What is the oracle to compute the bonus term? What are the terms $\tilde{\beta}_1$ and $\tilde{\beta}_2$? As far as I know, MARL often adopts the Centralized Training Decentralized Execution(CTDE) framework to avoid the action space growing exponentially with the number of users. However, it is unclear whether the proposed scenario follows "decentralized execution". Agents are supposed to execute based on partial observations in a decentralized manner, but the proposed approach appears to involve a central server consistently during execution. If the proposed scenario is inconsistent with CTDE, I would be interested to hear from the authors what the distinct advantages or necessity of this scenario is. A simple question: Why is the order of the Eluder dimension of regret reported as $O(\sqrt{\text{dim}_E})$ instead of $O(\text{dim}_E)$? The regret in Line 677 shows similar results to numerous other papers, but is written in $\sqrt{O(\text{dim}_E)}$ despite the fact that $O(\text{dim}_E)$ dominates. For the linear MDP case, [1] presents a lower bound of $\Omega(dH\sqrt{T})$, and considering $\text{dim}_E = \tilde{O}(d)$, the reported results seem lower than the known lower bound. Many papers report similar results as the authors, but am I missing something? What is the meaning of "fully asynchronous" in table 1?"}
{"input": "1. It seems that part of the techniques is from previous results, such as the bonus function oracle. It will be helpful if there is a section discussing technical novelty. 3. For the communication complexity bound in theorem 5.1, should it be $/\alpha$ instead of $\alpha$? In addition, why not choose $\alpha=1/M$ in both theorems? In this way, the communication cost can be improved. (Please correct me if I misunderstood anything) 4. Line ?? in line 214 of page 6. Please correct it. Theorem 5.1: Total communication complexity should be fixed to $O((1+M\alpha)^2 / \alpha)$. I think the major concern is non-optimal complexity bounds. Although it seems unreasonable to ask for a matching upper\&lower regret bound for the contextual bandit problem, the part about RL could be possibly improved (at least, the dependence on $H$ is not tight). Also I am curious that what is the current best lower bound for the communication cost to reach an $\sqrt{T}$ regret bound. It would be an interesting problem to study the exact trade-off between the communication cost and regret. A minor concern might be about the technical novelty given previous methods on measuring the uncertainty."}
{"input": "Typos: 1) An extra closing parenthesis appeared in Line 141. 2) Line 214, ?? -> 12 3) In Line 1 of algorithm 3, $k=[K]$ -> $k\in [K]$. 4) In Line 154, the trajectory should be $(s_h, a_h, \cdots, s_H, a_H)$. Typos: - Line 214: Reference to the label is not correctly written. Theorems 4.3 and 5.1: $\tilde{\beta}$ is not properly defined in the statement, and $\beta_t$ should be fixed to $\tilde{\beta}$."}
{"input": "3) The two sentences from Line 300 to Line 302 are confusing. Please clarify them. Please clarify the two sentences from Line 300 to Line 302."}
{"input": "What if there are more than one agents are to be activated instead of just having one agent at each time t?"}
{"input": "What does the MLP do? Given the mechinterp focused on attention solely, it is unclear what role MLPs played. Two experiments to try here are: (i) train attention only models to see if MLPs are even necessary, and (ii) perform the PCA analysis to uncover representations' geometry at the level of attentions and MLPs at each block in the model. Experiment (i) may require retraining models, so I understand if the authors are unable to conduct it, but my expectation will be that you will see that model "internalizes" task vectors and records them in MLPs. Attention only models can solve the task, but I expect the representations' geometry will be quite different. For experiment (ii) however, I expect that's easy to run and is merely repeating the plotting script on intermediate representations as a forward pass occurs through the model. If the geometry is primarily formed at attention layers, we'll see that in this experiment. vice versa, if it forms via MLPs, we'll see it explicitly. This work has many interesting experiments. I found Section 5.2 (Attention Heads Implement Essential Skills) pretty interesting. The paper claims that for larger models, early stopping is necessary (line 52). While I appreciate that the authors used GPT-like architectures to reflect realistic settings, the architectures in the experiments are not that large. Even amongst popular open source models, the smallest are usually around 7B parameters. The setting and experiments neatly isolate and clearly demonstrate several interesting phenomena of emergence of capabilities and shifting in the solutions found by deep networks throughout training, contributing to the field's developing catalogue of examples of these phenomena. The experiments are well-designed, providing compelling support for the claims."}
{"input": "PCA variance. Given this is a rather rich geometry in 2-D, I'm slightly surprised to see PCA captured it. Did you have to do some preprocessing? How much variance is explained by the two projected components? If there are other components that are not shown but have a large variance, what do those components encode---can you try 3D plots? I am left wondering if this batch construction methodology, as a further departure from the standard language modelling setting, has any other implications for the learning process that may affect the generality of the results. Note: This weakness is not decisive because the authors clearly document their training methodology and it's not that artificial anyway. However, I feel that this section could be improved if the authors attempted to offer greater insight into the relationship between these prior works and the present work. For example, the authors have an opportunity here to informally describe the in-context linear regression and the modular addition problem settings that the newly proposed setting generalises. The skill decomposition discussed in section 5 is great. The clear pattern in attention heads verifies it very well. (The hypotheses could be further verified if the author can link the values of $c_1, c_2$ to some weights in the network, see the question part.) Equation 2 is a bit hard to understand. How does it correlate to $z = ax+by$ ? (Although, from the latter explanations, I know the model relies on $c_1z_1^t + c_2z_2^t$ to get $z$, but it might be helpful to claim how it is derived.) - Better to define $GF(p)$, i.e., the Galois field, before using it."}
{"input": "The definition of task diversity is not well defined. Is the number of pretraining tasks truly indicative of task diversity? I think the paper could benefit from some justification of this assumption. Since multiplication can be viewed as repeated addition, isn’t skill 2 an extension of skill 3 (or can even be viewed as skill 3 composed with itself multiple times)? Is hierarchy of skills important here? 4. I noticed some minor text errors as follows, which I expect the authors can easily correct. Line 94: The notation $[1, p^2]$ to me suggests a closed continuous interval, whereas you appear to mean $\lbrace1, \ldots, p^2\rbrace$, also in some cases denoted $[p^2]$. It seems that equation 2 should read $\ldots = (z_1^t, z_2^2) \mod p$ and the equation on line 203 should read $c_1x + c_2y \mod p$. That is, $x$ and $y$ should swap places with $z_1^t$ and $z_2^t$. Is this indeed a mistake, or am I missing something?"}
{"input": "While some elements of the analysis are complex, the authors have done an exceptional job of clearly presenting their findings. I feel careful study of each section and figure in the main text was rewarded since there was no question that occurred to me that was not addressed in the authors' clear descriptions or figures. 2. The mechanistic analysis is only partial. The authors admit that they have not been able to identify an end-to-end mechanistic model of how the trained transformers perform the task. This leaves their posited skill decomposition and partial mechanistic analysis open to the possibility that they are incomplete. The task and batch sample selection in this paper have many constraints (e.g., the rectangular rule, the balanced number of samples in each batch, etc.). However, the practical systems usually cannot strictly satisfy all these assumptions. Hence a more detailed analysis of how these assumptions influence the generalization ability would provide more insights to practical systems."}
{"input": "The paper is fairly well written and clear. Going beyond the standard linear regression task to study ICL was great to see as well. The paper is easy to follow. Good presentation!"}
{"input": "In figure 6 (top row) there is a typo: "Qeury" on the vertical axis. 2. In the figure 1 caption, is it possible to offer a clearer summary of the difference between in-distribution generalisation and out-of-distribution memorisation? On my first read through, treating the figure and caption as an overview of the work's main results, I had trouble distinguishing these two concepts. The results in Figure 5 are cool. The paper claims in line 147 that “As the o.o.d. performance increases, the pre-training performance simultaneously degrades “. However, it is hard to read this information from Figure 3-a panel 1. Maybe a different color mapping or adding numbers on these patches would be helpful. In line 264, the paper claims that the pattern depends on $(a,b)$, but it is hard to read that from Figure 6b."}
{"input": "Many works in the continual learning and meta learning literature suggest that training on multiple tasks at once leads to better generalization. Perhaps it is worth including brief discussion on the connections between this point and the model’s ability to generalize ood which is predominantly determined by the number of pre-training tasks. The OOD settings studied in grokking or emergent ability setting are quite related to the compositional generalization and systematic generalization. It would be helpful to discuss them in the related works, here are some of them: [1] Schott, Lukas, et al. "Visual representation learning does not generalize strongly within the same domain." ICLR 2022 [2] Xu, Zhenlin, Marc Niethammer, and Colin A. Raffel. "Compositional generalization in unsupervised compositional representation learning: A study on disentanglement and emergent language." NeurIPS 2022 [3] Ren, Yi, et al. "Improving compositional generalization using iterated learning and simplicial embeddings." NeurIPS 2023"}
{"input": "PCA variance. Given this is a rather rich geometry in 2-D, I'm slightly surprised to see PCA captured it. Did you have to do some preprocessing? How much variance is explained by the two projected components? If there are other components that are not shown but have a large variance, what do those components encode---can you try 3D plots? Are the results in Figure 6 coming from $d=2$ or $d=4$? I can find the figure for all 8 attention heads for $d=2$ in the appendix, what about the $d=4$ case? It might be helpful to see if the pattern in later layers (i.e., attention focusing on different $z_i$) exists in shallow layers, and vice versa."}
{"input": "I am left wondering if this batch construction methodology, as a further departure from the standard language modelling setting, has any other implications for the learning process that may affect the generality of the results. Are the results in Figure 6 coming from $d=2$ or $d=4$? I can find the figure for all 8 attention heads for $d=2$ in the appendix, what about the $d=4$ case? It might be helpful to see if the pattern in later layers (i.e., attention focusing on different $z_i$) exists in shallow layers, and vice versa."}
{"input": "The results in Figure 5 are cool."}
{"input": "I have not noticed any weaknesses in the paper that would temper my overall recommendation to accept. However, I note the following weaknesses, some of which the authors have already acknowledged, and others which they may like to take into consideration if they are interested to improve the paper further."}
{"input": "Related Work. At this point, the topic this paper is focused on has a rather rich literature and I think a more detailed related work is warranted (perhaps in the appendix if space is an issue). For example, the results by Kirsch et al. (which is cited) are very similar to what authors show, especially results on scaling effects. The main different is width scaling in that paper and no geometric analysis, but nonetheless the relationship warranted more emphasis and discussion. Similarly, several recent works have explored OOD generalization of toy ICL tasks defined in prior works (e.g., see Ahuja and Lopez-Paz [1] for work on linear regression tasks and Ramesh et al. [2] for group arithmetic tasks). Regarding grokking, there are several works exploring the phase transition-y nature of this task. For example, see Kumar et al. [3]. The transient nature of ICL also has negative results (see Reddy [4]), which are worth discussion since they are the primary conclusion in depth scaling as I see it."}
{"input": "The main selling point for me are the empirics though---I really like the results! The visualization of how the model represents concepts relevant to this paper's setup is quite beautiful: the circle of circles was fascinating to look at and, arguably, not something I expected. In retrospect, I can rationalize this as making sense---we get circular embeddings in grokking, so circle of circles is the logical geometrical extension here. Results on scaling are interesting in their own right as well."}
{"input": "Moreover, the proposed synthetic problem is both rich and elegant. I expect this framework will become a fruitful test-best for follow-up work studying emergence phenomena, helping the field to improve our empirical and theoretical understanding of these phenomena. Note: I think the contribution the authors have given in terms of the setting, the generalisation phenomena, and the partial skill decomposition and mechanistic analysis are already significant."}
{"input": "The paper studies the emergence of the in-context ability of the GPT-style transformer model trained using autoregressive loss and arithmetic modular datasets. It analyzes the influence of the number of tasks, number of in-context examples, model capacity, etc., on the ICL capability of an appropriately trained model (i.e., using early stopping). It also provides a persuasive “task decomposition hypothesis”, which is well supported by the ablation study and various experiments. The white-box analysis on the attention heads provides convincing evidence of the proposed explanation. Although there is a gap between the grokking settings (i.e., small model and toy dataset) and practical systems, the paper does a good job of explaining many important trends and concepts related to the emergence of compositional in-context ability. I enjoy reading this paper and suggest an acceptance."}
{"input": "Note: I think the contribution the authors have given in terms of the setting, the generalisation phenomena, and the partial skill decomposition and mechanistic analysis are already significant."}
{"input": "To be honest this part of the title has puzzled me since I first looked at the paper. Even if my understanding above is wrong and the title has an accurate interpretation, that I have failed to notice it might be one data point suggesting that if you are going for a title that is both short and informative, this might not be the right choice."}
{"input": "1. Why is the title 'learning to grok'? Is this meant in the sense that the grokking of a modular addition task is occurring in-context? If so, this seems a little inaccurate, since the phenomenon analogous to 'grokking' seems to still be occurring during pre-training."}
{"input": "3. Relationship to prior work. The related work section does a good job of summarising the contributions of prior work in in-context linear regression and modular arithmetic in the context of transformer models. However, I feel that this section could be improved if the authors attempted to offer greater insight into the relationship between these prior works and the present work. For example, the authors have an opportunity here to informally describe the in-context linear regression and the modular addition problem settings that the newly proposed setting generalises."}
{"input": "Related Work. At this point, the topic this paper is focused on has a rather rich literature and I think a more detailed related work is warranted (perhaps in the appendix if space is an issue). For example, the results by Kirsch et al. (which is cited) are very similar to what authors show, especially results on scaling effects. The main different is width scaling in that paper and no geometric analysis, but nonetheless the relationship warranted more emphasis and discussion. Similarly, several recent works have explored OOD generalization of toy ICL tasks defined in prior works (e.g., see Ahuja and Lopez-Paz [1] for work on linear regression tasks and Ramesh et al. [2] for group arithmetic tasks). Regarding grokking, there are several works exploring the phase transition-y nature of this task. For example, see Kumar et al. [3]. The transient nature of ICL also has negative results (see Reddy [4]), which are worth discussion since they are the primary conclusion in depth scaling as I see it. The definition of task diversity is not well defined. Is the number of pretraining tasks truly indicative of task diversity? I think the paper could benefit from some justification of this assumption. Many works in the continual learning and meta learning literature suggest that training on multiple tasks at once leads to better generalization. Perhaps it is worth including brief discussion on the connections between this point and the model’s ability to generalize ood which is predominantly determined by the number of pre-training tasks. Since multiplication can be viewed as repeated addition, isn’t skill 2 an extension of skill 3 (or can even be viewed as skill 3 composed with itself multiple times)? Is hierarchy of skills important here? I am left wondering if this batch construction methodology, as a further departure from the standard language modelling setting, has any other implications for the learning process that may affect the generality of the results. However, I feel that this section could be improved if the authors attempted to offer greater insight into the relationship between these prior works and the present work. For example, the authors have an opportunity here to informally describe the in-context linear regression and the modular addition problem settings that the newly proposed setting generalises. 2. In the figure 1 caption, is it possible to offer a clearer summary of the difference between in-distribution generalisation and out-of-distribution memorisation? On my first read through, treating the figure and caption as an overview of the work's main results, I had trouble distinguishing these two concepts. The skill decomposition discussed in section 5 is great. The clear pattern in attention heads verifies it very well. (The hypotheses could be further verified if the author can link the values of $c_1, c_2$ to some weights in the network, see the question part.) The emergent ability (or grokking) usually refers to a phenomenon in the model “got stuck” in a non-generalization region and suddenly gained the generalization ability. Hence some discussion about the learning dynamics, i.e., how the accuracy, loss, representation, ability, attention pattern, etc., gradually evolve during training would make the paper stronger. The task and batch sample selection in this paper have many constraints (e.g., the rectangular rule, the balanced number of samples in each batch, etc.). However, the practical systems usually cannot strictly satisfy all these assumptions. Hence a more detailed analysis of how these assumptions influence the generalization ability would provide more insights to practical systems. The paper claims in line 147 that “As the o.o.d. performance increases, the pre-training performance simultaneously degrades “. However, it is hard to read this information from Figure 3-a panel 1. Maybe a different color mapping or adding numbers on these patches would be helpful."}
{"input": "PCA variance. Given this is a rather rich geometry in 2-D, I'm slightly surprised to see PCA captured it. Did you have to do some preprocessing? How much variance is explained by the two projected components? If there are other components that are not shown but have a large variance, what do those components encode---can you try 3D plots? 1. Why is the title 'learning to grok'? Is this meant in the sense that the grokking of a modular addition task is occurring in-context? If so, this seems a little inaccurate, since the phenomenon analogous to 'grokking' seems to still be occurring during pre-training. Equation 2 is a bit hard to understand. How does it correlate to $z = ax+by$ ? (Although, from the latter explanations, I know the model relies on $c_1z_1^t + c_2z_2^t$ to get $z$, but it might be helpful to claim how it is derived.) - Better to define $GF(p)$, i.e., the Galois field, before using it. The OOD settings studied in grokking or emergent ability setting are quite related to the compositional generalization and systematic generalization. It would be helpful to discuss them in the related works, here are some of them: [1] Schott, Lukas, et al. "Visual representation learning does not generalize strongly within the same domain." ICLR 2022 [2] Xu, Zhenlin, Marc Niethammer, and Colin A. Raffel. "Compositional generalization in unsupervised compositional representation learning: A study on disentanglement and emergent language." NeurIPS 2022 [3] Ren, Yi, et al. "Improving compositional generalization using iterated learning and simplicial embeddings." NeurIPS 2023"}
{"input": "What does the MLP do? Given the mechinterp focused on attention solely, it is unclear what role MLPs played. Two experiments to try here are: (i) train attention only models to see if MLPs are even necessary, and (ii) perform the PCA analysis to uncover representations' geometry at the level of attentions and MLPs at each block in the model. Experiment (i) may require retraining models, so I understand if the authors are unable to conduct it, but my expectation will be that you will see that model "internalizes" task vectors and records them in MLPs. Attention only models can solve the task, but I expect the representations' geometry will be quite different. For experiment (ii) however, I expect that's easy to run and is merely repeating the plotting script on intermediate representations as a forward pass occurs through the model. If the geometry is primarily formed at attention layers, we'll see that in this experiment. vice versa, if it forms via MLPs, we'll see it explicitly. As also mentioned in the strength part, is it possible to find some specific value in the weight space (e.g., attention weights, readout layers, etc.) that is highly correlated to $c_1, c_2$? If so, the hypothesis that the model first learns skill 2 (scale each example) and then skill 3 (weighted combine different examples) would be further verified."}
{"input": "Are the results in Figure 6 coming from $d=2$ or $d=4$? I can find the figure for all 8 attention heads for $d=2$ in the appendix, what about the $d=4$ case? It might be helpful to see if the pattern in later layers (i.e., attention focusing on different $z_i$) exists in shallow layers, and vice versa."}
{"input": "It seems that equation 2 should read $\ldots = (z_1^t, z_2^2) \mod p$ and the equation on line 203 should read $c_1x + c_2y \mod p$. That is, $x$ and $y$ should swap places with $z_1^t$ and $z_2^t$. Is this indeed a mistake, or am I missing something? In figure 6 (top row) there is a typo: "Qeury" on the vertical axis."}
{"input": "1. How do you confirm the prompt is reliable and the output of the GenAI is following the rules? While the author presents a comprehensive theoretical framework and provides clear and detailed insights into the prompt strategies, there is room for improvement in the experimental validation of the proposed techniques. As outlined in the paper, the experiments are limited to a single product tested on a sample of 236 participants. Given the potential applicability of this technique to a wide range of products, the scope of experimentation appears somewhat narrow. Expanding the experiment set to encompass a more diverse array of products would strengthen the paper's claims. Additionally, the manual design of prompts by the author may not be a scalable solution when considering the need for ad design across a vast array of products. Further exploration of automated or semi-automated prompt generation methods could enhance the paper's practicality and applicability in real-world scenarios. 1. Have you explored the possibility of automating the prompt generation process for various products? 2. Could you provide insights into any supplementary experiments conducted to further validate the effectiveness of the prompt strategy outlined in your paper? Also, the paper does not explain how the envisioned framework will be applied in practice. The paper’s evaluation defines a set of prompts that are very specific to the products that are studied and generates creatives that are then subsequently used to compare the user perceptions vs. ad creatives that simply show the product with a white background. Overall, it’s unclear on whether the envisioned framework can be applied in practice without great input and effort from experts that will guide the generation of the ad creatives. In addition, there is a disconnection between the motivation of the work and the framework/evaluation. The framework does not account for user personalization, which is an important aspect when considering the ad ecosystem. So I am wondering how the paper is planning to incorporate user personalization in this framework and how Generative AI models can assist in this, especially when considering the privacy concerns that may arise from sharing user-specific data with companies that offer LLM solutions (e.g., OpenAI). 1. How did you recruit participants, and why most of them are from India? How can the recruitment approach affect the presented results? 2. How are the two products selected? Are these products popular in India, where most participants are from? 3. How is this study different from previous efforts studying the use of neuromarketing methods vs. plain advertisements like the ones shown to the participants (plain background with the product in the middle)? Is the novelty of the work the use of ChatGPT to generate the ad creatives?"}
{"input": "Additionally, the paper fails to explain how this study is different from previous efforts that aim to understand the use of neuromarketing methods without the use of Generative AI to create the ad creatives. The presented framework can also be applied by people to generate ad creatives, so its unclear if the novelty of this work lies in the formulation/use of the framework or the combination of the framework with Generative AI models like ChatGPT. I suggest to the authors to better contextualize their work and better explain the novelty of this work. 3. The method mentioned in the paper, which utilizes ChatGTP to generate accurate prompts and generates high-quality digital advertisements using this prompts and a large text-to-image model, has been widely applied in the engineering field and therefore lacks innovation."}
{"input": "Thanks for your interests in ICLR! Overall, this is an interesting paper on a topic which is of interest to ICLR Conference. It offers valuable insights into the application of neuroscientifically designed content to enhance ad click-through rates. The paper astutely recognizes the promise of leveraging Generative AI for this purpose. Building upon this foundation, the author thoughtfully presents four distinct prompt strategies and supports them with well-structured experiments, thus substantiating the validity of their approach. The paper does a good job of motivating and explaining the problem, as well as providing all the necessary details and motivation to understand the necessary background. Also, the paper focuses on an interesting aspect of generative AI and how it can be used to generate ad creatives with the goal of increasing click-through rates that, over the years, have been declining. Overall, I think that this work has the potential to inform various interested stakeholders, including advertisers, policymakers, and social media operators. Also, I like the paper’s approach that aims to leverage the power of Generative AI (particularly ChatGPT) to generate content based on principles obtained from the neuroscience field."}
{"input": "1. This paper introduces a communication protocol to explain how the brain has been triggered, the entire process is fluent and reasonable. 2. Consider not using pie charts for the evaluation results, given that it is one of the worst visualization methods."}
{"input": "3. The number of samples in the experiment is too small, and the experiments should cover more scenarios. 2. Since it's an online experiment, why not invite more people? While the author presents a comprehensive theoretical framework and provides clear and detailed insights into the prompt strategies, there is room for improvement in the experimental validation of the proposed techniques. As outlined in the paper, the experiments are limited to a single product tested on a sample of 236 participants. Given the potential applicability of this technique to a wide range of products, the scope of experimentation appears somewhat narrow. Expanding the experiment set to encompass a more diverse array of products would strengthen the paper's claims. 2. Could you provide insights into any supplementary experiments conducted to further validate the effectiveness of the prompt strategy outlined in your paper?"}
{"input": "To summarize, I believe that this work is interesting and important, however, at this stage, I believe that the paper is not ready for publication. In addition to the above concerns, I would like to make the following suggestions to the authors (mainly minor issues): 1. There are a couple of references listed as Anonymous, when they are not Anonymous so I suggest fixing these issues."}
{"input": "1. This work is too simple, just using the existing GenAI to produce the context and comparing it with the corresponding items. 2. The prompt is hand-crafted, and cannot be applied flexibly."}
{"input": "1. The theme of this paper may not be closely related to the conference, as it is only an engineering specification and lacks theoretical explanation."}
{"input": "I have not found any strengths of this paper."}
{"input": "2. The presentation of the paper is chaotic, making it difficult to read."}
{"input": "Additionally, the paper fails to explain how this study is different from previous efforts that aim to understand the use of neuromarketing methods without the use of Generative AI to create the ad creatives. The presented framework can also be applied by people to generate ad creatives, so its unclear if the novelty of this work lies in the formulation/use of the framework or the combination of the framework with Generative AI models like ChatGPT. I suggest to the authors to better contextualize their work and better explain the novelty of this work."}
{"input": "In addition, there is a disconnection between the motivation of the work and the framework/evaluation. The framework does not account for user personalization, which is an important aspect when considering the ad ecosystem. So I am wondering how the paper is planning to incorporate user personalization in this framework and how Generative AI models can assist in this, especially when considering the privacy concerns that may arise from sharing user-specific data with companies that offer LLM solutions (e.g., OpenAI)."}
{"input": "2. Consider not using pie charts for the evaluation results, given that it is one of the worst visualization methods."}
{"input": "My main concerns with the paper are related to the framework’s evaluation. I believe that the evaluation is quite limited and simplistic, given that the sample of the recruited participants is biased (the overwhelming majority being from India) and the evaluation focuses on only two products. I suggest that the authors explain and motivate how they perform the user recruitment procedure and the reason why they selected the two products. Overall, given these limitations, it’s unclear whether the paper’s results are generalizable. 1. How did you recruit participants, and why most of them are from India? How can the recruitment approach affect the presented results?"}
{"input": "3. How is this study different from previous efforts studying the use of neuromarketing methods vs. plain advertisements like the ones shown to the participants (plain background with the product in the middle)? Is the novelty of the work the use of ChatGPT to generate the ad creatives?"}
{"input": "3. The number of samples in the experiment is too small, and the experiments should cover more scenarios. 2. Since it's an online experiment, why not invite more people? While the author presents a comprehensive theoretical framework and provides clear and detailed insights into the prompt strategies, there is room for improvement in the experimental validation of the proposed techniques. As outlined in the paper, the experiments are limited to a single product tested on a sample of 236 participants. Given the potential applicability of this technique to a wide range of products, the scope of experimentation appears somewhat narrow. Expanding the experiment set to encompass a more diverse array of products would strengthen the paper's claims. Additionally, the manual design of prompts by the author may not be a scalable solution when considering the need for ad design across a vast array of products. Further exploration of automated or semi-automated prompt generation methods could enhance the paper's practicality and applicability in real-world scenarios. 1. Have you explored the possibility of automating the prompt generation process for various products? Additionally, the paper fails to explain how this study is different from previous efforts that aim to understand the use of neuromarketing methods without the use of Generative AI to create the ad creatives. The presented framework can also be applied by people to generate ad creatives, so its unclear if the novelty of this work lies in the formulation/use of the framework or the combination of the framework with Generative AI models like ChatGPT. I suggest to the authors to better contextualize their work and better explain the novelty of this work. In addition, there is a disconnection between the motivation of the work and the framework/evaluation. The framework does not account for user personalization, which is an important aspect when considering the ad ecosystem. So I am wondering how the paper is planning to incorporate user personalization in this framework and how Generative AI models can assist in this, especially when considering the privacy concerns that may arise from sharing user-specific data with companies that offer LLM solutions (e.g., OpenAI). 2. Consider not using pie charts for the evaluation results, given that it is one of the worst visualization methods."}
{"input": "1. How do you confirm the prompt is reliable and the output of the GenAI is following the rules? 1. How did you recruit participants, and why most of them are from India? How can the recruitment approach affect the presented results? 2. How are the two products selected? Are these products popular in India, where most participants are from? 3. How is this study different from previous efforts studying the use of neuromarketing methods vs. plain advertisements like the ones shown to the participants (plain background with the product in the middle)? Is the novelty of the work the use of ChatGPT to generate the ad creatives?"}
{"input": "2. Could you provide insights into any supplementary experiments conducted to further validate the effectiveness of the prompt strategy outlined in your paper?"}
{"input": "Sec 4.3 para 1: "the changes to which the algorithm needs to remain invariant..." - p6, Stage 2: - "binary groups": Is there an advantage to using just 2 groups? I would have expected more. Variance is really just a simple difference unless there are more than two groups - "narrows the performance gap between the two groups": Relative to PPO, yes, but it still stays flat or increases over time. Why is that? p9, ablation study: "it enables the model to explore a larger action space on challenging samples, leading to the discovery of a superior policy". How does it enable that? p9, reward distribution: "which will aid in its generalization to unseen data." Why will it do that? Did you show this somewhere? 3. Good experimental results. The method demonstrates good experimental results, outperforming baselines on in-distribution data. This highlights its capabilities in known domains. On out-of-distribution data, it shows even greater performance gains, underscoring its generalization abilities. Cons: 1. The experiments seem to imply a static number of groups, binary in the experiments,(best and challenging groups) which is not ideal with multi-modal data in dynamics environments. Experimenting with varying number of groups would be better to see how well does the method perform when the number of data groups increases? 2. The approach relies on accurate group label inference. Incorrect initial grouping could potentially lead to improper KL penalties. How robust is the method to errors in initial group assignment? 3. question: how do the newly introduced hyperparameters add complexity to tuning the algorithm? as training PPO is already known to be hard to tune. The paper is well written and clear - The method is generally sound and intuitive - Promising experiment results 2- The adversarial objective may pickup noisy or outliers and divert the optimization. I think this deserves more investigations. 3- The learning objective is encouraging reduction of loss variance across groups, I was wondering why authors didn’t directly go for optimization macro average of loss which is a bit more intuitive and has less chance to have side-effects on the overall performance as mean is generally more stable than variance? 3. The first term of the final learning objective in Equation (8) does not appear to be directly related to group $g$, raising questions about its role in achieving invariant learning. 4. The paper would benefit from the inclusion of code or detailed pseudocode to clearly convey the training process, as the current description does not sufficiently outline the methodology. 5. The connection between group invariant learning and its ability to address issues of shortcut exploitation and neglect of challenging samples is not clearly articulated. More intuitive explanations or empirical evidence would strengthen the understanding of this relationship."}
{"input": "p7, Table 1: It would be helpful to know how statistically significant these results are. Is there some way to do that? It would also be helpful to add a calibration where a method is evaluated against itself. It should be a 100% tie, but obviously it will be more like 33/34/33. 3. Good experimental results. The method demonstrates good experimental results, outperforming baselines on in-distribution data. This highlights its capabilities in known domains. On out-of-distribution data, it shows even greater performance gains, underscoring its generalization abilities. The paper is well written and clear - The method is generally sound and intuitive - Promising experiment results 1- It is unclear how to find the optimal number of groups? The paper seems to miss discussion of this and sharing what settings used in the presented results, and how authors reached that setting."}
{"input": "3. Good experimental results. The method demonstrates good experimental results, outperforming baselines on in-distribution data. This highlights its capabilities in known domains. On out-of-distribution data, it shows even greater performance gains, underscoring its generalization abilities. Cons: 1. The experiments seem to imply a static number of groups, binary in the experiments,(best and challenging groups) which is not ideal with multi-modal data in dynamics environments. Experimenting with varying number of groups would be better to see how well does the method perform when the number of data groups increases? The paper is well written and clear - The method is generally sound and intuitive - Promising experiment results"}
{"input": "I don't have any major questions. The authors can feel free to respond to my questions in the previous section if they have time. 1. Organization and Accessibility: The paper is well-structured and the content is presented in a manner that is accessible to readers."}
{"input": "1. The explanation in Section 4.4 regarding the probability of assigning samples to the highest-performing group lacks clarity and warrants further detail."}
{"input": "4- Robust optimization has been extensively studied in the general optimization context. Many of such methods could be applicable to the RLHF/LLM problem and the method proposed in this paper is also also applicable to other settings. I do not see any comparisons to support this method is optimal for RLHF compared to existing robust optimization methods. 5- Related to the previous two comments, we we should have compared with other robust optimization methods and variations of applying the group loss 6- In Section 3, “The training of an AI assistant consists of three main stages...“ is not necessarily the case for all AI Assistants. I suggest revising this statement and connecting the presented method to more broader usecases as it is not really limited to this case."}
{"input": "Figure 1 is an excellent visualization of the problem and the proposed solution."}
{"input": "2. The term $R_{g}(\theta)$ is not clearly defined within the paper. It’s assumed to represent the expected return of group $g$, yet its relationship to the last term in Equation 6 is ambiguous and needs clarification."}
{"input": "The paper is well motivated, proposes a novel idea, describes it clearly, and presents compelling evidence that it works. In short, this is an excellent paper!"}
{"input": "p9, reward distribution: "which will aid in its generalization to unseen data." Why will it do that? Did you show this somewhere?"}
{"input": "3. Innovative Concept: The application of group-invariant learning to the alignment problem is both a novel and a promising idea."}
{"input": "2. Significance of Addressed Problems: The authors tackle critical issues within RLHF, such as reward hacking and the overlooking of complex samples, which are pertinent for the advancement of universal AI assistants."}
{"input": "p7, Table 1: It would be helpful to know how statistically significant these results are. Is there some way to do that? It would also be helpful to add a calibration where a method is evaluated against itself. It should be a 100% tie, but obviously it will be more like 33/34/33."}
{"input": "p7, Table 1: It would be helpful to know how statistically significant these results are. Is there some way to do that? It would also be helpful to add a calibration where a method is evaluated against itself. It should be a 100% tie, but obviously it will be more like 33/34/33. Cons: 1. The experiments seem to imply a static number of groups, binary in the experiments,(best and challenging groups) which is not ideal with multi-modal data in dynamics environments. Experimenting with varying number of groups would be better to see how well does the method perform when the number of data groups increases? 3- The learning objective is encouraging reduction of loss variance across groups, I was wondering why authors didn’t directly go for optimization macro average of loss which is a bit more intuitive and has less chance to have side-effects on the overall performance as mean is generally more stable than variance? 5- Related to the previous two comments, we we should have compared with other robust optimization methods and variations of applying the group loss 6- In Section 3, “The training of an AI assistant consists of three main stages...“ is not necessarily the case for all AI Assistants. I suggest revising this statement and connecting the presented method to more broader usecases as it is not really limited to this case. 4. The paper would benefit from the inclusion of code or detailed pseudocode to clearly convey the training process, as the current description does not sufficiently outline the methodology. 5. The connection between group invariant learning and its ability to address issues of shortcut exploitation and neglect of challenging samples is not clearly articulated. More intuitive explanations or empirical evidence would strengthen the understanding of this relationship."}
{"input": "Sec 4.3 para 1: "the changes to which the algorithm needs to remain invariant..." - p6, Stage 2: - "binary groups": Is there an advantage to using just 2 groups? I would have expected more. Variance is really just a simple difference unless there are more than two groups - "narrows the performance gap between the two groups": Relative to PPO, yes, but it still stays flat or increases over time. Why is that? p9, ablation study: "it enables the model to explore a larger action space on challenging samples, leading to the discovery of a superior policy". How does it enable that? p9, reward distribution: "which will aid in its generalization to unseen data." Why will it do that? Did you show this somewhere? 3. question: how do the newly introduced hyperparameters add complexity to tuning the algorithm? as training PPO is already known to be hard to tune. 1. The explanation in Section 4.4 regarding the probability of assigning samples to the highest-performing group lacks clarity and warrants further detail."}
{"input": "2. The approach relies on accurate group label inference. Incorrect initial grouping could potentially lead to improper KL penalties. How robust is the method to errors in initial group assignment? 2- The adversarial objective may pickup noisy or outliers and divert the optimization. I think this deserves more investigations."}
{"input": "Minor: - "hippocampus" is inconsistently upper-case/lower-case. I think it should be lower-case. p. 3: Eq. 1 should have brackets around the exponentiated term. p. 5: "dominate" -> "dominant" [also p. 8] - p. 6: "an increase the distance" -> "an increase in the distance" - p. 6: "severally" -> "severely" - p. 9: "environmnet" -> "environment" Please always explicitly describe the variables used in equations (also in figure captions)."}
{"input": "The paper is clearly written. The paper is well written, with clear pointers to mathematical details where appropriate."}
{"input": "The predictions depend on noise level. Is this something that can be tested experimentally using firing rate variability? “We postulate that selective inhibition along the hippocampus will lead to different types of memory impairment for spatial tasks”. How would be possible to test this hypothesis experimentally? The authors mention “performing confusion experiments” but it would be interesting to better discuss how such experiments would look like."}
{"input": "Please always explicitly describe the variables used in equations (also in figure captions)."}
{"input": "The tradeoff between spatial specificity and context separability is novel and sounding."}
{"input": "I think that this work is a nice example of a theoretical contribution in the field of computational neuroscience. I am not familiar enough with the related literature to evaluate the originality of the approach, but from my understanding the analyses are well-conducted and well-motivated. The paper is written in a clear way. The main issue with this submission, according to my non-expert opinion, is that it might have a limited relevance (and impact) on the NeurIPS community. Indeed, although I am aware that NeurIPS welcomes contributions more focused on neuroscientific aspects of neuronal computation, there is not a single NeurIPS paper cited in the literature, suggesting that this type of work might be more appropriate for other venues."}
{"input": "“To determine whether two contexts manifolds are separable, we use a strict criterion: the two manifolds are separable if and only if they do not have any intersections.”. This requirement seems quite strong, because it assumes that we need to decode context from any position in the manifold… Could it be replaced by a smoother criterion and/or by some form of probabilistic (linear) discriminability?"}
{"input": "The analysis is rigorous."}
{"input": "The paper only addresses global remapping, and did not study the implications of proposed model in terms of partial or rate remapping."}
{"input": "Overall my critical comments are relatively minor (see below). I do have one major comment pertaining to the model's empirical predictions. The paper makes an interesting and testable prediction that the more widely tuned cells in the dorsal hippocampus are specialized for context discrimination, whereas the more narrowly tuned cells in the ventral hippocampus are specialized for fine-grained spatial discrimination. First, I want to point out that this is backwards: ventral cells have wider tuning than dorsal cells. The classic study of this is Kjelstrup et al. (2008, Science), not cited here (see also Komorowski et al., 2013, Journal of Neuroscience). Oddly, the authors cite two papers to support their claim about the dorsal-ventral axis (Lee et al., 2020. Tanni et al., 2022), neither of which actually support this claim. The Lee et al. paper only measures activity in dorsal cells, and it's not clear which subregion was measured in the Tanni paper. Can the authors do a better job relating their work to existing literature (see Weaknesses)? I understand that due to space limitations it is unlikely that they will be able to comprehensively address this literature, but I want to make sure that the work is at least largely in alignment with what is known."}
{"input": "The predictions depend on noise level. Is this something that can be tested experimentally using firing rate variability? “We postulate that selective inhibition along the hippocampus will lead to different types of memory impairment for spatial tasks”. How would be possible to test this hypothesis experimentally? The authors mention “performing confusion experiments” but it would be interesting to better discuss how such experiments would look like."}
{"input": "The authors propose selective inhibition experiments to test these predictions. In fact, such experiments have been done, and unfortunately they don't consistently support the predictions (none of the studies mentioned below are cited in the paper). The model would make more sense in light of at least some of these studies if the dorsal/ventral division of labor was reversed from what the authors proposed, consistent with the electrophysiology data. The review by Fanselow & Dong (2010, Neuron) provides a more systematic discussion of studies dissociating dorsal and ventral subregions. Can the authors do a better job relating their work to existing literature (see Weaknesses)? I understand that due to space limitations it is unlikely that they will be able to comprehensively address this literature, but I want to make sure that the work is at least largely in alignment with what is known. Please always explicitly describe the variables used in equations (also in figure captions). “To determine whether two contexts manifolds are separable, we use a strict criterion: the two manifolds are separable if and only if they do not have any intersections.”. This requirement seems quite strong, because it assumes that we need to decode context from any position in the manifold… Could it be replaced by a smoother criterion and/or by some form of probabilistic (linear) discriminability? Please always explicitly state where the information can be found in the Supplemental material."}
{"input": "Minor: - "hippocampus" is inconsistently upper-case/lower-case. I think it should be lower-case. p. 3: Eq. 1 should have brackets around the exponentiated term. p. 5: "dominate" -> "dominant" [also p. 8] - p. 6: "an increase the distance" -> "an increase in the distance" - p. 6: "severally" -> "severely" - p. 9: "environmnet" -> "environment""}
{"input": "+ Rejection for fairness does give additional options for achieving fairness though this too comes with a weakness below. Unrelated, what is "ordering defined over the input space X" and why is it necessary? A discussion on when to use the proposed framework in contrast to other frameworks is absent (see also my questions below). FairDP-SGD" What is FairDP-SGD ? It doesn't seem to have been defined anywhere ? Is it an existing work ? Where is FAIR-PATE's result on CheXpert ? In addition to this, please also also address __W1,W2,W3,W4__."}
{"input": "+ The points of intervention discussions give a nice overview of the PATE approach and ways in which additional mechanisms can be independently injected. Note, however, the independent intervention assumption is a weakness below. Handling rejection in experiments will also need to be done but unsure what the best approach there would be. Perhaps a random human decision maker? Suggestion: either integrate suggestion regarding accounting for rejection above, or incorporate some form of rejection (or simulate it) in the existing methods being compared to. It may be that the best methodology is not FairPATE but some existing baselines if adjusted to include fairness rejection option. Question A: Is reasonable to ignore downstream decisions from queries rejected due to fairness (i.e. contrary to my suggestion in the weaknesses above)? Question B: C1 makes a point that adding privacy after fairness may break fairness. What about in expectation? Were one to view the demographic statistics defining fairness measures in expectations, wouldn't they remain fair? How does the framework work in case of some distribution shift? This is especially important in the context of my question above. Fig. 2 and 3 use orange colors for two different algorithms (Tran et al (Fig 2) and Jagielski et al. (Fig 3)). The authors should report all algorithms in all figures or justify their absence. 3. __W3__ Abstaining from prediction for fairness reasons The introduction justifies this as _"if a decision cannot be made without violating a pre-specified fairness metric, then the model can refuse to answer at which point the decision can be relegated to a human judge"_. If I have understood this correctly, this is a flawed argument. For example, consider the situation where there are group of data points from the majority group, on which if the algorithm predicts correctly the fairness will be violated and hence the algorithm abstains. Nevertheless, the prediction is easy for this group and the judge nevertheless predicts on them correctly and the overall fairness is still violated. Isn't this pointless then to relegate this to the judge ? Intuitively, it appears that any problem can be made fair this way by simply refusing to predict on certain majority groups thereby artificially boosting the fairness of the algorithm. The correct fairness solution should aim to improve performance on the minority group instead. 4. __W4__ Unfair Comparisons Fair-PATE and Loewy et. al's algorithm do not work under the same restrictions of differential privacy. Specifically, Fair-PATE uses public unlabelled data and Loewy et. al doesn't and we know that (even a small amount of) public data can severely help in improving accuracy of DP models (perhaps fairness too). Hence, it appears to me that Loewy et. al. uses a significantly stricter notion of privacy and achieves nearly comparable result and in some cases even better (for tabular datasets, there are no comparison on vision datasets with Loewy et. al.). Motivation I understand that there are several works showing that privacy incurs a sacrifice in fairness. However, one possible approach to this problem is simply improving the accuracy and perhaps the resultant accuracy also leads to high fairness. In fact Berrada et. al. (2023) makes this argument with some experiments in their paper. I did not see an argument in this paper why this approach is not expected to solve all problems. Why should there be a trade-off between privacy and fairness ? Is it known in the literature and are there theoretical studies arguing about the necessity of a trade-off ? Missing baselines Why is there no comparison with the algorithm of Zhang et. al. (2021), Kulynuych et. al. (2022) and Berrada et. al. (2023) ? They are mentioned in one or two sentences at the very end of the paper but without any comprehensive empirical comparison."}
{"input": "Suggestion: either integrate suggestion regarding accounting for rejection above, or incorporate some form of rejection (or simulate it) in the existing methods being compared to. It may be that the best methodology is not FairPATE but some existing baselines if adjusted to include fairness rejection option. Fig. 2 and 3 use orange colors for two different algorithms (Tran et al (Fig 2) and Jagielski et al. (Fig 3)). The authors should report all algorithms in all figures or justify their absence. Why Tran et and Jagielski et al. are not reported for the UTK-dataset experiment? Motivation I understand that there are several works showing that privacy incurs a sacrifice in fairness. However, one possible approach to this problem is simply improving the accuracy and perhaps the resultant accuracy also leads to high fairness. In fact Berrada et. al. (2023) makes this argument with some experiments in their paper. I did not see an argument in this paper why this approach is not expected to solve all problems. Why should there be a trade-off between privacy and fairness ? Is it known in the literature and are there theoretical studies arguing about the necessity of a trade-off ? Missing baselines Why is there no comparison with the algorithm of Zhang et. al. (2021), Kulynuych et. al. (2022) and Berrada et. al. (2023) ? They are mentioned in one or two sentences at the very end of the paper but without any comprehensive empirical comparison."}
{"input": "Suggestion: Include more experimental samples in the results to make sure the statistical validity of any improvement claims is good. This may require larger datasets. Related, the experiments show error bars but how they are derived is not explained. I can't judge what is the impact of IPP (the rejection step added at inference time) on the overall results. Can you provide some ablation study showing how the framework performs on various datasets with different majority/minority distributions with and without IPP? For the other datasets reported in the appendix the trends shown reverts, e.g., DP-Fermi produces better tradeoffs than FairPATE. Can you discuss why? What feature of the dataset makes this possible? 3. __W3__ Abstaining from prediction for fairness reasons The introduction justifies this as _"if a decision cannot be made without violating a pre-specified fairness metric, then the model can refuse to answer at which point the decision can be relegated to a human judge"_. If I have understood this correctly, this is a flawed argument. For example, consider the situation where there are group of data points from the majority group, on which if the algorithm predicts correctly the fairness will be violated and hence the algorithm abstains. Nevertheless, the prediction is easy for this group and the judge nevertheless predicts on them correctly and the overall fairness is still violated. Isn't this pointless then to relegate this to the judge ? Intuitively, it appears that any problem can be made fair this way by simply refusing to predict on certain majority groups thereby artificially boosting the fairness of the algorithm. The correct fairness solution should aim to improve performance on the minority group instead. 4. __W4__ Unfair Comparisons Fair-PATE and Loewy et. al's algorithm do not work under the same restrictions of differential privacy. Specifically, Fair-PATE uses public unlabelled data and Loewy et. al doesn't and we know that (even a small amount of) public data can severely help in improving accuracy of DP models (perhaps fairness too). Hence, it appears to me that Loewy et. al. uses a significantly stricter notion of privacy and achieves nearly comparable result and in some cases even better (for tabular datasets, there are no comparison on vision datasets with Loewy et. al.)."}
{"input": "The proposed framework is a simple adaptation of the existing PATE. Simplicity is a plus in my book. I also found the main algorithmic idea of this paper quite nice. The idea of that the algorithm chooses which points to label not only on the bassi of the privacy constraint but also the fairness constraint is quite neat and could be useful in other contexts. I appreciated this."}
{"input": "+ Fairly well written and easy to follow. The paper is written quite clearly and is easily readable. The arguments of the authors come out clearly without ambiguity and the reader can easily follow the train-of-thought. I appreciated that very much."}
{"input": "Algorithm 1 and several points throughout the work hint at this. However, there is also the consideration of intervention points 1,2,3 which seem odd as they points seen before any individual for whom fairness is considered is seen. That is, fairness about public individuals cannot be made there, independent of any other issues such as privacy budgeting. Further, Theorem 1 discusses a demographic parity pre-processor which achieves demographic parity on private data which I presume is irrelevant. The statement "DP that only protects privacy of a given sensitive feature" might be mischaracterizing DP. It is not focused on features or even data but rather the impact of individuals on visible results. Question C: Theorem 1 makes a statement about a pre-processor inducing privacy parameter degradation but FairPATE (or PATE) appears to fit the definition of a pre-processor. If the point of the Theorem is to argue against pre-processors, isn't it also arguing against PATE/FairPATE? 2. __ W2__ Wrong Conclusion from Theorem 2 I did not go through the detail of Theorem 1 but I assume it is correct and follows from Group privacy arguments. However, the sentence above Theorem 1 (Point C2) claims that the result proves that " pre-processing ... will necesarrily degrade the guarantee of the .... private learning mechanism". How ever this is not what the Theorem shows. The theorem only proves a privacy guarantee of $M\odot P_{\text{pre}}$ not that this is the tightest privacy guarantee possible for the composed mechanism. Am I missing something ?"}
{"input": "Smaller things: - Rejection rate is not shown in any experiments. One could view a misclassification as a rejection, however. Please include rejection rates or view them as misclassifications in the results. 1. __W1__ Significance: The paper aims to convey the significance of its contribution through experimental results (and I don't think there is anything wrong with it and in general support extensive empirical results), but the experiments present a rather bleak picture of the advantages of FairPATE."}
{"input": "Handling rejection in experiments will also need to be done but unsure what the best approach there would be. Perhaps a random human decision maker? Suggestion: Include more experimental samples in the results to make sure the statistical validity of any improvement claims is good. This may require larger datasets. Related, the experiments show error bars but how they are derived is not explained. Smaller things: - Rejection rate is not shown in any experiments. One could view a misclassification as a rejection, however. Please include rejection rates or view them as misclassifications in the results. 1. __W1__ Significance: The paper aims to convey the significance of its contribution through experimental results (and I don't think there is anything wrong with it and in general support extensive empirical results), but the experiments present a rather bleak picture of the advantages of FairPATE."}
{"input": "The statement "PATE relies on unlabeled public data, which lacks the ground truth labels Y" is a bit confusing unless one has already understood that fairness is with respect to public data. PATE also relies on private labeled data to create the teachers."}
{"input": "FairDP-SGD" What is FairDP-SGD ? It doesn't seem to have been defined anywhere ? Is it an existing work ? Where is FAIR-PATE's result on CheXpert ? In addition to this, please also also address __W1,W2,W3,W4__."}
{"input": "The experimental analysis should be improved. Some figures are misleading, e.g., same colors used for different algorithms (see questions below)."}
{"input": "The Privacy Analysis paragraph could be greatly simplified to just the last sentence regarding post-processing."}
{"input": "Misleading regarding diversity of experiments In the introduction, the paper sells itself regarding performing a wide range of experiments but results on nearly half of these datasets are hidden away in the appendix without any mention in the main text and without comparisons. Experiments on CheXpert, CelebA, FairFace, and Retired-Adults are not available in the main text and it is very hard to interpret the results presented for this in the appendix. In fact, there are no results of FAIR-PATE on CheXpert in the paper Please show comparison on these for demographic parity and equalized odds (similar to Figure 2,3 with comparison to Loewe et. al.'s algorithm) on these four datasets. In addition, there are results in the literature that run DP algorithms on CelebA and CheXpert. Comparisons should be made to them to show what is the sacrifice being made compared to state-of-the-art."}
{"input": "Fairness is "enforced" by adding parity constraint within the aggregator which acts as a rejection sampling step on the basis of fairness. The general idea was proposed in several other works in the past, including the ones cited by the authors and against which the authors compare. It is thus difficult to understand what is new in the proposed framework. An explicit mention would help."}
{"input": "The paper tackles a highly relevant issue in ML, addressing both theoretical and practical implications of fairness and privacy."}
{"input": "Fairness is "enforced" by adding parity constraint within the aggregator which acts as a rejection sampling step on the basis of fairness. The general idea was proposed in several other works in the past, including the ones cited by the authors and against which the authors compare. It is thus difficult to understand what is new in the proposed framework. An explicit mention would help."}
{"input": "Paper [Learning with Impartiality to Walk on the Pareto Frontier of Fairness, Privacy, and Utility]( discusses a similar topic (although the contributions from this work are different) and it could be added to your Related work section. Minor comments: A lot of the cited papers have appeared in conferences. But the authors cite their arxiv version. I suggest to update the references accordingly."}
{"input": "Misleading regarding diversity of experiments In the introduction, the paper sells itself regarding performing a wide range of experiments but results on nearly half of these datasets are hidden away in the appendix without any mention in the main text and without comparisons. Experiments on CheXpert, CelebA, FairFace, and Retired-Adults are not available in the main text and it is very hard to interpret the results presented for this in the appendix. In fact, there are no results of FAIR-PATE on CheXpert in the paper Please show comparison on these for demographic parity and equalized odds (similar to Figure 2,3 with comparison to Loewe et. al.'s algorithm) on these four datasets. In addition, there are results in the literature that run DP algorithms on CelebA and CheXpert. Comparisons should be made to them to show what is the sacrifice being made compared to state-of-the-art."}
{"input": "Comparisons against methods in which rejection due to fairness is not an option may not be fair."}
{"input": "Misleading regarding diversity of experiments In the introduction, the paper sells itself regarding performing a wide range of experiments but results on nearly half of these datasets are hidden away in the appendix without any mention in the main text and without comparisons. Experiments on CheXpert, CelebA, FairFace, and Retired-Adults are not available in the main text and it is very hard to interpret the results presented for this in the appendix. In fact, there are no results of FAIR-PATE on CheXpert in the paper Please show comparison on these for demographic parity and equalized odds (similar to Figure 2,3 with comparison to Loewe et. al.'s algorithm) on these four datasets. In addition, there are results in the literature that run DP algorithms on CelebA and CheXpert. Comparisons should be made to them to show what is the sacrifice being made compared to state-of-the-art."}
{"input": "The statement "DP that only protects privacy of a given sensitive feature" might be mischaracterizing DP. It is not focused on features or even data but rather the impact of individuals on visible results."}
{"input": "Question B: C1 makes a point that adding privacy after fairness may break fairness. What about in expectation? Were one to view the demographic statistics defining fairness measures in expectations, wouldn't they remain fair? Unrelated, what is "ordering defined over the input space X" and why is it necessary? How does the framework work in case of some distribution shift? This is especially important in the context of my question above. For the other datasets reported in the appendix the trends shown reverts, e.g., DP-Fermi produces better tradeoffs than FairPATE. Can you discuss why? What feature of the dataset makes this possible? Why Tran et and Jagielski et al. are not reported for the UTK-dataset experiment? Motivation I understand that there are several works showing that privacy incurs a sacrifice in fairness. However, one possible approach to this problem is simply improving the accuracy and perhaps the resultant accuracy also leads to high fairness. In fact Berrada et. al. (2023) makes this argument with some experiments in their paper. I did not see an argument in this paper why this approach is not expected to solve all problems. Why should there be a trade-off between privacy and fairness ? Is it known in the literature and are there theoretical studies arguing about the necessity of a trade-off ? Missing baselines Why is there no comparison with the algorithm of Zhang et. al. (2021), Kulynuych et. al. (2022) and Berrada et. al. (2023) ? They are mentioned in one or two sentences at the very end of the paper but without any comprehensive empirical comparison. FairDP-SGD" What is FairDP-SGD ? It doesn't seem to have been defined anywhere ? Is it an existing work ? Where is FAIR-PATE's result on CheXpert ? In addition to this, please also also address __W1,W2,W3,W4__."}
{"input": "Handling rejection in experiments will also need to be done but unsure what the best approach there would be. Perhaps a random human decision maker? I can't judge what is the impact of IPP (the rejection step added at inference time) on the overall results. Can you provide some ablation study showing how the framework performs on various datasets with different majority/minority distributions with and without IPP?"}
{"input": "Suggestion: incorporate ultimate decisions, whether by model or human, into the rejection mechanism. i.e. update counts m(z, k) based on human decisions. Given that humans might put the group counts into already violating territory, it may be necessary to rewrite Line 7 of Algorithm 1 to check whether the fairness criterion is improving or not due to the decision and allow queries that improve statistics even though those statistics already violate γ threshold. The Privacy Analysis paragraph could be greatly simplified to just the last sentence regarding post-processing. Question A: Is reasonable to ignore downstream decisions from queries rejected due to fairness (i.e. contrary to my suggestion in the weaknesses above)? Fairness is "enforced" by adding parity constraint within the aggregator which acts as a rejection sampling step on the basis of fairness. The general idea was proposed in several other works in the past, including the ones cited by the authors and against which the authors compare. It is thus difficult to understand what is new in the proposed framework. An explicit mention would help. The experimental analysis should be improved. Some figures are misleading, e.g., same colors used for different algorithms (see questions below). Fig. 2 and 3 use orange colors for two different algorithms (Tran et al (Fig 2) and Jagielski et al. (Fig 3)). The authors should report all algorithms in all figures or justify their absence. Paper [Learning with Impartiality to Walk on the Pareto Frontier of Fairness, Privacy, and Utility]( discusses a similar topic (although the contributions from this work are different) and it could be added to your Related work section. Minor comments: A lot of the cited papers have appeared in conferences. But the authors cite their arxiv version. I suggest to update the references accordingly."}
{"input": "Question C: Theorem 1 makes a statement about a pre-processor inducing privacy parameter degradation but FairPATE (or PATE) appears to fit the definition of a pre-processor. If the point of the Theorem is to argue against pre-processors, isn't it also arguing against PATE/FairPATE?"}
{"input": "A main objective of the paper is to compute numerics comparing models, but only one training run was executed for each experiment. If the compute budget is very limited, a more valuable approach may be to consider a smaller subset of models (e.g. keeping either GPT-2 or Llama, but not both) and simpler task parameterizations. Doing so will enable you to sweep across many more settings and increase your experiment replications, generating more convincing numerics. How is the ICL regression score meant to be interpreted? It seems to depend strongly on the task, where some tasks will feature a score uniformly less than 1 across models, and greater than 1 for others. It is interesting to find that even a small change in architecture (e.g., adding RMS to GPT-2) will lead to noticeable differences on some tasks (e.g., sparse linear). It would be interesting to investigate the root reason behind that. Additionally, use cases for ICL itself are not well-motivated. Are there any practical use cases that warrant such extensive evaluation? The writing is not easy to understand for a reader not very up-to-date with the ICL literature. Why are the specific hybrid architectures chosen (GPT2-Llama and Llama-Mamba)? Is it only because no one has studied them before or are there other profound reasons why these could be of interest to study? Why are some hybrid architectures converging to suboptimal regression schemes versus others are not? Do you have some intuition behind this finding? Similarly, why do some models escape suboptimal regression schemes while others fail to converge entirely? In lines 244-246, the authors mention that “certain hybrid architecture variations may place inductive biases on certain solution forms, resulting in convergence times when these solution forms greatly vary from the optimal predictor’s form”. Can the authors give some examples of such inductive biases for some architectures studied in this work? Quality - In considering 9 different model configurations for 8 different tasks, the author thoroughly consider a large number (72) of different LLM building blocks and their ICL capabilities across tasks of varying complexities."}
{"input": "The results are sometimes difficult to interpret. At times, this is simply because the plots are unreadable, with text that is too small. I'm unsure how to interpret the in-context regression score. It looks like it's often <1 across models. Does this mean they all fail to outperform the baseline? Is this score comparable across different tasks? The intuitive explanation behind the ICL regression score values in Figure 2(a) are well-appreciated and helpful to follow along the results. The results are presented in a manner where the performance metrics are reported for the 12 architectures and 5 tasks but it is not very clear what the reader or the scientific community working on ICL should take away from the results. Are there patterns regarding why certain hybrid architecture + task combinations make ICL shine compared to the baselines and why some others do not? A lot of the interpretation of such results is left to the reader to figure out. A lack of a deeper understanding and intuition about the reasons behind the results makes it hard to see solid/impactful takeaways that others could build on top of. The authors mention and describe a 6th task Vector MQAR but do not report or discuss any of its results in detail in the main text of the paper. One figure is present in the Appendix but it is not explained and it is too hard to read the text in the figure. Line 202: “Sparse Linear ~~on~~ adopts a suboptimal..” - References to result tables (Table 3 for lines 213, 230) and model descriptions (Figure 1a for lines 194, 204, 201, 16 etc.) could further enhance readability and the user’s understanding. ### Clarity There is significant room to improve the clarity of the presented work. Currently, several important details are missing (discussed below), key concepts are not fully explained, and the current paper could benefit from an editorial pass (it is currently difficult to read). It is also important to note that, while the authors presented the results of model + task pairs and detailed where models failed, no possible explanations and follow up ablation studies were conducted. the question of "why" remains for all the presented results. E.g., > Specific hybrid architectures can hesitate to learn/converge for certain function classes."}
{"input": "A main objective of the paper is to compute numerics comparing models, but only one training run was executed for each experiment. If the compute budget is very limited, a more valuable approach may be to consider a smaller subset of models (e.g. keeping either GPT-2 or Llama, but not both) and simpler task parameterizations. Doing so will enable you to sweep across many more settings and increase your experiment replications, generating more convincing numerics. The paper is not very well-motivated. Why are hybrid architectures (especially the two that are focused on) important to study? What intuitions or profound reasons drive the authors to make the experimental design choices that they did? Do the authors have intuition or an explanation which can be explored to explain this? Arguably, this is one of the most important contributions to be made in a large empirical review like the presented paper, i.e., to make sense of the experiments to gain a greater intuition for why an LLM behaves in a certain way."}
{"input": "As mentioned above, the present work seems to be closely related to [Park et al.]( with little added insight. While an attempt has been made to explore minor architectural aspects like the choice of normalization, positional encodings, and activations, for the most part the changes seem to matter little (according to Table 3). I'm unsure what the key take-away is. Is there a particular architectural configuration that works best? What concrete practices can a user apply to improve their models' performance? The authors appear to have an ambition towards answering questions like these, but do not ultimately resolve them. [2] Park J, Park J, Xiong Z, Lee N, Cho J, Oymak S, Lee K, Papailiopoulos D. Can mamba learn how to learn? a comparative study on in-context learning tasks. arXiv preprint arXiv:2402.04248. 2024. Could you briefly describe what kind of structural changes were necessary to the codebase of Garg et al.? A general understanding will make it more clear why the codebase itself is a major contribution as listed currently in the draft. Why aren’t the baselines and detailed results for Vector MQAR discussed in the paper draft? Why does Mamba struggle with MQAR but does pretty well on all other 5 tasks? [2] Jongho Park, Jaeseung Park, Zheyang Xiong, Nayoung Lee, Jaewoong Cho, Samet Oymak, Kangwook Lee, and Dimitris Papailiopoulos. Can mamba learn how to learn? a comparative study on in-context learning tasks. arXiv preprint arXiv:2402.04248, 2024."}
{"input": "I feel that it is hard to assess the contribution of this work. It seems that this work's main contribution is the implementation of the in-context learning ability benchmark codebase. While such a codebase is important and useful, I did not find what technical challenges the codebase is trying to address and the effectiveness of the codebase. Significance - The framework of [1] has grown as an interesting alternative to standard ICL, as a quick means to assess the ICL success and failure modes of different models. Thus, the presented work can aid in flushing out this information over new LLMs and previously proposed tasks."}
{"input": "On line 35, what is meant by “richly benchmarking”? Lines 60-61: What optimal train loss is being referred to here? There is no loss associated with ICL itself, right (as that only consists of in-context examples/demonstrations)? Does this loss refer to the ground truth functions being learned or the baselines? Line 80: which models are being referred to here? There is no “real training” during ICL itself. Are the functions/regression targets being referred to here or the baselines? The terminology used in the paper should be clarified. Wrt important missing information: - "We replicate the function classes Linear Regression, Sparse Linear Regression, 2-Layer MLP Regression, and Decision Tree Regression from Garg et al. [6] as they present a wide range of "difficulty" for sequence models. In addition, to capture the existence of some ICL ability, we also regress onto the two function classes examined in Park et al. [14]: parity function with induced sparsity (Sparse Parity) and parallel associative recall (Vector MQAR)." <- - How training instances are produced per task? How many test samples are produced per task? If this follows Garg et al., then each model is trained from scratch on 40 samples per task. Can you please clarify and state these in the main text? It is not clear what the author's mean by "zero estimator." Is this the zero shot prediction? Correspondingly, it is not clear exactly what the presented ICL regression score represents. "To determine task-specific ICL ability, our sequence models regress onto the functions shown above [14]." <- It would help to clearly state the paper trains the models "from scratch" to in-context learn, as in previous works."}
{"input": "Lines 60-61: What optimal train loss is being referred to here? There is no loss associated with ICL itself, right (as that only consists of in-context examples/demonstrations)? Does this loss refer to the ground truth functions being learned or the baselines? Line 80: which models are being referred to here? There is no “real training” during ICL itself. Are the functions/regression targets being referred to here or the baselines? The terminology used in the paper should be clarified. Wrt important missing information: - "We replicate the function classes Linear Regression, Sparse Linear Regression, 2-Layer MLP Regression, and Decision Tree Regression from Garg et al. [6] as they present a wide range of "difficulty" for sequence models. In addition, to capture the existence of some ICL ability, we also regress onto the two function classes examined in Park et al. [14]: parity function with induced sparsity (Sparse Parity) and parallel associative recall (Vector MQAR)." <- - How training instances are produced per task? How many test samples are produced per task? If this follows Garg et al., then each model is trained from scratch on 40 samples per task. Can you please clarify and state these in the main text? "To determine task-specific ICL ability, our sequence models regress onto the functions shown above [14]." <- It would help to clearly state the paper trains the models "from scratch" to in-context learn, as in previous works."}
{"input": "Additional minor formatting comments: - Line 25: consider compressing citations (e.g. with sort&compress) - Table 1: third hline from the top intersects with text - Consider plotting figures with point-markers for each data point, to clarify where exactly your data points fall - For related models that vary by parameter (e.g. training iteraitons), consider using the same color but with different shadings / style - Figure 15: text is unreadable - Figure text overall is small and hard to read The intuitive explanation behind the ICL regression score values in Figure 2(a) are well-appreciated and helpful to follow along the results. The authors mention and describe a 6th task Vector MQAR but do not report or discuss any of its results in detail in the main text of the paper. One figure is present in the Appendix but it is not explained and it is too hard to read the text in the figure. Some typos: - Line 161: Mention the word “Figure” before 2a. Line 164: “Figure 2b” instead of “Table 2b” - Line 185: Park et al. [14] show that .. Line 202: “Sparse Linear ~~on~~ adopts a suboptimal..” - References to result tables (Table 3 for lines 213, 230) and model descriptions (Figure 1a for lines 194, 204, 201, 16 etc.) could further enhance readability and the user’s understanding. Why are some values in the last column of Table 3 not filled in?"}
{"input": "The technical novelty of the work is limited. Originality - while the authors largely adapt the "trained to in-context learn" framework from other works, they consider new model configurations. This can be useful to determine what permutations of model components leads to failure modes in ICL."}
{"input": "The paper appears to be closely related to [Park et al.]( and I'm uncertain what additional scientific value it adds. As such, it's originality and significance appear to be limited. Originality - while the authors largely adapt the "trained to in-context learn" framework from other works, they consider new model configurations. This can be useful to determine what permutations of model components leads to failure modes in ICL."}
{"input": "Another contribution is the empirical findings of the relationship between architectures and per-task performance on in-context learning. However, I found the empirical results are not systematic and hard to interpret. I am unsure how these findings motivate future architecture design. As stated above, what's the main contribution/innovation of the codebase? What conclusion/takeaway/intuition can we learn from the empirical results? Why aren’t the baselines and detailed results for Vector MQAR discussed in the paper draft? Why does Mamba struggle with MQAR but does pretty well on all other 5 tasks?"}
{"input": "Significance - The framework of [1] has grown as an interesting alternative to standard ICL, as a quick means to assess the ICL success and failure modes of different models. Thus, the presented work can aid in flushing out this information over new LLMs and previously proposed tasks."}
{"input": "It is not clear what data the 12 hybrid architectures are trained on. Do the authors train them from scratch? If yes, what datasets are used? Else are pretrained weights used for the different blocks? Are the models finetuned? Something else? For Fig 6(a), could you try training Mamba checkpoints longer than 500k checkpoints to verify if it actually converges to the baseline (GPT-2’s performance)? Were these checkpoints trained by the authors themselves or used from a pretrained model? What data was used for training?"}
{"input": "Why do you pick the model subsets that you do in constructing each plot? For instance, plot 3b has a title "Decision Tree: All Architectures" but only plots GPT-2 and Mamba (what about Llama, and the other variations?) - Why apply Mamba only to Llama, and not to GPT-2? Since you mentioned compute was a bottleneck, why not keep either GPT-2 or Llama as a base model?"}
{"input": "Line 255 mentions that there is only a single run performed for each model-task pairs, yet many plots feature confidence intervals. Where do these confidence intervals come from? Why do you pick the model subsets that you do in constructing each plot? For instance, plot 3b has a title "Decision Tree: All Architectures" but only plots GPT-2 and Mamba (what about Llama, and the other variations?) - Why apply Mamba only to Llama, and not to GPT-2? Since you mentioned compute was a bottleneck, why not keep either GPT-2 or Llama as a base model?"}
{"input": "For Fig 6(a), could you try training Mamba checkpoints longer than 500k checkpoints to verify if it actually converges to the baseline (GPT-2’s performance)? Were these checkpoints trained by the authors themselves or used from a pretrained model? What data was used for training? What do the authors mean by “context length” in most of their figures? Does it refer to the number of ICL examples or the number of prompt tokens? Can the authors share a few examples of sample prompts that were used for ICL with varying context lengths? In Figure 6(b), what about GPT2-RMS might explain its inability to learn the decision tree function while all other hybrid architectures used do pretty well? Why is the RMS norm as the distinguishing factor for this hybrid architecture detrimental to the decision tree and sparse linear tasks but not others?"}
{"input": "Line 255 mentions that there is only a single run performed for each model-task pairs, yet many plots feature confidence intervals. Where do these confidence intervals come from? Why do you pick the model subsets that you do in constructing each plot? For instance, plot 3b has a title "Decision Tree: All Architectures" but only plots GPT-2 and Mamba (what about Llama, and the other variations?) - Why apply Mamba only to Llama, and not to GPT-2? Since you mentioned compute was a bottleneck, why not keep either GPT-2 or Llama as a base model? How is the ICL regression score meant to be interpreted? It seems to depend strongly on the task, where some tasks will feature a score uniformly less than 1 across models, and greater than 1 for others. It is interesting to find that even a small change in architecture (e.g., adding RMS to GPT-2) will lead to noticeable differences on some tasks (e.g., sparse linear). It would be interesting to investigate the root reason behind that. As stated above, what's the main contribution/innovation of the codebase? What conclusion/takeaway/intuition can we learn from the empirical results? The paper is not very well-motivated. Why are hybrid architectures (especially the two that are focused on) important to study? What intuitions or profound reasons drive the authors to make the experimental design choices that they did? The results are presented in a manner where the performance metrics are reported for the 12 architectures and 5 tasks but it is not very clear what the reader or the scientific community working on ICL should take away from the results. Are there patterns regarding why certain hybrid architecture + task combinations make ICL shine compared to the baselines and why some others do not? A lot of the interpretation of such results is left to the reader to figure out. A lack of a deeper understanding and intuition about the reasons behind the results makes it hard to see solid/impactful takeaways that others could build on top of. Why are the specific hybrid architectures chosen (GPT2-Llama and Llama-Mamba)? Is it only because no one has studied them before or are there other profound reasons why these could be of interest to study? On line 35, what is meant by “richly benchmarking”? Could you briefly describe what kind of structural changes were necessary to the codebase of Garg et al.? A general understanding will make it more clear why the codebase itself is a major contribution as listed currently in the draft. Why aren’t the baselines and detailed results for Vector MQAR discussed in the paper draft? Why does Mamba struggle with MQAR but does pretty well on all other 5 tasks? Why are some hybrid architectures converging to suboptimal regression schemes versus others are not? Do you have some intuition behind this finding? Similarly, why do some models escape suboptimal regression schemes while others fail to converge entirely? In Figure 6(b), what about GPT2-RMS might explain its inability to learn the decision tree function while all other hybrid architectures used do pretty well? Why is the RMS norm as the distinguishing factor for this hybrid architecture detrimental to the decision tree and sparse linear tasks but not others? In lines 244-246, the authors mention that “certain hybrid architecture variations may place inductive biases on certain solution forms, resulting in convergence times when these solution forms greatly vary from the optimal predictor’s form”. Can the authors give some examples of such inductive biases for some architectures studied in this work? Why are some values in the last column of Table 3 not filled in? Do the authors have intuition or an explanation which can be explored to explain this? Arguably, this is one of the most important contributions to be made in a large empirical review like the presented paper, i.e., to make sense of the experiments to gain a greater intuition for why an LLM behaves in a certain way."}
{"input": "A main objective of the paper is to compute numerics comparing models, but only one training run was executed for each experiment. If the compute budget is very limited, a more valuable approach may be to consider a smaller subset of models (e.g. keeping either GPT-2 or Llama, but not both) and simpler task parameterizations. Doing so will enable you to sweep across many more settings and increase your experiment replications, generating more convincing numerics. Additional minor formatting comments: - Line 25: consider compressing citations (e.g. with sort&compress) - Table 1: third hline from the top intersects with text - Consider plotting figures with point-markers for each data point, to clarify where exactly your data points fall - For related models that vary by parameter (e.g. training iteraitons), consider using the same color but with different shadings / style - Figure 15: text is unreadable - Figure text overall is small and hard to read The codebase for studying in-context learning ability could be useful to understand capabilities and limitations of hybrid models, which will accelerate research in this area. Additionally, use cases for ICL itself are not well-motivated. Are there any practical use cases that warrant such extensive evaluation? The writing is not easy to understand for a reader not very up-to-date with the ICL literature. Line 202: “Sparse Linear ~~on~~ adopts a suboptimal..” - References to result tables (Table 3 for lines 213, 230) and model descriptions (Figure 1a for lines 194, 204, 201, 16 etc.) could further enhance readability and the user’s understanding. ### Clarity There is significant room to improve the clarity of the presented work. Currently, several important details are missing (discussed below), key concepts are not fully explained, and the current paper could benefit from an editorial pass (it is currently difficult to read). It is also important to note that, while the authors presented the results of model + task pairs and detailed where models failed, no possible explanations and follow up ablation studies were conducted. the question of "why" remains for all the presented results. E.g., > Specific hybrid architectures can hesitate to learn/converge for certain function classes. Wrt important missing information: - "We replicate the function classes Linear Regression, Sparse Linear Regression, 2-Layer MLP Regression, and Decision Tree Regression from Garg et al. [6] as they present a wide range of "difficulty" for sequence models. In addition, to capture the existence of some ICL ability, we also regress onto the two function classes examined in Park et al. [14]: parity function with induced sparsity (Sparse Parity) and parallel associative recall (Vector MQAR)." <- - How training instances are produced per task? How many test samples are produced per task? If this follows Garg et al., then each model is trained from scratch on 40 samples per task. Can you please clarify and state these in the main text? "To determine task-specific ICL ability, our sequence models regress onto the functions shown above [14]." <- It would help to clearly state the paper trains the models "from scratch" to in-context learn, as in previous works."}
{"input": "The results are sometimes difficult to interpret. At times, this is simply because the plots are unreadable, with text that is too small. I'm unsure how to interpret the in-context regression score. It looks like it's often <1 across models. Does this mean they all fail to outperform the baseline? Is this score comparable across different tasks? What is the main motivation behind studying hybrid architectures for ICL? For a reader not well-versed with the ICL literature, it is not clear from the paper. Lines 60-61: What optimal train loss is being referred to here? There is no loss associated with ICL itself, right (as that only consists of in-context examples/demonstrations)? Does this loss refer to the ground truth functions being learned or the baselines? Line 80: which models are being referred to here? There is no “real training” during ICL itself. Are the functions/regression targets being referred to here or the baselines? The terminology used in the paper should be clarified. It is not clear what data the 12 hybrid architectures are trained on. Do the authors train them from scratch? If yes, what datasets are used? Else are pretrained weights used for the different blocks? Are the models finetuned? Something else? What do the authors mean by “context length” in most of their figures? Does it refer to the number of ICL examples or the number of prompt tokens? Can the authors share a few examples of sample prompts that were used for ICL with varying context lengths? It is not clear what the author's mean by "zero estimator." Is this the zero shot prediction? Correspondingly, it is not clear exactly what the presented ICL regression score represents."}
{"input": "For Fig 6(a), could you try training Mamba checkpoints longer than 500k checkpoints to verify if it actually converges to the baseline (GPT-2’s performance)? Were these checkpoints trained by the authors themselves or used from a pretrained model? What data was used for training?"}
{"input": "Some typos: - Line 161: Mention the word “Figure” before 2a. Line 164: “Figure 2b” instead of “Table 2b” - Line 185: Park et al. [14] show that .."}
{"input": "Are the limitations related to the cap of two-collider paths only? Are there any issues related to computational complexity? This was partially referenced in the attached form. What was the reason of choosing only three models for benchmarks? It is unclear the actual advantage of such approach w.r.t. the existing FCI algorithm. 1) What is the main Novelty of Theorem 1 compared to [14]? Can the theoretical motivation for the proposed search-and-score algorithm be deduced from the results in [14]? 2) Do you have convergence guarantees for the algorithm? In other words, how many edge orientations need to be probed before convergence in step 1? How many loops over nodes and edges are needed until convergence? 4) Is there any class of distributions or graphs for which algorithm is guaranteed to converge to a true hidden causal graph? 5) Did you perform any experiment to understand how sensitive is the proposed algorithm to the noise? 6) In the first step of the algorithm does it matter in which order vertices are processed? do you select any specific order? 2. I am curious about how to find ac-connected subset, and what the computational complexity is. 3. One important thing that the readers may wonder is, what is the performance difference between the method based on approximate local scores limited to the closed surrounding vertices and the method with exact scores. Could the authors present more clues? The likelihood decomposition (Theorem 1) is nice and there seems to be a slight improvement in the empirical testing. Then in the simulation studies, relevant methods from the references are excluded, even though the greedy search approaches (like references 13 and 32 for example) are not exact, but also heuristic. These (and similar) would need to be included in the benchmarking to judge the importance of the work."}
{"input": "The paper proposes a new empirical algorithm for causal structure discovery in ancestral graphs. The practical algorithm is motivated by a theoretical decomposition of likelihood function in terms of multivariate cross-information summed over ac-connected components. The developed algorithm is scalable to graphs with several dozens of vertices and links (thousands of unknown parameters) The authors provide experimental results on both synthetic datasets and challenging benchmarks from the bnlearn repository, showing that developed algorithm typically has higher precision and similar recall to prior MIIC algorithm. Also as the authors point out Theorem 1 is equivalent to the decomposition established in prior work, which makes novelty of the main theorem somewhat questionable. 1. The likelihood decomposition of ancestral graphs is very novel and could be useful in the future. There are close overlaps to published and preprint works by Hu and Evans, so it is hard to assess the novelty here. For example Theorem 1 (in another form) is from reference 14."}
{"input": "I also think that it will be beneficial to the reader if the connection between the theoretical results in Theorem 1 and the proposed algorithm are explained a bit more deeply with a more clear connection of how that theorem is being used. 1. Could authors elaborate more on where Theorem 1 can be applied?"}
{"input": "The performance of the proposed solution is poor. For instance, Figure 2 most of the metrics overlap with existing solutions. The proposed algorithm seems to be only empirical and lacks theoretical guarantees and relies on MIIC as a base of the algorithm. To my understanding, the paper does not provide a guarantee that the algorithm always converges in a reasonable number of steps (even if not to a global minimum). Multivariate cross-information over a large set of variables may be very sensitive to noise, which I think is a weakness compared to algorithms that use cross-information only over small number of (e.g. triples) of variables."}
{"input": "Why keeping the sample size range fixed even if the number of possible states increases dramatically? Observing 100 samples for BARLEY means that (ideally) each algorithm observe less than 1% of the state space, which limits the experimental evidence. 5) Did you perform any experiment to understand how sensitive is the proposed algorithm to the noise?"}
{"input": "4. Could the authors present some efficiency comparisons empirically, as the authors say "to implement an efficient and reliable heuristic method applicable to more challenging graphical modes and large datasets"."}
{"input": "3. The paper is clearly written."}
{"input": "4. Could the authors present some efficiency comparisons empirically, as the authors say "to implement an efficient and reliable heuristic method applicable to more challenging graphical modes and large datasets"."}
{"input": "I also think that it will be beneficial to the reader if the connection between the theoretical results in Theorem 1 and the proposed algorithm are explained a bit more deeply with a more clear connection of how that theorem is being used."}
{"input": "The paper proposes a new empirical algorithm for causal structure discovery in ancestral graphs. The practical algorithm is motivated by a theoretical decomposition of likelihood function in terms of multivariate cross-information summed over ac-connected components. The developed algorithm is scalable to graphs with several dozens of vertices and links (thousands of unknown parameters) The authors provide experimental results on both synthetic datasets and challenging benchmarks from the bnlearn repository, showing that developed algorithm typically has higher precision and similar recall to prior MIIC algorithm."}
{"input": "The paper proposes a new empirical algorithm for causal structure discovery in ancestral graphs. The practical algorithm is motivated by a theoretical decomposition of likelihood function in terms of multivariate cross-information summed over ac-connected components. The developed algorithm is scalable to graphs with several dozens of vertices and links (thousands of unknown parameters) The authors provide experimental results on both synthetic datasets and challenging benchmarks from the bnlearn repository, showing that developed algorithm typically has higher precision and similar recall to prior MIIC algorithm."}
{"input": "2. For me, I encourage the studies on the MAG/PAG learning methods. It is a hard and fundamental problem."}
{"input": "There are close overlaps to published and preprint works by Hu and Evans, so it is hard to assess the novelty here. For example Theorem 1 (in another form) is from reference 14."}
{"input": "The performance of the proposed solution is poor. For instance, Figure 2 most of the metrics overlap with existing solutions."}
{"input": "The experiments keep the number of samples fixed even is the number of the parameters of the model increase, reducing the information available for the biggest models. Moreover, a sample size of 100 for BARLEY (114,005 parameters) is rather unrealistic."}
{"input": "Then in the simulation studies, relevant methods from the references are excluded, even though the greedy search approaches (like references 13 and 32 for example) are not exact, but also heuristic. These (and similar) would need to be included in the benchmarking to judge the importance of the work. Can the authors more clearly delineate what is novel and different here as compared to the other recent works of search-and-score for ancestral graphs?"}
{"input": "In the case of ancestral graphs, it's crucial to understand the differences between edge patterns. I would suggest to refactor Figure 1 to improve readability, e.g. isolating different edge patterns combinations with horizontal lines."}
{"input": "In the case of ancestral graphs, it's crucial to understand the differences between edge patterns. I would suggest to refactor Figure 1 to improve readability, e.g. isolating different edge patterns combinations with horizontal lines. I also think that it will be beneficial to the reader if the connection between the theoretical results in Theorem 1 and the proposed algorithm are explained a bit more deeply with a more clear connection of how that theorem is being used. 1. Could authors elaborate more on where Theorem 1 can be applied? 4. Could the authors present some efficiency comparisons empirically, as the authors say "to implement an efficient and reliable heuristic method applicable to more challenging graphical modes and large datasets". Then in the simulation studies, relevant methods from the references are excluded, even though the greedy search approaches (like references 13 and 32 for example) are not exact, but also heuristic. These (and similar) would need to be included in the benchmarking to judge the importance of the work. Can the authors more clearly delineate what is novel and different here as compared to the other recent works of search-and-score for ancestral graphs?"}
{"input": "Are the limitations related to the cap of two-collider paths only? Are there any issues related to computational complexity? This was partially referenced in the attached form. What was the reason of choosing only three models for benchmarks? It is unclear the actual advantage of such approach w.r.t. the existing FCI algorithm. Why keeping the sample size range fixed even if the number of possible states increases dramatically? Observing 100 samples for BARLEY means that (ideally) each algorithm observe less than 1% of the state space, which limits the experimental evidence. 1) What is the main Novelty of Theorem 1 compared to [14]? Can the theoretical motivation for the proposed search-and-score algorithm be deduced from the results in [14]? 2) Do you have convergence guarantees for the algorithm? In other words, how many edge orientations need to be probed before convergence in step 1? How many loops over nodes and edges are needed until convergence? 3. One important thing that the readers may wonder is, what is the performance difference between the method based on approximate local scores limited to the closed surrounding vertices and the method with exact scores. Could the authors present more clues?"}
{"input": "4) Is there any class of distributions or graphs for which algorithm is guaranteed to converge to a true hidden causal graph? 5) Did you perform any experiment to understand how sensitive is the proposed algorithm to the noise? 6) In the first step of the algorithm does it matter in which order vertices are processed? do you select any specific order?"}
{"input": "1. The main issue of the proposed method lies in the experimental evaluation. Only one learned-based method is adopted for comparison in the main paper on a rather simple dataset. More methods including some traditional methods should be also evaluated for better comparison. The experiment on the real dataset should be also provided to show the robustness of the proposed method. My major concern is with the experimental setup. While the experiments on ShapeNet is common in the community and shows good result, I am in general doubtful whether such an approach could be really applied to the real world. In motivation, the authors talk about usage in vision, graphics, and robotics. In vision and robotics, we are interested in fitting real-world scans to templates (e.g. [Scan2CAD, CVPR 2019]), where in most cases, only noisy, partial, and sparse point clouds are provided. The authors do not have experiments or discussions in such cases. It would be nice if the authors could conduct a minimal set of experiments in the real-world setup (e.g., extract a reconstruction from a ScanNet model and attempt to apply keypoint/semantic part transfer). Otherwise, it would be good to see a justification that this paper itself is an inevitable intermediate step toward real-world usage, and what can be done to further extend it. 3) Experiments on the KeypointNet dataset show the effectiveness of the proposed method. 3) Experimental results are good. 1) The main weakness of this paper could be all experiments are performed on synthetic datasets, with simple point cloud. It's good for authors' to show some examples/experiments on real-world datasets. For example, the 3Dmatch dataset. 2.2.2 Second, the so-called local shape transform is predicted by a multi-layer perception from some SO(3)-invariant features that obtained from the input. Why after transforming the "global" features by such a mechanism, the features turn to "local"? I cannot see any specific design that enables it. It should be further explained. (I personally do not think so) 3. Regarding the experiments: 3.1 The experiments are only conducted on synthetic data, which cannot support the proposed method can work for real applications. I think it would be better to have additional real-data experiments. 3.2 As this paper also targets on correspondence estimation, whose typical downstream task is pose estimation. Therefore, I consider it worthwhile to also conduct experiments on tasks of 6D pose estimation or point cloud registration (there you always use real data), to further validate the estimated correspondences. 3.3 In Tab.1, only CPAE proposed in 2021 is used as the baseline. Some recent methods, e.g., [1], should also be included. Otherwise the results are not convincing at all (only compared to a single baseline which was proposed years ago). And it seems CPAE is the only baseline method for all the experiments. More baselines are required on both tasks."}
{"input": "2. The proposed method is straightforward and shown to be effective on the test data. 1. The main issue of the proposed method lies in the experimental evaluation. Only one learned-based method is adopted for comparison in the main paper on a rather simple dataset. More methods including some traditional methods should be also evaluated for better comparison. The experiment on the real dataset should be also provided to show the robustness of the proposed method. 3. How about the performance of other methods with a rough alignment of the initial shape? If a rough alignment is enough for the existing methods, why should we learn SO(3)-invariant correspondence in an end-to-end manner? 1. Would SO(3) invariance be sufficient? Do we need SE(3) or even Sim(3) invariance, if we cannot easily normalize the input due to the noise and sparsity? 2. Will the network still be functional if the density distributions are different across input and output? 4. Would non-gt and/or biased key points and semantic parts be transferred properly? 3) Experiments on the KeypointNet dataset show the effectiveness of the proposed method. 2) Since the proposed method can estimate dense correspondences, I wonder whether the proposed method can be used to estimate the relative rotation/translation for a point cloud pair. For example, the estimated dense correspondences can be fed to an ICP method to estimate the relative rotation/translation. 2.2.1 First, why are the features obtained by the Encoder global? They are generated by a DGCNN-based VNN, but DGCNN is not guaranteed to capture the global context, as it is graph-based and really depends on the number of layers together with the number of rings of each layer. 2.2.2 Second, the so-called local shape transform is predicted by a multi-layer perception from some SO(3)-invariant features that obtained from the input. Why after transforming the "global" features by such a mechanism, the features turn to "local"? I cannot see any specific design that enables it. It should be further explained. (I personally do not think so) 3. Regarding the experiments: 3.1 The experiments are only conducted on synthetic data, which cannot support the proposed method can work for real applications. I think it would be better to have additional real-data experiments. 3.3 In Tab.1, only CPAE proposed in 2021 is used as the baseline. Some recent methods, e.g., [1], should also be included. Otherwise the results are not convincing at all (only compared to a single baseline which was proposed years ago). And it seems CPAE is the only baseline method for all the experiments. More baselines are required on both tasks. 3.4 The method is claimed to generate SO(3)-invariant correspondences. However, in Tab. 1, even on the synthetic data, the I/SO(3) and SO(3)/SO(3) experiments perform unsimilarly (I would expect to have similar results per category, as it is on synthetic and clean data). Could this be explained?"}
{"input": "1. The paper is in general well organized and easy to follow. 1) This paper is generally well-written. 2. The overall writing is good and the methodology part is well-organized and easy to follow."}
{"input": "2. The proposed method is straightforward and shown to be effective on the test data. 1. The main issue of the proposed method lies in the experimental evaluation. Only one learned-based method is adopted for comparison in the main paper on a rather simple dataset. More methods including some traditional methods should be also evaluated for better comparison. The experiment on the real dataset should be also provided to show the robustness of the proposed method. 3. Will it work out of the 16-category domain? Do we need more training data, or would it work out-of-box? 3) Experiments on the KeypointNet dataset show the effectiveness of the proposed method. 1) The main weakness of this paper could be all experiments are performed on synthetic datasets, with simple point cloud. It's good for authors' to show some examples/experiments on real-world datasets. For example, the 3Dmatch dataset."}
{"input": "4. The whole method is mainly built upon the existing SO(3)-equivariant representation. The main contribution lies in introducing this representation to the specific task. I didn't get too much novel insight in terms of network design. 1. The novelty of this work seems insufficient for ICLR. The whole pipeline heavily relies on VNNs and the main contribution I personally consider is the local shape transform and the self-supervised mechanism for correspondences."}
{"input": "2. From Fig. 6 in the supplementary, we can see that the performance of the proposed method on the I/I scenario is much worse than the SOTA method. More analysis of the drop of performance should be given. Moreover, the performance of different methods with different rotation angles should be provided for better comparison. 4) Please compare the proposed method with more recent papers, e.g., [SC3K: Self-supervised and Coherent 3D Keypoints Estimation from Rotated, Noisy, and Decimated Point Cloud Data]."}
{"input": "The reason why other methods are much better than LSTNet under the setting of I/I should be clarified. 4. For the SO(3)-equivariant and -invariant methods, some works for point cloud registration [2, 3, 4, 5] should also be discussed."}
{"input": "2) The idea of factorizing point cloud descriptors into SO(3)-equivariant global shape descriptor and dynamic SO(3)-invariant point-wise local shape transforms seems to be novel. 1. The idea of cross-reconstruction for generating inter-object correspondences in a self-supervised way is interesting."}
{"input": "4. For the SO(3)-equivariant and -invariant methods, some works for point cloud registration [2, 3, 4, 5] should also be discussed."}
{"input": "2. From Fig. 6 in the supplementary, we can see that the performance of the proposed method on the I/I scenario is much worse than the SOTA method. More analysis of the drop of performance should be given. Moreover, the performance of different methods with different rotation angles should be provided for better comparison."}
{"input": "4) Please compare the proposed method with more recent papers, e.g., [SC3K: Self-supervised and Coherent 3D Keypoints Estimation from Rotated, Noisy, and Decimated Point Cloud Data]."}
{"input": "3.4 The method is claimed to generate SO(3)-invariant correspondences. However, in Tab. 1, even on the synthetic data, the I/SO(3) and SO(3)/SO(3) experiments perform unsimilarly (I would expect to have similar results per category, as it is on synthetic and clean data). Could this be explained?"}
{"input": "1. The main issue of the proposed method lies in the experimental evaluation. Only one learned-based method is adopted for comparison in the main paper on a rather simple dataset. More methods including some traditional methods should be also evaluated for better comparison. The experiment on the real dataset should be also provided to show the robustness of the proposed method. 2. From Fig. 6 in the supplementary, we can see that the performance of the proposed method on the I/I scenario is much worse than the SOTA method. More analysis of the drop of performance should be given. Moreover, the performance of different methods with different rotation angles should be provided for better comparison. 1. Would SO(3) invariance be sufficient? Do we need SE(3) or even Sim(3) invariance, if we cannot easily normalize the input due to the noise and sparsity? 1) The main weakness of this paper could be all experiments are performed on synthetic datasets, with simple point cloud. It's good for authors' to show some examples/experiments on real-world datasets. For example, the 3Dmatch dataset. 4) Please compare the proposed method with more recent papers, e.g., [SC3K: Self-supervised and Coherent 3D Keypoints Estimation from Rotated, Noisy, and Decimated Point Cloud Data]. 2.2.2 Second, the so-called local shape transform is predicted by a multi-layer perception from some SO(3)-invariant features that obtained from the input. Why after transforming the "global" features by such a mechanism, the features turn to "local"? I cannot see any specific design that enables it. It should be further explained. (I personally do not think so) 3. Regarding the experiments: 3.1 The experiments are only conducted on synthetic data, which cannot support the proposed method can work for real applications. I think it would be better to have additional real-data experiments. 3.3 In Tab.1, only CPAE proposed in 2021 is used as the baseline. Some recent methods, e.g., [1], should also be included. Otherwise the results are not convincing at all (only compared to a single baseline which was proposed years ago). And it seems CPAE is the only baseline method for all the experiments. More baselines are required on both tasks. 4. For the SO(3)-equivariant and -invariant methods, some works for point cloud registration [2, 3, 4, 5] should also be discussed."}
{"input": "2. Will the network still be functional if the density distributions are different across input and output? 3. Will it work out of the 16-category domain? Do we need more training data, or would it work out-of-box? It would be nice if the authors could conduct a minimal set of experiments in the real-world setup (e.g., extract a reconstruction from a ScanNet model and attempt to apply keypoint/semantic part transfer). Otherwise, it would be good to see a justification that this paper itself is an inevitable intermediate step toward real-world usage, and what can be done to further extend it. 2) Since the proposed method can estimate dense correspondences, I wonder whether the proposed method can be used to estimate the relative rotation/translation for a point cloud pair. For example, the estimated dense correspondences can be fed to an ICP method to estimate the relative rotation/translation. 3.2 As this paper also targets on correspondence estimation, whose typical downstream task is pose estimation. Therefore, I consider it worthwhile to also conduct experiments on tasks of 6D pose estimation or point cloud registration (there you always use real data), to further validate the estimated correspondences."}
{"input": "3. How about the performance of other methods with a rough alignment of the initial shape? If a rough alignment is enough for the existing methods, why should we learn SO(3)-invariant correspondence in an end-to-end manner? The reason why other methods are much better than LSTNet under the setting of I/I should be clarified. 2.2.1 First, why are the features obtained by the Encoder global? They are generated by a DGCNN-based VNN, but DGCNN is not guaranteed to capture the global context, as it is graph-based and really depends on the number of layers together with the number of rings of each layer. 3.4 The method is claimed to generate SO(3)-invariant correspondences. However, in Tab. 1, even on the synthetic data, the I/SO(3) and SO(3)/SO(3) experiments perform unsimilarly (I would expect to have similar results per category, as it is on synthetic and clean data). Could this be explained?"}
{"input": "4. Would non-gt and/or biased key points and semantic parts be transferred properly?"}
