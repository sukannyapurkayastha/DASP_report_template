Attitude_roots,Frequency,Descriptions,Comments
Substance(MET),1.0,Incomplete details on perfromance of the method,"[['Reviewer cgwB', ['Sec 4.3 para 1: ""the changes to which the algorithm needs to remain invariant..."" - p6, Stage 2: - ""binary groups"": Is there an advantage to using just 2 groups? I would have expected more. Variance is really just a simple difference unless there are more than two groups - ""narrows the performance gap between the two groups"": Relative to PPO, yes, but it still stays flat or increases over time. Why is that?', 'p9, ablation study: ""it enables the model to explore a larger action space on challenging samples, leading to the discovery of a superior policy"". How does it enable that?', 'p9, reward distribution: ""which will aid in its generalization to unseen data."" Why will it do that? Did you show this somewhere?']], ['Reviewer cgwB', ['3. Good experimental results. The method demonstrates good experimental results, outperforming baselines on in-distribution data. This highlights its capabilities in known domains. On out-of-distribution data, it shows even greater performance gains, underscoring its generalization abilities.', 'Cons: 1. The experiments seem to imply a static number of groups, binary in the experiments,(best and challenging groups) which is not ideal with multi-modal data in dynamics environments. Experimenting with varying number of groups would be better to see how well does the method perform when the number of data groups increases?', '2. The approach relies on accurate group label inference. Incorrect initial grouping could potentially lead to improper KL penalties. How robust is the method to errors in initial group assignment?', '3. question: how do the newly introduced hyperparameters add complexity to tuning the algorithm? as training PPO is already known to be hard to tune.']], ['Reviewer 7D9U', ['The paper is well written and clear - The method is generally sound and intuitive - Promising experiment results', '2- The adversarial objective may pickup noisy or outliers and divert the optimization. I think this deserves more investigations.', '3- The learning objective is encouraging reduction of loss variance across groups, I was wondering why authors didn’t directly go for optimization macro average of loss which is a bit more intuitive and has less chance to have side-effects on the overall performance as mean is generally more stable than variance?']], ['Reviewer cgwB', ['3. The first term of the final learning objective in Equation (8) does not appear to be directly related to group $g$, raising questions about its role in achieving invariant learning.', '4. The paper would benefit from the inclusion of code or detailed pseudocode to clearly convey the training process, as the current description does not sufficiently outline the methodology.', '5. The connection between group invariant learning and its ability to address issues of shortcut exploitation and neglect of challenging samples is not clearly articulated. More intuitive explanations or empirical evidence would strengthen the understanding of this relationship.']]]"
Substance(RES),0.75,generalizability of results is questionable,"[['Reviewer cgwB', ['p7, Table 1: It would be helpful to know how statistically significant these results are. Is there some way to do that? It would also be helpful to add a calibration where a method is evaluated against itself. It should be a 100% tie, but obviously it will be more like 33/34/33.']], ['Reviewer cgwB', ['3. Good experimental results. The method demonstrates good experimental results, outperforming baselines on in-distribution data. This highlights its capabilities in known domains. On out-of-distribution data, it shows even greater performance gains, underscoring its generalization abilities.']], ['Reviewer 7D9U', ['The paper is well written and clear - The method is generally sound and intuitive - Promising experiment results', '1- It is unclear how to find the optimal number of groups? The paper seems to miss discussion of this and sharing what settings used in the presented results, and how authors reached that setting.']]]"
Substance(EXP),0.5,Experimental study not strong enough,"[['Reviewer cgwB', ['3. Good experimental results. The method demonstrates good experimental results, outperforming baselines on in-distribution data. This highlights its capabilities in known domains. On out-of-distribution data, it shows even greater performance gains, underscoring its generalization abilities.', 'Cons: 1. The experiments seem to imply a static number of groups, binary in the experiments,(best and challenging groups) which is not ideal with multi-modal data in dynamics environments. Experimenting with varying number of groups would be better to see how well does the method perform when the number of data groups increases?']], ['Reviewer cgwB', ['The paper is well written and clear - The method is generally sound and intuitive - Promising experiment results']]]"
Clarity(OAL),0.5,3. The paper is not nicely written or rather easy to follow.,"[['Reviewer cgwB', [""I don't have any major questions. The authors can feel free to respond to my questions in the previous section if they have time.""]], ['Reviewer cgwB', ['1. Organization and Accessibility: The paper is well-structured and the content is presented in a manner that is accessible to readers.']]]"
Clarity(DAT),0.25,"Lack of discussion on datasets (size, motivation for use, etc)","[['Reviewer cgwB', ['1. The explanation in Section 4.4 regarding the probability of assigning samples to the highest-performing group lacks clarity and warrants further detail.']]]"
Meaningful-comparison(MET),0.25,Missing theoretical comparisons,"[['Reviewer cgwB', ['4- Robust optimization has been extensively studied in the general optimization context. Many of such methods could be applicable to the RLHF/LLM problem and the method proposed in this paper is also also applicable to other settings. I do not see any comparisons to support this method is optimal for RLHF compared to existing robust optimization methods.', '5- Related to the previous two comments, we we should have compared with other robust optimization methods and variations of applying the group loss 6- In Section 3, “The training of an AI assistant consists of three main stages...“ is not necessarily the case for all AI Assistants. I suggest revising this statement and connecting the presented method to more broader usecases as it is not really limited to this case.']]]"
Clarity(TNF),0.25,"Incorrect presentation of tables and figures (plots, typos in tables and figs. description of plot)","[['Reviewer cgwB', ['Figure 1 is an excellent visualization of the problem and the proposed solution.']]]"
Clarity(MET),0.25,Unclear description of method,"[['Reviewer cgwB', ['2. The term $R_{g}(\\theta)$ is not clearly defined within the paper. It’s assumed to represent the expected return of group $g$, yet its relationship to the last term in Equation 6 is ambiguous and needs clarification.']]]"
Motivation-impact(PDI),0.25,Limited contribution in problem definition,"[['Reviewer cgwB', ['The paper is well motivated, proposes a novel idea, describes it clearly, and presents compelling evidence that it works. In short, this is an excellent paper!']]]"
Substance(DAT),0.25,Less datasets used,"[['Reviewer cgwB', ['p9, reward distribution: ""which will aid in its generalization to unseen data."" Why will it do that? Did you show this somewhere?']]]"
Originality(PDI),0.25,"The main reason is that from the narration, I cannot figure out what is the idea or technique of other works and what is the contribution of this paper.","[['Reviewer cgwB', ['3. Innovative Concept: The application of group-invariant learning to the alignment problem is both a novel and a promising idea.']]]"
Substance(PDI),0.25,Need more interesting problem definition,"[['Reviewer cgwB', ['2. Significance of Addressed Problems: The authors tackle critical issues within RLHF, such as reward hacking and the overlooking of complex samples, which are pertinent for the advancement of universal AI assistants.']]]"
Substance(TNF),0.25,"Incomprehensible tables and figures (what is the point of the plot, no decription of figure in main text, etc)","[['Reviewer cgwB', ['p7, Table 1: It would be helpful to know how statistically significant these results are. Is there some way to do that? It would also be helpful to add a calibration where a method is evaluated against itself. It should be a 100% tie, but obviously it will be more like 33/34/33.']]]"
