Request Information,Frequency,Comments
Improvement,1.0,"[['Reviewer 7D9U', ['p7, Table 1: It would be helpful to know how statistically significant these results are. Is there some way to do that? It would also be helpful to add a calibration where a method is evaluated against itself. It should be a 100% tie, but obviously it will be more like 33/34/33.']], ['Reviewer bjuC', ['Cons: 1. The experiments seem to imply a static number of groups, binary in the experiments,(best and challenging groups) which is not ideal with multi-modal data in dynamics environments. Experimenting with varying number of groups would be better to see how well does the method perform when the number of data groups increases?']], ['Reviewer cSKU', ['3- The learning objective is encouraging reduction of loss variance across groups, I was wondering why authors didn’t directly go for optimization macro average of loss which is a bit more intuitive and has less chance to have side-effects on the overall performance as mean is generally more stable than variance?', '5- Related to the previous two comments, we we should have compared with other robust optimization methods and variations of applying the group loss 6- In Section 3, “The training of an AI assistant consists of three main stages...“ is not necessarily the case for all AI Assistants. I suggest revising this statement and connecting the presented method to more broader usecases as it is not really limited to this case.']], ['Reviewer cgwB', ['4. The paper would benefit from the inclusion of code or detailed pseudocode to clearly convey the training process, as the current description does not sufficiently outline the methodology.', '5. The connection between group invariant learning and its ability to address issues of shortcut exploitation and neglect of challenging samples is not clearly articulated. More intuitive explanations or empirical evidence would strengthen the understanding of this relationship.']]]"
Explanation,0.75,"[['Reviewer 7D9U', ['Sec 4.3 para 1: ""the changes to which the algorithm needs to remain invariant..."" - p6, Stage 2: - ""binary groups"": Is there an advantage to using just 2 groups? I would have expected more. Variance is really just a simple difference unless there are more than two groups - ""narrows the performance gap between the two groups"": Relative to PPO, yes, but it still stays flat or increases over time. Why is that?', 'p9, ablation study: ""it enables the model to explore a larger action space on challenging samples, leading to the discovery of a superior policy"". How does it enable that?', 'p9, reward distribution: ""which will aid in its generalization to unseen data."" Why will it do that? Did you show this somewhere?']], ['Reviewer bjuC', ['3. question: how do the newly introduced hyperparameters add complexity to tuning the algorithm? as training PPO is already known to be hard to tune.']], ['Reviewer cgwB', ['1. The explanation in Section 4.4 regarding the probability of assigning samples to the highest-performing group lacks clarity and warrants further detail.']]]"
Experiment,0.5,"[['Reviewer bjuC', ['2. The approach relies on accurate group label inference. Incorrect initial grouping could potentially lead to improper KL penalties. How robust is the method to errors in initial group assignment?']], ['Reviewer cSKU', ['2- The adversarial objective may pickup noisy or outliers and divert the optimization. I think this deserves more investigations.']]]"
