Attitude_roots,Frequency,Descriptions,Comments
Clarity(MET),1.0,Unclear description of method,"[['Reviewer Mex5', ['+ Rejection for fairness does give additional options for achieving fairness though this too comes with a weakness below.', 'Unrelated, what is ""ordering defined over the input space X"" and why is it necessary?']], ['Reviewer Mex5', ['A discussion on when to use the proposed framework in contrast to other frameworks is absent (see also my questions below).']], ['Reviewer XkhF', ['FairDP-SGD"" What is FairDP-SGD ? It doesn\'t seem to have been defined anywhere ? Is it an existing work ? Where is FAIR-PATE\'s result on CheXpert ? In addition to this, please also also address __W1,W2,W3,W4__.']]]"
Substance(MET),1.0,Incomplete details on perfromance of the method,"[['Reviewer Mex5', ['+ The points of intervention discussions give a nice overview of the PATE approach and ways in which additional mechanisms can be independently injected. Note, however, the independent intervention assumption is a weakness below.', 'Handling rejection in experiments will also need to be done but unsure what the best approach there would be. Perhaps a random human decision maker?', 'Suggestion: either integrate suggestion regarding accounting for rejection above, or incorporate some form of rejection (or simulate it) in the existing methods being compared to. It may be that the best methodology is not FairPATE but some existing baselines if adjusted to include fairness rejection option.', 'Question A: Is reasonable to ignore downstream decisions from queries rejected due to fairness (i.e. contrary to my suggestion in the weaknesses above)?', ""Question B: C1 makes a point that adding privacy after fairness may break fairness. What about in expectation? Were one to view the demographic statistics defining fairness measures in expectations, wouldn't they remain fair?""]], ['Reviewer Mex5', ['How does the framework work in case of some distribution shift? This is especially important in the context of my question above.', 'Fig. 2 and 3 use orange colors for two different algorithms (Tran et al (Fig 2) and Jagielski et al. (Fig 3)). The authors should report all algorithms in all figures or justify their absence.']], ['Reviewer XkhF', ['3. __W3__ Abstaining from prediction for fairness reasons The introduction justifies this as _""if a decision cannot be made without violating a pre-specified fairness metric, then the model can refuse to answer at which point the decision can be relegated to a human judge""_. If I have understood this correctly, this is a flawed argument. For example, consider the situation where there are group of data points from the majority group, on which if the algorithm predicts correctly the fairness will be violated and hence the algorithm abstains. Nevertheless, the prediction is easy for this group and the judge nevertheless predicts on them correctly and the overall fairness is still violated. Isn\'t this pointless then to relegate this to the judge ? Intuitively, it appears that any problem can be made fair this way by simply refusing to predict on certain majority groups thereby artificially boosting the fairness of the algorithm. The correct fairness solution should aim to improve performance on the minority group instead. 4. __W4__ Unfair Comparisons Fair-PATE and Loewy et. al\'s algorithm do not work under the same restrictions of differential privacy. Specifically, Fair-PATE uses public unlabelled data and Loewy et. al doesn\'t and we know that (even a small amount of) public data can severely help in improving accuracy of DP models (perhaps fairness too). Hence, it appears to me that Loewy et. al. uses a significantly stricter notion of privacy and achieves nearly comparable result and in some cases even better (for tabular datasets, there are no comparison on vision datasets with Loewy et. al.).', 'Motivation I understand that there are several works showing that privacy incurs a sacrifice in fairness. However, one possible approach to this problem is simply improving the accuracy and perhaps the resultant accuracy also leads to high fairness. In fact Berrada et. al. (2023) makes this argument with some experiments in their paper. I did not see an argument in this paper why this approach is not expected to solve all problems. Why should there be a trade-off between privacy and fairness ? Is it known in the literature and are there theoretical studies arguing about the necessity of a trade-off ? Missing baselines Why is there no comparison with the algorithm of Zhang et. al. (2021), Kulynuych et. al. (2022) and Berrada et. al. (2023) ? They are mentioned in one or two sentences at the very end of the paper but without any comprehensive empirical comparison.']]]"
Substance(RWK),1.0,Limited improvement over baselines,"[['Reviewer Mex5', ['Suggestion: either integrate suggestion regarding accounting for rejection above, or incorporate some form of rejection (or simulate it) in the existing methods being compared to. It may be that the best methodology is not FairPATE but some existing baselines if adjusted to include fairness rejection option.']], ['Reviewer Mex5', ['Fig. 2 and 3 use orange colors for two different algorithms (Tran et al (Fig 2) and Jagielski et al. (Fig 3)). The authors should report all algorithms in all figures or justify their absence.', 'Why Tran et and Jagielski et al. are not reported for the UTK-dataset experiment?']], ['Reviewer XkhF', ['Motivation I understand that there are several works showing that privacy incurs a sacrifice in fairness. However, one possible approach to this problem is simply improving the accuracy and perhaps the resultant accuracy also leads to high fairness. In fact Berrada et. al. (2023) makes this argument with some experiments in their paper. I did not see an argument in this paper why this approach is not expected to solve all problems. Why should there be a trade-off between privacy and fairness ? Is it known in the literature and are there theoretical studies arguing about the necessity of a trade-off ? Missing baselines Why is there no comparison with the algorithm of Zhang et. al. (2021), Kulynuych et. al. (2022) and Berrada et. al. (2023) ? They are mentioned in one or two sentences at the very end of the paper but without any comprehensive empirical comparison.']]]"
Substance(DAT),1.0,Less datasets used,"[['Reviewer Mex5', ['Suggestion: Include more experimental samples in the results to make sure the statistical validity of any improvement claims is good. This may require larger datasets. Related, the experiments show error bars but how they are derived is not explained.']], ['Reviewer Mex5', [""I can't judge what is the impact of IPP (the rejection step added at inference time) on the overall results. Can you provide some ablation study showing how the framework performs on various datasets with different majority/minority distributions with and without IPP?"", 'For the other datasets reported in the appendix the trends shown reverts, e.g., DP-Fermi produces better tradeoffs than FairPATE. Can you discuss why? What feature of the dataset makes this possible?']], ['Reviewer XkhF', ['3. __W3__ Abstaining from prediction for fairness reasons The introduction justifies this as _""if a decision cannot be made without violating a pre-specified fairness metric, then the model can refuse to answer at which point the decision can be relegated to a human judge""_. If I have understood this correctly, this is a flawed argument. For example, consider the situation where there are group of data points from the majority group, on which if the algorithm predicts correctly the fairness will be violated and hence the algorithm abstains. Nevertheless, the prediction is easy for this group and the judge nevertheless predicts on them correctly and the overall fairness is still violated. Isn\'t this pointless then to relegate this to the judge ? Intuitively, it appears that any problem can be made fair this way by simply refusing to predict on certain majority groups thereby artificially boosting the fairness of the algorithm. The correct fairness solution should aim to improve performance on the minority group instead. 4. __W4__ Unfair Comparisons Fair-PATE and Loewy et. al\'s algorithm do not work under the same restrictions of differential privacy. Specifically, Fair-PATE uses public unlabelled data and Loewy et. al doesn\'t and we know that (even a small amount of) public data can severely help in improving accuracy of DP models (perhaps fairness too). Hence, it appears to me that Loewy et. al. uses a significantly stricter notion of privacy and achieves nearly comparable result and in some cases even better (for tabular datasets, there are no comparison on vision datasets with Loewy et. al.).']]]"
Originality(MET),0.6666666666666666,Limited novelty in theoretical contribution,"[['Reviewer Mex5', ['The proposed framework is a simple adaptation of the existing PATE. Simplicity is a plus in my book.']], ['Reviewer Mex5', ['I also found the main algorithmic idea of this paper quite nice. The idea of that the algorithm chooses which points to label not only on the bassi of the privacy constraint but also the fairness constraint is quite neat and could be useful in other contexts. I appreciated this.']]]"
Clarity(OAL),0.6666666666666666,3. The paper is not nicely written or rather easy to follow.,"[['Reviewer Mex5', ['+ Fairly well written and easy to follow.']], ['Reviewer Mex5', ['The paper is written quite clearly and is easily readable. The arguments of the authors come out clearly without ambiguity and the reader can easily follow the train-of-thought. I appreciated that very much.']]]"
Soundness-correctness(MET),0.6666666666666666,Correctness of algorithm proposed is questionable,"[['Reviewer Mex5', ['Algorithm 1 and several points throughout the work hint at this. However, there is also the consideration of intervention points 1,2,3 which seem odd as they points seen before any individual for whom fairness is considered is seen. That is, fairness about public individuals cannot be made there, independent of any other issues such as privacy budgeting. Further, Theorem 1 discusses a demographic parity pre-processor which achieves demographic parity on private data which I presume is irrelevant.', 'The statement ""DP that only protects privacy of a given sensitive feature"" might be mischaracterizing DP. It is not focused on features or even data but rather the impact of individuals on visible results.', ""Question C: Theorem 1 makes a statement about a pre-processor inducing privacy parameter degradation but FairPATE (or PATE) appears to fit the definition of a pre-processor. If the point of the Theorem is to argue against pre-processors, isn't it also arguing against PATE/FairPATE?""]], ['Reviewer Mex5', ['2. __ W2__ Wrong Conclusion from Theorem 2 I did not go through the detail of Theorem 1 but I assume it is correct and follows from Group privacy arguments. However, the sentence above Theorem 1 (Point C2) claims that the result proves that "" pre-processing ... will necesarrily degrade the guarantee of the .... private learning mechanism"". How ever this is not what the Theorem shows. The theorem only proves a privacy guarantee of $M\\odot P_{\\text{pre}}$ not that this is the tightest privacy guarantee possible for the composed mechanism. Am I missing something ?']]]"
Substance(RES),0.6666666666666666,generalizability of results is questionable,"[['Reviewer Mex5', ['Smaller things: - Rejection rate is not shown in any experiments. One could view a misclassification as a rejection, however. Please include rejection rates or view them as misclassifications in the results.']], ['Reviewer Mex5', [""1. __W1__ Significance: The paper aims to convey the significance of its contribution through experimental results (and I don't think there is anything wrong with it and in general support extensive empirical results), but the experiments present a rather bleak picture of the advantages of FairPATE.""]]]"
Substance(EXP),0.6666666666666666,Experimental study not strong enough,"[['Reviewer Mex5', ['Handling rejection in experiments will also need to be done but unsure what the best approach there would be. Perhaps a random human decision maker?', 'Suggestion: Include more experimental samples in the results to make sure the statistical validity of any improvement claims is good. This may require larger datasets. Related, the experiments show error bars but how they are derived is not explained.', 'Smaller things: - Rejection rate is not shown in any experiments. One could view a misclassification as a rejection, however. Please include rejection rates or view them as misclassifications in the results.']], ['Reviewer Mex5', [""1. __W1__ Significance: The paper aims to convey the significance of its contribution through experimental results (and I don't think there is anything wrong with it and in general support extensive empirical results), but the experiments present a rather bleak picture of the advantages of FairPATE.""]]]"
Clarity(DAT),0.3333333333333333,"Lack of discussion on datasets (size, motivation for use, etc)","[['Reviewer Mex5', ['The statement ""PATE relies on unlabeled public data, which lacks the ground truth labels Y"" is a bit confusing unless one has already understood that fairness is with respect to public data. PATE also relies on private labeled data to create the teachers.']]]"
Clarity(RES),0.3333333333333333,"Improper explanation of results (as in advantages, why are results better, etc)","[['Reviewer Mex5', ['FairDP-SGD"" What is FairDP-SGD ? It doesn\'t seem to have been defined anywhere ? Is it an existing work ? Where is FAIR-PATE\'s result on CheXpert ? In addition to this, please also also address __W1,W2,W3,W4__.']]]"
Clarity(EXP),0.3333333333333333,"Lack of discussion of experimental setup (conclusion from experiments, poor experimental details, etc)","[['Reviewer Mex5', ['The experimental analysis should be improved. Some figures are misleading, e.g., same colors used for different algorithms (see questions below).']]]"
Clarity(ANA),0.3333333333333333,Lack of discussion of analysis,"[['Reviewer Mex5', ['The Privacy Analysis paragraph could be greatly simplified to just the last sentence regarding post-processing.']]]"
Meaningful-comparison(DAT),0.3333333333333333,"- Iâ€™d also like to see more extensive comparisons between FICM and ICM across different datasets, for example, Super Mario Bros. and the Atari games, instead of only comparing FICM against ICM on ViZDoom.","[['Reviewer Mex5', [""Misleading regarding diversity of experiments In the introduction, the paper sells itself regarding performing a wide range of experiments but results on nearly half of these datasets are hidden away in the appendix without any mention in the main text and without comparisons. Experiments on CheXpert, CelebA, FairFace, and Retired-Adults are not available in the main text and it is very hard to interpret the results presented for this in the appendix. In fact, there are no results of FAIR-PATE on CheXpert in the paper Please show comparison on these for demographic parity and equalized odds (similar to Figure 2,3 with comparison to Loewe et. al.'s algorithm) on these four datasets. In addition, there are results in the literature that run DP algorithms on CelebA and CheXpert. Comparisons should be made to them to show what is the sacrifice being made compared to state-of-the-art.""]]]"
Motivation-impact(RWK),0.3333333333333333,Limited novelty as compared to related work,"[['Reviewer Mex5', ['Fairness is ""enforced"" by adding parity constraint within the aggregator which acts as a rejection sampling step on the basis of fairness. The general idea was proposed in several other works in the past, including the ones cited by the authors and against which the authors compare. It is thus difficult to understand what is new in the proposed framework. An explicit mention would help.']]]"
Motivation-impact(PDI),0.3333333333333333,Limited contribution in problem definition,"[['Reviewer Mex5', ['The paper tackles a highly relevant issue in ML, addressing both theoretical and practical implications of fairness and privacy.']]]"
Motivation-impact(MET),0.3333333333333333,Limited insights based on design choices,"[['Reviewer Mex5', ['Fairness is ""enforced"" by adding parity constraint within the aggregator which acts as a rejection sampling step on the basis of fairness. The general idea was proposed in several other works in the past, including the ones cited by the authors and against which the authors compare. It is thus difficult to understand what is new in the proposed framework. An explicit mention would help.']]]"
Meaningful-comparison(RWK),0.3333333333333333,Missing baselines,"[['Reviewer Mex5', ['Paper [Learning with Impartiality to Walk on the Pareto Frontier of Fairness, Privacy, and Utility]( discusses a similar topic (although the contributions from this work are different) and it could be added to your Related work section.', 'Minor comments: A lot of the cited papers have appeared in conferences. But the authors cite their arxiv version. I suggest to update the references accordingly.']]]"
Meaningful-comparison(RES),0.3333333333333333,More comparisons needed with variations of the proposed method,"[['Reviewer Mex5', [""Misleading regarding diversity of experiments In the introduction, the paper sells itself regarding performing a wide range of experiments but results on nearly half of these datasets are hidden away in the appendix without any mention in the main text and without comparisons. Experiments on CheXpert, CelebA, FairFace, and Retired-Adults are not available in the main text and it is very hard to interpret the results presented for this in the appendix. In fact, there are no results of FAIR-PATE on CheXpert in the paper Please show comparison on these for demographic parity and equalized odds (similar to Figure 2,3 with comparison to Loewe et. al.'s algorithm) on these four datasets. In addition, there are results in the literature that run DP algorithms on CelebA and CheXpert. Comparisons should be made to them to show what is the sacrifice being made compared to state-of-the-art.""]]]"
Meaningful-comparison(MET),0.3333333333333333,Missing theoretical comparisons,"[['Reviewer Mex5', ['Comparisons against methods in which rejection due to fairness is not an option may not be fair.']]]"
Meaningful-comparison(EXP),0.3333333333333333,More experiments needed with related work,"[['Reviewer Mex5', [""Misleading regarding diversity of experiments In the introduction, the paper sells itself regarding performing a wide range of experiments but results on nearly half of these datasets are hidden away in the appendix without any mention in the main text and without comparisons. Experiments on CheXpert, CelebA, FairFace, and Retired-Adults are not available in the main text and it is very hard to interpret the results presented for this in the appendix. In fact, there are no results of FAIR-PATE on CheXpert in the paper Please show comparison on these for demographic parity and equalized odds (similar to Figure 2,3 with comparison to Loewe et. al.'s algorithm) on these four datasets. In addition, there are results in the literature that run DP algorithms on CelebA and CheXpert. Comparisons should be made to them to show what is the sacrifice being made compared to state-of-the-art.""]]]"
Soundness-correctness(DAT),0.3333333333333333,claims on the datasets is questionable,"[['Reviewer Mex5', ['The statement ""DP that only protects privacy of a given sensitive feature"" might be mischaracterizing DP. It is not focused on features or even data but rather the impact of individuals on visible results.']]]"
