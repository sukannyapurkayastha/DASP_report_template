Request Information,Frequency,Comments
Explanation,1.0,"[['Reviewer Mex5', [""Question B: C1 makes a point that adding privacy after fairness may break fairness. What about in expectation? Were one to view the demographic statistics defining fairness measures in expectations, wouldn't they remain fair?"", 'Unrelated, what is ""ordering defined over the input space X"" and why is it necessary?']], ['Reviewer XkhF', ['How does the framework work in case of some distribution shift? This is especially important in the context of my question above.', 'For the other datasets reported in the appendix the trends shown reverts, e.g., DP-Fermi produces better tradeoffs than FairPATE. Can you discuss why? What feature of the dataset makes this possible?', 'Why Tran et and Jagielski et al. are not reported for the UTK-dataset experiment?']], ['Reviewer p5Rv', ['Motivation I understand that there are several works showing that privacy incurs a sacrifice in fairness. However, one possible approach to this problem is simply improving the accuracy and perhaps the resultant accuracy also leads to high fairness. In fact Berrada et. al. (2023) makes this argument with some experiments in their paper. I did not see an argument in this paper why this approach is not expected to solve all problems. Why should there be a trade-off between privacy and fairness ? Is it known in the literature and are there theoretical studies arguing about the necessity of a trade-off ? Missing baselines Why is there no comparison with the algorithm of Zhang et. al. (2021), Kulynuych et. al. (2022) and Berrada et. al. (2023) ? They are mentioned in one or two sentences at the very end of the paper but without any comprehensive empirical comparison.', 'FairDP-SGD"" What is FairDP-SGD ? It doesn\'t seem to have been defined anywhere ? Is it an existing work ? Where is FAIR-PATE\'s result on CheXpert ? In addition to this, please also also address __W1,W2,W3,W4__.']]]"
Experiment,0.6666666666666666,"[['Reviewer Mex5', ['Handling rejection in experiments will also need to be done but unsure what the best approach there would be. Perhaps a random human decision maker?']], ['Reviewer XkhF', [""I can't judge what is the impact of IPP (the rejection step added at inference time) on the overall results. Can you provide some ablation study showing how the framework performs on various datasets with different majority/minority distributions with and without IPP?""]]]"
Improvement,0.6666666666666666,"[['Reviewer Mex5', ['Suggestion: incorporate ultimate decisions, whether by model or human, into the rejection mechanism. i.e. update counts m(z, k) based on human decisions. Given that humans might put the group counts into already violating territory, it may be necessary to rewrite Line 7 of Algorithm 1 to check whether the fairness criterion is improving or not due to the decision and allow queries that improve statistics even though those statistics already violate Î³ threshold.', 'The Privacy Analysis paragraph could be greatly simplified to just the last sentence regarding post-processing.', 'Question A: Is reasonable to ignore downstream decisions from queries rejected due to fairness (i.e. contrary to my suggestion in the weaknesses above)?']], ['Reviewer XkhF', ['Fairness is ""enforced"" by adding parity constraint within the aggregator which acts as a rejection sampling step on the basis of fairness. The general idea was proposed in several other works in the past, including the ones cited by the authors and against which the authors compare. It is thus difficult to understand what is new in the proposed framework. An explicit mention would help.', 'The experimental analysis should be improved. Some figures are misleading, e.g., same colors used for different algorithms (see questions below).', 'Fig. 2 and 3 use orange colors for two different algorithms (Tran et al (Fig 2) and Jagielski et al. (Fig 3)). The authors should report all algorithms in all figures or justify their absence.', 'Paper [Learning with Impartiality to Walk on the Pareto Frontier of Fairness, Privacy, and Utility]( discusses a similar topic (although the contributions from this work are different) and it could be added to your Related work section.', 'Minor comments: A lot of the cited papers have appeared in conferences. But the authors cite their arxiv version. I suggest to update the references accordingly.']]]"
Clarification,0.3333333333333333,"[['Reviewer Mex5', [""Question C: Theorem 1 makes a statement about a pre-processor inducing privacy parameter degradation but FairPATE (or PATE) appears to fit the definition of a pre-processor. If the point of the Theorem is to argue against pre-processors, isn't it also arguing against PATE/FairPATE?""]]]"
