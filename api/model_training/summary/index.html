
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="Dokumentation des DASP Report Template Projekts inklusive automatischer Docstring-Integration.">
      
      
        <meta name="author" content="Johannes Lemken">
      
      
      
        <link rel="prev" href="../../backend/">
      
      
        <link rel="next" href="../request_classifier/">
      
      
      <link rel="icon" href="../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.2">
    
    
      
        <title>Summary Generator - DASP Report Template Dokumentation</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/main.d7758b05.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../../assets/_mkdocstrings.css">
    
    <script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#model_training.nlp.summary.compute_metrics" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../.." title="DASP Report Template Dokumentation" class="md-header__button md-logo" aria-label="DASP Report Template Dokumentation" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            DASP Report Template Dokumentation
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Summary Generator
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/dein_username/DASP_report_template" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../.." title="DASP Report Template Dokumentation" class="md-nav__button md-logo" aria-label="DASP Report Template Dokumentation" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    DASP Report Template Dokumentation
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/dein_username/DASP_report_template" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../readme/main_page/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Main Page
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../readme/setup/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Setup and Installation
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../readme/developer_guide/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Developer Guide
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../readme/architecture/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Architecture and Design Notes
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../readme/files/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Files
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../readme/data/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Data
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../readme/testing/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Testing
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_8" checked>
        
          
          <label class="md-nav__link" for="__nav_8" id="__nav_8_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Code
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_8_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_8">
            <span class="md-nav__icon md-icon"></span>
            Code
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../frontend/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Frontend
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../backend/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Backend
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_8_3" checked>
        
          
          <label class="md-nav__link" for="__nav_8_3" id="__nav_8_3_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Model Training
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_8_3_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_8_3">
            <span class="md-nav__icon md-icon"></span>
            Model Training
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    Summary Generator
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    Summary Generator
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#model_training.nlp.summary.compute_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      compute_metrics
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#model_training.nlp.summary.compute_metrics.compute_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      compute_metrics
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#model_training.nlp.summary.dfs_to_input_converter" class="md-nav__link">
    <span class="md-ellipsis">
      dfs_to_input_converter
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#model_training.nlp.summary.dfs_to_input_converter.generate_attitude_roots_prompts" class="md-nav__link">
    <span class="md-ellipsis">
      generate_attitude_roots_prompts
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#model_training.nlp.summary.dfs_to_input_converter.generate_dummy_input_text" class="md-nav__link">
    <span class="md-ellipsis">
      generate_dummy_input_text
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#model_training.nlp.summary.dfs_to_input_converter.generate_overview_prompts" class="md-nav__link">
    <span class="md-ellipsis">
      generate_overview_prompts
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#model_training.nlp.summary.dfs_to_input_converter.generate_request_information_prompts" class="md-nav__link">
    <span class="md-ellipsis">
      generate_request_information_prompts
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#model_training.nlp.summary.dummy_data" class="md-nav__link">
    <span class="md-ellipsis">
      dummy_data
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#model_training.nlp.summary.input_to_prompt_converter" class="md-nav__link">
    <span class="md-ellipsis">
      input_to_prompt_converter
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#model_training.nlp.summary.main" class="md-nav__link">
    <span class="md-ellipsis">
      main
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#model_training.nlp.summary.predict_BART" class="md-nav__link">
    <span class="md-ellipsis">
      predict_BART
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#model_training.nlp.summary.predict_BART.load_BART_model" class="md-nav__link">
    <span class="md-ellipsis">
      load_BART_model
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#model_training.nlp.summary.predict_BART.predict" class="md-nav__link">
    <span class="md-ellipsis">
      predict
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#model_training.nlp.summary.predict_BLOOM" class="md-nav__link">
    <span class="md-ellipsis">
      predict_BLOOM
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#model_training.nlp.summary.predict_BLOOM.load_BLOOM_model" class="md-nav__link">
    <span class="md-ellipsis">
      load_BLOOM_model
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#model_training.nlp.summary.predict_BLOOM.predict" class="md-nav__link">
    <span class="md-ellipsis">
      predict
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#model_training.nlp.summary.predict_compare" class="md-nav__link">
    <span class="md-ellipsis">
      predict_compare
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#model_training.nlp.summary.predict_compare.main" class="md-nav__link">
    <span class="md-ellipsis">
      main
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#model_training.nlp.summary.predict_LLAMA2" class="md-nav__link">
    <span class="md-ellipsis">
      predict_LLAMA2
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#model_training.nlp.summary.predict_LLAMA2.load_LLAMA2_model" class="md-nav__link">
    <span class="md-ellipsis">
      load_LLAMA2_model
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#model_training.nlp.summary.predict_LLAMA2.predict" class="md-nav__link">
    <span class="md-ellipsis">
      predict
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../request_classifier/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Request Classifier
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../attitude/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Attitude Classifier
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../theme/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Theme Classifier
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../theme_to_desc/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Theme to Description
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../attitude_classifier/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Attitude Classifier
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../request_classifier/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Request Classifier
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../summary_generator/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Summary Generator
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../tests.md" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Tests
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../readme/contact/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Contact
    
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#model_training.nlp.summary.compute_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      compute_metrics
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#model_training.nlp.summary.compute_metrics.compute_metrics" class="md-nav__link">
    <span class="md-ellipsis">
      compute_metrics
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#model_training.nlp.summary.dfs_to_input_converter" class="md-nav__link">
    <span class="md-ellipsis">
      dfs_to_input_converter
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#model_training.nlp.summary.dfs_to_input_converter.generate_attitude_roots_prompts" class="md-nav__link">
    <span class="md-ellipsis">
      generate_attitude_roots_prompts
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#model_training.nlp.summary.dfs_to_input_converter.generate_dummy_input_text" class="md-nav__link">
    <span class="md-ellipsis">
      generate_dummy_input_text
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#model_training.nlp.summary.dfs_to_input_converter.generate_overview_prompts" class="md-nav__link">
    <span class="md-ellipsis">
      generate_overview_prompts
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#model_training.nlp.summary.dfs_to_input_converter.generate_request_information_prompts" class="md-nav__link">
    <span class="md-ellipsis">
      generate_request_information_prompts
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#model_training.nlp.summary.dummy_data" class="md-nav__link">
    <span class="md-ellipsis">
      dummy_data
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#model_training.nlp.summary.input_to_prompt_converter" class="md-nav__link">
    <span class="md-ellipsis">
      input_to_prompt_converter
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#model_training.nlp.summary.main" class="md-nav__link">
    <span class="md-ellipsis">
      main
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#model_training.nlp.summary.predict_BART" class="md-nav__link">
    <span class="md-ellipsis">
      predict_BART
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#model_training.nlp.summary.predict_BART.load_BART_model" class="md-nav__link">
    <span class="md-ellipsis">
      load_BART_model
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#model_training.nlp.summary.predict_BART.predict" class="md-nav__link">
    <span class="md-ellipsis">
      predict
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#model_training.nlp.summary.predict_BLOOM" class="md-nav__link">
    <span class="md-ellipsis">
      predict_BLOOM
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#model_training.nlp.summary.predict_BLOOM.load_BLOOM_model" class="md-nav__link">
    <span class="md-ellipsis">
      load_BLOOM_model
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#model_training.nlp.summary.predict_BLOOM.predict" class="md-nav__link">
    <span class="md-ellipsis">
      predict
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#model_training.nlp.summary.predict_compare" class="md-nav__link">
    <span class="md-ellipsis">
      predict_compare
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#model_training.nlp.summary.predict_compare.main" class="md-nav__link">
    <span class="md-ellipsis">
      main
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#model_training.nlp.summary.predict_LLAMA2" class="md-nav__link">
    <span class="md-ellipsis">
      predict_LLAMA2
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#model_training.nlp.summary.predict_LLAMA2.load_LLAMA2_model" class="md-nav__link">
    <span class="md-ellipsis">
      load_LLAMA2_model
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#model_training.nlp.summary.predict_LLAMA2.predict" class="md-nav__link">
    <span class="md-ellipsis">
      predict
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


  
  


  <h1>Summary Generator</h1>

<div class="doc doc-object doc-module">



<a id="model_training.nlp.summary.compute_metrics"></a>
    <div class="doc doc-contents first">








  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h2 id="model_training.nlp.summary.compute_metrics.compute_metrics" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">compute_metrics</span><span class="p">(</span><span class="n">eval_pred</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">remove_prompt_portion</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">prompt_delimiter</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

</h2>


    <div class="doc doc-contents ">

        <p>Compute BERT-Score between predictions and labels.
Includes detailed debug logs to trace data shapes and types.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>eval_pred</code>
            </td>
            <td>
                  <code>Tuple</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The (predictions, labels) tuple from Trainer.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>tokenizer</code>
            </td>
            <td>
                  <code>PreTrainedTokenizer</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The tokenizer used for decoding.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>remove_prompt_portion</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>If True, tries to remove the prompt portion
in each decoded string before scoring. Defaults to False.</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>prompt_delimiter</code>
            </td>
            <td>
                  <code>str</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>If provided, is used to split out the prompt portion.
For example, if your text is "PROMPT\nOUTPUT", then prompt_delimiter="\n"
will remove everything up to the first newline.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
<th>Name</th>          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
<td><code>dict</code></td>            <td>
            </td>
            <td>
              <div class="doc-md-description">
                <p>A dictionary containing "bertscore_precision", "bertscore_recall",
  and "bertscore_f1".</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>model_training\nlp\summary\compute_metrics.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 18</span>
<span class="normal"> 19</span>
<span class="normal"> 20</span>
<span class="normal"> 21</span>
<span class="normal"> 22</span>
<span class="normal"> 23</span>
<span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">compute_metrics</span><span class="p">(</span>
    <span class="n">eval_pred</span><span class="p">,</span>
    <span class="n">tokenizer</span><span class="p">,</span>
    <span class="n">remove_prompt_portion</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">prompt_delimiter</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute BERT-Score between predictions and labels.</span>
<span class="sd">    Includes detailed debug logs to trace data shapes and types.</span>

<span class="sd">    Args:</span>
<span class="sd">        eval_pred (Tuple): The (predictions, labels) tuple from Trainer.</span>
<span class="sd">        tokenizer (PreTrainedTokenizer): The tokenizer used for decoding.</span>
<span class="sd">        remove_prompt_portion (bool): If True, tries to remove the prompt portion</span>
<span class="sd">            in each decoded string before scoring. Defaults to False.</span>
<span class="sd">        prompt_delimiter (str): If provided, is used to split out the prompt portion.</span>
<span class="sd">            For example, if your text is &quot;PROMPT\\nOUTPUT&quot;, then prompt_delimiter=&quot;\\n&quot;</span>
<span class="sd">            will remove everything up to the first newline.</span>

<span class="sd">    Returns:</span>
<span class="sd">        dict: A dictionary containing &quot;bertscore_precision&quot;, &quot;bertscore_recall&quot;,</span>
<span class="sd">              and &quot;bertscore_f1&quot;.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="s2">&quot;[compute_metrics] Entering compute_metrics function.&quot;</span><span class="p">)</span>

    <span class="n">predictions</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">eval_pred</span>

    <span class="c1"># Debug: Check types</span>
    <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;[compute_metrics] Type of predictions: </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">predictions</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;[compute_metrics] Type of labels: </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="c1"># Handle predictions</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
        <span class="n">predictions</span> <span class="o">=</span> <span class="n">predictions</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="s2">&quot;[compute_metrics] Predictions were a tuple; took first element.&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;[compute_metrics] Predictions is a torch.Tensor with shape </span><span class="si">{</span><span class="n">predictions</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">predictions</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="s2">&quot;[compute_metrics] Predictions have logits; applying argmax over last dimension.&quot;</span><span class="p">)</span>
            <span class="n">predictions</span> <span class="o">=</span> <span class="n">predictions</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">predictions</span> <span class="o">=</span> <span class="n">predictions</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">):</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;[compute_metrics] Predictions is a numpy.ndarray with shape </span><span class="si">{</span><span class="n">predictions</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">predictions</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="s2">&quot;[compute_metrics] Predictions have logits; applying argmax over last dimension.&quot;</span><span class="p">)</span>
            <span class="n">predictions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="s2">&quot;[compute_metrics] Predictions is a list; converting to numpy array.&quot;</span><span class="p">)</span>
        <span class="n">predictions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">predictions</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;[compute_metrics] Unexpected type for predictions: </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">predictions</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Unexpected type for predictions: </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">predictions</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="c1"># Handle labels</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;[compute_metrics] Labels is a torch.Tensor with shape </span><span class="si">{</span><span class="n">labels</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="n">labels</span> <span class="o">=</span> <span class="n">labels</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">):</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;[compute_metrics] Labels is a numpy.ndarray with shape </span><span class="si">{</span><span class="n">labels</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="s2">&quot;[compute_metrics] Labels is a list; converting to numpy array.&quot;</span><span class="p">)</span>
        <span class="n">labels</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;[compute_metrics] Unexpected type for labels: </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Unexpected type for labels: </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="c1"># Replace -100 with pad_token_id</span>
    <span class="n">pad_id</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token_id</span> <span class="k">if</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token_id</span>
    <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;[compute_metrics] Using pad_token_id=</span><span class="si">{</span><span class="n">pad_id</span><span class="si">}</span><span class="s2"> to replace -100 in labels.&quot;</span><span class="p">)</span>
    <span class="n">labels</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">labels</span> <span class="o">==</span> <span class="o">-</span><span class="mi">100</span><span class="p">,</span> <span class="n">pad_id</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>

    <span class="c1"># Ensure predictions and labels are 2D arrays</span>
    <span class="k">if</span> <span class="n">predictions</span><span class="o">.</span><span class="n">ndim</span> <span class="o">!=</span> <span class="mi">2</span><span class="p">:</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;[compute_metrics] Predictions should be a 2D array, but got shape </span><span class="si">{</span><span class="n">predictions</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Predictions should be a 2D array, but got shape </span><span class="si">{</span><span class="n">predictions</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">labels</span><span class="o">.</span><span class="n">ndim</span> <span class="o">!=</span> <span class="mi">2</span><span class="p">:</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;[compute_metrics] Labels should be a 2D array, but got shape </span><span class="si">{</span><span class="n">labels</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Labels should be a 2D array, but got shape </span><span class="si">{</span><span class="n">labels</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="c1"># Decode predictions and labels</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">decoded_preds</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">decoded_labels</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="s2">&quot;[compute_metrics] Successfully decoded predictions and labels.&quot;</span><span class="p">)</span>
    <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;[compute_metrics] Error during batch_decode: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">raise</span> <span class="n">e</span>

    <span class="c1"># Optionally remove prompt portion from decoded strings</span>
    <span class="k">if</span> <span class="n">remove_prompt_portion</span> <span class="ow">and</span> <span class="n">prompt_delimiter</span><span class="p">:</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="s2">&quot;[compute_metrics] Removing prompt portion using the given delimiter.&quot;</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">decoded_preds</span><span class="p">)):</span>
            <span class="k">if</span> <span class="n">prompt_delimiter</span> <span class="ow">in</span> <span class="n">decoded_preds</span><span class="p">[</span><span class="n">i</span><span class="p">]:</span>
                <span class="c1"># Split at the first occurrence of the delimiter, keep only the &#39;output&#39; portion</span>
                <span class="n">parts</span> <span class="o">=</span> <span class="n">decoded_preds</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">prompt_delimiter</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">parts</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                    <span class="n">decoded_preds</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">parts</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>  <span class="c1"># Keep text after the delimiter</span>
            <span class="k">if</span> <span class="n">prompt_delimiter</span> <span class="ow">in</span> <span class="n">decoded_labels</span><span class="p">[</span><span class="n">i</span><span class="p">]:</span>
                <span class="n">parts</span> <span class="o">=</span> <span class="n">decoded_labels</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">prompt_delimiter</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">parts</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                    <span class="n">decoded_labels</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">parts</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

    <span class="c1"># Debug sample decoded strings</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">decoded_preds</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">decoded_labels</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;[compute_metrics] Sample prediction: &#39;</span><span class="si">{</span><span class="n">decoded_preds</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">...&#39;&quot;</span><span class="p">)</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;[compute_metrics] Sample label: &#39;</span><span class="si">{</span><span class="n">decoded_labels</span><span class="p">[</span><span class="mi">0</span><span class="p">][:</span><span class="mi">60</span><span class="p">]</span><span class="si">}</span><span class="s2">...&#39;&quot;</span><span class="p">)</span>

    <span class="c1"># Compute BERT-Score</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="s2">&quot;[compute_metrics] Computing BERT-Score.&quot;</span><span class="p">)</span>
        <span class="n">bert_results</span> <span class="o">=</span> <span class="n">bertscore_metric</span><span class="o">.</span><span class="n">compute</span><span class="p">(</span>
            <span class="n">predictions</span><span class="o">=</span><span class="n">decoded_preds</span><span class="p">,</span>
            <span class="n">references</span><span class="o">=</span><span class="n">decoded_labels</span><span class="p">,</span>
            <span class="n">model_type</span><span class="o">=</span><span class="s2">&quot;bert-base-uncased&quot;</span>
        <span class="p">)</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="s2">&quot;[compute_metrics] BERT-Score computation successful.&quot;</span><span class="p">)</span>
    <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;[compute_metrics] Error during BERT-Score computation: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">raise</span> <span class="n">e</span>

    <span class="c1"># Aggregate scores</span>
    <span class="n">precision_arr</span> <span class="o">=</span> <span class="n">bert_results</span><span class="p">[</span><span class="s2">&quot;precision&quot;</span><span class="p">]</span>
    <span class="n">recall_arr</span> <span class="o">=</span> <span class="n">bert_results</span><span class="p">[</span><span class="s2">&quot;recall&quot;</span><span class="p">]</span>
    <span class="n">f1_arr</span> <span class="o">=</span> <span class="n">bert_results</span><span class="p">[</span><span class="s2">&quot;f1&quot;</span><span class="p">]</span>

    <span class="n">precision</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">precision_arr</span><span class="p">)</span> <span class="o">*</span> <span class="mi">100</span>
    <span class="n">recall</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">recall_arr</span><span class="p">)</span> <span class="o">*</span> <span class="mi">100</span>
    <span class="n">f1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">f1_arr</span><span class="p">)</span> <span class="o">*</span> <span class="mi">100</span>

    <span class="n">metrics</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;bertscore_precision&quot;</span><span class="p">:</span> <span class="nb">round</span><span class="p">(</span><span class="n">precision</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
        <span class="s2">&quot;bertscore_recall&quot;</span><span class="p">:</span> <span class="nb">round</span><span class="p">(</span><span class="n">recall</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
        <span class="s2">&quot;bertscore_f1&quot;</span><span class="p">:</span> <span class="nb">round</span><span class="p">(</span><span class="n">f1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="p">}</span>

    <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;[compute_metrics] BERT-Score Metrics: </span><span class="si">{</span><span class="n">metrics</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">metrics</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<a id="model_training.nlp.summary.dfs_to_input_converter"></a>
    <div class="doc doc-contents first">








  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h2 id="model_training.nlp.summary.dfs_to_input_converter.generate_attitude_roots_prompts" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">generate_attitude_roots_prompts</span><span class="p">(</span><span class="n">attitude_df</span><span class="p">)</span></code>

</h2>


    <div class="doc doc-contents ">

        <p>Summarize the attitude roots, frequencies, and comments.</p>

            <details class="quote">
              <summary>Source code in <code>model_training\nlp\summary\dfs_to_input_converter.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">generate_attitude_roots_prompts</span><span class="p">(</span><span class="n">attitude_df</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Summarize the attitude roots, frequencies, and comments.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">attitude_df</span><span class="p">[</span><span class="s1">&#39;Frequency_Percent&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">attitude_df</span><span class="p">[</span><span class="s1">&#39;Frequency&#39;</span><span class="p">]</span> <span class="o">*</span> <span class="mi">100</span>
    <span class="n">lines</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">attitude_df</span><span class="o">.</span><span class="n">iterrows</span><span class="p">():</span>
        <span class="n">root</span> <span class="o">=</span> <span class="n">row</span><span class="p">[</span><span class="s1">&#39;Attitude_roots&#39;</span><span class="p">]</span>
        <span class="n">freq</span> <span class="o">=</span> <span class="n">row</span><span class="p">[</span><span class="s1">&#39;Frequency_Percent&#39;</span><span class="p">]</span>
        <span class="n">descr</span> <span class="o">=</span> <span class="n">row</span><span class="p">[</span><span class="s1">&#39;Descriptions&#39;</span><span class="p">]</span>
        <span class="n">comments_list</span> <span class="o">=</span> <span class="n">row</span><span class="p">[</span><span class="s1">&#39;Comments&#39;</span><span class="p">]</span>

        <span class="n">combined_comments</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">comment_chunk</span> <span class="ow">in</span> <span class="n">comments_list</span><span class="p">:</span>
            <span class="n">combined_comments</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot; &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">comment_chunk</span><span class="p">))</span>

        <span class="n">all_comments</span> <span class="o">=</span> <span class="s2">&quot; &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">combined_comments</span><span class="p">)</span>
        <span class="n">line</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;- </span><span class="si">{</span><span class="n">root</span><span class="si">}</span><span class="s2"> appears </span><span class="si">{</span><span class="n">freq</span><span class="si">:</span><span class="s2">.0f</span><span class="si">}</span><span class="s2">% of the time. </span><span class="si">{</span><span class="n">descr</span><span class="si">}</span><span class="s2">. Comments: </span><span class="si">{</span><span class="n">all_comments</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="n">lines</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">line</span><span class="p">)</span>

    <span class="k">return</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">lines</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="model_training.nlp.summary.dfs_to_input_converter.generate_dummy_input_text" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">generate_dummy_input_text</span><span class="p">()</span></code>

</h2>


    <div class="doc doc-contents ">

        <p>Returns dummy input data string for debug purposes</p>

            <details class="quote">
              <summary>Source code in <code>model_training\nlp\summary\dfs_to_input_converter.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">generate_dummy_input_text</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns dummy input data string for debug purposes</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="kn">import</span> <span class="nn">dummy_data</span>

    <span class="c1"># Import dummy data from respective module</span>
    <span class="n">overview_df</span><span class="p">,</span> <span class="n">attitude_df</span><span class="p">,</span> <span class="n">request_df</span> <span class="o">=</span> <span class="n">dummy_data</span><span class="o">.</span><span class="n">get_dummy_data</span><span class="p">()</span>

    <span class="c1"># Translate dummy data into input string expected</span>
    <span class="n">dummy_input_data</span> <span class="o">=</span> <span class="n">generate_input_text</span><span class="p">(</span><span class="n">overview_df</span><span class="p">,</span> <span class="n">attitude_df</span><span class="p">,</span> <span class="n">request_df</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">dummy_input_data</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="model_training.nlp.summary.dfs_to_input_converter.generate_overview_prompts" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">generate_overview_prompts</span><span class="p">(</span><span class="n">overview_df</span><span class="p">)</span></code>

</h2>


    <div class="doc doc-contents ">

        <p>Build lines summarizing rating, soundness, etc.</p>

            <details class="quote">
              <summary>Source code in <code>model_training\nlp\summary\dfs_to_input_converter.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">generate_overview_prompts</span><span class="p">(</span><span class="n">overview_df</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Build lines summarizing rating, soundness, etc.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">max_scores</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;Rating&#39;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span> <span class="s1">&#39;Soundness&#39;</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span> <span class="s1">&#39;Presentation&#39;</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span> <span class="s1">&#39;Contribution&#39;</span><span class="p">:</span> <span class="mi">4</span><span class="p">}</span>
    <span class="n">threshold</span> <span class="o">=</span> <span class="mf">1.5</span>
    <span class="n">overview_sentences</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">overview_df</span><span class="o">.</span><span class="n">iterrows</span><span class="p">():</span>
        <span class="n">category</span> <span class="o">=</span> <span class="n">row</span><span class="p">[</span><span class="s1">&#39;Category&#39;</span><span class="p">]</span>
        <span class="n">avg_score</span> <span class="o">=</span> <span class="n">row</span><span class="p">[</span><span class="s1">&#39;Avg_Score&#39;</span><span class="p">]</span>
        <span class="n">individual_scores</span> <span class="o">=</span> <span class="n">row</span><span class="p">[</span><span class="s1">&#39;Individual_scores&#39;</span><span class="p">]</span>
        <span class="n">max_score</span> <span class="o">=</span> <span class="n">max_scores</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">category</span><span class="p">,</span> <span class="s2">&quot;Unknown&quot;</span><span class="p">)</span>

        <span class="n">scores</span> <span class="o">=</span> <span class="p">[</span><span class="n">score</span> <span class="k">for</span> <span class="p">(</span><span class="n">_</span><span class="p">,</span> <span class="n">score</span><span class="p">)</span> <span class="ow">in</span> <span class="n">individual_scores</span> <span class="k">if</span> <span class="n">score</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">]</span>
        <span class="n">outliers</span> <span class="o">=</span> <span class="p">[</span><span class="n">sc</span> <span class="k">for</span> <span class="n">sc</span> <span class="ow">in</span> <span class="n">scores</span> <span class="k">if</span> <span class="nb">abs</span><span class="p">(</span><span class="n">sc</span> <span class="o">-</span> <span class="n">avg_score</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">threshold</span><span class="p">]</span>

        <span class="n">sentence</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;- </span><span class="si">{</span><span class="n">category</span><span class="si">}</span><span class="s2"> is </span><span class="si">{</span><span class="n">avg_score</span><span class="si">}</span><span class="s2"> out of </span><span class="si">{</span><span class="n">max_score</span><span class="si">}</span><span class="s2">.&quot;</span>
        <span class="k">if</span> <span class="n">outliers</span><span class="p">:</span>
            <span class="n">unique_outliers</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">outliers</span><span class="p">))</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">unique_outliers</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">outliers_text</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;a rating at </span><span class="si">{</span><span class="n">unique_outliers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">outliers_text</span> <span class="o">=</span> <span class="s2">&quot;, &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="sa">f</span><span class="s2">&quot;a rating at </span><span class="si">{</span><span class="n">sc</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">for</span> <span class="n">sc</span> <span class="ow">in</span> <span class="n">unique_outliers</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]])</span>
                <span class="n">outliers_text</span> <span class="o">+=</span> <span class="sa">f</span><span class="s2">&quot; and a rating at </span><span class="si">{</span><span class="n">unique_outliers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="n">sentence</span> <span class="o">+=</span> <span class="sa">f</span><span class="s2">&quot; Outlier was </span><span class="si">{</span><span class="n">outliers_text</span><span class="si">}</span><span class="s2">.&quot;</span>

        <span class="n">overview_sentences</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">sentence</span><span class="p">)</span>

    <span class="k">return</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">overview_sentences</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="model_training.nlp.summary.dfs_to_input_converter.generate_request_information_prompts" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">generate_request_information_prompts</span><span class="p">(</span><span class="n">request_df</span><span class="p">)</span></code>

</h2>


    <div class="doc doc-contents ">

        <p>Summarize requests (like "Typo" or "Clarification"), freq, and comments.</p>

            <details class="quote">
              <summary>Source code in <code>model_training\nlp\summary\dfs_to_input_converter.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span>
<span class="normal">82</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">generate_request_information_prompts</span><span class="p">(</span><span class="n">request_df</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Summarize requests (like &quot;Typo&quot; or &quot;Clarification&quot;), freq, and comments.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">request_df</span><span class="p">[</span><span class="s1">&#39;Frequency_Percent&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">request_df</span><span class="p">[</span><span class="s1">&#39;Frequency&#39;</span><span class="p">]</span> <span class="o">*</span> <span class="mi">100</span>
    <span class="n">lines</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">request_df</span><span class="o">.</span><span class="n">iterrows</span><span class="p">():</span>
        <span class="n">request_type</span> <span class="o">=</span> <span class="n">row</span><span class="p">[</span><span class="s1">&#39;Request Information&#39;</span><span class="p">]</span>
        <span class="n">freq</span> <span class="o">=</span> <span class="n">row</span><span class="p">[</span><span class="s1">&#39;Frequency_Percent&#39;</span><span class="p">]</span>
        <span class="n">comments_list</span> <span class="o">=</span> <span class="n">row</span><span class="p">[</span><span class="s1">&#39;Comments&#39;</span><span class="p">]</span>

        <span class="n">combined_comments</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">comments_list</span><span class="p">:</span>
            <span class="n">combined_comments</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot; &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">chunk</span><span class="p">))</span>

        <span class="n">all_comments</span> <span class="o">=</span> <span class="s2">&quot; &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">combined_comments</span><span class="p">)</span>
        <span class="n">line</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;- </span><span class="si">{</span><span class="n">request_type</span><span class="si">}</span><span class="s2"> was requested </span><span class="si">{</span><span class="n">freq</span><span class="si">:</span><span class="s2">.0f</span><span class="si">}</span><span class="s2">% of the time. Comments: </span><span class="si">{</span><span class="n">all_comments</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="n">lines</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">line</span><span class="p">)</span>

    <span class="k">return</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">lines</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<a id="model_training.nlp.summary.dummy_data"></a>
    <div class="doc doc-contents first">








  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<a id="model_training.nlp.summary.input_to_prompt_converter"></a>
    <div class="doc doc-contents first">








  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<a id="model_training.nlp.summary.main"></a>
    <div class="doc doc-contents first">

        <p>main.py</p>
<p>This file 
(1) checks wheather cuda is availibile 
(2) executes the model training for all models we want to investigate and require training, 
(3.1) computes evaluation measures for each model using a test-set and saves it into a model_comparison.csv
(3.2) makes a prediciton for a dummy data set for each model to make it more human readable and saves the results into output.txt</p>








  <div class="doc doc-children">











  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<a id="model_training.nlp.summary.predict_BART"></a>
    <div class="doc doc-contents first">

        <p>prediction_BART.py</p>
<p>A script that:
1) Loads a previously fine-tuned BART model from ./models/bart
2) Predicts summary of input</p>








  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h2 id="model_training.nlp.summary.predict_BART.load_BART_model" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">load_BART_model</span><span class="p">(</span><span class="n">model_dir</span><span class="o">=</span><span class="s1">&#39;./models/bart&#39;</span><span class="p">)</span></code>

</h2>


    <div class="doc doc-contents ">

        <p>Load BART model previously trained by train_BART.py</p>
<p>Args:
      model_dir (str): path to directory where the model is supposed to be stored.</p>

            <details class="quote">
              <summary>Source code in <code>model_training\nlp\summary\predict_BART.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">load_BART_model</span><span class="p">(</span><span class="n">model_dir</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;./models/bart&quot;</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Load BART model previously trained by train_BART.py</span>

<span class="sd">      Args:</span>
<span class="sd">          model_dir (str): path to directory where the model is supposed to be stored.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Loading BART model from: </span><span class="si">{</span><span class="n">model_dir</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">BartTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_dir</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">BartForConditionalGeneration</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_dir</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="model_training.nlp.summary.predict_BART.predict" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">predict</span><span class="p">(</span><span class="n">input_text</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">model_dir</span><span class="o">=</span><span class="s1">&#39;./models/bart&#39;</span><span class="p">,</span> <span class="n">min_new_tokens</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span></code>

</h2>


    <div class="doc doc-contents ">

        <p>Generates a BART prediction such that the output is around
two-thirds the token length of the input text.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>input_text</code>
            </td>
            <td>
                  <code>str</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The raw input text to be summarized/transformed.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>model_dir</code>
            </td>
            <td>
                  <code>str</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Path to the directory where the BART model is stored.</p>
              </div>
            </td>
            <td>
                  <code>&#39;./models/bart&#39;</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>min_new_tokens</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>A minimum number of tokens to generate
                  (avoids extremely short outputs).</p>
              </div>
            </td>
            <td>
                  <code>20</code>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                  <code>str</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>A string containing the BART model's output.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>model_training\nlp\summary\predict_BART.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span>
<span class="normal">82</span>
<span class="normal">83</span>
<span class="normal">84</span>
<span class="normal">85</span>
<span class="normal">86</span>
<span class="normal">87</span>
<span class="normal">88</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">predict</span><span class="p">(</span>
    <span class="n">input_text</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">model</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">tokenizer</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">model_dir</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;./models/bart&quot;</span><span class="p">,</span>
    <span class="n">min_new_tokens</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">20</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generates a BART prediction such that the output is around</span>
<span class="sd">    two-thirds the token length of the input text.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_text (str): The raw input text to be summarized/transformed.</span>
<span class="sd">        model_dir (str): Path to the directory where the BART model is stored.</span>
<span class="sd">        min_new_tokens (int): A minimum number of tokens to generate</span>
<span class="sd">                              (avoids extremely short outputs).</span>

<span class="sd">    Returns:</span>
<span class="sd">        A string containing the BART model&#39;s output.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># 1) Load tokenizer and model</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">model</span> <span class="o">==</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">tokenizer</span> <span class="o">==</span> <span class="kc">None</span><span class="p">):</span>
        <span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">load_BART_model</span><span class="p">(</span><span class="n">model_dir</span><span class="p">)</span>

    <span class="c1"># Move model to GPU if available</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

    <span class="c1"># 2) Tokenize the input</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">input_text</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="c1">#input_length = inputs[&quot;input_ids&quot;].shape[1]</span>

    <span class="c1"># 3) Calculate desired output length (~2/3 of input length)</span>
    <span class="c1">#desired_output_length = int(round((2.0 / 3.0) * input_length))</span>
    <span class="c1"># Ensure it&#39;s at least `min_new_tokens` to avoid producing almost nothing</span>
    <span class="c1">#desired_output_length = max(desired_output_length, min_new_tokens)</span>

    <span class="c1">#logger.info(f&quot;Detected input length (tokens): {input_length}&quot;)</span>
    <span class="c1">#logger.info(f&quot;Desired output length (tokens): {desired_output_length}&quot;)</span>

    <span class="c1"># 4) Generate</span>
    <span class="n">input_ids</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

    <span class="c1"># We use `max_new_tokens=desired_output_length` to limit generation</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
        <span class="n">input_ids</span><span class="o">=</span><span class="n">input_ids</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
        <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">150</span><span class="p">,</span>
        <span class="n">num_beams</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>               <span class="c1"># optional: beam search</span>
        <span class="n">no_repeat_ngram_size</span><span class="o">=</span><span class="mi">2</span>     <span class="c1"># optional: avoid repeating phrases</span>
    <span class="p">)</span>

    <span class="c1"># 5) Decode</span>
    <span class="n">prediction</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">prediction</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<a id="model_training.nlp.summary.predict_BLOOM"></a>
    <div class="doc doc-contents first">

        <p>prediction_BLOOM.py</p>
<p>A script that:
1) Loads a previously fine-tuned BLOOM model from ./models/bloom
2) Prints ONLY the newly generated text (omitting the prompt)</p>








  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h2 id="model_training.nlp.summary.predict_BLOOM.load_BLOOM_model" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">load_BLOOM_model</span><span class="p">(</span><span class="n">model_dir</span><span class="o">=</span><span class="s1">&#39;./models/bloom&#39;</span><span class="p">)</span></code>

</h2>


    <div class="doc doc-contents ">

        <p>Load BLOOM model previously trained by train_BLOOM.py</p>
<p>Args:
      model_dir (str): path to directory where the model is supposed to be stored.</p>

            <details class="quote">
              <summary>Source code in <code>model_training\nlp\summary\predict_BLOOM.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">load_BLOOM_model</span><span class="p">(</span><span class="n">model_dir</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;./models/bloom&quot;</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Load BLOOM model previously trained by train_BLOOM.py</span>

<span class="sd">      Args:</span>
<span class="sd">          model_dir (str): path to directory where the model is supposed to be stored.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Loading BLOOM model from: </span><span class="si">{</span><span class="n">model_dir</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">BloomTokenizerFast</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_dir</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">BloomForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_dir</span><span class="p">)</span>

    <span class="c1"># Option 1: Set pad_token to eos_token</span>
    <span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>
    <span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">pad_token_id</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token_id</span>

    <span class="c1"># Option 2: Add a new pad_token (if preferred)</span>
    <span class="c1"># tokenizer.add_special_tokens({&#39;pad_token&#39;: &#39;[PAD]&#39;})</span>
    <span class="c1"># model.resize_token_embeddings(len(tokenizer))</span>
    <span class="c1"># model.config.pad_token_id = tokenizer.pad_token_id</span>

    <span class="k">return</span> <span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="model_training.nlp.summary.predict_BLOOM.predict" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">predict</span><span class="p">(</span><span class="n">input_text</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">model_dir</span><span class="o">=</span><span class="s1">&#39;./models/bloom&#39;</span><span class="p">,</span> <span class="n">min_new_tokens</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span></code>

</h2>


    <div class="doc doc-contents ">

        <p>Generates a BLOOM prediction such that the output is around
two-thirds the token length of the (prompted) input text,
and returns ONLY the newly generated tokens (excluding the prompt).</p>

            <details class="quote">
              <summary>Source code in <code>model_training\nlp\summary\predict_BLOOM.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span>
<span class="normal">82</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">predict</span><span class="p">(</span>
    <span class="n">input_text</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">model</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">tokenizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">model_dir</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;./models/bloom&quot;</span><span class="p">,</span>
    <span class="n">min_new_tokens</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">20</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generates a BLOOM prediction such that the output is around</span>
<span class="sd">    two-thirds the token length of the (prompted) input text,</span>
<span class="sd">    and returns ONLY the newly generated tokens (excluding the prompt).</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># 1) Load tokenizer and model if not provided</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">model</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">tokenizer</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">):</span>
        <span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">load_BLOOM_model</span><span class="p">(</span><span class="n">model_dir</span><span class="p">)</span>

    <span class="c1"># Move model to GPU if available</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

    <span class="c1"># 2) Build and tokenize the input</span>
    <span class="n">prompt_tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">input_text</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">,</span> <span class="n">add_special_tokens</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">prompt_length</span> <span class="o">=</span> <span class="n">prompt_tokens</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>


    <span class="c1"># 4) Generate text</span>
    <span class="n">input_ids</span> <span class="o">=</span> <span class="n">prompt_tokens</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
        <span class="n">input_ids</span><span class="o">=</span><span class="n">input_ids</span><span class="p">,</span>
        <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">150</span><span class="p">,</span>
        <span class="n">num_beams</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
        <span class="n">no_repeat_ngram_size</span><span class="o">=</span><span class="mi">2</span>
    <span class="p">)</span>

    <span class="c1"># 5) Decode only the newly generated tokens</span>
    <span class="n">full_sequence</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>                     <span class="c1"># shape: [total_length]</span>
    <span class="n">new_tokens</span> <span class="o">=</span> <span class="n">full_sequence</span><span class="p">[</span><span class="n">prompt_length</span><span class="p">:]</span>     <span class="c1"># slice off the prompt</span>
    <span class="n">prediction</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">new_tokens</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">prediction</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<a id="model_training.nlp.summary.predict_compare"></a>
    <div class="doc doc-contents first">








  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h2 id="model_training.nlp.summary.predict_compare.main" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">main</span><span class="p">()</span></code>

</h2>


    <div class="doc doc-contents ">

        <p>Main function to load test data, generate predictions using each model,
compute evaluation metrics, and compare the performance of each model.</p>

            <details class="quote">
              <summary>Source code in <code>model_training\nlp\summary\predict_compare.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 22</span>
<span class="normal"> 23</span>
<span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">main</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Main function to load test data, generate predictions using each model,</span>
<span class="sd">    compute evaluation metrics, and compare the performance of each model.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">test_file</span> <span class="o">=</span> <span class="s2">&quot;data/test.jsonl&quot;</span>
    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Loading test data from </span><span class="si">{</span><span class="n">test_file</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">test_data</span> <span class="o">=</span> <span class="n">load_jsonl</span><span class="p">(</span><span class="n">test_file</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">test_data</span><span class="p">:</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="s2">&quot;No test data found. Exiting.&quot;</span><span class="p">)</span>
        <span class="k">return</span>

    <span class="c1"># Extract inputs and labels</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">entry</span><span class="p">[</span><span class="s1">&#39;input&#39;</span><span class="p">]</span> <span class="k">for</span> <span class="n">entry</span> <span class="ow">in</span> <span class="n">test_data</span><span class="p">]</span>
    <span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="n">entry</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">]</span> <span class="k">for</span> <span class="n">entry</span> <span class="ow">in</span> <span class="n">test_data</span><span class="p">]</span>

    <span class="c1"># Initialize a dictionary to store predictions for each model</span>
    <span class="n">model_predictions</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;T5&quot;</span><span class="p">:</span> <span class="p">[],</span>
        <span class="s2">&quot;BART&quot;</span><span class="p">:</span> <span class="p">[],</span>
        <span class="s2">&quot;BLOOM&quot;</span><span class="p">:</span> <span class="p">[],</span>
        <span class="s2">&quot;LLaMA2&quot;</span><span class="p">:</span> <span class="p">[]</span>
    <span class="p">}</span>

    <span class="c1"># Load tokenizers and each model</span>
    <span class="n">model_T5</span><span class="p">,</span> <span class="n">tokenizer_T5</span> <span class="o">=</span> <span class="n">load_T5_model</span><span class="p">(</span><span class="n">model_dir</span> <span class="o">=</span> <span class="s2">&quot;./models/t5&quot;</span><span class="p">)</span>
    <span class="n">model_BART</span><span class="p">,</span> <span class="n">tokenizer_BART</span> <span class="o">=</span> <span class="n">load_BART_model</span><span class="p">(</span><span class="n">model_dir</span> <span class="o">=</span> <span class="s2">&quot;./models/bart&quot;</span><span class="p">)</span>
    <span class="n">model_BLOOM</span><span class="p">,</span> <span class="n">tokenizer_BLOOM</span> <span class="o">=</span> <span class="n">load_BLOOM_model</span><span class="p">(</span><span class="n">model_dir</span> <span class="o">=</span> <span class="s2">&quot;./models/bloom&quot;</span><span class="p">)</span>
    <span class="n">model_LLAMA2</span><span class="p">,</span> <span class="n">tokenizer_LLAMA2</span> <span class="o">=</span> <span class="n">load_LLAMA2_model</span><span class="p">()</span>

    <span class="c1"># Generate predictions for each model</span>
    <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">input_text</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">inputs</span><span class="p">):</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Processing sample </span><span class="si">{</span><span class="n">idx</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">pred_t5</span> <span class="o">=</span> <span class="n">predict_T5</span><span class="p">(</span><span class="n">input_text</span><span class="o">=</span><span class="n">input_text</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">model_T5</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer_T5</span><span class="p">)</span>
            <span class="n">model_predictions</span><span class="p">[</span><span class="s2">&quot;T5&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">pred_t5</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;T5 prediction failed for sample </span><span class="si">{</span><span class="n">idx</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="n">model_predictions</span><span class="p">[</span><span class="s2">&quot;T5&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;&quot;</span><span class="p">)</span>

        <span class="k">try</span><span class="p">:</span>
            <span class="n">pred_bart</span> <span class="o">=</span> <span class="n">predict_BART</span><span class="p">(</span><span class="n">input_text</span><span class="o">=</span><span class="n">input_text</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">model_BART</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer_BART</span><span class="p">)</span>
            <span class="n">model_predictions</span><span class="p">[</span><span class="s2">&quot;BART&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">pred_bart</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;BART prediction failed for sample </span><span class="si">{</span><span class="n">idx</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="n">model_predictions</span><span class="p">[</span><span class="s2">&quot;BART&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;&quot;</span><span class="p">)</span>

        <span class="k">try</span><span class="p">:</span>
            <span class="n">pred_bloom</span> <span class="o">=</span> <span class="n">predict_BLOOM</span><span class="p">(</span><span class="n">input_text</span><span class="o">=</span><span class="n">input_text</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">model_BLOOM</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer_BLOOM</span><span class="p">)</span>
            <span class="n">model_predictions</span><span class="p">[</span><span class="s2">&quot;BLOOM&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">pred_bloom</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;BLOOM prediction failed for sample </span><span class="si">{</span><span class="n">idx</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="n">model_predictions</span><span class="p">[</span><span class="s2">&quot;BLOOM&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;&quot;</span><span class="p">)</span>

        <span class="k">try</span><span class="p">:</span>
            <span class="n">pred_llama2</span> <span class="o">=</span> <span class="n">predict_LLAMA2</span><span class="p">(</span><span class="n">input_text</span><span class="o">=</span><span class="n">input_text</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">model_LLAMA2</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer_LLAMA2</span><span class="p">)</span>
            <span class="n">model_predictions</span><span class="p">[</span><span class="s2">&quot;LLaMA2&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">pred_llama2</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;LLaMA2 prediction failed for sample </span><span class="si">{</span><span class="n">idx</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="n">model_predictions</span><span class="p">[</span><span class="s2">&quot;LLaMA2&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;&quot;</span><span class="p">)</span>

    <span class="c1"># Encode predictions and labels for compute_metrics</span>
    <span class="n">model_metrics</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">]]</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="k">for</span> <span class="n">model_name</span><span class="p">,</span> <span class="n">predictions</span> <span class="ow">in</span> <span class="n">model_predictions</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Computing metrics for model: </span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">model_name</span> <span class="o">==</span> <span class="s2">&quot;T5&quot;</span><span class="p">:</span>
                <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">tokenizer_T5</span>
            <span class="k">elif</span> <span class="n">model_name</span> <span class="o">==</span> <span class="s2">&quot;BART&quot;</span><span class="p">:</span>
                <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">tokenizer_BART</span>
            <span class="k">elif</span> <span class="n">model_name</span> <span class="o">==</span> <span class="s2">&quot;BLOOM&quot;</span><span class="p">:</span>
                <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">tokenizer_BLOOM</span>
            <span class="k">elif</span> <span class="n">model_name</span> <span class="o">==</span> <span class="s2">&quot;LLaMA2&quot;</span><span class="p">:</span>
                <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">tokenizer_LLAMA2</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;No tokenizer found for model: </span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                <span class="n">tokenizer</span> <span class="o">=</span> <span class="kc">None</span>

            <span class="k">if</span> <span class="n">tokenizer</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Tokenizer for model </span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="s2"> is not loaded.&quot;</span><span class="p">)</span>

            <span class="c1"># Encode predictions and labels</span>
            <span class="c1"># Replace empty predictions with tokenizer.pad_token or appropriate token</span>
            <span class="n">encoded_predictions</span> <span class="o">=</span> <span class="p">[</span>
                <span class="n">pred</span> <span class="k">if</span> <span class="n">pred</span> <span class="k">else</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="k">for</span> <span class="n">pred</span> <span class="ow">in</span> <span class="n">predictions</span>
            <span class="p">]</span>
            <span class="n">encoded_labels</span> <span class="o">=</span> <span class="p">[</span>
                <span class="n">label</span> <span class="k">if</span> <span class="n">label</span> <span class="k">else</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="k">for</span> <span class="n">label</span> <span class="ow">in</span> <span class="n">labels</span>
            <span class="p">]</span>

            <span class="c1"># Tokenize the predictions and labels to get token IDs</span>
            <span class="n">tokenized_predictions</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span>
                <span class="n">encoded_predictions</span><span class="p">,</span>
                <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="n">max_length</span><span class="o">=</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">model_max_length</span><span class="p">,</span>
                <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;np&quot;</span>
            <span class="p">)[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]</span>

            <span class="n">tokenized_labels</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span>
                <span class="n">encoded_labels</span><span class="p">,</span>
                <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="n">max_length</span><span class="o">=</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">model_max_length</span><span class="p">,</span>
                <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;np&quot;</span>
            <span class="p">)[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]</span>

            <span class="c1"># Compute metrics</span>
            <span class="n">metrics</span> <span class="o">=</span> <span class="n">compute_metrics</span><span class="p">((</span><span class="n">tokenized_predictions</span><span class="p">,</span> <span class="n">tokenized_labels</span><span class="p">),</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">)</span>
            <span class="n">model_metrics</span><span class="p">[</span><span class="n">model_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">metrics</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Metrics for </span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">metrics</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Error computing metrics for model </span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="n">model_metrics</span><span class="p">[</span><span class="n">model_name</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="c1"># Display the comparison of metrics</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">=== Model Performance Comparison ===</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">model_name</span><span class="p">,</span> <span class="n">metrics</span> <span class="ow">in</span> <span class="n">model_metrics</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Model: </span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">metrics</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">metric_name</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">metrics</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  </span><span class="si">{</span><span class="n">metric_name</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">value</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;  Metrics not available due to errors.&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">()</span>

    <span class="c1"># Save the comparison to a JSON file</span>
    <span class="n">comparison_file</span> <span class="o">=</span> <span class="s2">&quot;model_performance_comparison.json&quot;</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">comparison_file</span><span class="p">,</span> <span class="s2">&quot;w&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">json</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">model_metrics</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">indent</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Model performance comparison saved to </span><span class="si">{</span><span class="n">comparison_file</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<a id="model_training.nlp.summary.predict_LLAMA2"></a>
    <div class="doc doc-contents first">

        <p>predict_LLAMA2.py</p>
<p>A script that:
1) Loads a LLaMA2 model from Hugging Face
2) Predicts summary of input</p>








  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h2 id="model_training.nlp.summary.predict_LLAMA2.load_LLAMA2_model" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">load_LLAMA2_model</span><span class="p">(</span><span class="n">model_dir</span><span class="o">=</span><span class="s1">&#39;/models/llama2&#39;</span><span class="p">)</span></code>

</h2>


    <div class="doc doc-contents ">

        <p>Load a LLaMA 2 model from Hugging Face that is supposed to be stored locally at the given path.
For a real-world scenario, ensure you have:
  - 'transformers&gt;=4.30'
  - 'sentencepiece'
  - You have accepted the license for LLaMA2 if it's gated.</p>
<p>Args:
      model_dir (str): path to directory where the model is supposed to be stored.</p>

            <details class="quote">
              <summary>Source code in <code>model_training\nlp\summary\predict_LLAMA2.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">load_LLAMA2_model</span><span class="p">(</span><span class="n">model_dir</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;/models/llama2&quot;</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Load a LLaMA 2 model from Hugging Face that is supposed to be stored locally at the given path.</span>
<span class="sd">    For a real-world scenario, ensure you have:</span>
<span class="sd">      - &#39;transformers&gt;=4.30&#39;</span>
<span class="sd">      - &#39;sentencepiece&#39;</span>
<span class="sd">      - You have accepted the license for LLaMA2 if it&#39;s gated.</span>

<span class="sd">      Args:</span>
<span class="sd">          model_dir (str): path to directory where the model is supposed to be stored.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Loading model &#39;</span><span class="si">{</span><span class="n">model_dir</span><span class="si">}</span><span class="s2">&#39; from Hugging Face...&quot;</span><span class="p">)</span>

    <span class="c1"># &#39;legacy=True&#39; is sometimes needed for older model conversions,</span>
    <span class="c1"># but if you see deprecation warnings, remove it or try without it.</span>
    <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">LlamaTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_dir</span><span class="p">,</span> <span class="n">legacy</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">LlamaForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
        <span class="n">model_dir</span><span class="p">,</span>
        <span class="n">device_map</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># Set pad_token to eos_token</span>
    <span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>
    <span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">pad_token_id</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token_id</span>

    <span class="k">return</span> <span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="model_training.nlp.summary.predict_LLAMA2.predict" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">predict</span><span class="p">(</span><span class="n">input_text</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">model_dir</span><span class="o">=</span><span class="s1">&#39;/storage/ukp/shared/shared_model_weights/models--llama-2-hf/7B-Chat&#39;</span><span class="p">,</span> <span class="n">min_new_tokens</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">max_new_tokens_cap</span><span class="o">=</span><span class="mi">512</span><span class="p">)</span></code>

</h2>


    <div class="doc doc-contents ">

        <p>Build the prompt from real data, run generation on LLaMA 2, 
and return an output that is ~2/3 of the prompt length (with a min
of <code>min_new_tokens</code>), but also capped by <code>max_new_tokens_cap</code>.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>input_text</code>
            </td>
            <td>
                  <code>str</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Raw text or data for the prompt builder.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>model_dir</code>
            </td>
            <td>
                  <code>str</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Path or HF repo for your LLaMA2 model.</p>
              </div>
            </td>
            <td>
                  <code>&#39;/storage/ukp/shared/shared_model_weights/models--llama-2-hf/7B-Chat&#39;</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>min_new_tokens</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Ensures we generate at least this many tokens.</p>
              </div>
            </td>
            <td>
                  <code>20</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>max_new_tokens_cap</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Hard upper bound on new tokens generated.</p>
              </div>
            </td>
            <td>
                  <code>512</code>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
<th>Name</th>          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
<td><code>str</code></td>            <td>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The final generated text (prompt echo stripped).</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>model_training\nlp\summary\predict_LLAMA2.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">predict</span><span class="p">(</span>
    <span class="n">input_text</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">model</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">tokenizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">model_dir</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;/storage/ukp/shared/shared_model_weights/models--llama-2-hf/7B-Chat&quot;</span><span class="p">,</span>
    <span class="n">min_new_tokens</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">20</span><span class="p">,</span>
    <span class="n">max_new_tokens_cap</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">512</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Build the prompt from real data, run generation on LLaMA 2, </span>
<span class="sd">    and return an output that is ~2/3 of the prompt length (with a min</span>
<span class="sd">    of `min_new_tokens`), but also capped by `max_new_tokens_cap`.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_text (str): Raw text or data for the prompt builder.</span>
<span class="sd">        model_dir (str): Path or HF repo for your LLaMA2 model.</span>
<span class="sd">        min_new_tokens (int): Ensures we generate at least this many tokens.</span>
<span class="sd">        max_new_tokens_cap (int): Hard upper bound on new tokens generated.</span>

<span class="sd">    Returns:</span>
<span class="sd">        str: The final generated text (prompt echo stripped).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># 1) Load model &amp; tokenizer if needed</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">model</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">tokenizer</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">):</span>
        <span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">load_LLAMA2_model</span><span class="p">(</span><span class="n">model_dir</span><span class="p">)</span>

    <span class="c1"># 2) Build the final prompt from the input data</span>
    <span class="n">prompt</span> <span class="o">=</span> <span class="n">input_to_prompt_converter</span><span class="o">.</span><span class="n">build_llama2_prompt</span><span class="p">(</span><span class="n">input_text</span><span class="p">)</span>
    <span class="c1">#logger.info(&quot;Prompt is ready. Calculating lengths and generating...&quot;)</span>

    <span class="c1"># 3) Measure the prompt and input length in tokens</span>
    <span class="n">prompt_tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">,</span> <span class="n">add_special_tokens</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="c1">#input_tokens = tokenizer(input_text, return_tensors=&quot;pt&quot;, add_special_tokens=False)</span>
    <span class="c1">#input_length = input_tokens[&quot;input_ids&quot;].shape[1]</span>

    <span class="c1">#logger.debug(f&quot;Detected input length (tokens): {input_length}&quot;)</span>

    <span class="c1"># 4) Desired generation length: ~2/3 of prompt length, but at least min_new_tokens</span>
    <span class="c1">#desired_length = int(round((2.0 / 3.0) * input_length))</span>
    <span class="c1">#desired_length = max(desired_length, min_new_tokens)</span>

    <span class="c1">#logger.info(f&quot;Desired output length (tokens): {desired_length}&quot;)</span>

    <span class="c1"># 5) Move everything to the model&#39;s device</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">device</span>
    <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">prompt_tokens</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="n">prompt_tokens</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">v</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

    <span class="c1"># 6) Create generation config</span>
    <span class="n">gen_config</span> <span class="o">=</span> <span class="n">GenerationConfig</span><span class="p">(</span>
        <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">150</span><span class="p">,</span>
        <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">temperature</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span>
        <span class="n">top_p</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span>
        <span class="n">repetition_penalty</span><span class="o">=</span><span class="mf">1.2</span><span class="p">,</span>  <span class="c1"># helps reduce repeated tokens</span>
        <span class="c1"># Additional parameters (e.g., num_beams) if desired</span>
    <span class="p">)</span>

    <span class="c1"># 7) Generate</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">prompt_tokens</span><span class="p">,</span> <span class="n">generation_config</span><span class="o">=</span><span class="n">gen_config</span><span class="p">)</span>

    <span class="c1"># 8) Decode</span>
    <span class="n">decoded</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="c1"># LLaMA often re-echos the prompt. Remove it if present:</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">decoded</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">result</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="n">prompt</span><span class="p">):</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">result</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">prompt</span><span class="p">):]</span>

    <span class="k">return</span> <span class="n">result</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div><!-- ::: model_training.nlp.summary.predict_T5 -->












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "../../..", "features": [], "search": "../../../assets/javascripts/workers/search.f8cc74c7.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../../../assets/javascripts/bundle.f13b1293.min.js"></script>
      
    
  </body>
</html>